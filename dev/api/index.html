<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Index of functions · Jchemo.jl</title><meta name="title" content="Index of functions · Jchemo.jl"/><meta property="og:title" content="Index of functions · Jchemo.jl"/><meta property="twitter:title" content="Index of functions · Jchemo.jl"/><meta name="description" content="Documentation for Jchemo.jl."/><meta property="og:description" content="Documentation for Jchemo.jl."/><meta property="twitter:description" content="Documentation for Jchemo.jl."/><meta property="og:url" content="https://mlesnoff.github.io/Jchemo.jl/api/"/><meta property="twitter:url" content="https://mlesnoff.github.io/Jchemo.jl/api/"/><link rel="canonical" href="https://mlesnoff.github.io/Jchemo.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Jchemo.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../domains/">Available methods</a></li><li class="is-active"><a class="tocitem" href>Index of functions</a></li><li><a class="tocitem" href="../news/">News</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Index of functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Index of functions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/mlesnoff/Jchemo.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/mlesnoff/Jchemo.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Index-of-functions"><a class="docs-heading-anchor" href="#Index-of-functions">Index of functions</a><a id="Index-of-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Index-of-functions" title="Permalink"></a></h1><p>Here is a list of all exported functions from Jchemo.jl. </p><p>For more details, click on the link and you&#39;ll be directed to the function help.</p><ul><li><a href="#Jchemo.aggmean-Tuple{Any, Any}"><code>Jchemo.aggmean</code></a></li><li><a href="#Jchemo.aggstat-Tuple{Any, Any}"><code>Jchemo.aggstat</code></a></li><li><a href="#Jchemo.aggsumv-Tuple{Vector, Union{BitVector, Vector}}"><code>Jchemo.aggsumv</code></a></li><li><a href="#Jchemo.aicplsr-Tuple{Any, Any}"><code>Jchemo.aicplsr</code></a></li><li><a href="#Jchemo.aov1-Tuple{Any, Any}"><code>Jchemo.aov1</code></a></li><li><a href="#Jchemo.bias-Tuple{Any, Any}"><code>Jchemo.bias</code></a></li><li><a href="#Jchemo.blockscal-Tuple{}"><code>Jchemo.blockscal</code></a></li><li><a href="#Jchemo.calds-Tuple{Any, Any}"><code>Jchemo.calds</code></a></li><li><a href="#Jchemo.calpds-Tuple{Any, Any}"><code>Jchemo.calpds</code></a></li><li><a href="#Jchemo.cca-Tuple{}"><code>Jchemo.cca</code></a></li><li><a href="#Jchemo.ccawold-Tuple{}"><code>Jchemo.ccawold</code></a></li><li><a href="#Jchemo.center-Tuple{}"><code>Jchemo.center</code></a></li><li><a href="#Jchemo.cglsr-Tuple{}"><code>Jchemo.cglsr</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Rr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Kplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Dkplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Krr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Pcr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Cglsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.colmad-Tuple{Any}"><code>Jchemo.colmad</code></a></li><li><a href="#Jchemo.colmean-Tuple{Any}"><code>Jchemo.colmean</code></a></li><li><a href="#Jchemo.colmed-Tuple{Any}"><code>Jchemo.colmed</code></a></li><li><a href="#Jchemo.colnorm-Tuple{Any}"><code>Jchemo.colnorm</code></a></li><li><a href="#Jchemo.colstd-Tuple{Any}"><code>Jchemo.colstd</code></a></li><li><a href="#Jchemo.colsum-Tuple{Any}"><code>Jchemo.colsum</code></a></li><li><a href="#Jchemo.colvar-Tuple{Any}"><code>Jchemo.colvar</code></a></li><li><a href="#Jchemo.comdim-Tuple{}"><code>Jchemo.comdim</code></a></li><li><a href="#Jchemo.conf-Tuple{Any, Any}"><code>Jchemo.conf</code></a></li><li><a href="#Jchemo.convertdf-Tuple{DataFrames.DataFrame}"><code>Jchemo.convertdf</code></a></li><li><a href="#Jchemo.cor2-Tuple{Any, Any}"><code>Jchemo.cor2</code></a></li><li><a href="#Jchemo.corm-Tuple{Any}"><code>Jchemo.corm</code></a></li><li><a href="#Jchemo.corv-Tuple{Any, Any}"><code>Jchemo.corv</code></a></li><li><a href="#Jchemo.cosm-Tuple{Any}"><code>Jchemo.cosm</code></a></li><li><a href="#Jchemo.cosv-Tuple{Any, Any}"><code>Jchemo.cosv</code></a></li><li><a href="#Jchemo.covm-Tuple{Any}"><code>Jchemo.covm</code></a></li><li><a href="#Jchemo.covsel-Tuple{}"><code>Jchemo.covsel</code></a></li><li><a href="#Jchemo.covv-Tuple{Any, Any}"><code>Jchemo.covv</code></a></li><li><a href="#Jchemo.cscale-Tuple{}"><code>Jchemo.cscale</code></a></li><li><a href="#Jchemo.cweight-Tuple{Any, Any}"><code>Jchemo.cweight</code></a></li><li><a href="#Jchemo.detrend_airpls-Tuple{}"><code>Jchemo.detrend_airpls</code></a></li><li><a href="#Jchemo.detrend_arpls-Tuple{}"><code>Jchemo.detrend_arpls</code></a></li><li><a href="#Jchemo.detrend_asls-Tuple{}"><code>Jchemo.detrend_asls</code></a></li><li><a href="#Jchemo.detrend_lo-Tuple{}"><code>Jchemo.detrend_lo</code></a></li><li><a href="#Jchemo.detrend_pol-Tuple{}"><code>Jchemo.detrend_pol</code></a></li><li><a href="#Jchemo.dfplsr_cg-Tuple{Any, Any}"><code>Jchemo.dfplsr_cg</code></a></li><li><a href="#Jchemo.difmean-Tuple{Any, Any}"><code>Jchemo.difmean</code></a></li><li><a href="#Jchemo.dkplskdeda-Tuple{}"><code>Jchemo.dkplskdeda</code></a></li><li><a href="#Jchemo.dkplslda-Tuple{}"><code>Jchemo.dkplslda</code></a></li><li><a href="#Jchemo.dkplsqda-Tuple{}"><code>Jchemo.dkplsqda</code></a></li><li><a href="#Jchemo.dkplsr-Tuple{}"><code>Jchemo.dkplsr</code></a></li><li><a href="#Jchemo.dkplsrda-Tuple{}"><code>Jchemo.dkplsrda</code></a></li><li><a href="#Jchemo.dmkern-Tuple{}"><code>Jchemo.dmkern</code></a></li><li><a href="#Jchemo.dmnorm-Tuple{}"><code>Jchemo.dmnorm</code></a></li><li><a href="#Jchemo.dmnormlog-Tuple{}"><code>Jchemo.dmnormlog</code></a></li><li><a href="#Jchemo.dummy-Tuple{Any}"><code>Jchemo.dummy</code></a></li><li><a href="#Jchemo.dupl-Tuple{Any}"><code>Jchemo.dupl</code></a></li><li><a href="#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><code>Jchemo.ensure_df</code></a></li><li><a href="#Jchemo.ensure_mat-Tuple{AbstractMatrix}"><code>Jchemo.ensure_mat</code></a></li><li><a href="#Jchemo.eposvd-Tuple{Any}"><code>Jchemo.eposvd</code></a></li><li><a href="#Jchemo.errp-Tuple{Any, Any}"><code>Jchemo.errp</code></a></li><li><a href="#Jchemo.euclsq-Tuple{Any, Any}"><code>Jchemo.euclsq</code></a></li><li><a href="#Jchemo.expand_tab2d-Tuple{Any}"><code>Jchemo.expand_tab2d</code></a></li><li><a href="#Jchemo.fcenter-Tuple{Any, Any}"><code>Jchemo.fcenter</code></a></li><li><a href="#Jchemo.fconcat-Tuple{Any}"><code>Jchemo.fconcat</code></a></li><li><a href="#Jchemo.fcscale-Tuple{Any, Any, Any}"><code>Jchemo.fcscale</code></a></li><li><a href="#Jchemo.fda-Tuple{}"><code>Jchemo.fda</code></a></li><li><a href="#Jchemo.fdasvd-Tuple{}"><code>Jchemo.fdasvd</code></a></li><li><a href="#Jchemo.fdif-Tuple{}"><code>Jchemo.fdif</code></a></li><li><a href="#Jchemo.findmax_cla-Tuple{Any}"><code>Jchemo.findmax_cla</code></a></li><li><a href="#Jchemo.findmiss-Tuple{Any}"><code>Jchemo.findmiss</code></a></li><li><a href="#Jchemo.finduniq-Tuple{Any}"><code>Jchemo.finduniq</code></a></li><li><a href="#Jchemo.frob-Tuple{Any}"><code>Jchemo.frob</code></a></li><li><a href="#Jchemo.fscale-Tuple{Any, Any}"><code>Jchemo.fscale</code></a></li><li><a href="#Jchemo.getknn-Tuple{Any, Any}"><code>Jchemo.getknn</code></a></li><li><a href="#Jchemo.gridcv-Tuple{Any, Any, Any}"><code>Jchemo.gridcv</code></a></li><li><a href="#Jchemo.gridcv_br-Tuple{Any, Any}"><code>Jchemo.gridcv_br</code></a></li><li><a href="#Jchemo.gridcv_lb-Tuple{Any, Any}"><code>Jchemo.gridcv_lb</code></a></li><li><a href="#Jchemo.gridcv_lv-Tuple{Any, Any}"><code>Jchemo.gridcv_lv</code></a></li><li><a href="#Jchemo.gridscore-NTuple{5, Any}"><code>Jchemo.gridscore</code></a></li><li><a href="#Jchemo.gridscore-Tuple{Jchemo.Pipeline, Vararg{Any, 4}}"><code>Jchemo.gridscore</code></a></li><li><a href="#Jchemo.gridscore_br-NTuple{4, Any}"><code>Jchemo.gridscore_br</code></a></li><li><a href="#Jchemo.gridscore_lb-NTuple{4, Any}"><code>Jchemo.gridscore_lb</code></a></li><li><a href="#Jchemo.gridscore_lv-NTuple{4, Any}"><code>Jchemo.gridscore_lv</code></a></li><li><a href="#Jchemo.head-Tuple{Any}"><code>Jchemo.head</code></a></li><li><a href="#Jchemo.interpl-Tuple{}"><code>Jchemo.interpl</code></a></li><li><a href="#Jchemo.iqrv-Tuple{Any}"><code>Jchemo.iqrv</code></a></li><li><a href="#Jchemo.isel!"><code>Jchemo.isel!</code></a></li><li><a href="#Jchemo.kdeda-Tuple{}"><code>Jchemo.kdeda</code></a></li><li><a href="#Jchemo.knnda-Tuple{}"><code>Jchemo.knnda</code></a></li><li><a href="#Jchemo.knnr-Tuple{}"><code>Jchemo.knnr</code></a></li><li><a href="#Jchemo.kpca-Tuple{}"><code>Jchemo.kpca</code></a></li><li><a href="#Jchemo.kplskdeda-Tuple{}"><code>Jchemo.kplskdeda</code></a></li><li><a href="#Jchemo.kplslda-Tuple{}"><code>Jchemo.kplslda</code></a></li><li><a href="#Jchemo.kplsqda-Tuple{}"><code>Jchemo.kplsqda</code></a></li><li><a href="#Jchemo.kplsr-Tuple{}"><code>Jchemo.kplsr</code></a></li><li><a href="#Jchemo.kplsrda-Tuple{}"><code>Jchemo.kplsrda</code></a></li><li><a href="#Jchemo.kpol-Tuple{Any, Any}"><code>Jchemo.kpol</code></a></li><li><a href="#Jchemo.krbf-Tuple{Any, Any}"><code>Jchemo.krbf</code></a></li><li><a href="#Jchemo.krr-Tuple{}"><code>Jchemo.krr</code></a></li><li><a href="#Jchemo.krrda-Tuple{}"><code>Jchemo.krrda</code></a></li><li><a href="#Jchemo.lda-Tuple{}"><code>Jchemo.lda</code></a></li><li><a href="#Jchemo.list-Tuple{Integer}"><code>Jchemo.list</code></a></li><li><a href="#Jchemo.list-Tuple{Any, Integer}"><code>Jchemo.list</code></a></li><li><a href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>Jchemo.locw</code></a></li><li><a href="#Jchemo.locwlv-Tuple{Any, Any, Any}"><code>Jchemo.locwlv</code></a></li><li><a href="#Jchemo.loessr-Tuple{}"><code>Jchemo.loessr</code></a></li><li><a href="#Jchemo.lwmlr-Tuple{}"><code>Jchemo.lwmlr</code></a></li><li><a href="#Jchemo.lwmlrda-Tuple{}"><code>Jchemo.lwmlrda</code></a></li><li><a href="#Jchemo.lwplslda-Tuple{}"><code>Jchemo.lwplslda</code></a></li><li><a href="#Jchemo.lwplsqda-Tuple{}"><code>Jchemo.lwplsqda</code></a></li><li><a href="#Jchemo.lwplsr-Tuple{}"><code>Jchemo.lwplsr</code></a></li><li><a href="#Jchemo.lwplsravg-Tuple{}"><code>Jchemo.lwplsravg</code></a></li><li><a href="#Jchemo.lwplsrda-Tuple{}"><code>Jchemo.lwplsrda</code></a></li><li><a href="#Jchemo.madv-Tuple{Any}"><code>Jchemo.madv</code></a></li><li><a href="#Jchemo.mae-Tuple{Any, Any}"><code>Jchemo.mae</code></a></li><li><a href="#Jchemo.mahsq-Tuple{Any, Any}"><code>Jchemo.mahsq</code></a></li><li><a href="#Jchemo.mahsqchol-Tuple{Any, Any}"><code>Jchemo.mahsqchol</code></a></li><li><a href="#Jchemo.matB"><code>Jchemo.matB</code></a></li><li><a href="#Jchemo.matW"><code>Jchemo.matW</code></a></li><li><a href="#Jchemo.mavg-Tuple{}"><code>Jchemo.mavg</code></a></li><li><a href="#Jchemo.mbconcat-Tuple{}"><code>Jchemo.mbconcat</code></a></li><li><a href="#Jchemo.mbin"><code>Jchemo.mbin</code></a></li><li><a href="#Jchemo.mblock-Tuple{Any, Any}"><code>Jchemo.mblock</code></a></li><li><a href="#Jchemo.mbpca-Tuple{}"><code>Jchemo.mbpca</code></a></li><li><a href="#Jchemo.mbplskdeda-Tuple{}"><code>Jchemo.mbplskdeda</code></a></li><li><a href="#Jchemo.mbplslda-Tuple{}"><code>Jchemo.mbplslda</code></a></li><li><a href="#Jchemo.mbplsqda-Tuple{}"><code>Jchemo.mbplsqda</code></a></li><li><a href="#Jchemo.mbplsr-Tuple{}"><code>Jchemo.mbplsr</code></a></li><li><a href="#Jchemo.mbplsrda-Tuple{}"><code>Jchemo.mbplsrda</code></a></li><li><a href="#Jchemo.mbplswest-Tuple{}"><code>Jchemo.mbplswest</code></a></li><li><a href="#Jchemo.meanv-Tuple{Any}"><code>Jchemo.meanv</code></a></li><li><a href="#Jchemo.merrp-Tuple{Any, Any}"><code>Jchemo.merrp</code></a></li><li><a href="#Jchemo.mlev-Tuple{Any}"><code>Jchemo.mlev</code></a></li><li><a href="#Jchemo.mlr-Tuple{}"><code>Jchemo.mlr</code></a></li><li><a href="#Jchemo.mlrchol-Tuple{}"><code>Jchemo.mlrchol</code></a></li><li><a href="#Jchemo.mlrda-Tuple{}"><code>Jchemo.mlrda</code></a></li><li><a href="#Jchemo.mlrpinv-Tuple{}"><code>Jchemo.mlrpinv</code></a></li><li><a href="#Jchemo.mlrpinvn-Tuple{}"><code>Jchemo.mlrpinvn</code></a></li><li><a href="#Jchemo.mlrvec-Tuple{}"><code>Jchemo.mlrvec</code></a></li><li><a href="#Jchemo.mpar"><code>Jchemo.mpar</code></a></li><li><a href="#Jchemo.mse-Tuple{Any, Any}"><code>Jchemo.mse</code></a></li><li><a href="#Jchemo.msep-Tuple{Any, Any}"><code>Jchemo.msep</code></a></li><li><a href="#Jchemo.mweight-Tuple{Vector}"><code>Jchemo.mweight</code></a></li><li><a href="#Jchemo.mweightcla-Tuple{AbstractVector}"><code>Jchemo.mweightcla</code></a></li><li><a href="#Jchemo.nco-Tuple{Any}"><code>Jchemo.nco</code></a></li><li><a href="#Jchemo.nipals-Tuple{Any}"><code>Jchemo.nipals</code></a></li><li><a href="#Jchemo.nipalsmiss-Tuple{Any}"><code>Jchemo.nipalsmiss</code></a></li><li><a href="#Jchemo.normv-Tuple{Any}"><code>Jchemo.normv</code></a></li><li><a href="#Jchemo.nro-Tuple{Any}"><code>Jchemo.nro</code></a></li><li><a href="#Jchemo.occknn-Tuple{}"><code>Jchemo.occknn</code></a></li><li><a href="#Jchemo.occlknn-Tuple{}"><code>Jchemo.occlknn</code></a></li><li><a href="#Jchemo.occod-Tuple{}"><code>Jchemo.occod</code></a></li><li><a href="#Jchemo.occsd-Tuple{}"><code>Jchemo.occsd</code></a></li><li><a href="#Jchemo.occsdod-Tuple{}"><code>Jchemo.occsdod</code></a></li><li><a href="#Jchemo.occstah-Tuple{}"><code>Jchemo.occstah</code></a></li><li><a href="#Jchemo.out-Tuple{Any, Any}"><code>Jchemo.out</code></a></li><li><a href="#Jchemo.outeucl-Tuple{Any}"><code>Jchemo.outeucl</code></a></li><li><a href="#Jchemo.outknn-Tuple{Any}"><code>Jchemo.outknn</code></a></li><li><a href="#Jchemo.outlknn-Tuple{Any}"><code>Jchemo.outlknn</code></a></li><li><a href="#Jchemo.outod-Tuple{Any, Any}"><code>Jchemo.outod</code></a></li><li><a href="#Jchemo.outsd-Tuple{Any}"><code>Jchemo.outsd</code></a></li><li><a href="#Jchemo.outsdod-Tuple{Any, Any}"><code>Jchemo.outsdod</code></a></li><li><a href="#Jchemo.outstah-Tuple{Any, Any}"><code>Jchemo.outstah</code></a></li><li><a href="#Jchemo.parsemiss-Tuple{Any, Vector{Union{Missing, String}}}"><code>Jchemo.parsemiss</code></a></li><li><a href="#Jchemo.pcaeigen-Tuple{}"><code>Jchemo.pcaeigen</code></a></li><li><a href="#Jchemo.pcaeigenk-Tuple{}"><code>Jchemo.pcaeigenk</code></a></li><li><a href="#Jchemo.pcanipals-Tuple{}"><code>Jchemo.pcanipals</code></a></li><li><a href="#Jchemo.pcanipalsmiss-Tuple{}"><code>Jchemo.pcanipalsmiss</code></a></li><li><a href="#Jchemo.pcaout-Tuple{}"><code>Jchemo.pcaout</code></a></li><li><a href="#Jchemo.pcapp-Tuple{}"><code>Jchemo.pcapp</code></a></li><li><a href="#Jchemo.pcasph-Tuple{}"><code>Jchemo.pcasph</code></a></li><li><a href="#Jchemo.pcasvd-Tuple{}"><code>Jchemo.pcasvd</code></a></li><li><a href="#Jchemo.pcr-Tuple{}"><code>Jchemo.pcr</code></a></li><li><a href="#Jchemo.pip-Tuple"><code>Jchemo.pip</code></a></li><li><a href="#Jchemo.plotconf-Tuple{Any}"><code>Jchemo.plotconf</code></a></li><li><a href="#Jchemo.plotgrid-Tuple{AbstractVector, Any}"><code>Jchemo.plotgrid</code></a></li><li><a href="#Jchemo.plotlv-Tuple{Any}"><code>Jchemo.plotlv</code></a></li><li><a href="#Jchemo.plotsp"><code>Jchemo.plotsp</code></a></li><li><a href="#Jchemo.plotxy-Tuple{Any, Any}"><code>Jchemo.plotxy</code></a></li><li><a href="#Jchemo.plotxyz-Tuple{Any, Any, Any}"><code>Jchemo.plotxyz</code></a></li><li><a href="#Jchemo.plscan-Tuple{}"><code>Jchemo.plscan</code></a></li><li><a href="#Jchemo.plskdeda-Tuple{}"><code>Jchemo.plskdeda</code></a></li><li><a href="#Jchemo.plskern-Tuple{}"><code>Jchemo.plskern</code></a></li><li><a href="#Jchemo.plslda-Tuple{}"><code>Jchemo.plslda</code></a></li><li><a href="#Jchemo.plsnipals-Tuple{}"><code>Jchemo.plsnipals</code></a></li><li><a href="#Jchemo.plsqda-Tuple{}"><code>Jchemo.plsqda</code></a></li><li><a href="#Jchemo.plsravg-Tuple{}"><code>Jchemo.plsravg</code></a></li><li><a href="#Jchemo.plsrda-Tuple{}"><code>Jchemo.plsrda</code></a></li><li><a href="#Jchemo.plsrosa-Tuple{}"><code>Jchemo.plsrosa</code></a></li><li><a href="#Jchemo.plsrout-Tuple{}"><code>Jchemo.plsrout</code></a></li><li><a href="#Jchemo.plssimp-Tuple{}"><code>Jchemo.plssimp</code></a></li><li><a href="#Jchemo.plstuck-Tuple{}"><code>Jchemo.plstuck</code></a></li><li><a href="#Jchemo.plswold-Tuple{}"><code>Jchemo.plswold</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Calds, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Calpds, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsprobda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Pcr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mbplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mbplsprobda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mbplswest, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Krr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Treeda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occod, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Lda, Jchemo.Qda}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Loessr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsravg, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Treer, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.pval-Tuple{Distributions.Distribution, Any}"><code>Jchemo.pval</code></a></li><li><a href="#Jchemo.qda-Tuple{}"><code>Jchemo.qda</code></a></li><li><a href="#Jchemo.r2-Tuple{Any, Any}"><code>Jchemo.r2</code></a></li><li><a href="#Jchemo.rasvd-Tuple{}"><code>Jchemo.rasvd</code></a></li><li><a href="#Jchemo.rd-Tuple{Any, Any}"><code>Jchemo.rd</code></a></li><li><a href="#Jchemo.rda-Tuple{}"><code>Jchemo.rda</code></a></li><li><a href="#Jchemo.recod_catbydict-Tuple{Any, Any}"><code>Jchemo.recod_catbydict</code></a></li><li><a href="#Jchemo.recod_catbyind-Tuple{Any, Any}"><code>Jchemo.recod_catbyind</code></a></li><li><a href="#Jchemo.recod_catbyint-Tuple{Any}"><code>Jchemo.recod_catbyint</code></a></li><li><a href="#Jchemo.recod_catbylev-Tuple{Any, Any}"><code>Jchemo.recod_catbylev</code></a></li><li><a href="#Jchemo.recod_contbyint-Tuple{Any, Any}"><code>Jchemo.recod_contbyint</code></a></li><li><a href="#Jchemo.recod_indbylev-Tuple{Union{Int64, Array{Int64}}, Array}"><code>Jchemo.recod_indbylev</code></a></li><li><a href="#Jchemo.recod_miss-Tuple{AbstractArray}"><code>Jchemo.recod_miss</code></a></li><li><a href="#Jchemo.recovkw-Tuple{DataType, Any}"><code>Jchemo.recovkw</code></a></li><li><a href="#Jchemo.residcla-Tuple{Any, Any}"><code>Jchemo.residcla</code></a></li><li><a href="#Jchemo.residreg-Tuple{Any, Any}"><code>Jchemo.residreg</code></a></li><li><a href="#Jchemo.rfda-Tuple{}"><code>Jchemo.rfda</code></a></li><li><a href="#Jchemo.rfr-Tuple{}"><code>Jchemo.rfr</code></a></li><li><a href="#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, BitVector, Vector}}"><code>Jchemo.rmcol</code></a></li><li><a href="#Jchemo.rmgap-Tuple{}"><code>Jchemo.rmgap</code></a></li><li><a href="#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, BitVector, Vector}}"><code>Jchemo.rmrow</code></a></li><li><a href="#Jchemo.rmsep-Tuple{Any, Any}"><code>Jchemo.rmsep</code></a></li><li><a href="#Jchemo.rmsepstand-Tuple{Any, Any}"><code>Jchemo.rmsepstand</code></a></li><li><a href="#Jchemo.rosaplsr-Tuple{}"><code>Jchemo.rosaplsr</code></a></li><li><a href="#Jchemo.rowmean-Tuple{Any}"><code>Jchemo.rowmean</code></a></li><li><a href="#Jchemo.rownorm-Tuple{Any}"><code>Jchemo.rownorm</code></a></li><li><a href="#Jchemo.rowstd-Tuple{Any}"><code>Jchemo.rowstd</code></a></li><li><a href="#Jchemo.rowsum-Tuple{Any}"><code>Jchemo.rowsum</code></a></li><li><a href="#Jchemo.rowvar-Tuple{Any}"><code>Jchemo.rowvar</code></a></li><li><a href="#Jchemo.rp-Tuple{}"><code>Jchemo.rp</code></a></li><li><a href="#Jchemo.rpd-Tuple{Any, Any}"><code>Jchemo.rpd</code></a></li><li><a href="#Jchemo.rpdr-Tuple{Any, Any}"><code>Jchemo.rpdr</code></a></li><li><a href="#Jchemo.rpmatgauss"><code>Jchemo.rpmatgauss</code></a></li><li><a href="#Jchemo.rpmatli"><code>Jchemo.rpmatli</code></a></li><li><a href="#Jchemo.rr-Tuple{}"><code>Jchemo.rr</code></a></li><li><a href="#Jchemo.rrchol-Tuple{}"><code>Jchemo.rrchol</code></a></li><li><a href="#Jchemo.rrda-Tuple{}"><code>Jchemo.rrda</code></a></li><li><a href="#Jchemo.rrmsep-Tuple{Any, Any}"><code>Jchemo.rrmsep</code></a></li><li><a href="#Jchemo.rrr-Tuple{}"><code>Jchemo.rrr</code></a></li><li><a href="#Jchemo.rv-Tuple{Any, Any}"><code>Jchemo.rv</code></a></li><li><a href="#Jchemo.rweight-Tuple{Any, Any}"><code>Jchemo.rweight</code></a></li><li><a href="#Jchemo.sampcla"><code>Jchemo.sampcla</code></a></li><li><a href="#Jchemo.sampdf"><code>Jchemo.sampdf</code></a></li><li><a href="#Jchemo.sampdp-Tuple{Any, Int64}"><code>Jchemo.sampdp</code></a></li><li><a href="#Jchemo.sampks-Tuple{Any, Int64}"><code>Jchemo.sampks</code></a></li><li><a href="#Jchemo.samprand-Tuple{Int64, Int64}"><code>Jchemo.samprand</code></a></li><li><a href="#Jchemo.sampsys-Tuple{Any, Int64}"><code>Jchemo.sampsys</code></a></li><li><a href="#Jchemo.sampwsp-Tuple{Any, Any}"><code>Jchemo.sampwsp</code></a></li><li><a href="#Jchemo.savgk-Tuple{Int64, Int64, Int64}"><code>Jchemo.savgk</code></a></li><li><a href="#Jchemo.savgol-Tuple{}"><code>Jchemo.savgol</code></a></li><li><a href="#Jchemo.scale-Tuple{}"><code>Jchemo.scale</code></a></li><li><a href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>Jchemo.segmkf</code></a></li><li><a href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>Jchemo.segmts</code></a></li><li><a href="#Jchemo.selwold-Tuple{Any, Any}"><code>Jchemo.selwold</code></a></li><li><a href="#Jchemo.sep-Tuple{Any, Any}"><code>Jchemo.sep</code></a></li><li><a href="#Jchemo.snorm-Tuple{}"><code>Jchemo.snorm</code></a></li><li><a href="#Jchemo.snv-Tuple{}"><code>Jchemo.snv</code></a></li><li><a href="#Jchemo.softmax-Tuple{AbstractVector}"><code>Jchemo.softmax</code></a></li><li><a href="#Jchemo.soplsr-Tuple{}"><code>Jchemo.soplsr</code></a></li><li><a href="#Jchemo.sourcedir-Tuple{Any}"><code>Jchemo.sourcedir</code></a></li><li><a href="#Jchemo.spca-Tuple{}"><code>Jchemo.spca</code></a></li><li><a href="#Jchemo.spcr-Tuple{}"><code>Jchemo.spcr</code></a></li><li><a href="#Jchemo.splskdeda-Tuple{}"><code>Jchemo.splskdeda</code></a></li><li><a href="#Jchemo.splslda-Tuple{}"><code>Jchemo.splslda</code></a></li><li><a href="#Jchemo.splsqda-Tuple{}"><code>Jchemo.splsqda</code></a></li><li><a href="#Jchemo.splsr-Tuple{}"><code>Jchemo.splsr</code></a></li><li><a href="#Jchemo.splsrda-Tuple{}"><code>Jchemo.splsrda</code></a></li><li><a href="#Jchemo.ssr-Tuple{Any, Any}"><code>Jchemo.ssr</code></a></li><li><a href="#Jchemo.stdv-Tuple{Any}"><code>Jchemo.stdv</code></a></li><li><a href="#Jchemo.summ-Tuple{Any}"><code>Jchemo.summ</code></a></li><li><a href="#Jchemo.sumv-Tuple{Any}"><code>Jchemo.sumv</code></a></li><li><a href="#Jchemo.svmda-Tuple{}"><code>Jchemo.svmda</code></a></li><li><a href="#Jchemo.svmr-Tuple{}"><code>Jchemo.svmr</code></a></li><li><a href="#Jchemo.tab-Tuple{AbstractArray}"><code>Jchemo.tab</code></a></li><li><a href="#Jchemo.tabcont-Tuple{Any, Any}"><code>Jchemo.tabcont</code></a></li><li><a href="#Jchemo.tabdupl-Tuple{Any}"><code>Jchemo.tabdupl</code></a></li><li><a href="#Jchemo.thresh_hard-Tuple{Any, Any}"><code>Jchemo.thresh_hard</code></a></li><li><a href="#Jchemo.thresh_soft-Tuple{Any, Any}"><code>Jchemo.thresh_soft</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Snv, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Savgol, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Spca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Umap, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Cscale, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rmgap, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Covsel, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rp, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.DetrendLo, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Fdif, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Center, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Blockscal, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.DetrendArpls, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Snorm, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbplsprobda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.DetrendAirpls, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.DetrendPol, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbplswest, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Plsprobda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.DetrendAsls, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Interpl, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Scale, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mavg, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.treeda-Tuple{}"><code>Jchemo.treeda</code></a></li><li><a href="#Jchemo.treer-Tuple{}"><code>Jchemo.treer</code></a></li><li><a href="#Jchemo.umap-Tuple{}"><code>Jchemo.umap</code></a></li><li><a href="#Jchemo.varv-Tuple{Any}"><code>Jchemo.varv</code></a></li><li><a href="#Jchemo.vcatdf-Tuple{Any}"><code>Jchemo.vcatdf</code></a></li><li><a href="#Jchemo.vcol-Tuple{Any, Any}"><code>Jchemo.vcol</code></a></li><li><a href="#Jchemo.vip-Tuple{Union{Jchemo.Mbplsr, Jchemo.Pcr, Jchemo.Plsr, Jchemo.Spcr, Jchemo.Splsr}}"><code>Jchemo.vip</code></a></li><li><a href="#Jchemo.viperm!-Tuple{Any, Any, Any}"><code>Jchemo.viperm!</code></a></li><li><a href="#Jchemo.vrow-Tuple{Any, Any}"><code>Jchemo.vrow</code></a></li><li><a href="#Jchemo.wdis-Tuple{Any}"><code>Jchemo.wdis</code></a></li><li><a href="#Jchemo.winvs-Tuple{Any}"><code>Jchemo.winvs</code></a></li><li><a href="#Jchemo.wtal-Tuple{Any}"><code>Jchemo.wtal</code></a></li><li><a href="#Jchemo.xfit-Tuple{Any}"><code>Jchemo.xfit</code></a></li><li><a href="#Jchemo.xresid-Tuple{Any, Any}"><code>Jchemo.xresid</code></a></li><li><a href="#Jchemo.@names-Tuple{Any}"><code>Jchemo.@names</code></a></li><li><a href="#Jchemo.@namvar-Tuple{Any}"><code>Jchemo.@namvar</code></a></li><li><a href="#Jchemo.@pars-Tuple{Any}"><code>Jchemo.@pars</code></a></li><li><a href="#Jchemo.@plist-Tuple{Any}"><code>Jchemo.@plist</code></a></li><li><a href="#Jchemo.@pmod-Tuple{Any}"><code>Jchemo.@pmod</code></a></li><li><a href="#Jchemo.@type-Tuple{Any}"><code>Jchemo.@type</code></a></li></ul><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Cca, Any, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Cca, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Cca, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/cca.jl#L185-L191">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Ccawold, Any, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Ccawold, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Ccawold, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/ccawold.jl#L246-L252">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Comdim, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Comdim, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Comdim, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/comdim.jl#L245-L250">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Covsel}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Covsel}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Covsel)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/covsel.jl#L151-L155">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Fda}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Fda}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Fda)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/fda.jl#L142-L147">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Kpca}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Kpca}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Kpca)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kpca.jl#L120-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Mbpca, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Mbpca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Mbpca, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbpca.jl#L275-L280">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Mbplsr, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Mbplsr, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Mbplsr, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsr.jl#L196-L201">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Mbplswest, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Mbplswest, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Mbplswest, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to    fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplswest.jl#L246-L252">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Pca, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Pca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Pca, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcasvd.jl#L113-L118">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Plscan, Any, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Plscan, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Plscan, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plscan.jl#L197-L203">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Plstuck, Any, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Plstuck, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Plstuck, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plstuck.jl#L145-L151">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Rasvd, Any, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Rasvd, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Rasvd, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rasvd.jl#L180-L186">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Jchemo.Spca, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Jchemo.Spca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Spca, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/spca.jl#L195-L200">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Union{Pcr, Spcr}, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcr.jl#L146-L151">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><a class="docstring-binding" href="#Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summary(object::Union{Plsr, Splsr}, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plskern.jl#L230-L235">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.aggmean-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.aggmean-Tuple{Any, Any}"><code>Jchemo.aggmean</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">aggmean(X, y)</code></pre><p>Compute column-wise means by group in a dataset.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>y</code> : A group variable (n).</li></ul><p>This is a (faster) particular case of <code>aggstat</code>: computes means from a single group variable. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, :auto) 
y = rand(1:3, n)

res = aggmean(X, y)
res.X
res.lev 

aggmean(df, y).X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L61-L84">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.aggstat-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.aggstat-Tuple{Any, Any}"><code>Jchemo.aggstat</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">aggstat(X, y; algo = mean)
aggstat(X::DataFrame; sel, group, algo = mean)</code></pre><p>Compute column-wise statistics by group in a dataset.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>y</code> : A categorical variable (n) defining the groups.</li><li><code>algo</code> : Function to compute (default = mean).</li></ul><p>Specific for <code>X::DataFrame</code>:</p><ul><li><code>sel</code> : Vector of the names of the variables to summarize.</li><li><code>group</code> : Vector of the names of the categorical variables defining the groups.</li></ul><p>Variables defined in <code>sel</code> and <code>group</code> must be columns of <code>X</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, DataFrames, Statistics

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, :auto)
y = rand(1:3, n)

res = aggstat(X, y; algo = sum)
@names res
res.lev 
res.X

aggstat(df, y; algo = sum).X

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, string.(&quot;v&quot;, 1:p))
df.y1 = rand(1:2, n)
df.y2 = rand([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], n)
df

aggstat(df; sel = [:v1, :v2] , group = [:y1, :y2], algo = var)  # return a dataframe </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L1-L39">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.aggsumv-Tuple{Vector, Union{BitVector, Vector}}"><a class="docstring-binding" href="#Jchemo.aggsumv-Tuple{Vector, Union{BitVector, Vector}}"><code>Jchemo.aggsumv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">aggsumv(x::Vector, y::Union{Vector, BitVector})</code></pre><p>Compute the sum by group over a categorical variable.</p><ul><li><code>x</code> : A vector representing the quantitative variable to sum (n) </li><li><code>y</code> : A vector representing the group variable (n).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = ones(1000)
y = vcat(rand([&quot;a&quot; ; &quot;c&quot;], 900), repeat([&quot;b&quot;], 100))

aggsumv(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L99-L115">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.aicplsr-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.aicplsr-Tuple{Any, Any}"><code>Jchemo.aicplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">aicplsr(X, y; alpha = 2, kwargs...)</code></pre><p>Compute Akaike&#39;s (AIC) and Mallows&#39;s (Cp) criteria for univariate PLSR models.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate Y-data.</li></ul><p>Keyword arguments:</p><ul><li>Same arguments as those of function <code>cglsr</code>.</li><li><code>alpha</code> : Coefficient multiplicating the model complexity (df) to compute AIC. </li></ul><p>The function uses function <code>dfplsr_cg</code>. </p><p><strong>References</strong></p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3. Numer Algor 46, 189–194.  https://doi.org/10.1007/s11075-007-9136-9</p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating Mallows’s Cp and AIC criteria  for PLSR models. Illustration on agronomic spectroscopic NIR data. Journal of Chemometrics n/a, e3369.  https://doi.org/10.1002/cem.3369</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 40
res = aicplsr(X, y; nlv) ;
res.crit
res.opt
res.delta

zaic = res.crit.aic
f, ax = plotgrid(0:nlv, zaic; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;AIC&quot;)
scatter!(ax, 0:nlv, zaic)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/aicplsr.jl#L1-L51">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.aov1-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.aov1-Tuple{Any, Any}"><code>Jchemo.aov1</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">aov1(x, Y)
One-factor ANOVA test.</code></pre><ul><li><code>x</code> : Univariate categorical (factor) data (n).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
@names dat
@head dat.X
x = dat.X[:, 5]
Y = dat.X[:, 1:4]
tab(x) 

res = aov1(x, Y) ;
@names res
res.SSF
res.SSR 
res.F 
res.pval</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/aov1.jl#L1-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.bias-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.bias-Tuple{Any, Any}"><code>Jchemo.bias</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">bias(pred, Y)</code></pre><p>Compute the prediction bias, i.e. the opposite of the mean prediction error.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
bias(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
bias(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L268-L295">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.blockscal-Tuple{}"><a class="docstring-binding" href="#Jchemo.blockscal-Tuple{}"><code>Jchemo.blockscal</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">blockscal(; kwargs...)
blockscal(Xbl; kwargs...)
blockscal(Xbl, weights::Weight; kwargs...)</code></pre><p>Scale multiblock X-data.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>weights</code> : Weights (n) of the observations (rows of the blocks). Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>centr</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> is centered (before the block scaling).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> is scaled by its uncorrected standard deviation (before the block scaling).</li><li><code>bscal</code> : Type of block scaling. Possible values are: <code>:none</code>, <code>:frob</code>, <code>:mfa</code>, <code>:ncol</code>, <code>:sd</code>. See thereafter.</li></ul><p>If implemented, the data transformations follow the order: column centering, column scaling and finally block scaling. </p><p>Types of block scaling:</p><ul><li><code>:none</code> : No block scaling. </li><li><code>:frob</code> : Let D be the diagonal matrix of vector <code>weights.w</code>. Each block X is divided by its Frobenius norm  = sqrt(tr(X&#39; * D * X)).    After this scaling, tr(X&#39; * D * X) = 1.</li><li><code>:mfa</code> : Each block X is divided by sv, where sv is the dominant singular value of X (this is the &quot;MFA&quot; approach; &quot;AFM &quot;in French).</li><li><code>:ncol</code> : Each block X is divided by the nb. of columns of the block.</li><li><code>:sd</code> : Each block X is divided by sqrt(sum(weighted variances of the block-columns)). After this scaling, sum(weighted variances of    the block-columns) = 1.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
n = 5 ; m = 3 ; p = 10 
X = rand(n, p) 
Xnew = rand(m, p)
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl) 
Xblnew = mblock(Xnew, listbl) 
@head Xbl[3]

centr = true ; scal = true
bscal = :frob
model = blockscal(; centr, scal, bscal)
fit!(model, Xbl)
## Data transformation
zXbl = transf(model, Xbl) ; 
@head zXbl[3]

zXblnew = transf(model, Xblnew) ; 
zXblnew[3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_mb.jl#L33-L78">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.calds-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.calds-Tuple{Any, Any}"><code>Jchemo.calds</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">calds(; algo = plskern, kwargs...)
calds(X1, X2; algo = plskern, kwargs...)</code></pre><p>Direct standardization (DS) for calibration transfer of spectral data.</p><ul><li><code>X1</code> : Spectra (n, p) to transfer to the target.</li><li><code>X2</code> : Target spectra (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>algo</code> : Function used as transfer model.  </li><li><code>kwargs</code> : Optional arguments for <code>algo</code>.</li></ul><p><code>X1</code> and <code>X2</code> must represent the same n samples (&quot;standards&quot;).</p><p>The objective is to transform spectra <code>X1</code> to new spectra as close as possible as the target <code>X2</code>.  Method DS fits a model (defined in <code>algo</code>) that predicts <code>X2</code> from <code>X1</code>.</p><p><strong>References</strong></p><p>Y. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,” Anal. Chem., vol. 63,  no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
@names dat
## Objects X1 and X2 are spectra collected 
## on the same samples. 
## X2 represents the target space. 
## We want to transfer X1 in the same space
## as X2.
## Data to transfer
X1cal = dat.X1cal
X1val = dat.X1val
n = nro(X1cal)
m = nro(X1val)
## Target space
X2cal = dat.X2cal
X2val = dat.X2val

## Fitting the model
fitm = calds(X1cal, X2cal; algo = plskern, nlv = 10) 
#fitm = calds(X1cal, X2cal; algo = mlrpinv)   # less robust 

## Transfer of new spectra X1val 
## expected to be close to X2val
pred = predict(fitm, X1val).pred

i = 1
f = Figure(size = (500, 300))
ax = Axis(f[1, 1])
lines!(X2val[i, :]; label = &quot;x2&quot;)
lines!(ax, X1val[i, :]; label = &quot;x1&quot;)
lines!(pred[i, :]; linestyle = :dash, label = &quot;x1_corrected&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/calds.jl#L1-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.calpds-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.calpds-Tuple{Any, Any}"><code>Jchemo.calpds</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">calpds(; npoint = 5, algo = plskern, kwargs...)
calpds(X1, X2; npoint = 5, algo = plskern, kwargs...)</code></pre><p>Piecewise direct standardization (PDS) for calibration transfer of spectral data.</p><ul><li><code>X1</code> : Spectra (n, p) to transfer to the target.</li><li><code>X2</code> : Target spectra (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Half-window size (nb. points left or right to the given wavelength). </li><li><code>algo</code> : Function used as transfer model.  </li><li><code>kwargs</code> : Optional arguments for <code>algo</code>.</li></ul><p><code>X1</code> and <code>X2</code> must represent the same n standard samples.</p><p>The objective is to transform spectra <code>X1</code> to new spectra as close as possible as the target <code>X2</code>.  Method PDS fits models (defined in <code>algo</code>) that predict <code>X2</code> from <code>X1</code>.</p><p>The window used in <code>X1</code> to predict wavelength &quot;i&quot; in <code>X2</code> is:</p><ul><li>i - <code>npoint</code>, i - <code>npoint</code> + 1, ..., i, ..., i + <code>npoint</code> - 1, i + <code>npoint</code></li></ul><p><strong>References</strong></p><p>Bouveresse, E., Massart, D.L., 1996. Improvement of the piecewise direct targetisation procedure  for the transfer of NIR spectra for multivariate calibration. Chemometrics and Intelligent Laboratory  Systems 32, 201–213. https://doi.org/10.1016/0169-7439(95)00074-7</p><p>Y. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.</p><p>Wülfert, F., Kok, W.Th., Noord, O.E. de, Smilde, A.K., 2000. Correction of Temperature-Induced  Spectral Variation by Continuous Piecewise Direct Standardization. Anal. Chem. 72, 1639–1644. https://doi.org/10.1021/ac9906835</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
@names dat
## Objects X1 and X2 are spectra collected 
## on the same samples. 
## X2 represents the target space. 
## We want to transfer X1 in the same space
## as X2.
## Data to transfer
X1cal = dat.X1cal
X1val = dat.X1val
n = nro(X1cal)
m = nro(X1val)
## Target space
X2cal = dat.X2cal
X2val = dat.X2val

## Fitting the model
fitm = calpds(X1cal, X2cal; npoint = 2, algo = plskern, nlv = 2) 

## Transfer of new spectra X1val 
## expected to be close to X2val
pred = predict(fitm, X1val).pred

i = 1
f = Figure(size = (500, 300))
ax = Axis(f[1, 1])
lines!(X2val[i, :]; label = &quot;x2&quot;)
lines!(ax, X1val[i, :]; label = &quot;x1&quot;)
lines!(pred[i, :]; linestyle = :dash, label = &quot;x1_corrected&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/calpds.jl#L1-L70">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cca-Tuple{}"><a class="docstring-binding" href="#Jchemo.cca-Tuple{}"><code>Jchemo.cca</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cca(; kwargs...)
cca(X, Y; kwargs...)
cca(X, Y, weights::Weight; kwargs...)
cca!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Canonical correlation Analysis (CCA, RCCA).</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:<code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks <code>X</code> and <code>Y</code> is scaled by its uncorrected standard    deviation (before the block scaling).</li></ul><p>This function implements a CCA algorithm using SVD decompositions and presented in Weenink 2003 section 2. </p><p>A continuum regularization is available (parameter <code>tau</code>). After block centering and scaling,  the function returns block LVs (Tx and Ty) that are proportionnal to the eigenvectors of Projx * Projy  and Projy * Projx, respectively, defined as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li><li>Cy = (1 - <code>tau</code>) * Y&#39;DY + <code>tau</code> * Iy</li><li>Cxy = X&#39;DY </li><li>Projx = sqrt(D) * X * invCx * X&#39; * sqrt(D)</li><li>Projy = sqrt(D) * Y * invCx * Y&#39; * sqrt(D)</li></ul><p>where D is the observation (row) metric. Value <code>tau</code> = 0 can generate unstability when inverting the covariance  matrices. Often, a better alternative is to use an epsilon value (e.g. <code>tau</code> = 1e-8) to get similar results as  with pseudo-inverses.  </p><p>After normalized (and using uniform <code>weights</code>), the scores returned by the function are expected to be the same as  those returned by functions <code>rcc</code> of the R packages <code>CCA</code> (González et al.) and <code>mixOmics</code> (Lê Cao et al.) whith their  parameters lambda1 and lambda2 set to:</p><ul><li>lambda1 = lambda2 = <code>tau</code> / (1 - <code>tau</code>) * n / (n - 1)</li></ul><p>See function <code>plscan</code> for the details on the <code>summary</code> outputs.</p><p><strong>References</strong></p><p>González, I., Déjean, S., Martin, P.G.P., Baccini, A., 2008. CCA: An R Package to Extend Canonical Correlation Analysis. Journal of Statistical Software 23, 1-14. https://doi.org/10.18637/jss.v023.i12</p><p>Hotelling, H. (1936): “Relations between two sets of variates”, Biometrika 28: pp. 321–377.</p><p>Lê Cao, K.-A., Rohart, F., Gonzalez, I., Dejean, S., Abadi, A.J., Gautier, B., Bartolo, F., Monget, P.,  Coquery, J., Yao, F., Liquet, B., 2022. mixOmics: Omics Data Integration Project.  https://doi.org/10.18129/B9.bioc.mixOmics</p><p>Weenink, D. 2003. Canonical Correlation Analysis, Institute of Phonetic Sciences, Univ. of Amsterdam,  Proceedings 25, 81-99.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
@names dat
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 3
bscal = :frob ; tau = 1e-8
model = cca(; nlv, bscal, tau)
fit!(model, X, Y)
@names model
@names model.fitm

@head model.fitm.Tx
@head transfbl(model, X, Y).Tx

@head model.fitm.Ty
@head transfbl(model, X, Y).Ty

res = summary(model, X, Y) ;
@names res
res.cortx2ty
res.rvx2tx
res.rvy2ty
res.rdx2tx
res.rdy2ty
res.corx2tx 
res.cory2ty </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/cca.jl#L1-L86">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.ccawold-Tuple{}"><a class="docstring-binding" href="#Jchemo.ccawold-Tuple{}"><code>Jchemo.ccawold</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ccawold(; kwargs...)
ccawold(X, Y; kwargs...)
ccawold(X, Y, weights::Weight; kwargs...)
ccawold!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Canonical correlation analysis (CCA, RCCA) - Wold Nipals algorithm.</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:<code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>tol</code> : Tolerance value for convergence (Nipals).</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks <code>X</code> and <code>Y</code> is scaled by its uncorrected standard    deviation (before the block scaling).</li></ul><p>This function implements the Nipals ccawold algorithm presented by Tenenhaus 1998 p.204 (related to Wold et al. 1984). </p><p>In this implementation, after each step of LVs computation, X and Y are deflated relatively to their respective scores  (tx and ty). </p><p>A continuum regularization is available (parameter <code>tau</code>). After block centering and scaling, the covariances matrices  are computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li><li>Cy = (1 - <code>tau</code>) * Y&#39;DY + <code>tau</code> * Iy</li></ul><p>where D is the observation (row) metric. Value <code>tau</code> = 0 can generate unstability when inverting the covariance  matrices. Often, a better alternative is to use an epsilon value (e.g. <code>tau</code> = 1e-8) to get similar results as with  pseudo-inverses.   </p><p>The normed scores returned by the function are expected (using uniform <code>weights</code>) to be the same as those returned  by function <code>rgcca</code> of the R package <code>RGCCA</code> (Tenenhaus &amp; Guillemot 2017, Tenenhaus et al. 2017). </p><p>See function <code>plscan</code> for the details on the <code>summary</code> outputs.</p><p><strong>References</strong></p><p>Tenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and Sparse Generalized Canonical Correlation Analysis for  Multiblock Data Multiblock data analysis.https://cran.r-project.org/web/packages/RGCCA/index.html </p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p>Tenenhaus, M., Tenenhaus, A., Groenen, P.J.F., 2017. Regularized Generalized Canonical Correlation Analysis: A Framework  for Sequential Multiblock Component Methods. Psychometrika 82, 737–777. https://doi.org/10.1007/s11336-017-9573-x</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear Regression. The Partial Least  Squares (PLS) Approach to Generalized Inverses. SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
@names dat
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 2
bscal = :frob ; tau = 1e-4
model = ccawold(; nlv, bscal, tau, tol = 1e-10)
fit!(model, X, Y)
@names model
@names model.fitm

@head model.fitm.Tx
@head transfbl(model, X, Y).Tx

@head model.fitm.Ty
@head transfbl(model, X, Y).Ty

res = summary(model, X, Y) ;
@names res
res.explvarx
res.explvary
res.cortx2ty
res.rvx2tx
res.rvy2ty
res.rdx2tx
res.rdy2ty
res.corx2tx 
res.cory2ty </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/ccawold.jl#L1-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.center-Tuple{}"><a class="docstring-binding" href="#Jchemo.center-Tuple{}"><code>Jchemo.center</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">center()
center(X)
center(X, weights::Weight)</code></pre><p>Column-wise centering of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = center() 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
colmean(Xptrain)
@head Xptest 
@head Xtest .- colmean(Xtrain)&#39;
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L106-L139">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cglsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.cglsr-Tuple{}"><code>Jchemo.cglsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cglsr(; kwargs...)
cglsr(X, y; kwargs...)
cglsr!(X::Matrix, y::Matrix; kwargs...)</code></pre><p>Conjugate gradient algorithm for the normal equations (CGLS; Björck 1996).</p><ul><li><code>X</code> : X-data  (n, p).</li><li><code>y</code> : Univariate Y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. CG iterations.</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt orthogonalization of the normal equation residual    vectors is done.</li><li><code>filt</code> : Boolean. If <code>true</code>, CG filter factors are computed (output <code>F</code>). Default = <code>false</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>CGLS algorithm &quot;7.4.1&quot; Bjorck 1996, p.289. In the present function, the part of the code computing the  re-orthogonalization (Hansen 1998) and filter factors (Vogel 1987, Hansen 1998) is a transcription (with few  adaptations) of the Matlab function <code>cgls</code>  (Saunders et al. https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).</p><p><strong>References</strong></p><p>Björck, A., 1996. Numerical Methods for Least Squares Problems, Other Titles in Applied Mathematics.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9781611971484</p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3. Numer Algor 46, 189–194.  https://doi.org/10.1007/s11075-007-9136-9</p><p>Manne R. Analysis of two partial-least-squares algorithms for multivariate calibration. Chemometrics Intell.  Lab. Syst. 1987, 2: 187–197.</p><p>Phatak A, De Hoog F. Exploiting the connection between PLS, Lanczos methods and conjugate gradients: alternative proofs  of some properties of PLS. J. Chemometrics 2002; 16: 361–367.</p><p>Vogel, C. R.,  &quot;Solving ill-conditioned linear systems using the conjugate gradient method&quot;, Report, Dept. of Mathematical  Sciences, Montana State University, 1987.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 5 ; scal = true
model = cglsr(; nlv, scal)
fit!(model, Xtrain, ytrain)
@names model.fitm 
@head model.fitm.B
coef(model.fitm).B
coef(model.fitm).int

pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/cglsr.jl#L1-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Cglsr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Cglsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Cglsr)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/cglsr.jl#L158-L162">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Dkplsr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Dkplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Dkplsr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplsr.jl#L136-L141">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Kplsr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Kplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Kplsr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplsr.jl#L184-L190">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Krr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Krr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Krr; lb = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>lb</code> : Ridge regularization parameter &#39;lambda&#39;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/krr.jl#L145-L150">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Pcr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Pcr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Pcr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a LV model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul><p>For a model fitted from X (n, p) and Y (n, q), the returned object <code>B</code> is a matrix (p, q). If <code>nlv</code> = 0, <code>B</code> is a matrix  of zeros. The returned object <code>int</code> is the intercept.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcr.jl#L103-L111">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Rosaplsr; nlv = nothing)</code></pre><p>Compute the X b-coefficients of a model fitted with <code>nlv</code> LVs.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rosaplsr.jl#L211-L216">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Jchemo.Rr}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Jchemo.Rr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Rr; lb = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>lb</code> : Ridge regularization parameter &#39;lambda&#39;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rr.jl#L108-L113">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Union{Mlr, MlrNoArg, Rrchol})</code></pre><p>Compute the coefficients of the fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L283-L287">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.coef-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}}"><a class="docstring-binding" href="#Jchemo.coef-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">coef(object::Union{Plsr, Pcr, Splsr}; nlv = nothing)</code></pre><p>Compute the b-coefficients of a LV model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul><p>For a model fitted from X (n, p) and Y (n, q), the returned object <code>B</code> is a matrix (p, q). If <code>nlv</code> = 0, <code>B</code> is a matrix  of zeros. The returned object <code>int</code> is the intercept.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plskern.jl#L187-L195">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colmad-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colmad-Tuple{Any}"><code>Jchemo.colmad</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colmad(X)</code></pre><p>Compute column-wise median absolute deviations (MAD) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)

colmad(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L217-L233">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colmean-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colmean-Tuple{Any}"><code>Jchemo.colmean</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colmean(X)
colmean(X, weights::Weight)</code></pre><p>Compute column-wise means of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colmean(X)
colmean(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L34-L54">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colmed-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colmed-Tuple{Any}"><code>Jchemo.colmed</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colmed(X)</code></pre><p>Compute column-wise medians of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)

colmed(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L190-L206">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colnorm-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colnorm-Tuple{Any}"><code>Jchemo.colnorm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colnorm(X)
colnorm(X, weights::Weight)</code></pre><p>Compute column-wise norms of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>The norm of each column x of <code>X</code> is computed by:</p><ul><li>sqrt(x&#39; * x)</li></ul><p>The weighted norm is:</p><ul><li>sqrt(x&#39; * D * x), where D is the diagonal matrix of <code>weights.w</code></li></ul><p><strong>Warning:</strong> <code>colnorm(X, mweight(ones(n)))</code> = <code>colnorm(X) / sqrt(n)</code>.</p><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colnorm(X)
colnorm(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L59-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colstd-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colstd-Tuple{Any}"><code>Jchemo.colstd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colstd(X)
colstd(X, weights::Weight)</code></pre><p>Compute column-wise standard deviations (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colstd(X)
colstd(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L108-L128">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colsum-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colsum-Tuple{Any}"><code>Jchemo.colsum</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colsum(X)
colsum(X, weights::Weight)</code></pre><p>Compute column-wise sums of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colsum(X)
colsum(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L1-L21">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.colvar-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.colvar-Tuple{Any}"><code>Jchemo.colvar</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">colvar(X)
colvar(X, weights::Weight)</code></pre><p>Compute column-wise variances (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colvar(X)
colvar(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_colwise.jl#L149-L169">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.comdim-Tuple{}"><a class="docstring-binding" href="#Jchemo.comdim-Tuple{}"><code>Jchemo.comdim</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">comdim(; kwargs...)
comdim(Xbl; kwargs...)
comdim(Xbl, weights::Weight; kwargs...)
comdim!(Xbl::Matrix, weights::Weight; kwargs...)</code></pre><p>Common components and specific weights analysis (CCSWA, a.k.a ComDim and HPCA).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code>.  </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. global latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>tol</code> : Tolerance value for convergence (Nipals).</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>ComDim &quot;SVD&quot; algorithm of Hannafi &amp; Qannari 2008 p.84.</p><p>The function returns several objects, in particular:</p><ul><li><code>T</code> : Global LVs (not-normed).</li><li><code>U</code> : Global LVs (normed).</li><li><code>W</code> : Block weights (normed).</li><li><code>Tb</code> : Block LVs (in the metric scale), <strong>grouped by LV</strong>.</li><li><code>Tbl</code> : Block LVs (in the original scale), <strong>grouped by block</strong>.</li><li><code>Vbl</code> : Block loadings (normed).</li><li><code>lb</code> : Block specific weights (saliences) &#39;lambda&#39;.</li><li><code>mu</code> : Sum of the block specific weights.</li></ul><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total X inertia (squared Frobenious norm) explained by the global LVs.</li><li><code>explvarxx</code> : Proportion of the total XX&#39; inertia explained by the global LVs (= indicator &quot;V&quot; in Qannari    et al. 2000, Hanafi et al. 2008).</li><li><code>explxbl</code> : Proportion of the inertia of each block (= Xbl[k]) explained by the global LVs.</li><li><code>psal2</code> : Proportion of the squared saliences of each block within each global score. </li><li><code>contrxbl2t</code> : Contribution of each block to the global LVs (= lb proportions). </li><li><code>rvxbl2t</code> : RV coefficients between each block and the global LVs.</li><li><code>rdxbl2t</code> : Rd coefficients between each block and the global LVs.</li><li><code>cortbl2t</code> : Correlations between the block LVs (= Tbl[k]) and the global LVs.</li><li><code>corx2t</code> : Correlation between the X-variables and the global LVs.  </li></ul><p><strong>References</strong></p><p>Cariou, V., Qannari, E.M., Rutledge, D.N., Vigneau, E., 2018. ComDim: From multiblock data analysis to  path modeling. Food Quality and Preference, Sensometrics 2016: Sensometrics-by-the-Sea 67, 27–34.  https://doi.org/10.1016/j.foodqual.2017.02.012</p><p>Cariou, V., Jouan-Rimbaud Bouveresse, D., Qannari, E.M., Rutledge, D.N., 2019. Chapter 7 - ComDim Methods  for the Analysis of Multiblock Data in a Data Fusion Perspective, in: Cocchi, M. (Ed.),  Data Handling in Science and Technology, Data Fusion Methodology and Applications. Elsevier, pp. 179–204.  https://doi.org/10.1016/B978-0-444-63984-4.00007-7</p><p>Ghaziri, A.E., Cariou, V., Rutledge, D.N., Qannari, E.M., 2016. Analysis of multiblock datasets using ComDim:  Overview and extension to the analysis of (K + 1) datasets. Journal of Chemometrics 30, 420–429.  https://doi.org/10.1002/cem.2810</p><p>Hanafi, M., 2008. Nouvelles propriétés de l’analyse en composantes communes et poids spécifiques. Journal de la  société française de statistique 149, 75–97.</p><p>Qannari, E.M., Wakeling, I., Courcoux, P., MacFie, H.J.H., 2000. Defining the underlying sensory dimensions.  Food Quality and Preference 11, 151–154. https://doi.org/10.1016/S0950-3293(99)00069-5</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
@names dat 
X = dat.X
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X[1:6, :], listbl)
Xblnew = mblock(X[7:8, :], listbl)
n = nro(Xbl[1]) 

nlv = 3
bscal = :frob
scal = false
#scal = true
model = comdim(; nlv, bscal, scal)
fit!(model, Xbl)
@names model 
@names model.fitm
## Global scores 
@head model.fitm.T
@head transf(model, Xbl)
transf(model, Xblnew)
## Blocks scores
i = 1
@head model.fitm.Tbl[i]
@head transfbl(model, Xbl)[i]

res = summary(model, Xbl) ;
@names res 
res.explvarx
res.explvarxx
res.psal2 
res.contrxbl2t
res.explxbl   # = model.fitm.lb if bscal = :frob
rowsum(Matrix(res.explxbl))
res.rvxbl2t
res.rdxbl2t
res.cortbl2t
res.corx2t </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/comdim.jl#L1-L105">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.conf-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.conf-Tuple{Any, Any}"><code>Jchemo.conf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">conf(pred, y; digits = 1)</code></pre><p>Confusion matrix.</p><ul><li><code>pred</code> : Univariate predictions.</li><li><code>y</code> : Univariate observed data.</li></ul><p>Keyword arguments:</p><ul><li><code>digits</code> : Nb. digits used to round percentages.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie

y = [&quot;d&quot;; &quot;c&quot;; &quot;b&quot;; &quot;c&quot;; &quot;a&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; 
    &quot;b&quot;; &quot;b&quot;; &quot;a&quot;; &quot;a&quot;; &quot;c&quot;; &quot;d&quot;; &quot;d&quot;]
pred = [&quot;a&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; 
    &quot;b&quot;; &quot;b&quot;; &quot;a&quot;; &quot;a&quot;; &quot;d&quot;; &quot;d&quot;; &quot;d&quot;]
#y = rand(1:10, 200); pred = rand(1:10, 200)

res = conf(pred, y) ;
@names res
res.cnt       # Counts (dataframe built from `A`) 
res.pct       # Row %  (dataframe built from `Apct`))
res.A         
res.Apct
res.diagpct
res.accpct    # Accuracy (% classification successes)
res.lev       # Levels

plotconf(res).f

plotconf(res; cnt = false, ptext = false).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/conf.jl#L1-L33">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.convertdf-Tuple{DataFrames.DataFrame}"><a class="docstring-binding" href="#Jchemo.convertdf-Tuple{DataFrames.DataFrame}"><code>Jchemo.convertdf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">convertdf(df::DataFrame; typ, miss = nothing)</code></pre><p>Convert the columns of a dataframe to given types.</p><ul><li><code>df</code> : A dataframe.</li><li><code>typ</code> : A vector of the targeted types for the columns of the new dataframe.  </li><li><code>miss</code> : The code used in <code>df</code> to identify the data to be declared as <code>missing</code> (of type <code>Missing</code>).   See function <code>recod_miss</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, DataFrames</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L276-L289">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cor2-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.cor2-Tuple{Any, Any}"><code>Jchemo.cor2</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cor2(pred, Y)</code></pre><p>Compute the squared linear correlation between data and predictions.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
cor2(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
cor2(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L301-L328">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.corm-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.corm-Tuple{Any}"><code>Jchemo.corm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">corm(X) 
corm(X, Y) 
corm(X, weights::Weight)
corm(X, Y, weights::Weight)</code></pre><p>Compute a weighted correlation matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Object of type <code>Weight</code> (e.g. generated by function <code>mweight</code>).</li></ul><p>Uncorrected correlation matrix </p><ul><li>of the <code>X</code>-columns :  return a (p, p) matrix, </li><li>or between the <code>X</code>-columns and the <code>Y</code>-columns :  return a matrix (p, q).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)
w = mweight(rand(n))

corm(X, w)
corm(X, Y, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L340-L366">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.corv-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.corv-Tuple{Any, Any}"><code>Jchemo.corv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">corv(x, y)</code></pre><p>Compute correlation between two vectors.</p><ul><li><code>x</code> : vector (n).</li><li><code>y</code> : vector (n).</li></ul><p><strong>References</strong></p><p>@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 5
x = rand(n)
y = rand(n)

corv(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L212-L232">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cosm-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.cosm-Tuple{Any}"><code>Jchemo.cosm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cosm(X)
cosm(X, Y)</code></pre><p>Compute a cosinus matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li></ul><p>The function computes the cosinus matrix: </p><ul><li>of the columns of <code>X</code>:  return a matrix (p, p),</li><li>or between the columns of <code>X</code> and <code>Y</code> :  return a matrix (p, q).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)

cosm(X)
cosm(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L389-L411">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cosv-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.cosv-Tuple{Any, Any}"><code>Jchemo.cosv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cosv(x, y)</code></pre><p>Compute cosinus between two vectors.</p><ul><li><code>x</code> : vector (n).</li><li><code>y</code> : vector (n).</li></ul><p><strong>References</strong></p><p>@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 5
x = rand(n)
y = rand(n)

cosv(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L264-L284">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.covm-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.covm-Tuple{Any}"><code>Jchemo.covm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">covm(X)
covm(X, weights::Weight)
covm(X, Y) 
covm(X, Y, weights::Weight)</code></pre><p>Compute a weighted covariance matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Object of type <code>Weight</code> (e.g. generated by function <code>mweight</code>).</li></ul><p>The function computes the uncorrected covariance matrix: </p><ul><li>of the columns of <code>X</code>:  return a matrix (p, p),</li><li>or between the columns of <code>X</code> and <code>Y</code> :  return a matrix (p, q).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)
w = mweight(rand(n))

covm(X, w)
covm(X, Y, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L294-L320">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.covsel-Tuple{}"><a class="docstring-binding" href="#Jchemo.covsel-Tuple{}"><code>Jchemo.covsel</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">covsel(; kwargs...)
covsel(X, Y; kwargs...)
covsel(X, Y, weights::Weight; kwargs...)
covsel!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Variable (feature) selection from partial covariance (Covsel).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. variables to select.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected    standard deviation.</li></ul><p>This is the Covsel algorithm described in Roger et al. 2011 for variable selection (see also  Höskuldsson, A., 1992).</p><p>The selection is sequential and based on the <em>partial</em> covariance principle. One first variable (that  maximizes the selection criterion: squared partial covariance) is selected, <code>X</code> and <code>Y</code> are orthogonolized  (deflated) to this variable, the selection criterion is recomputed and the next variable is selected,  and so on.</p><p>When <code>Y</code>is multivariate (q &gt; 1), it is recommended to use scaling to give the same importance to the  variables when computing covariances.</p><p><strong>Note:</strong> A faster alternative to function <code>covsel</code> is function <code>splsr</code> (with <code>nlv = 1</code>) that implements  the kernel PLS algorithm of Dayal&amp;McGregor 1997. Another faster algorithm is described in Mishra 2022.   </p><p><strong>References</strong></p><p>Höskuldsson, A., 1992. The H-principle in modelling with applications to chemometrics. Chemometrics  and Intelligent Laboratory Systems, Proceedings of the 2nd Scandinavian Symposium on Chemometrics 14,  139–153. https://doi.org/10.1016/0169-7439(92)80099-P</p><p>Mishra, P., 2022. A brief note on a new faster covariate’s selection (fCovSel) algorithm.  Journal of Chemometrics 36, e3397. https://doi.org/10.1002/cem.3397</p><p>Roger, J.M., Palagos, B., Bertrand, D., Fernandez-Ahumada, E., 2011. covsel: Variable selection for highly  multivariate and multi-response calibration: Application to IR spectroscopy.  Chem. Lab. Int. Syst. 106, 216-223.</p><p>Wikipedia https://en.wikipedia.org/wiki/Partial_correlation</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat

X = dat.X
y = dat.Y.tbc

nlv = 10
model = covsel(; nlv)
fit!(model, X, y)
fitm = model.fitm ;
fitm.sel
fitm.selc

@head transf(model, X; nlv = 3)

res = summary(model)
res.explvarx 
res.explvary

plotxy(1:nlv, fitm.selc; xlabel = &quot;Variable&quot;, ylabel = &quot;Importance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/covsel.jl#L1-L70">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.covv-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.covv-Tuple{Any, Any}"><code>Jchemo.covv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">covv(x, y)</code></pre><p>Compute uncorrected covariance between two vectors.</p><ul><li><code>x</code> : vector (n).</li><li><code>y</code> : vector (n).</li></ul><p><strong>References</strong></p><p>@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 5
x = rand(n)
y = rand(n)

covv(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L179-L199">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cscale-Tuple{}"><a class="docstring-binding" href="#Jchemo.cscale-Tuple{}"><code>Jchemo.cscale</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cscale()
cscale(X)
cscale(X, weights::Weight)</code></pre><p>Column-wise centering and scaling of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))

db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = cscale() 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
colmean(Xptrain)
colstd(Xptrain)
@head Xptest 
@head (Xtest .- colmean(Xtrain)&#39;) ./ colstd(Xtrain)&#39;
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L232-L267">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.cweight-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.cweight-Tuple{Any, Any}"><code>Jchemo.cweight</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cweight(X, v)
cweight!(X::AbstractMatrix, v)</code></pre><p>Weight each column of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>v</code> : A weighting vector (p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, LinearAlgebra

X = rand(5, 2) 
w = rand(2) 
cweight(X, w)
X * diagm(w)

cweight!(X, w)
X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_weighting.jl#L115-L134">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.detrend_airpls-Tuple{}"><a class="docstring-binding" href="#Jchemo.detrend_airpls-Tuple{}"><code>Jchemo.detrend_airpls</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">detrend_airpls(; kwargs...)
detrend_airpls(X; kwargs...)</code></pre><p>Baseline correction of each row of X-data by adaptive iteratively reweighted penalized least      squares algorithm (AIRPLS).</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Penalizing (smoothing) parameter &quot;lambda&quot;.</li><li><code>maxit</code> : Maximum number of iterations.</li><li><code>verbose</code> : If <code>true</code>, nb. iterations are printed.</li></ul><p>De-trend transformation: the function fits a baseline by AIRPLS (see Zhang et al. 2010, and Baek et al. 2015  section 2) for each observation and returns the residuals (= signals corrected from the baseline).</p><p><strong>References</strong></p><p>Baek, S.-J., Park, A., Ahn, Y.-J., Choo, J., 2015. Baseline correction using asymmetrically reweighted penalized  least squares smoothing. Analyst 140, 250–257. https://doi.org/10.1039/C4AN01061B</p><p>Zhang, Z.-M., Chen, S., Liang, Y.-Z., 2010. Baseline correction using adaptive iteratively reweighted penalized  least squares. Analyst 135, 1138–1146. https://doi.org/10.1039/B922045C</p><p>https://github.com/zmzhang/airPLS/tree/master </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

## Example on 1 spectrum
i = 2
zX = Matrix(X)[i:i, :]
lb = 1e6
model = detrend_airpls(; lb)
fit!(model, zX)
zXc = transf(model, zX)   # = corrected spectrum 
B = zX - zXc              # = estimated baseline
f, ax = plotsp(zX, wl)
lines!(wl, vec(B); color = :blue)
lines!(wl, vec(zXc); color = :black)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/detrend_airpls.jl#L1-L54">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.detrend_arpls-Tuple{}"><a class="docstring-binding" href="#Jchemo.detrend_arpls-Tuple{}"><code>Jchemo.detrend_arpls</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">detrend_arpls(; kwargs...)
detrend_arpls(X; kwargs...)</code></pre><p>Baseline correction of each row of X-data by asymmetrically reweighted penalized least      squares smoothing (ARPLS).</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Penalizing (smoothness) parameter &quot;lambda&quot;.</li><li><code>tol</code> : Tolerance value for stopping the iterations.  </li><li><code>maxit</code> : Maximum number of iterations.</li><li><code>verbose</code> : If <code>true</code>, nb. iterations are printed.</li></ul><p>De-trend transformation: the function fits a baseline by ARPLS (see Baek et al. 2015 section 3) for each observation  and returns the residuals (= signals corrected from the baseline).</p><p><strong>References</strong></p><p>Baek, S.-J., Park, A., Ahn, Y.-J., Choo, J., 2015. Baseline correction using asymmetrically reweighted penalized  least squares smoothing. Analyst 140, 250–257. https://doi.org/10.1039/C4AN01061B</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

## Example on 1 spectrum
i = 2
zX = Matrix(X)[i:i, :]
lb = 1e4
model = detrend_arpls(; lb, p)
fit!(model, zX)
zXc = transf(model, zX)   # = corrected spectrum 
B = zX - zXc              # = estimated baseline
f, ax = plotsp(zX, wl)
lines!(wl, vec(B); color = :blue)
lines!(wl, vec(zXc); color = :black)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/detrend_arpls.jl#L1-L50">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.detrend_asls-Tuple{}"><a class="docstring-binding" href="#Jchemo.detrend_asls-Tuple{}"><code>Jchemo.detrend_asls</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">detrend_asls(; kwargs...)
detrend_asls(X; kwargs...)</code></pre><p>Baseline correction of each row of X-data by asymmetric least squares algorithm (ASLS).</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Penalizing (smoothness) parameter &quot;lambda&quot;.</li><li><code>p</code> : Asymmetry parameter (0 &lt; <code>p</code> &lt;&lt; 1).</li><li><code>tol</code> : Tolerance value for stopping the iterations.  </li><li><code>maxit</code> : Maximum number of iterations.</li><li><code>verbose</code> : If <code>true</code>, nb. iterations are printed.</li></ul><p>De-trend transformation: the function fits a baseline by ASLS (see Baek et al. 2015 section 2) for each observation  and returns the residuals (= signals corrected from the baseline).</p><p>Generally <code>0.001 ≤ p ≤ 0.1</code> is a good choice (for a signal with positive peaks) and <code>1e2 ≤ lb ≤ 1e9</code>, but exceptions may occur (Eilers &amp; Boelens 2005).</p><p><strong>References</strong></p><p>Baek, S.-J., Park, A., Ahn, Y.-J., Choo, J., 2015. Baseline correction using asymmetrically reweighted penalized  least squares smoothing. Analyst 140, 250–257. https://doi.org/10.1039/C4AN01061B</p><p>Eilers, P. H., &amp; Boelens, H. F. (2005). Baseline correction with asymmetric least squares smoothing. Leiden  University Medical Centre Report, 1(1).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

## Example on 1 spectrum
i = 2
zX = Matrix(X)[i:i, :]
lb = 1e5 ; p = .001
model = detrend_asls(; lb, p)
fit!(model, zX)
zXc = transf(model, zX)   # = corrected spectrum 
B = zX - zXc              # = estimated baseline
f, ax = plotsp(zX, wl)
lines!(wl, vec(B); color = :blue)
lines!(wl, vec(zXc); color = :black)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/detrend_asls.jl#L1-L56">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.detrend_lo-Tuple{}"><a class="docstring-binding" href="#Jchemo.detrend_lo-Tuple{}"><code>Jchemo.detrend_lo</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">detrend_lo(; kwargs...)
detrend_lo(X; kwargs...)</code></pre><p>Baseline correction of each row of X-data by LOESS regression.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>span</code> : Window for neighborhood selection (level of smoothing) for the local fitting, typically proportion    within [0, 1].</li><li><code>degree</code> : Polynomial degree for the local fitting.</li></ul><p>De-trend transformation: The function fits a baseline by LOESS regression (function <code>loessr</code>) for each  observation and returns the residuals (= signals corrected from the baseline).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = detrend_lo(span = .8)
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain, wl).f
plotsp(Xptest, wl).f

## Example on 1 spectrum
i = 2
zX = Matrix(X)[i:i, :]
model = detrend_lo(span = .75)
fit!(model, zX)
zXc = transf(model, zX)   # = corrected spectrum 
B = zX - zXc            # = estimated baseline
f, ax = plotsp(zX, wl)
lines!(wl, vec(B); color = :blue)
lines!(wl, vec(zXc); color = :black)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L1-L49">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.detrend_pol-Tuple{}"><a class="docstring-binding" href="#Jchemo.detrend_pol-Tuple{}"><code>Jchemo.detrend_pol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">detrend_pol(; kwargs...)
detrend_pol(X; kwargs...)</code></pre><p>Baseline correction of each row of X-data by polynomial linear regression.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>degree</code> : Polynom degree.</li></ul><p>De-trend transformation: the function fits a baseline by polynomial regression for each observation  and returns the residuals (= signals corrected from the baseline).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = detrend_pol(degree = 2)
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain, wl).f
plotsp(Xptest, wl).f

## Example on 1 spectrum
i = 2
zX = Matrix(X)[i:i, :]
model = detrend_pol(degree = 1)
fit!(model, zX)
zXc = transf(model, zX)   # = corrected spectrum 
B = zX - zXc            # = estimated baseline
f, ax = plotsp(zX, wl)
lines!(wl, vec(B); color = :blue)
lines!(wl, vec(zXc); color = :black)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L84-L130">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dfplsr_cg-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.dfplsr_cg-Tuple{Any, Any}"><code>Jchemo.dfplsr_cg</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dfplsr_cg(X, y; kwargs...)</code></pre><p>Compute the model complexity (df) of PLSR models with the CGLS algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate Y-data.</li></ul><p>Keyword arguments:</p><ul><li>Same as function <code>cglsr</code>.</li></ul><p>The number of degrees of freedom (<code>df</code>) of the PLSR model is returned for 0, 1, ..., <code>nlv</code> LVs.</p><p><strong>References</strong></p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3. Numer Algor 46, 189–194.  https://doi.org/10.1007/s11075-007-9136-9</p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating Mallows’s Cp and AIC criteria  for PLSR models. Illustration on agronomic spectroscopic NIR data. Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">## The example below reproduces the numerical illustration given by Kramer &amp; Sugiyama 2011 
## on the Ozone data (Fig. 1, center).
## Function &quot;pls.model&quot; used for df calculations in the R package &quot;plsdof&quot; v0.2-9 (Kramer &amp; Braun 2019)
## automatically scales the X matrix before PLS. The example scales X for consistency with plsdof.

using Jchemo, JchemoData, JLD2, DataFrames, CairoMakie 
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ozone.jld2&quot;) 
@load db dat
@names dat
X = dat.X
dropmissing!(X) 
zX = rmcol(Matrix(X), 4) 
y = X[:, 4] 
## For consistency with plsdof
xscales = colstd(zX)
zXs = fscale(zX, xscales)
## End

nlv = 12 ; gs = true
res = dfplsr_cg(zXs, y; nlv, gs) ;
res.df 
df_kramer = [1.000000, 3.712373, 6.456417, 11.633565, 
    12.156760, 11.715101, 12.349716,
    12.192682, 13.000000, 13.000000, 
    13.000000, 13.000000, 13.000000]
f, ax = plotgrid(0:nlv, df_kramer; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;df&quot;)
scatter!(ax, 0:nlv, res.df; color = &quot;red&quot;)
ablines!(ax, 1, 1; color = :grey, linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dfplsr_cg.jl#L1-L55">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.difmean-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.difmean-Tuple{Any, Any}"><code>Jchemo.difmean</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">difmean(X1, X2; normx::Bool = false)</code></pre><p>Compute a 1-D detrimental matrix by difference of the column-means of two X-datas.</p><ul><li><code>X1</code> : Spectra (n1, p).</li><li><code>X2</code> : Spectra (n2, p).</li></ul><p>Keyword arguments:</p><ul><li><code>normx</code> : Boolean. If <code>true</code>, the column-means vectors of <code>X1</code> and <code>X2</code> are normed before computing their difference.</li></ul><p>The function returns a matrix <code>D</code> (1, p) computed by the difference between two mean-spectra, i.e. the  column-means of <code>X1</code> and <code>X2</code>. </p><p><code>D</code> is assumed to contain the detrimental information that can be removed (by orthogonalization)  from <code>X1</code> and <code>X2</code> for calibration transfer. For instance, <code>D</code> can be used as input of function <code>eposvd</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
@names dat
X1cal = dat.X1cal
X1val = dat.X1val
X2cal = dat.X2cal
X2val = dat.X2val

## The objective is to remove a detrimental 
## information (here, D) from spaces X1 and X2
D = difmean(X1cal, X2cal).D
res = eposvd(D; nlv = 1)
## Corrected Val matrices
X1val_c = X1val * res.M
X2val_c = X2val * res.M

i = 1
f = Figure(size = (800, 300))
ax1 = Axis(f[1, 1])
ax2 = Axis(f[1, 2])
lines!(ax1, X1val[i, :]; label = &quot;x1&quot;)
lines!(ax1, X2val[i, :]; label = &quot;x2&quot;)
axislegend(ax1, position = :cb, framevisible = false)
lines!(ax2, X1val_c[i, :]; label = &quot;x1_correct&quot;)
lines!(ax2, X2val_c[i, :]; label = &quot;x2_correct&quot;)
axislegend(ax2, position = :cb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/difmean.jl#L1-L47">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dkplskdeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.dkplskdeda-Tuple{}"><code>Jchemo.dkplskdeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dkplskdeda(; kwargs...)
dkplskdeda(X, y; kwargs...)
dkplskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>DKPLS-KDEDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>Same as function <code>plskdeda</code> (PLS-KDEDA) except that a direct kernel PLSR (function <code>dkplsr</code>), instead of a PLSR  (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>dkplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplskdeda.jl#L1-L25">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dkplslda-Tuple{}"><a class="docstring-binding" href="#Jchemo.dkplslda-Tuple{}"><code>Jchemo.dkplslda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dkplslda(; kwargs...)
dkplslda(X, y; kwargs...)
dkplslda(X, y, weights::Weight; kwargs...)</code></pre><p>DKPLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Eventual keyword arguments of function <code>dmkern</code> (bandwidth definition).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plslda</code> (PLS-LDA) except that a direct kernel PLSR (function <code>dkplsr</code>), instead of a PLSR  (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
gamma = .1
model = dkplslda(; nlv, gamma) 
#model = dkplslda(; nlv, gamma, prior = :unif) 
#model = dkplsqda(; nlv, gamma, alpha = .5) 
#model = dkplskdeda(; nlv, gamma, a = .5) 
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm

fitm.lev
fitm.ni
fitm.priors

fitm_emb = fitm.fitm_emb ;
typeof(fitm_emb)
@names fitm_emb 
typeof(fitm_emb.fitm)

@head transf(model, Xtrain)
@head fitm_emb.fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplslda.jl#L1-L77">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dkplsqda-Tuple{}"><a class="docstring-binding" href="#Jchemo.dkplsqda-Tuple{}"><code>Jchemo.dkplsqda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dkplsqda(; kwargs...)
dkplsqda(X, y; kwargs...)
dkplsqda(X, y, weights::Weight; kwargs...)</code></pre><p>DKPLS-QDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>Same as function <code>plsqda</code> (PLS-QDA) except that a direct kernel PLSR (function <code>dkplsr</code>), instead of a PLSR  (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>dkplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplsqda.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dkplsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.dkplsr-Tuple{}"><code>Jchemo.dkplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dkplsr(; kwargs...)
dkplsr(X, Y; kwargs...)
dkplsr(X, Y, weights::Weight; kwargs...)
dkplsr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Direct kernel partial least squares regression (DKPLSR) (Bennett &amp; Embrechts 2003).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective functions    <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>The method builds kernel Gram matrices and then runs a usual PLSR algorithm on them. This is faster (but not equivalent) to the  &quot;true&quot; KPLSR (Nipals) algorithm (function <code>kplsr</code>) described in Rosipal &amp; Trejo (2001).</p><p><strong>References</strong></p><p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and Applications, NATO Science Series III: Computer &amp; Systems Sciences.  IOS Press Amsterdam, pp. 227-250.</p><p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space.  Journal of Machine Learning Research 2, 97-123.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
kern = :krbf ; gamma = 1e-1 ; scal = false
#gamma = 1e-4 ; scal = true
model = dkplsr(; nlv, kern, gamma, scal) ;
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm)
@names fitm.fitm

@head transf(model, Xtrain)
@head fitm.fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

coef(model)
coef(model; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f  

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
nlv = 2
gamma = 1 / 3
model = dkplsr(; nlv, gamma) ;
fit!(model, x, y)
pred = predict(model, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplsr.jl#L1-L89">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dkplsrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.dkplsrda-Tuple{}"><code>Jchemo.dkplsrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dkplsrda(; kwargs...)
dkplsrda(X, y; kwargs...)
dkplsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on direct kernel partial least squares regression (KPLSR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that a direct kernel PLSR (function <code>dkplsr</code>), instead of a PLSR  (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
kern = :krbf ; gamma = .001 
scal = true
model = dkplsrda(; nlv, kern, gamma, scal) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm
typeof(fitm.fitm.fitm) 
@names fitm.fitm.fitm

fitm.lev
fitm.ni
fitm.priors

@head transf(model, Xtrain)
@head fitm.fitm.fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplsrda.jl#L1-L75">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dmkern-Tuple{}"><a class="docstring-binding" href="#Jchemo.dmkern-Tuple{}"><code>Jchemo.dmkern</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dmkern(; kwargs...)
dmkern(X; kwargs...)</code></pre><p>Gaussian kernel density estimation (KDE).</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>h</code> : Define the bandwith, see examples.</li><li><code>a</code> : Constant for the Scott&#39;s rule (default bandwith), see thereafter.</li></ul><p>Estimation of the probability density of <code>X</code> (column space) by non parametric Gaussian kernels. </p><p>Data <code>X</code> can be univariate (p = 1) or multivariate (p &gt; 1). In the last case, function <code>dmkern</code> computes a multiplicative  kernel such as in Scott &amp; Sain 2005 Eq.19, and the internal bandwidth matrix <code>H</code> is diagonal (see the code). </p><p><strong>Note:</strong>  <code>H</code> in the <code>dmkern</code> code is often noted &quot;H^(1/2)&quot; in the litterature (e.g. Wikipedia).</p><p>The default bandwith is computed by:</p><ul><li><code>h</code> = <code>a</code> * n^(-1 / (p + 4)) * colstd(<code>X</code>)</li></ul><p>(<code>a</code> = 1 in Scott &amp; Sain 2005).</p><p><strong>References</strong></p><p>Scott, D.W., Sain, S.R., 2005. 9 - Multidimensional Density Estimation, in: Rao, C.R., Wegman, E.J., Solka, J.L. (Eds.),  Handbook of Statistics, Data Mining and Data Visualization. Elsevier, pp. 229–261. https://doi.org/10.1016/S0169-7161(04)24009-3</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
@names dat
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

nlv = 2
model0 = fda(; nlv)
fit!(model0, X, y)
@head T = transf(model0, X)
n, p = size(T)

#### Probability density in the FDA score space (2-D)

model = dmkern()
fit!(model, T) 
@names model.fitm
model.fitm.H
u = [1; 4; 150]
predict(model, T[u, :]).pred

h = .3
model = dmkern(; h)
fit!(model, T) 
model.fitm.H
predict(model, T[u, :]).pred

h = [.3; .1]
model = dmkern(; h)
fit!(model, T) 
model.fitm.H
predict(model, T[u, :]).pred

## Bivariate distribution
npoints = 2^7
nlv = 2
lims = [(minimum(T[:, j]), maximum(T[:, j])) for j = 1:nlv]
x1 = LinRange(lims[1][1], lims[1][2], npoints)
x2 = LinRange(lims[2][1], lims[2][2], npoints)
z = mpar(x1 = x1, x2 = x2)
grid = reduce(hcat, z)
m = nro(grid)
model = dmkern() 
#model = dmkern(a = .5) 
#model = dmkern(h = .3) 
fit!(model, T) 

res = predict(model, grid) ;
pred_grid = vec(res.pred)
f = Figure(size = (600, 400))
ax = Axis(f[1, 1];  title = &quot;Density for FDA scores (Iris)&quot;, xlabel = &quot;Score 1&quot;, 
    ylabel = &quot;Score 2&quot;)
co = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10, labels = true)
scatter!(ax, T[:, 1], T[:, 2], color = :red, markersize = 5)
#xlims!(ax, -15, 15) ;ylims!(ax, -15, 15)
f

## Univariate distribution
x = T[:, 1]
model = dmkern() 
#model = dmkern(a = .5) 
#model = dmkern(h = .3) 
fit!(model, x) 
pred = predict(model, x).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
scatter!(ax, x, vec(pred); color = :red)
f

x = T[:, 1]
npoints = 2^8
lims = [minimum(x), maximum(x)]
#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]
grid = LinRange(lims[1], lims[2], npoints)
model = dmkern() 
#model = dmkern(a = .5) 
#model = dmkern(h = .3) 
fit!(model, x) 
pred_grid = predict(model, grid).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
lines!(ax, grid, vec(pred_grid); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dmkern.jl#L1-L117">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dmnorm-Tuple{}"><a class="docstring-binding" href="#Jchemo.dmnorm-Tuple{}"><code>Jchemo.dmnorm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dmnorm(; kwargs...)
dmnorm(X; kwargs...)
dmnorm!(X::Matrix; kwargs...)
dmnorm(mu, S; kwargs...)
dmnorm!(mu::Vector, S::Matrix; kwargs...)</code></pre><p>Normal probability density estimation.</p><ul><li><code>X</code> : X-data (n, p) used to estimate the mean <code>mu</code> and the covariance matrix <code>S</code>. If <code>X</code> is not given,    <code>mu</code> and <code>S</code> must be provided in <code>kwargs</code>.</li><li><code>mu</code> : Mean vector of the normal distribution. </li><li><code>S</code> : Covariance matrix of the Normal distribution.</li></ul><p>Keyword arguments:</p><ul><li><code>simpl</code> : Boolean. If <code>true</code>, the constant term and the determinant in the Normal density formula    are set to 1.</li></ul><p>Data <code>X</code> can be univariate (p = 1) or multivariate (p &gt; 1). See examples.</p><p>When <code>simple</code> = <code>true</code>, the determinant of the covariance matrix (object <code>detS</code>) and the constant  (2 * pi)^(-p / 2) (object <code>cst</code>) in the density formula are set to 1. The function returns a pseudo density  that resumes to exp(-d / 2), where d is the squared Mahalanobis distance to the center <code>mu</code>. This can for instance  be useful when the number of columns (p) of <code>X</code> becomes too large, with the possible consequences that:</p><ul><li><code>detS</code> tends to 0 or, conversely, to infinity;</li><li><code>cst</code> tends to 0,</li></ul><p>which makes impossible to compute the true density. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
@names dat
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

nlv = 2
model0 = fda(; nlv)
fit!(model0, X, y)
@head T = transf(model0, X)
n, p = size(T)

#### Probability density in the FDA score space (2-D)
#### Example of class Setosa 
s = y .== &quot;setosa&quot;
zT = T[s, :]
m = nro(zT)

#### Bivariate distribution
model = dmnorm()
fit!(model, zT)
fitm = model.fitm
@names fitm
fitm.Uinv 
fitm.detS
@head pred = predict(model, zT).pred

## Direct syntax
mu = colmean(zT)
S = covm(zT, mweight(ones(m))) * m / (m - 1) # corrected cov. matrix
fitm = dmnorm(mu, S) ; 
@names fitm
fitm.Uinv
fitm.detS

npoints = 2^7
lims = [(minimum(zT[:, j]), maximum(zT[:, j])) for j = 1:nlv]
x1 = LinRange(lims[1][1], lims[1][2], npoints)
x2 = LinRange(lims[2][1], lims[2][2], npoints)
z = mpar(x1 = x1, x2 = x2)
grid = reduce(hcat, z)
model = dmnorm()
fit!(model, zT)
res = predict(model, grid) ;
pred_grid = vec(res.pred)
f = Figure(size = (600, 400))
ax = Axis(f[1, 1];  title = &quot;Density for FDA scores (Iris - Setosa)&quot;, 
    xlabel = &quot;Score 1&quot;, ylabel = &quot;Score 2&quot;)
co = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10, labels = true)
scatter!(ax, T[:, 1], T[:, 2], color = :red, markersize = 5)
scatter!(ax, zT[:, 1], zT[:, 2], color = :blue, markersize = 5)
#xlims!(ax, -12, 12) ;ylims!(ax, -12, 12)
f

#### Univariate distribution
j = 1
x = zT[:, j]
model = dmnorm()
fit!(model, x)
pred = predict(model, x).pred 
f = Figure()
ax = Axis(f[1, 1]; xlabel = string(&quot;FDA-score &quot;, j))
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
scatter!(ax, x, vec(pred); color = :red)
f

x = zT[:, j]
npoints = 2^8
lims = [minimum(x), maximum(x)]
#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]
grid = LinRange(lims[1], lims[2], npoints)
model = dmnorm()
fit!(model, x)
pred_grid = predict(model, grid).pred 
f = Figure()
ax = Axis(f[1, 1]; xlabel = string(&quot;FDA-score &quot;, j))
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
lines!(ax, grid, vec(pred_grid); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dmnorm.jl#L1-L112">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dmnormlog-Tuple{}"><a class="docstring-binding" href="#Jchemo.dmnormlog-Tuple{}"><code>Jchemo.dmnormlog</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dmnormlog(; kwargs...)
dmnormlog(X; kwargs...)
dmnormlog!(X::Matrix; kwargs...)
dmnormlog(mu, S; kwargs...)
dmnormlog!(mu::Vector, S::Matrix; kwargs...)</code></pre><p>Logarithm of the normal probability density estimation.     * <code>X</code> : X-data (n, p) used to estimate the mean <code>mu</code> and the covariance matrix <code>S</code>. If <code>X</code> is not given,          <code>mu</code> and <code>S</code> must be provided in <code>kwargs</code>.     * <code>mu</code> : Mean vector of the normal distribution.      * <code>S</code> : Covariance matrix of the Normal distribution. Keyword arguments:     * <code>simpl</code> : Boolean. If <code>true</code>, the constant term and the determinant in the Normal density formula          are set to 1.</p><p>See the help page of function <code>dmnorm</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
@names dat
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

## Example of class Setosa 
s = y .== &quot;setosa&quot;
zX = X[s, :]

model = dmnormlog()
fit!(model, zX)
fitm = model.fitm
@names fitm
fitm.Uinv 
fitm.logdetS
@head pred = predict(model, zX).pred

## Consistency with dmnorm
model0 = dmnorm()
fit!(model0, zX)
@head pred0 = predict(model0, zX).pred
@head log.(pred0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dmnormlog.jl#L1-L48">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dummy-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.dummy-Tuple{Any}"><code>Jchemo.dummy</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dummy(y)</code></pre><p>Compute dummy table from a categorical variable.</p><ul><li><code>y</code> : A categorical variable.</li></ul><p>The output <code>Y</code> (dummy table) is a BitMatrix.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

y = [&quot;d&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;b&quot;, &quot;c&quot;]
#y =  rand(1:3, 7)
res = dummy(y)
@names res
res.Y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L310-L327">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.dupl-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.dupl-Tuple{Any}"><code>Jchemo.dupl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">dupl(X; digits = 3)</code></pre><p>Find duplicated rows in a dataset.</p><ul><li><code>X</code> : A dataset.</li><li><code>digits</code> : Nb. digits used to round <code>X</code> before checking.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = rand(5, 3)
Z = vcat(X, X[1:3, :], X[1:1, :])
dupl(X)
dupl(Z)

M = hcat(X, fill(missing, 5))
Z = vcat(M, M[1:3, :])
dupl(M)
dupl(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L125-L145">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><a class="docstring-binding" href="#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><code>Jchemo.ensure_df</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ensure_df(X)</code></pre><p>Reshape <code>X</code> to a dataframe if necessary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L169-L172">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.ensure_mat-Tuple{AbstractMatrix}"><a class="docstring-binding" href="#Jchemo.ensure_mat-Tuple{AbstractMatrix}"><code>Jchemo.ensure_mat</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ensure_mat(X)</code></pre><p>Reshape <code>X</code> to a matrix if necessary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L177-L180">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.eposvd-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.eposvd-Tuple{Any}"><code>Jchemo.eposvd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">eposvd(D; nlv = 1)</code></pre><p>Compute an orthogonalization matrix for calibration transfer of spectral data.</p><ul><li><code>D</code> : Data (m, p) containing the detrimental information on which spectra (rows of a matrix X) have    to be orthogonalized.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of first loadings vectors of <code>D</code> considered for the orthogonalization.</li></ul><p>The objective is to remove some detrimental information (e.g. humidity patterns in signals, multiple spectrometers,  etc.) from a X-dataset (n, p).  The detrimental information is defined by the main row-directions computed from a  matrix <code>D</code> (m, p). </p><p>Function <code>eposvd</code> returns two objects:</p><ul><li><code>V</code> (p, <code>nlv</code>) : The matrix of the <code>nlv</code> first loading vectors of the SVD decomposition (non centered PCA)    of <code>D</code>. </li><li><code>M</code> (p, p) : The orthogonalization matrix, used to orthogonolize a given matrix X to directions contained    in <code>V</code>.</li></ul><p>Any matrix X can then be corrected from <code>D</code> by:</p><ul><li>X_corrected = X * <code>M</code>.</li></ul><p>Matrix <code>D</code> can be built from many methods. For instance, two common methods are:</p><ul><li>EPO (Roger et al. 2003, 2018): <code>D</code> is built from a set of differences between spectra collected under different    conditions. </li><li>TOP (Andrew &amp; Fearn 2004): Each row of <code>D</code> is the mean spectrum computed for a given spectrometer instrument.</li></ul><p>A particular situation is the following. Assume that <code>D</code> is built from some differences between matrices X1 and X2,  and that a bilinear model (e.g. PLSR) is fitted on the data {X1<em>corrected, Y} where X1</em>corrected = X1 * <code>M</code>.  To predict new data X2<em>new with the fitted model, there is no need to correct X2</em>new.</p><p><strong>References</strong></p><p>Andrew, A., Fearn, T., 2004. Transfer by orthogonal projection: making near-infrared calibrations robust to  between-instrument variation. Chemometrics and Intelligent Laboratory Systems 72, 51–56.  https://doi.org/10.1016/j.chemolab.2004.02.004</p><p>Roger, J.-M., Chauchard, F., Bellon-Maurel, V., 2003. EPO-PLS external parameter orthogonalisation of PLS  application to temperature-independent measurement of sugar content of intact fruits. Chemometrics and Intelligent  Laboratory Systems 66, 191-204. https://doi.org/10.1016/S0169-7439(03)00051-0</p><p>Roger, J.-M., Boulet, J.-C., 2018. A review of orthogonal projections for calibration. Journal of Chemometrics 32, e3045.  https://doi.org/10.1002/cem.3045</p><p>Zeaiter, M., Roger, J.M., Bellon-Maurel, V., 2006. Dynamic orthogonal projection. A new method to maintain the on-line  robustness of multivariate calibrations. Application to NIR-based monitoring of wine fermentations. Chemometrics and  Intelligent Laboratory Systems, 80, 227–235. https://doi.org/10.1016/j.chemolab.2005.06.011</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
@names dat
X1cal = dat.X1cal
X1val = dat.X1val
X2cal = dat.X2cal
X2val = dat.X2val

## The objective is to remove a detrimental 
## information (here, D) from spaces X1 and X2
D = X1cal - X2cal
nlv = 2
res = eposvd(D; nlv)
res.M # orthogonalization matrix
res.V # detrimental directions (columns of matrix V = loadings of D)

## Corrected Val matrices
X1val_c = X1val * res.M
X2val_c = X2val * res.M

i = 1
f = Figure(size = (800, 300))
ax1 = Axis(f[1, 1])
ax2 = Axis(f[1, 2])
lines!(ax1, X1val[i, :]; label = &quot;x1&quot;)
lines!(ax1, X2val[i, :]; label = &quot;x2&quot;)
axislegend(ax1, position = :cb, framevisible = false)
lines!(ax2, X1val_c[i, :]; label = &quot;x1_correct&quot;)
lines!(ax2, X2val_c[i, :]; label = &quot;x2_correct&quot;)
axislegend(ax2, position = :cb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/eposvd.jl#L2-L84">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.errp-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.errp-Tuple{Any, Any}"><code>Jchemo.errp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">errp(pred, y)</code></pre><p>Compute the classification error rate (ERRP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data (class membership).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

model = plsrda(; nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
errp(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L530-L550">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.euclsq-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.euclsq-Tuple{Any, Any}"><code>Jchemo.euclsq</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">euclsq(X, Y)</code></pre><p>Squared Euclidean distances between the rows of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (m, p).</li></ul><p>For <code>X</code>(n, p) and <code>Y</code> (m, p), the function returns an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)

euclsq(X, Y)

euclsq(X[1:1, :], Y[1:1, :])

euclsq(X[:, 1], 4)
euclsq(1, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/distances.jl#L1-L22">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.expand_tab2d-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.expand_tab2d-Tuple{Any}"><code>Jchemo.expand_tab2d</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">expand_tab2d(X; namr = nothing, namc = nothing, namv = nothing)</code></pre><p>Expand a 2-D contingency table in a dataframe of two categorical variables.</p><ul><li><code>X</code> : 2-D contincency table (m, p).</li></ul><p>Keyword arguments:</p><ul><li><code>namr</code> : Vector (m) of names of the <code>X</code>-rows. </li><li><code>namc</code> : Vector (p) of names of the `X-columns.</li><li><code>namv</code> : Vector (2) of the names of the output categorical variables.</li></ul><p>The eventual names in <code>namr</code> (<code>namc</code>) must have the same length and be in the same order as the rows (columns) of <code>X</code>.  </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = [5 3 4 ; 1 2 3]

expand_tab2d(X)

res = expand_tab2d(X; namr = [:B1, :AA], namc = collect(&#39;a&#39;:&#39;c&#39;))
tab(string.(res.v1, &quot;-&quot;, res.v2))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L336-L358">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fcenter-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.fcenter-Tuple{Any, Any}"><code>Jchemo.fcenter</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fcenter(X, v)
fcenter!(X::AbstractMatrix, v)</code></pre><p>Center each column of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>v</code> : Centering vector (p).</li></ul><p><strong>examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
xmeans = colmean(X)
fcenter(X, xmeans)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L1-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fconcat-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.fconcat-Tuple{Any}"><code>Jchemo.fconcat</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fconcat()</code></pre><p>Concatenate horizontaly multiblock X-data.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
n = 5 ; m = 3 ; p = 9 
X = rand(n, p) 
Xnew = rand(m, p)
listbl = [3:4, 1, [6; 8:9]]
Xbl = mblock(X, listbl) 
Xblnew = mblock(Xnew, listbl) 
@head Xbl[3]

fconcat(Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_mb.jl#L232-L250">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fcscale-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#Jchemo.fcscale-Tuple{Any, Any, Any}"><code>Jchemo.fcscale</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fcscale(X, u, v)
fcscale!(X, u, v)</code></pre><p>Center and scale each column of a matrix.</p><ul><li><code>X</code> : Data  (n, p).</li><li><code>u</code> : Centering vector (p).</li><li><code>v</code> : Scaling vector (p).</li></ul><p><strong>examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
xmeans = colmean(X)
xscales = colstd(X)
fcscale(X, xmeans, xscales)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L69-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fda-Tuple{}"><a class="docstring-binding" href="#Jchemo.fda-Tuple{}"><code>Jchemo.fda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fda(; kwargs...)
fda(X, y; kwargs...)
fda(X, y, weights; kwargs...)
fda!(X::Matrix, y, weights; kwargs...)</code></pre><p>Factorial discriminant analysis (FDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : y-data (n) (class membership).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of discriminant components.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;. Can be used when <code>X</code> has collinearities. </li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>FDA by eigen factorization of Inverse(W) * B, where W is the &quot;Within&quot;-covariance matrix (pooled over the classes),  and B the &quot;Between&quot;-covariance matrix.</p><p>The function maximizes the consensus:</p><ul><li>p&#39;Bp / p&#39;Wp </li></ul><p>i.e. max p&#39;Bp with constraint p&#39;Wp = 1. Vectors p (columns of <code>V</code>) are the linear discrimant coefficients  often referred to as &quot;LD&quot;.</p><p>If <code>X</code> is ill-conditionned, a ridge regularization can be used:</p><ul><li>If <code>lb</code> &gt; 0, W is replaced by W + <code>lb</code> * I, where I is the Idendity matrix.</li></ul><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
tab(ytrain)
tab(ytest)

nlv = 2
model = fda(; nlv)
#model = fdasvd(; nlv)
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm
fitm = model.fitm ;

lev = fitm.lev
nlev = length(lev)
fitm.priors
aggsumv(fitm.weights.w, ytrain)

@head fitm.T 
@head transf(model, Xtrain)
@head transf(model, Xtest)

## X-loadings matrix
## = coefficients of the linear discriminant function
## = &quot;LD&quot; of function lda of the R package MASS
fitm.V
fitm.V&#39; * fitm.V

## Explained variance computed by weighted PCA of the class centers 
## in transformed scale
summary(model).explvarx

## Projections of the class centers to the score space
ct = fitm.Tcenters 
f, ax = plotxy(fitm.T[:, 1], fitm.T[:, 2], ytrain; ellipse = true, title = &quot;FDA&quot;,
    xlabel = &quot;Score-1&quot;, ylabel = &quot;Score-2&quot;)
scatter!(ax, ct[:, 1], ct[:, 2], marker = :star5, markersize = 15, color = :red)  # see available_marker_symbols()
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/fda.jl#L1-L92">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fdasvd-Tuple{}"><a class="docstring-binding" href="#Jchemo.fdasvd-Tuple{}"><code>Jchemo.fdasvd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fdasvd(; kwargs...)
fdasvd(X, y, weights; kwargs...)
fdasvd!(X::Matrix, y, weights; kwargs...)</code></pre><p>Factorial discriminant analysis (FDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : y-data (n) (class membership).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of discriminant components.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;. Can be used when <code>X</code> has collinearities. </li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>FDA by a weighted SVD factorization of the matrix of the class centers (after spherical transformaton). The function  gives the same results as function <code>fda</code>.</p><p>See function <code>fda</code> for details and examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/fdasvd.jl#L1-L22">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fdif-Tuple{}"><a class="docstring-binding" href="#Jchemo.fdif-Tuple{}"><code>Jchemo.fdif</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fdif(; kwargs...)
fdif(X; kwargs...)</code></pre><p>Finite differences (discrete derivates) for each row of X-data. </p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Nb. points involved in the window for the finite differences. The range of the window (= nb. intervals </li></ul><p>of two successive colums) is npoint - 1.</p><p>The method reduces the column-dimension: </p><ul><li>(n, p) –&gt; (n, p - npoint + 1). </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = fdif(npoint = 2) 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L169-L204">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.findmax_cla-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.findmax_cla-Tuple{Any}"><code>Jchemo.findmax_cla</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">findmax_cla(x)
findmax_cla(x, weights::Weight)</code></pre><p>Find the most occurent level in <code>x</code>.</p><ul><li><code>x</code> : A categorical variable.</li><li><code>weights</code> : Weights (n) of the observations. Object of type <code>Weight</code> (e.g. generated by function <code>mweight</code>).</li></ul><p>If ex-aequos, the function returns the first.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = rand(1:3, 10)
tab(x)
findmax_cla(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L190-L207">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.findmiss-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.findmiss-Tuple{Any}"><code>Jchemo.findmiss</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">findmiss(X)</code></pre><p>Find rows with missing data in a dataset.</p><ul><li><code>X</code> : A dataset.</li></ul><p>For dataframes, see also <code>DataFrames.completecases</code> and <code>DataFrames.dropmissing</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = rand(5, 4)
zX = hcat(rand(2, 3), fill(missing, 2))
Z = vcat(X, zX)
findmiss(X)
findmiss(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L219-L236">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.finduniq-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.finduniq-Tuple{Any}"><code>Jchemo.finduniq</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">finduniq(id)</code></pre><p>Find the indexes making unique the IDs in a ID vector.</p><ul><li><code>id</code> : A vector of IDs.</li></ul><p>Can be used to remove duplicated rows in a dataset, identified by a single ID variable.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

v = [&quot;a&quot;, &quot;d&quot;, &quot;c&quot;, &quot;b&quot;, &quot;a&quot;, &quot;d&quot;, &quot;a&quot;]  # a vector of IDs

s = finduniq(v)  # indexes of the IDs without duplicates
v[s]  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L244-L260">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.frob-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.frob-Tuple{Any}"><code>Jchemo.frob</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">frob(X)
frob(X, weights::Weight)
frob2(X)
frob2(X, weights::Weight)</code></pre><p>Frobenius norm of a matrix.</p><ul><li><code>X</code> : A matrix (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Object of type <code>Weight</code> (e.g. generated by function <code>mweight</code>).</li></ul><p>The Frobenius norm of <code>X</code> is:</p><ul><li>sqrt(tr(X&#39; * X)).</li></ul><p>The Frobenius weighted norm is:</p><ul><li>sqrt(tr(X&#39; * D * X)), where D is the diagonal matrix of vector <code>w</code>.</li></ul><p>Functions <code>frob2</code> are the squared versions of <code>frob</code>.</p><p><strong>References</strong></p><p>@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L427-L448">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.fscale-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.fscale-Tuple{Any, Any}"><code>Jchemo.fscale</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fscale(X, v)
fscale!(X::AbstractMatrix, v)</code></pre><p>Scale each column of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>v</code> : Scaling vector (p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = rand(5, 2) 
fscale(X, colstd(X))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L36-L50">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.getknn-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.getknn-Tuple{Any, Any}"><code>Jchemo.getknn</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">getknn(Xtrain, X; metric = :eucl, k = 1)</code></pre><p>Return the k nearest neighbors in <code>Xtrain</code> of each row of the query <code>X</code>.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>X</code> : Query X-data.</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of distance used for the query. Possible values are <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>k</code> : Number of neighbors to return.</li></ul><p>In addition to neighbors, the function also returns the distances. </p><p>The angular distances used in this function are scaled to [0, 1]. Consider two vectors x and y, theta the (unknown)  angle between x and y in radiants, and costheta = cosinus(x, y). Then, costheta is computed as:</p><ul><li>costheta = x&#39;y / (||x|| ||y||) </li></ul><p>and distances as (see file distances.jl):</p><ul><li>Spectral angular distance (x, y) = acos(costheta) / pi    </li><li>Cosine distance (x, y) = (1 - costheta) / 2                     </li><li>Correlation distance (x, y) = (1 - corr(x, y)) / 2  </li></ul><p>Note: If x and y are centered, costheta = corr(x, y).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
Xtrain = rand(5, 3)
X = rand(2, 3)
x = X[1:1, :]

k = 3
res = getknn(Xtrain, X; k)
res.ind  # indexes
res.d    # distances

res = getknn(Xtrain, x; k)
res.ind

res = getknn(Xtrain, X; metric = :mah, k)
res.ind</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/getknn.jl#L1-L42">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridcv-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#Jchemo.gridcv-Tuple{Any, Any, Any}"><code>Jchemo.gridcv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridcv(model, X, Y; segm, score, pars = nothing, nlv = nothing, lb = nothing, verbose = false)</code></pre><p>Cross-validation (CV) of a model over a grid of parameters.</p><ul><li><code>model</code> : Model to evaluate.</li><li><code>X</code> : Training X-data (n, p).</li><li><code>Y</code> : Training Y-data (n, q).</li></ul><p>Keyword arguments: </p><ul><li><code>segm</code> : Segments of observations used for the CV (output of functions <a href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>segmts</code></a>, <a href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>segmkf</code></a>, etc.).</li><li><code>score</code> : Function computing the prediction score (e.g. <code>rmsep</code>).</li><li><code>pars</code> : tuple of named vectors of same length defining the parameter combinations (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If <code>true</code>, predicting information are printed.</li><li><code>nlv</code> : Value, or vector of values, of the nb. of latent variables (LVs).</li><li><code>lb</code> : Value, or vector of values, of the ridge regularization parameter &quot;lambda&quot;.</li></ul><p>The function is used for grid-search: it computes a prediction score (= error rate) for the specified <code>model</code>  for each parameter combination defined in <code>pars</code>.</p><p>For models based on LV or ridge regularization, using arguments <code>nlv</code> and <code>lb</code> allow faster computations than including  these parameters in argument `pars. See the examples.   </p><p>The function returns two outputs: </p><ul><li><code>res</code> : mean results</li><li><code>res_p</code> : results per replication.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">####### Regression

using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
model = savgol(npoint = 21, deriv = 2, degree = 2)
fit!(model, X)
Xp = transf(model, X)
s = year .&lt;= 2012
Xtrain = Xp[s, :]
ytrain = y[s]
Xtest = rmrow(Xp, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## a) Replicated K-fold CV 
K = 3 ; rep = 10
segm = segmkf(ntrain, K; rep)
## b) Replicated test-set validation
#m = round(Int, ntrain / 3) ; rep = 30
#segm = segmts(ntrain, m; rep)

####---- Plsr
model = plskern()
nlv = 0:30
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, nlv) ;
@names rescv
res = rescv.res 
plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP-CV&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = plskern(; nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## Example of plot showing replications
res_rep = rescv.res_rep
f, ax = plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP-CV&quot;)
for i = 1:rep, j = 1:K
    zres = res_rep[res_rep.rep .== i .&amp;&amp; res_rep.segm .== j, :]
    lines!(ax, zres.nlv, zres.y1; color = (:grey, .2))
end
lines!(ax, res.nlv, res.y1; color = :red, linewidth = 1)
f

## Adding pars 
pars = mpar(scal = [false; true])
rescv = gridcv(model, Xtrain, ytrain; segm,  score = rmsep, pars, nlv) ;
res = rescv.res 
typ = res.scal
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP-CV&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = plskern(nlv = res.nlv[u], scal = res.scal[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####---- Rr 
lb = (10).^(-8:.1:3)
model = rr() 
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, lb) ;
res = rescv.res 
loglb = log.(10, res.lb)
plotgrid(loglb, res.y1; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP-CV&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = rr(lb = res.lb[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f     
    
## Adding pars 
pars = mpar(scal = [false; true])
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, lb) ;
res = rescv.res 
loglb = log.(10, res.lb)
typ = string.(res.scal)
plotgrid(loglb, res.y1, typ; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP-CV&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = rr(lb = res.lb[u], scal = res.scal[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####---- Kplsr 
model = kplsr()
nlv = 0:30
gamma = (10).^(-5:1.:5)
pars = mpar(gamma = gamma)
rescv = gridcv(model, Xtrain, ytrain; segm,  score = rmsep, pars, nlv) ;
res = rescv.res 
loggamma = round.(log.(10, res.gamma), digits = 1)
plotgrid(res.nlv, res.y1, loggamma; step = 2, xlabel = &quot;Nb. LVs&quot;,  ylabel = &quot;RMSEP-CV&quot;, 
    leg_title = &quot;Log(gamma)&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = kplsr(nlv = res.nlv[u], gamma = res.gamma[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####---- Knnr 
nlvdis = [0; 15; 25] ; metric = [:sam]
h = [1, 2.5, 5]
k = [1; 5; 10; 20; 50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
model = knnr()
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, verbose = true) ;
res = rescv.res 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = knnr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], k = res.k[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####---- Lwplsr 
nlvdis = 15 ; metric = [:mah]
h = [1, 2, 5] ; k = [200, 350, 500] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
nlv = 0:20
model = lwplsr()
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, nlv, verbose = true) ;
res = rescv.res 
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP-CV&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], k = res.k[u], 
    nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####---- LwplsrAvg 
nlvdis = 15 ; metric = [:mah]
h = [1, 2, 5] ; k = [200, 350, 500] 
nlv = [0:20, 5:20] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k, nlv = nlv)
length(pars[1]) 
model = lwplsravg()
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, verbose = true) ;
res = rescv.res 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = lwplsravg(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], k = res.k[u], 
  nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   
    
####------ Mbplsr
listbl = [1:525, 526:1050]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 
Xblcal = mblock(Xcal, listbl) 
Xblval = mblock(Xval, listbl) 

model = mbplsr()
bscal = [:none, :frob]
pars = mpar(bscal = bscal) 
nlv = 0:30
rescv = gridcv(model, Xbltrain, ytrain; segm,  score = rmsep, pars, nlv) ;
res = rescv.res 
group = res.bscal 
plotgrid(res.nlv, res.y1, group; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP-CV&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = mbplsr(bscal = res.bscal[u], nlv = res.nlv[u])
fit!(model, Xbltrain, ytrain)
pred = predict(model, Xbltest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####### Discrimination
## The principle is the same as for regression

using JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## a) Replicated K-fold CV 
K = 3 ; rep = 10
segm = segmkf(ntrain, K; rep)
## b) Replicated test-set validation
#m = round(Int, ntrain / 3) ; rep = 30
#segm = segmts(ntrain, m; rep)

####---- Plslda
model = plslda()
nlv = 1:30
pars = mpar(scal = [false; true])
rescv = gridcv(model, Xtrain, ytrain; segm, score = errp, pars, nlv)
res = rescv.res
typ = res.scal
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;ERR&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = plslda(nlv = res.nlv[u], scal = res.scal[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show errp(pred, ytest)
conf(pred, ytest).pct

## Computation of the confusion matrix within CV (average over the replications), for the best model 
matpred = Vector{Matrix{String}}(undef, rep * K)
k = 1
for i = 1:rep
    listsegm = segm[i]
    for j = 1:K
        s = listsegm[j]
        model = plslda(nlv = res.nlv[u], scal = res.scal[u])
        fit!(model, rmrow(Xtrain, s), rmrow(ytrain, s))
        pred = predict(model, Xtrain[s, :]).pred
        matpred[k] = hcat(pred, ytrain[s])
        k = k + 1
    end
end
respred = reduce(vcat, matpred)
conf(respred[:, 1], respred[:, 2]).pct</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridcv.jl#L1-L293">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridcv_br-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.gridcv_br-Tuple{Any, Any}"><code>Jchemo.gridcv_br</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridcv_br(X, Y; segm, algo, score, pars, verbose = false)</code></pre><p>Working function for <code>gridcv</code>.</p><p>See function <code>gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridcv_br.jl#L1-L6">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridcv_lb-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.gridcv_lb-Tuple{Any, Any}"><code>Jchemo.gridcv_lb</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridcv_lb(X, Y; segm, algo, score, pars = nothing, lb, verbose = false)</code></pre><p>Working function for <code>gridcv</code>.</p><p>Specific and faster than <code>gridcv_br</code> for models using ridge regularization (e.g. RR). Argument <code>pars</code> must  not contain <code>nlv</code>.</p><p>See function <code>gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridcv_lb.jl#L1-L9">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridcv_lv-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.gridcv_lv-Tuple{Any, Any}"><code>Jchemo.gridcv_lv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridcv_lv((X, Y; segm, algo, score, pars = nothing, nlv, verbose = false)</code></pre><p>Working function for <code>gridcv</code>.</p><p>Specific and faster than <code>gridcv_br</code> for models using latent variables (e.g. PLSR). Argument <code>pars</code> must  not contain <code>nlv</code>.</p><p>See function <code>gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridcv_lv.jl#L1-L9">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridscore-NTuple{5, Any}"><a class="docstring-binding" href="#Jchemo.gridscore-NTuple{5, Any}"><code>Jchemo.gridscore</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridscore(model, Xtrain, Ytrain, X, Y; score, pars = nothing, nlv = nothing, 
    lb = nothing, verbose = false)</code></pre><p>Test-set validation of a model over a grid of parameters.</p><ul><li><code>model</code> : Model to evaluate.</li><li><code>Xtrain</code> : Training X-data (n, p).</li><li><code>Ytrain</code> : Training Y-data (n, q).</li><li><code>X</code> : Validation X-data (m, p).</li><li><code>Y</code> : Validation Y-data (m, q).</li></ul><p>Keyword arguments: </p><ul><li><code>score</code> : Function computing the prediction score (e.g. <code>rmsep</code>).</li><li><code>pars</code> : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If <code>true</code>, predicting information are printed.</li><li><code>nlv</code> : Value, or vector of values, of the nb. of latent variables (LVs).</li><li><code>lb</code> : Value, or vector of values, of the ridge regularization    parameter &quot;lambda&quot;.</li></ul><p>The function is used for grid-search: it computes a prediction score (= error rate) for the specified <code>model</code>  for each parameter combination defined in <code>pars</code>. The score is computed over sets {<code>X,</code>Y`}. </p><p>For models based on LV or ridge regularization, using arguments <code>nlv</code> and <code>lb</code> allow faster computations than including  these parameters in argument `pars. See the examples.   </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">####### Regression 

using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
model = savgol(npoint = 21, deriv = 2, degree = 2)
fit!(model, X)
Xp = transf(model, X)
s = year .&lt;= 2012
Xtrain = Xp[s, :]
ytrain = y[s]
Xtest = rmrow(Xp, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
## Train ==&gt; Cal + Val 
nval = round(Int, .3 * ntrain)
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]

####---- Plsr
model = plskern()
nlv = 0:30
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, nlv)
plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = plskern(nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## Adding pars 
pars = mpar(scal = [false; true])
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv)
typ = res.scal
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = plskern(nlv = res.nlv[u], scal = res.scal[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####---- Rr 
lb = (10).^(-8:.1:3)
model = rr() 
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, lb)
loglb = log.(10, res.lb)
plotgrid(loglb, res.y1; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = rr(lb = res.lb[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    
    
## Adding pars 
pars = mpar(scal = [false; true])
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, lb)
loglb = log.(10, res.lb)
typ = string.(res.scal)
plotgrid(loglb, res.y1, typ; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = rr(lb = res.lb[u], scal = res.scal[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####---- Kplsr 
model = kplsr()
nlv = 0:30
gamma = (10).^(-5:1.:5)
pars = mpar(gamma = gamma)
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv)
loggamma = round.(log.(10, res.gamma), digits = 1)
plotgrid(res.nlv, res.y1, loggamma; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;,
    leg_title = &quot;Log(gamma)&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = kplsr(nlv = res.nlv[u], gamma = res.gamma[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####---- Knnr 
nlvdis = [0; 15; 25] ; metric = [:cos]
h = [1, 2.5, 5]
k = [1, 5, 10, 20, 50, 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
model = knnr()
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = knnr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], k = res.k[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####---- Lwplsr 
nlvdis = 15 ; metric = [:mah]
h = [1, 2, 5] ; k = [200, 350, 500] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
nlv = 0:20
model = lwplsr()
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv, verbose = true)
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u], nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####---- LwplsrAvg 
nlvdis = 15 ; metric = [:mah]
h = [1, 2, 5] ; k = [200, 350, 500] 
nlv = [0:20, 5:20] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k, nlv = nlv)
length(pars[1]) 
model = lwplsravg()
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = lwplsravg(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u], nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####---- Mbplsr
listbl = [1:525, 526:1050]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 
Xblcal = mblock(Xcal, listbl) 
Xblval = mblock(Xval, listbl) 

model = mbplsr()
bscal = [:none, :frob]
pars = mpar(bscal = bscal) 
nlv = 0:30
res = gridscore(model, Xblcal, ycal, Xblval, yval; score = rmsep, pars, nlv)
group = res.bscal 
plotgrid(res.nlv, res.y1, group; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = mbplsr(bscal = res.bscal[u], nlv = res.nlv[u])
fit!(model, Xbltrain, ytrain)
pred = predict(model, Xbltest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    
    
####### Discrimination
## The principle is the same as for regression

using JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
## Train ==&gt; Cal + Val 
nval = round(Int, .3 * ntrain)
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]

####---- Plslda
model = plslda()
nlv = 1:30
prior = [:unif]
scal = [false; true]
pars = mpar(prior = prior, scal = scal)
res = gridscore(model, Xcal, ycal, Xval, yval; score = errp, pars, nlv)
typ = string.(res.prior, &quot;-&quot;, res.scal)
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model = plslda(nlv = res.nlv[u], prior = res.prior[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
@show errp(pred, ytest)
conf(pred, ytest).pct</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridscore.jl#L1-L257">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridscore-Tuple{Jchemo.Pipeline, Vararg{Any, 4}}"><a class="docstring-binding" href="#Jchemo.gridscore-Tuple{Jchemo.Pipeline, Vararg{Any, 4}}"><code>Jchemo.gridscore</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridscore(model::Pipeline, Xtrain, Ytrain, X, Y; score, pars = nothing, nlv = nothing, lb = nothing, 
    verbose = false)</code></pre><p>Test-set validation of a model pipeline over a grid of parameters.</p><ul><li><code>model</code> : A pipeline of models to evaluate.</li><li><code>Xtrain</code> : Training X-data (n, p).</li><li><code>Ytrain</code> : Training Y-data (n, q).</li><li><code>X</code> : Validation X-data (m, p).</li><li><code>Y</code> : Validation Y-data (m, q).</li></ul><p>Keyword arguments: </p><ul><li><code>score</code> : Function computing the prediction score (e.g. <code>rmsep</code>).</li><li><code>pars</code> : tuple of named vectors of same length defining the parameter combinations (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If <code>true</code>, predicting information are printed.</li><li><code>nlv</code> : Value, or vector of values, of the nb. of latent variables (LVs).</li><li><code>lb</code> : Value, or vector of values, of the ridge regularization parameter &quot;lambda&quot;.</li></ul><p>In the present version of the function, only the last model of the pipeline (= the final predictor) is validated.</p><p>For other details, see function <code>gridscore</code> for simple models. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
## Building Cal and Val 
## within Train
nval = round(Int, .3 * ntrain)
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]

####-- Pipeline Snv :&gt; Savgol :&gt; Plsr
## Only the last model is validated
## model1
model1 = snv()
## model2 
npoint = 11 ; deriv = 2 ; degree = 3
model2 = savgol(; npoint, deriv, degree)
## model3
nlv = 0:30
model3 = plskern()
##
model = pip(model1, model2, model3)
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, nlv) ;
plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model3 = plskern(nlv = res.nlv[u])
model = pip(model1, model2, model3)
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ; 
@head res.pred 
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,
      ylabel = &quot;Observed&quot;).f

####-- Pipeline Pca :&gt; Svmr
## Only the last model is validated
## model1
nlv = 15 ; scal = true
model1 = pcasvd(; nlv, scal)
## model2
kern = [:krbf]
gamma = (10).^(-5:1.:5)
cost = (10).^(1:3)
epsilon = [.1, .2, .5]
pars = mpar(kern = kern, gamma = gamma, cost = cost, epsilon = epsilon)
model2 = svmr()
##
model = pip(model1, model2)
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
model2 = svmr(kern = res.kern[u], gamma = res.gamma[u], cost = res.cost[u], epsilon = res.epsilon[u])
model = pip(model1, model2) 
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ; 
@head res.pred 
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,
      ylabel = &quot;Observed&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridscore_pip.jl#L1-L101">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridscore_br-NTuple{4, Any}"><a class="docstring-binding" href="#Jchemo.gridscore_br-NTuple{4, Any}"><code>Jchemo.gridscore_br</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridscore_br(Xtrain, Ytrain, X, Y; algo, score, pars, verbose = false)</code></pre><p>Working function for <code>gridscore</code>.</p><p>See function <code>gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridscore_br.jl#L1-L6">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridscore_lb-NTuple{4, Any}"><a class="docstring-binding" href="#Jchemo.gridscore_lb-NTuple{4, Any}"><code>Jchemo.gridscore_lb</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridscore_lb(Xtrain, Ytrain, X, Y; algo, score, pars = nothing, lb, verbose = false)</code></pre><p>Working function for <code>gridscore</code>.</p><p>Specific and faster than <code>gridscore_br</code> for models using ridge regularization (e.g. RR). Argument <code>pars</code> must  not contain <code>lb</code>.</p><p>See function <code>gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridscore_lb.jl#L1-L9">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.gridscore_lv-NTuple{4, Any}"><a class="docstring-binding" href="#Jchemo.gridscore_lv-NTuple{4, Any}"><code>Jchemo.gridscore_lv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gridscore_lv(Xtrain, Ytrain, X, Y; algo, score, pars = nothing, nlv, verbose = false)</code></pre><p>Working function for <code>gridscore</code>.</p><p>Specific and faster than <code>gridscore_br</code> for models using latent variables (e.g. PLSR). Argument <code>pars</code> must  not contain <code>nlv</code>.</p><p>See function <code>gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/gridscore_lv.jl#L1-L9">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.head-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.head-Tuple{Any}"><code>Jchemo.head</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">@head X</code></pre><p>Display the first rows of a dataset.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = rand(100, 5)
@head X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L273-L284">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.interpl-Tuple{}"><a class="docstring-binding" href="#Jchemo.interpl-Tuple{}"><code>Jchemo.interpl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">interpl(; kwargs...)
interpl(X; kwargs...)</code></pre><p>Sampling spectra by interpolation.</p><ul><li><code>X</code> : Matrix (n, p) of spectra (rows).</li></ul><p>Keyword arguments:</p><ul><li><code>wl</code> : Values representing the column &quot;names&quot; of <code>X</code>. Must be a numeric vector of length p, or an AbstractRange,    with increasing values.</li><li><code>wlfin</code> : Final values (within the range of <code>wl</code>) where to interpolate each spectrum. Must be a numeric vector,    or an AbstractRange, with increasing values.</li></ul><p>The function implements a cubic spline interpolation using package DataInterpolations.jl.</p><p><strong>References</strong></p><p>http://github.com/SciML/DataInterpolations.jl</p><p>Bhagavan et al. 2024, https://doi.org/10.21105/joss.06917</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

wlfin = range(500, 2400, length = 10)
#wlfin = collect(range(500, 2400, length = 10))
model = interpl(; wl, wlfin)
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L239-L283">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.iqrv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.iqrv-Tuple{Any}"><code>Jchemo.iqrv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">iqrv(x)</code></pre><p>Compute the interquartile interval (IQR) of a vector.</p><ul><li><code>x</code> : A vector (n).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(100)
iqrv(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L109-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.isel!"><a class="docstring-binding" href="#Jchemo.isel!"><code>Jchemo.isel!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">isel!(model, X, Y, wl = 1:nco(X); score = rmsep, psamp = .3, nint = 5, rep = 1)</code></pre><p>Interval variable selection.</p><ul><li><code>model</code> : Model to evaluate.</li><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>wl</code> : Optional numeric labels (p) of the X-columns.</li></ul><p>Keyword arguments:  </p><ul><li><code>score</code> : Function computing the prediction score (an error rate).</li><li><code>psamp</code> : Proportion of data used as validation set to compute the <code>score</code>.</li><li><code>nint</code> : Nb. intervals. </li><li><code>rep</code> : Number of replications of the splitting calibration/validation (see below). </li></ul><p>The principle is as follows:</p><ul><li>Data (X, Y) are splitted randomly to a calibration set (Xcal, Ycal) and a validation set (Xval, Yval).</li><li>The range 1:p in <code>X</code> is segmented to <code>nint</code> intervals of equal size (when possible). </li><li>The model is fitted on the calibration set and used to compute the predictions from Xval, firtsly accounting    for all the p variables (reference) and secondly for each (separately) of the <code>nint</code> intervals. The error    rates are computed by comparing the predictions to Yval. The interval-variable importance is the difference between    the reference error rate and the error rate computed for each interval.</li></ul><p>The overall process above is replicated <code>rep</code> times. The outputs provided by the function are the average results  (i.e. over the <code>rep</code> replications;<code>imp</code>) and the results per replication (<code>res_rep</code>).</p><p>Note: the function is inplace (modifies object <code>model</code>).</p><p><strong>References</strong></p><ul><li>Nørgaard, L., Saudland, A., Wagner, J., Nielsen, J.V., Munck, L., Engelsen, S.B., 2000. Interval Partial </li></ul><p>Least-Squares Regression (iPLS): A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419. https://doi.org/10.1366/0003702001949500</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, DataFrames, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;tecator.jld2&quot;) 
@load db dat
@names dat
X = dat.X
Y = dat.Y 
wl_str = names(X)
wl = parse.(Float64, wl_str) 
ntot, p = size(X)
typ = Y.typ
namy = names(Y)[1:3]
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

s = typ .== &quot;train&quot;
Xtrain = X[s, :]
Ytrain = Y[s, namy]
Xtest = rmrow(X, s)
Ytest = rmrow(Y[:, namy], s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Work on the j-th y-variable 
j = 2
nam = namy[j]
ytrain = Ytrain[:, nam]
ytest = Ytest[:, nam]

model = plskern(nlv = 5)
nint = 10
res = isel!(model, Xtrain, ytrain, wl; nint, rep = 50) ;
dat = res.dat
res.imp 
res.res_rep

imp = res.imp[:, 1]
imp[imp .&lt; 0] .= 0  # option: negative values are set to 0
lo = round.(dat.lo)
f = Figure(size = (650, 300))
ax = Axis(f[1, 1], xlabel = &quot;Wawelength (nm)&quot;, ylabel = &quot;Importance&quot;, xticks = lo)
scatter!(ax, dat.avg, imp; color = (:red, .5))
vlines!(ax, lo; color = :grey, linestyle = :dash, linewidth = 1)
hlines!(ax, [0]; color = :grey)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/isel.jl#L1-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kdeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.kdeda-Tuple{}"><code>Jchemo.kdeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kdeda(; kwargs...)
kdeda(X, y; kwargs...)
kdeda(X, y, weights::Weight; kwargs...)</code></pre><p>Discriminant analysis using non-parametric kernel Gaussian density estimation (KDE-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth definition) can also be specified here.</li></ul><p>Same as function <code>qda</code> except that class densities are estimated from function <code>dmkern</code> instead of function <code>dmnorm</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

prior = :unif
#prior = :prop
model = kdeda(; prior)
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 

fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

model = kdeda(; prior, a = .5) 
#model = kdeda(; prior, h = .1) 
fit!(model, Xtrain, ytrain)
model.fitm.fitm[1].H</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kdeda.jl#L1-L64">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.knnda-Tuple{}"><a class="docstring-binding" href="#Jchemo.knnda-Tuple{}"><code>Jchemo.knnda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">knnda(; kwargs...)
knnda(X, y; kwargs...)</code></pre><p>k-Nearest-Neighbours weighted discrimination (kNN-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights (see function <code>getknn</code>).    Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis), <code>:sam</code> (spectral angular distance),    <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h, sharper is the function.    See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of the global <code>X</code> is scaled by its uncorrected standard deviation before the distance     and weight computations.</li></ul><p>This function has the same principle as function <code>knnr</code> except that a discrimination replaces the regression. A weighted vote  is done over the neighborhood, and the prediction corresponds to the most frequent class.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

metric = :eucl ; h = 2 ; k = 10
model = knnda(; metric, h, k) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm

fitm.lev
fitm.ni

res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show errp(res.pred, ytest)
conf(res.pred, ytest).cnt

## With dimension reduction and function &#39;pip&#39;
nlv = 15
metric = :cos ; h = 1 ; k = 3 
model1 = pcasvd(; nlv)
model2 = knnda(; metric, h, k) 
model = pip(model1, model2)
fit!(model, Xtrain, ytrain)
@head pred = predict(model, Xtest).pred 
errp(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/knnda.jl#L1-L72">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.knnr-Tuple{}"><a class="docstring-binding" href="#Jchemo.knnr-Tuple{}"><code>Jchemo.knnr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">knnr(; kwargs...)
knnr(X, Y; kwargs...)</code></pre><p>k-Nearest-Neighbours weighted regression (KNNR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of the global <code>X</code> is scaled by its uncorrected standard    deviation before the distance and weight computations.</li></ul><p>The general principle of this function is as follows (many other variants of kNNR pipelines can be built): a) For each new observation to predict, the prediction is the weighted mean of <code>y</code> over a selected neighborhood     (in <code>X</code>) of size <code>k</code>.  b) Within the selected neighborhood, the weights  are defined from the dissimilarities between the new observation      and the neighborhood, and are computed from function &#39;winvs&#39;.</p><p>In general, for X-data with high dimensions, using the Mahalanobis distance requires a preliminary dimensionality  reduction (see examples).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

h = 1 ; k = 3 
model = knnr(; h, k) 
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm
dump(model.fitm.par)
res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## With preliminary dimension reduction
model1 = pcasvd(nlv = 15)
metric = :eucl ; h = 1 ; k = 3 
model2 = knnr(; metric, h, k) 
model = pip(model1, model2)
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ; 
@head res.pred
@show rmsep(res.pred, ytest)

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
model = knnr(k = 15, h = 5) 
fit!(model, x, y)
pred = predict(model, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/knnr.jl#L1-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kpca-Tuple{}"><a class="docstring-binding" href="#Jchemo.kpca-Tuple{}"><code>Jchemo.kpca</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kpca(; kwargs...)
kpca(X; kwargs...)
kpca(X, weights::Weight; kwargs...)</code></pre><p>Kernel PCA  (Scholkopf et al. 1997, Scholkopf &amp; Smola 2002, Tipping 2001).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. principal components (PCs) to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective functions <code>krbf</code>    and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The method is implemented by SVD factorization of the weighted Gram matrix: </p><ul><li>D^(1/2) * Phi(X) * Phi(X)&#39; * D^(1/2)</li></ul><p>where X is the cenetred matrix and D is a diagonal matrix of weights (<code>weights.w</code>) of the observations (rows of X).</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A., MÃ¼ller, K.-R., 1997. Kernel principal component analysis, in: Gerstner, W., Germond, A., Hasler,  M., Nicoud, J.-D. (Eds.), Artificial Neural Networks, ICANN 97, Lecture Notes in Computer Science. Springer, Berlin,  Heidelberg, pp. 583-588. https://doi.org/10.1007/BFb0020217</p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.</p><p>Tipping, M.E., 2001. Sparse kernel principal component analysis. Advances in neural information processing systems, MIT Press.  http://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
Xtest = X[s.test, :]

nlv = 3
kern = :krbf ; gamma = 1e-4
model = kpca(; nlv, kern, gamma) 
fit!(model, Xtrain)
fitm = model.fitm ;
@names fitm

@head transf(model, Xtrain)
@head fitm.T

@head transf(model, Xtest) 

res = summary(model) ;
@names res
res.explvarx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kpca.jl#L1-L60">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kplskdeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.kplskdeda-Tuple{}"><code>Jchemo.kplskdeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kplskdeda(; kwargs...)
kplskdeda(X, y; kwargs...)
kplskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>KPLS-KDEDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Eventual keyword arguments of function <code>dmkern</code> (bandwidth definition).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>Same as function <code>plskdeda</code> (PLS-KDEDA) except that a kernel PLSR (function <code>kplsr</code>), instead of a  PLSR (function <code>plskern</code>),  is run on the Y-dummy table. </p><p>See function <code>kplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplskdeda.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kplslda-Tuple{}"><a class="docstring-binding" href="#Jchemo.kplslda-Tuple{}"><code>Jchemo.kplslda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kplslda(; kwargs...)
kplslda(X, y; kwargs...)
kplslda(X, y, weights::Weight; kwargs...)</code></pre><p>KPLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>Same as function <code>plslda</code> (PLS-LDA) except that a kernel PLSR (function <code>kplsr</code>), instead of a PLSR (function <code>plskern</code>),  is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
gamma = .1
model = kplslda(; nlv, gamma) 
#model = kplslda(; nlv, gamma, prior = :unif) 
#model = kplsqda(; nlv, gamma, alpha = .5) 
#model = kplskdeda(; nlv, gamma, a = .5) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm

fitm.lev
fitm.ni
fitm.priors

fitm_emb = fitm.fitm_emb ;
typeof(fitm_emb)
@names fitm_emb 
@head transf(model, Xtrain)
@head fitm_emb.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

fitm_da = fitm.fitm_da ;
typeof(fitm_da)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplslda.jl#L1-L80">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kplsqda-Tuple{}"><a class="docstring-binding" href="#Jchemo.kplsqda-Tuple{}"><code>Jchemo.kplsqda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kplsqda(; kwargs...)
kplsqda(X, y; kwargs...)
kplsqda(X, y, weights::Weight; kwargs...)</code></pre><p>KPLS-QDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>Same as function <code>plsqda</code> (PLS-QDA) except that a kernel PLSR (function <code>kplsr</code>), instead of a PLSR (function <code>plskern</code>),  is run on the Y-dummy table. </p><p>See function <code>kplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplsqda.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kplsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.kplsr-Tuple{}"><code>Jchemo.kplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kplsr(; kwargs...)
kplsr(X, Y; kwargs...)
kplsr(X, Y, weights::Weight; kwargs...)
kplsr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Kernel partial least squares regression (KPLSR) implemented with a Nipals algorithm (Rosipal &amp; Trejo, 2001).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective functions    <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>This algorithm becomes slow for n &gt; 1000. Use function <code>dkplsr</code> instead.</p><p><strong>References</strong></p><p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space.  Journal of Machine Learning Research 2, 97-123.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
kern = :krbf ; gamma = 1e-1
model = kplsr(; nlv, kern, gamma) ;
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm
@head model.fitm.T

coef(model)
coef(model; nlv = 3)

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
nlv = 2
kern = :krbf ; gamma = 1 / 3
model = kplsr(; nlv, kern, gamma) 
fit!(model, x, y)
pred = predict(model, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplsr.jl#L1-L77">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kplsrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.kplsrda-Tuple{}"><code>Jchemo.kplsrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kplsrda(; kwargs...)
kplsrda(X, y; kwargs...)
kplsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on kernel partial least squares regression (KPLSR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that a kernel PLSR (function <code>kplsr</code>), instead of a PLSR (function <code>plskern</code>),  is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
kern = :krbf ; gamma = .001 
scal = true
model = kplsrda(; nlv, kern, gamma, scal) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

@head transf(model, Xtrain)
@head fitm.fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplsrda.jl#L1-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.kpol-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.kpol-Tuple{Any, Any}"><code>Jchemo.kpol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kpol(X, Y; kwargs...)</code></pre><p>Compute a polynomial kernel Gram matrix. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (m, p).</li></ul><p>Keyword arguments:</p><ul><li><code>gamma</code> : Scale of the polynom.</li><li><code>coef0</code> : Offset of the polynom.</li><li><code>degree</code> : Degree of the polynom.</li></ul><p>Given matrices <code>X</code> and <code>Y</code> of sizes (n, p) and (m, p), respectively, the function returns the (n, m) Gram matrix:</p><ul><li>K(X, Y) = Phi(X) * Phi(Y)&#39;.</li></ul><p>The polynomial kernel between two vectors x and y is computed by (<code>gamma</code> * (x&#39; * y) + <code>coef0</code>)^<code>degree</code>.</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization,  and beyond, Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
X = rand(5, 3)
Y = rand(2, 3)
kpol(X, Y; gamma = .1, coef0 = 10, degree = 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kernels.jl#L35-L61">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.krbf-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.krbf-Tuple{Any, Any}"><code>Jchemo.krbf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">krbf(X, Y; kwargs...)</code></pre><p>Compute a Radial-Basis-Function (RBF) kernel Gram matrix. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (m, p).</li></ul><p>Keyword arguments:</p><ul><li><code>gamma</code> : Scale parameter.</li></ul><p>Given matrices <code>X</code> and <code>Y</code> of sizes (n, p) and (m, p), respectively, the function returns the (n, m) Gram matrix:</p><ul><li>K(X, Y) = Phi(X) * Phi(Y)&#39;.</li></ul><p>The RBF kernel between two vectors x and y is computed by exp(-<code>gamma</code> * ||x - y||^2).</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization,  and beyond, Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
X = rand(5, 3)
Y = rand(2, 3)
krbf(X, Y; gamma = .1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kernels.jl#L1-L25">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.krr-Tuple{}"><a class="docstring-binding" href="#Jchemo.krr-Tuple{}"><code>Jchemo.krr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">krr(; kwargs...)
krr(X, Y; kwargs...)
krr(X, Y, weights::Weight; kwargs...)
krr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Kernel ridge regression (KRR) implemented by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective functions    <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of `X    is scaled by its uncorrected standard deviation.</li></ul><p>KRR is also referred to as least squared SVM regression (LS-SVMR). The method is close to the particular case of  SVM regression where there is no marge excluding the observations (epsilon coefficient set to zero). The difference  is that a L2-norm optimization is done, instead of L1 in SVM.</p><p><strong>References</strong></p><p>Bennett, K.V., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and Applications, NATO Science Series III: Computer &amp; Systems  Sciences. IOS Press Amsterdam, pp. 227-250.</p><p>Cawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel Ridge Regression. Neural Processing Letters 16, 293-302.  https://doi.org/10.1023/A:1021798002258</p><p>Krell, M.M., 2018. Generalizing, Decoding, and Optimizing Support Vector Machine Classification. arXiv:1801.04929.</p><p>Saunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression Learning Algorithm in Dual Variables, in: In Proceedings of the  15th International Conference on Machine Learning. Morgan Kaufitmann, pp. 515-521.</p><p>Suykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse approximation using least squares support vector machines. 2000 IEEE  International Symposium on Circuits and Systems. Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439</p><p>Welling, M., n.d. Kernel ridge regression. Department of Computer Science, University of Toronto, Toronto, Canada.  https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

lb = 1e-3
kern = :krbf ; gamma = 1e-1
model = krr(; lb, kern, gamma) ;
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm

coef(model)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
   ylabel = &quot;Observed&quot;).f    

coef(model; lb = 1e-1)
res = predict(model, Xtest; lb = [.1 ; .01])
@head res.pred[1]
@head res.pred[2]

lb = 1e-3
kern = :kpol ; degree = 1
model = krr(; lb, kern, degree) 
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest)
rmsep(res.pred, ytest)

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
lb = 1e-1
kern = :krbf ; gamma = 1 / 3
model = krr(; lb, kern, gamma) 
fit!(model, x, y)
pred = predict(model, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/krr.jl#L1-L103">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.krrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.krrda-Tuple{}"><code>Jchemo.krrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">krrda(; kwargs...)
krrda(X, y; kwargs...)
krrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on kernel ridge regression (KRR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments: </p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>rrda</code> (RR-DA) except that a kernel RR (function <code>krr</code>), instead of a RR (function <code>rr</code>),  is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

lb = 1e-5
kern = :krbf ; gamma = .001 
scal = true
model = krrda(; lb, kern, gamma, scal) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

coef(fitm.fitm)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; lb = [.1, .001]).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/krrda.jl#L1-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lda-Tuple{}"><a class="docstring-binding" href="#Jchemo.lda-Tuple{}"><code>Jchemo.lda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lda(; kwargs...)
lda(; kwargs...)
lda(X, y; kwargs...)
lda(X, y, weights::Weight; kwargs...)</code></pre><p>Linear discriminant analysis (LDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li></ul><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

model = lda()
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
@names fitm
typeof(fitm.fitm) 

fitm.lev
fitm.ni
fitm.priors
aggsumv(fitm.weights.w, ytrain)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lda.jl#L1-L66">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.list-Tuple{Any, Integer}"><a class="docstring-binding" href="#Jchemo.list-Tuple{Any, Integer}"><code>Jchemo.list</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">list(Q, n::Integer)</code></pre><p>Create a Vector <code>{Q}(undef, n)</code>.</p><p><code>isassigned(object, i)</code> can be used to check if cell i is empty.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

list(Float64, 5)
list(Array{Float64}, 5)
list(Matrix{Int}, 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L318-L332">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.list-Tuple{Integer}"><a class="docstring-binding" href="#Jchemo.list-Tuple{Integer}"><code>Jchemo.list</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">list(n::Integer)</code></pre><p>Create a Vector{Any}(nothing, n).</p><p><code>isnothing(object, i)</code> can be used to check if cell i is empty.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

list(5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L303-L315">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.locw-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>Jchemo.locw</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">locw(Xtrain, Ytrain, X; listnn, listw = nothing, algo, verbose = false, kwargs...)</code></pre><p>Compute predictions for a given kNN model.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>Ytrain</code> : Training Y-data.</li><li><code>X</code> : X-data (m observations) to predict.</li></ul><p>Keyword arguments:</p><ul><li><code>listnn</code> : List (vector) of m vectors of indexes.</li><li><code>listw</code> : List (vector) of m vectors of weights.</li><li><code>algo</code> : Function computing the model on the m neighborhoods.</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li><li><code>kwargs</code> : Keywords arguments to pass in function <code>algo</code>. Each argument must have length = 1 (not be a collection).</li></ul><p>Each component i of <code>listnn</code> and <code>listw</code> contains the indexes and weights, respectively, of the nearest neighbors  of x_i in Xtrain. The sizes of the neighborhood for i = 1,...,m can be different.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/locw.jl#L1-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.locwlv-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#Jchemo.locwlv-Tuple{Any, Any, Any}"><code>Jchemo.locwlv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">locwlv(Xtrain, Ytrain, X; listnn, listw = nothing, algo, nlv, verbose = true, kwargs...)</code></pre><p>Compute predictions for a given kNN model.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>Ytrain</code> : Training Y-data.</li><li><code>X</code> : X-data (m observations) to predict.</li></ul><p>Keyword arguments:</p><ul><li><code>listnn</code> : List (vector) of m vectors of indexes.</li><li><code>listw</code> : List (vector) of m vectors of weights.</li><li><code>algo</code> : Function computing the model on the m neighborhoods.</li><li><code>nlv</code> : Nb. or collection of nb. of latent variables (LVs).</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li><li><code>kwargs</code> : Keywords arguments to pass in function <code>algo</code>. Each argument must have length = 1 (not be a collection).</li></ul><p>Same as <a href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>locw</code></a> but specific and much faster for LV-based models (e.g. PLSR).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/locwlv.jl#L1-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.loessr-Tuple{}"><a class="docstring-binding" href="#Jchemo.loessr-Tuple{}"><code>Jchemo.loessr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">loessr(; kwargs...)
loessr(X, y; kwargs...)</code></pre><p>Compute a locally weighted regression model (LOESS).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>span</code> : Window for neighborhood selection (level of smoothing) for the local fitting, typically proportion    within [0, 1].</li><li><code>degree</code> : Polynomial degree for the local fitting.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function fits a LOESS model using package `Loess.jl&#39;. </p><p>Smaller values of <code>span</code> result in smaller local context in fitting (less smoothing).</p><p><strong>References</strong></p><p>https://github.com/JuliaStats/Loess.jl</p><p>Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of the  American statistical association, 74(368), 829-836. DOI: 10.1080/01621459.1979.10481038</p><p>Cleveland, W. S., &amp; Devlin, S. J. (1988). Locally weighted regression: an approach to regression analysis by  local fitting. Journal of the American statistical association, 83(403), 596-610. DOI: 10.1080/01621459.1988.10478639</p><p>Cleveland, W. S., &amp; Grosse, E. (1991). Computational methods for local regression. Statistics and computing,  1(1), 47-62. DOI: 10.1007/BF01890836</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
model = loessr(span = 1 / 3) 
fit!(model, x, y)
pred = predict(model, x).pred 
f = Figure(size = (700, 300))
ax = Axis(f[1, 1], xlabel = &quot;x&quot;, ylabel = &quot;y&quot;)
scatter!(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred); label = &quot;Loess&quot;)
f[1, 2] = Legend(f, ax, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/loessr.jl#L1-L52">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwmlr-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwmlr-Tuple{}"><code>Jchemo.lwmlr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwmlr(; kwargs...)
lwmlr(X, Y; kwargs...)</code></pre><p>k-Nearest-Neighbours locally weighted multiple linear regression (kNN-LWMLR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of the global <code>X</code> is scaled by its uncorrected standard deviation before    the distance and weight computations.</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>This is the same principle as function <code>lwplsr</code> except that MLR models are fitted on the neighborhoods, instead of  PLSR models.  The neighborhoods are computed directly on <code>X</code> (there is no preliminary dimension reduction).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
model0 = pcasvd(; nlv) ;
fit!(model0, Xtrain) 
@head Ttrain = model0.fitm.T 
@head Ttest = transf(model0, Xtest)

metric = :eucl 
h = 2 ; k = 100 
model = lwmlr(; metric, h, k) 
fit!(model, Ttrain, ytrain)
@names model
@names model.fitm

res = predict(model, Ttest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f    

## Same but with function &#39;pip&#39;
nlv = 20
metric = :eucl 
h = 2 ; k = 100 
model1 = pcasvd(; nlv) ;
model2 = lwmlr(; metric, h, k) 
model = pip(model1, model2)
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ;
@head res.pred
rmsep(res.pred, ytest)

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
model = lwmlr(metric = :eucl, h = 1.5, k = 20) ;
fit!(model, x, y)
pred = predict(model, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwmlr.jl#L1-L92">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwmlrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwmlrda-Tuple{}"><code>Jchemo.lwmlrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwmlrda(; kwargs...)
lwmlrda(X, y; kwargs...)</code></pre><p>k-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of the global <code>X</code> is scaled by its uncorrected standard deviation before    the distance and weight computations.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>This is the same principle as function <code>lwmlr</code> except that MLR-DA models, instead of MLR models, are fitted on the  neighborhoods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

metric = :mah
h = 2 ; k = 10
model = lwmlrda(; metric, h, k) 
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm

fitm.lev
fitm.ni

res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwmlrda.jl#L1-L64">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwplslda-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwplslda-Tuple{}"><code>Jchemo.lwplslda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwplslda(; kwargs...)
lwplslda(X, y; kwargs...)</code></pre><p>kNN-LWPLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS used for the dimension    reduction before computing the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.   If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional). This argument only concerns the preliminary global PLS dimension reduction (if any)   used to compute the distances. In the local models, the priors are not used since the weights are given by the    distance-based decreasing weight function.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e. inside each neighborhood) models.</li><li><code>scal</code> : Boolean. If <code>true</code>, (a) each column of the global <code>X</code> (and of the global <code>Y</code> if there    is a preliminary PLS reduction dimension) is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Y scaling is also done within each neighborhood (local level)    for the weighted PLSR.</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>This is the same principle as function <code>lwplsr</code> except that a PLS-LDA model, instead of a PLSR model, is fitted  on each neighborhoods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 200
nlv = 10
model = lwplslda(; nlvdis, metric, h, k, prior = :unif, nlv) 
#model = lwplsqda(; nlvdis, metric, h, k, nlv, alpha = .5) 
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm
fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplslda.jl#L1-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwplsqda-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwplsqda-Tuple{}"><code>Jchemo.lwplsqda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwplsqda(; kwargs...)
lwplsqda(X, y; kwargs...)</code></pre><p>kNN-LWPLS-QDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS used for the dimension    reduction before computing the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.   If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional). This argument only concerns the preliminary global PLS dimension reduction (if any)   used to compute the distances. In the local models, the priors are not used since the weights are given by the    distance-based decreasing weight function.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e. inside each neighborhood) models.</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, (a) each column of the global <code>X</code> (and of the global <code>Y</code> if there    is a preliminary PLS reduction dimension) is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Y scaling is also done within each neighborhood (local level)    for the weighted PLSR.</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>This is the same principle as function <code>lwplsr</code> except that a PLS-QDA model, instead of a PLSR model, is fitted  on each neighborhoods.</p><ul><li><strong>Warning:</strong> The present version of this function can suffer from stops due to non positive definite matrices    when doing QDA on neighborhoods. This is due to that some classes within the neighborhood can have very few    observations. It is recommended to select a sufficiantly large number of neighbors or/and to use a regularized    QDA (<code>alpha &gt; 0</code>).</li></ul><p>See function <code>lwplslda</code> for examples. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsqda.jl#L1-L42">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwplsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwplsr-Tuple{}"><code>Jchemo.lwplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwplsr(; kwargs...)
lwplsr(X, Y; kwargs...)</code></pre><p>k-Nearest-Neighbours locally weighted partial least squares regression (kNN-LWPLSR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS used for the dimension    reduction before computing the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e. inside each neighborhood) models.</li><li><code>scal</code> : Boolean. If <code>true</code>, (a) each column of the global <code>X</code> (and of the global <code>Y</code> if there    is a preliminary PLS reduction dimension) is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Y scaling is also done within each neighborhood (local level)    for the weighted PLSR.</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>Function <code>lwplsr</code> fits kNN-LWPLSR models such as in Lesnoff et al. 2020. The general principle of  the pipeline is as follows (many other variants of pipelines can be built):</p><p>LWPLSR is a particular case of weighted PLSR (WPLSR) (e.g. Schaal et al. 2002). In WPLSR, a priori weights,  different from the usual 1/n (standard PLSR), are given to the n training observations. These weights are used to calculate (i) the scores and loadings of the WPLS and (ii) the regression model that fits (by weighted least  squares) the Y-response(s) to the WPLS scores. The specificity of LWPLSR (compared to WPLSR) is that the weights  are computed from dissimilarities (e.g. distances) between the new observation to predict and the training  observations (&quot;L&quot; in LWPLSR comes from &quot;localized&quot;). Note that in LWPLSR the weights, and therefore the fitted  WPLSR model, change for each new observation to predict (there are no a &#39;unique&#39; fitted model).</p><p>In the original LWPLSR, all the n training observations are used for each observation to predict (e.g. Sicard &amp; Sabatier 2006, Kim et al 2011). This can be very time consuming when n is large. A faster (and often more efficient)  strategy is to preliminary select, in the training set, a number of <code>k</code> nearest neighbors to the observation  to predict (= &quot;weighting 1&quot;) and then to apply LWPLSR only to this pre-selected neighborhood (= &quot;weighting 2&quot;). This strategy corresponds to kNN-LWPLSR implemented in function <code>lwplsr</code>.</p><p>In <code>lwplsr</code>, the dissimilarities used for weightings 1 and 2 are computed directely from the raw X-data, or after  a dimension reduction,depending on argument <code>nlvdis</code>. In the last case, global PLS2 scores (LVs) are computed  from {<code>X</code>, <code>Y</code>} and the dissimilarities are computed over these scores. </p><p>In general, for high dimensional X-data, using the Mahalanobis distance requires preliminary dimensionality  reduction of the data.</p><p><strong>References</strong></p><p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active pharmaceutical ingredients content using  locally weighted partial least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.</p><p>Lesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS strategies for regression and  discrimination on agronomic NIR data. Journal of Chemometrics, e3209. https://doi.org/10.1002/cem.3209</p><p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric statistics for the real time  robot learning. Applied Intell., 17, 49-60.</p><p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression and application to a rainfall dataset.  Comput. Stat. Data Anal., 51, 1393-1410.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 15 ; metric = :mah 
k = 500 ; h = 1 ; nlv = 10
model = lwplsr(; nlvdis, metric, h, k, nlv) 
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm

res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f    

## Storage of the local models fitted on the neighborhoods
model = lwplsr(; nlvdis, metric, h, k, nlv, store = true) 
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ; 
@show rmsep(res.pred, ytest)
@names res
length(res.fitm)
typeof(res.fitm[1]) 
@names res.fitm[1]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsr.jl#L1-L107">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwplsravg-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwplsravg-Tuple{}"><code>Jchemo.lwplsravg</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwplsravg(; kwargs...)
lwplsravg(X, Y; kwargs...)</code></pre><p>Averaging kNN-LWPLSR models with different numbers of latent variables (kNN-LWPLSR-AVG).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS used for the dimension    reduction before computing the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.   If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : A range of nb. of latent variables (LVs) to compute for the local (i.e. inside each neighborhood) models.</li><li><code>scal</code> : Boolean. If <code>true</code>, (a) each column of the global <code>X</code> (and of the global <code>Y</code> if there is a preliminary PLS    reduction dimension) is scaled by its uncorrected standard deviation before to compute the distances and the weights,    and (b) the X and Y scaling is also done within each neighborhood (local level) for the weighted PLSR.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>Ensemblist method where the predictions are computed by averaging the predictions of a set of models built with  different numbers of LVs, such as in Lesnoff 2023. On each neighborhood, a PLSR-averaging (Lesnoff et al. 2022) is done  instead of a PLSR.</p><p>For instance, if argument <code>nlv</code> is set to <code>nlv</code> = <code>5:10</code>, the prediction for a new observation is the simple average of  the predictions returned by the models with 5 LVs, 6 LVs, ... 10 LVs, respectively.</p><p><strong>References</strong></p><p>Lesnoff, M., Andueza, D., Barotin, C., Barre, V., Bonnal, L., Fernández Pierna, J.A., Picard, F.,  Vermeulen, V., Roger, J.-M., 2022. Averaging and Stacking Partial Least Squares  Regression Models to Predict the Chemical Compositions and the Nutritive Values of Forages from  Spectral Near Infrared Data. Applied Sciences 12, 7850. https://doi.org/10.3390/app12157850</p><p>M. Lesnoff, Averaging a local PLSR pipeline to predict chemical compositions and nutritive values of forages  and feed from spectral near infrared data, Chemometrics and Intelligent Laboratory Systems. 244 (2023) 105031.  https://doi.org/10.1016/j.chemolab.2023.105031.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 5 ; metric = :mah 
h = 1 ; k = 200 ; nlv = 4:20
model = lwplsravg(; nlvdis, metric, h, k, nlv) ;
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm

res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsravg.jl#L1-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.lwplsrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.lwplsrda-Tuple{}"><code>Jchemo.lwplsrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lwplsrda(; kwargs...)
lwplsrda(X, y; kwargs...)</code></pre><p>kNN-LWPLSR-DA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS used for the dimension    reduction before computing the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.   If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and to compute the weights    (see function <code>getknn</code>). Possible values are: <code>:eucl</code> (Euclidean), <code>:mah</code> (Mahalanobis),    <code>:sam</code> (spectral angular distance), <code>:cos</code> (cosine distance), <code>:cor</code> (correlation distance).</li><li><code>h</code> : A scalar defining the shape of the weight function computed by function <code>winvs</code>. Lower is h,    sharper is the function. See function <code>winvs</code> for details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>winvs</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional). This argument only concerns the preliminary global PLS dimension reduction (if any)   used to compute the distances. In the local models, the priors are not used since the weights are given by the    distance-based decreasing weight function.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e. inside each neighborhood) models.</li><li><code>scal</code> : Boolean. If <code>true</code>, (a) each column of the global <code>X</code> (and of the global <code>Y</code> if there    is a preliminary PLS reduction dimension) is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Y scaling is also done within each neighborhood (local level)    for the weighted PLSR.</li><li><code>store</code> : Boolean. If <code>true</code>, the local models fitted on the neighborhoods are stored and returned by function <code>predict</code>.</li><li><code>verbose</code> : Boolean. If <code>true</code>, predicting information are printed.</li></ul><p>This is the same principle as function <code>lwplsr</code> except that PLSR-DA models, instead of PLSR models, are fitted  on the neighborhoods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 200
nlv = 10
model = lwplsrda(; nlvdis, metric, h, k, prior = :unif, nlv) 
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm

fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ; 
@names res 
res.listnn
res.listd
res.listw
@head res.pred
@show errp(res.pred, ytest)
conf(res.pred, ytest).cnt

## Storage of the local models fitted on the neighborhoods
model = lwplsrda(; nlvdis, metric, h, k, prior = :unif, nlv, store = true) 
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ; 
@show errp(res.pred, ytest)
@names res
length(res.fitm)
typeof(res.fitm[1]) 
@names res.fitm[1]
@names res.fitm[1].fitm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsrda.jl#L1-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.madv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.madv-Tuple{Any}"><code>Jchemo.madv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">madv(x)</code></pre><p>Compute the median absolute deviation (MAD) of a vector. </p><ul><li><code>x</code> : A vector (n).</li></ul><p>This is the MAD adjusted by a factor (1.4826) for asymptotically normal consistency.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = rand(100)
madv(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L122-L138">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mae-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.mae-Tuple{Any, Any}"><code>Jchemo.mae</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mae(pred, Y)</code></pre><p>Compute the median absolute error (MAE).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
mae(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
mae(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L200-L227">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mahsq-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.mahsq-Tuple{Any, Any}"><code>Jchemo.mahsq</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mahsq(X, Y)
mahsq(X, Y, Sinv)</code></pre><p>Squared Mahalanobis distances between the rows of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (m, p).</li><li><code>Sinv</code> : Inverse of a covariance matrix S. If not given, S is computed as the uncorrected covariance matrix of <code>X</code>.</li></ul><p>When <code>X</code> and <code>Y</code> are (n, p) and (m, p), repectively, it returns an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

X = rand(5, 3)
Y = rand(2, 3)

mahsq(X, Y)

S = covm(X)
Sinv = inv(S)
mahsq(X, Y, Sinv)
mahsq(X[1:1, :], Y[1:1, :], Sinv)

mahsq(X[:, 1], 4)
mahsq(1, 4, 2.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/distances.jl#L29-L57">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mahsqchol-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.mahsqchol-Tuple{Any, Any}"><code>Jchemo.mahsqchol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mahsqchol(X, Y)
mahsqchol(X, Y, Uinv)</code></pre><p>Compute the squared Mahalanobis distances (with a Cholesky factorization) between the observations (rows)      of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (m, p).</li><li><code>Uinv</code> : Inverse of the upper matrix of a Cholesky factorization of a covariance matrix S. If not given,    the factorization is done on S, the uncorrected covariance matrix of <code>X</code>.</li></ul><p>When <code>X</code> and <code>Y</code> are (n, p) and (m, p), repectively, it returns an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using LinearAlgebra, StatsBase

X = rand(5, 3)
Y = rand(2, 3)

mahsqchol(X, Y)

S = cov(X, corrected = false)
U = cholesky(Hermitian(S)).U 
Uinv = inv(U)
mahsqchol(X, Y, Uinv)

mahsqchol(X[:, 1], 4)
mahsqchol(1, 4, sqrt(2.1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/distances.jl#L73-L103">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.matB"><a class="docstring-binding" href="#Jchemo.matB"><code>Jchemo.matB</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">matB(X, y, weights::Weight)</code></pre><p>Between-class covariance matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : A vector (n) defining the class membership.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Compute the between-class covariance matrix (output <code>B</code>)  of <code>X</code>. This is the (non-corrected) covariance matrix of  the weighted class centers.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, StatsBase

n = 20 ; p = 3
X = rand(n, p)
y = rand(1:3, n)
tab(y) 
weights = mweight(ones(n)) 

res = matB(X, y, weights) ;
res.B
res.priors
res.ni
res.lev

res = matW(X, y, weights) ;
res.W
res.Wi

matW(X, y, weights).W + matB(X, y, weights).B
cov(X; corrected = false)

v = mweight(collect(1:n))
matW(X, y, v).priors 
matB(X, y, v).priors 
matW(X, y, v).W + matB(X, y, v).B
covm(X, v)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/matW.jl#L1-L42">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.matW"><a class="docstring-binding" href="#Jchemo.matW"><code>Jchemo.matW</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">matW(X, y, weights::Weight)</code></pre><p>Within-class covariance matrices.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : A vector (n) defing the class membership.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Compute the (non-corrected) within-class and pooled covariance  matrices  (outputs <code>Wi</code> and <code>W</code>, respectively) of <code>X</code>. </p><p>If class i contains only one observation, Wi is computed by:</p><ul><li><code>covm(</code>X<code>,</code>weights<code>)</code>.</li></ul><p>For examples, see function <code>matB</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/matW.jl#L61-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mavg-Tuple{}"><a class="docstring-binding" href="#Jchemo.mavg-Tuple{}"><code>Jchemo.mavg</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mavg(; kwargs...)
mavg(X; kwargs...)</code></pre><p>Smoothing by moving averages of each row of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Nb. points involved in the window (length of the kernel).</li></ul><p>The function returns a matrix (n, p).</p><p>The smoothing is computed by convolution with padding, using function <code>imfilter</code> of package ImageFiltering.jl.  The centered kernel is <code>ones(npoint) / npoint</code>. Each returned point is located on the center of the kernel. Assume a signal x of length p (row of <code>X</code>) correponding to a vector wl of p wavelengths (or other indexes). </p><p><strong>For instance:</strong></p><p>If <code>npoint = 3</code>, the  kernel is kern = [.33, .33, .33], and: </p><ul><li>The output value at index i = 1 is: dot(kern, [x[1], x[1], x[2]]) (padding). The corresponding wavelength    is: wl[1].</li><li>The output value at index i = 4 is: dot(kern, [x[3], x[4], x[5]]). The corresponding wavelength is: wl[4]</li></ul><p>If <code>npoint = 4</code>, the  kernel is kern = [.25, .25, .25, .25], and: </p><ul><li>The output value at index i = 1 is: dot(kern, x[1], x[1], x[2], x[3]) (padding). The corresponding    wavelength is: (wl[1] + wl[2]) / 2.</li><li>The output value at index i = 4 is: dot(kern, [x[3], x[4], x[5], x[6]]). The corresponding wavelength    is: (wl[4] + wl[5]) / 2.</li></ul><p><strong>References</strong></p><p>https://github.com/JuliaImages/ImageFiltering.jl</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = mavg(npoint = 10) 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L327-L380">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbconcat-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbconcat-Tuple{}"><code>Jchemo.mbconcat</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbconcat()
mbconcat(Xbl)</code></pre><p>Concatenate horizontaly multiblock X-data.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
n = 5 ; m = 3 ; p = 9 
X = rand(n, p) 
Xnew = rand(m, p)
listbl = [3:4, 1, [6; 8:9]]
Xbl = mblock(X, listbl) 
Xblnew = mblock(Xnew, listbl) 
@head Xbl[3]

model = mbconcat() 
fit!(model, Xbl)
transf(model, Xbl)
transf(model, Xblnew)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_mb.jl#L195-L217">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbin"><a class="docstring-binding" href="#Jchemo.mbin"><code>Jchemo.mbin</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mbin(q)</code></pre><p>Build histogram-bin intervals.</p><ul><li><code>q</code> : Numerical values (K) defining the limits of the intervals. </li></ul><p>For a given vector <code>q</code> of length K, the function returns K + 1 intervals: </p><ul><li>(-Inf, q[1]]</li><li>(q[1], q[2]]</li><li>etc.</li><li>(q[K - 1], q[K]]</li><li>(q[K], Inf)</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

q = [.01; .5; .500001; .9; 1.1]
mbin(q)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_table.jl#L110-L129">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mblock-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.mblock-Tuple{Any, Any}"><code>Jchemo.mblock</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mblock(X, listbl)</code></pre><p>Make blocks from a matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>listbl</code> : A vector whose each component defines the colum numbers defining a block in <code>X</code>.   The length of <code>listbl</code> is the number of blocks.</li></ul><p>The function returns a list (vector) of blocks.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
n = 5 ; p = 10 
X = rand(n, p) 
listbl = [3:4, 1, [6; 8:10]]

Xbl = mblock(X, listbl)
Xbl[1]
Xbl[2]
Xbl[3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_mb.jl#L1-L22">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbpca-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbpca-Tuple{}"><code>Jchemo.mbpca</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbpca(; kwargs...)
mbpca(Xbl; kwargs...)
mbpca(Xbl, weights::Weight; kwargs...)
mbpca!(Xbl::Matrix, weights::Weight; kwargs...)</code></pre><p>Consensus principal components analysis (CPCA, a.k.a MBPCA) by Nipals.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code>.  </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. global latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>tol</code> : Tolerance value for Nipals convergence.</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> is scaled by its uncorrected standard    deviation (before the block scaling).</li></ul><p>CPCA Nipals algorithm (Westerhuis et a; 1998). CPCA is also known as MBPCA, and was referred to as CPCA-W in Smilde  et al. 2003. Besides eventual block scaling, MBPCA is equivalent to a PCA on the horizontally concatenated matrix  X = [X1 X2 ... Xk] and is referred to as SUM-PCA in Smilde et al 2003.</p><p>The function returns several objects, in particular:</p><ul><li><code>T</code> : Global LVs (not-normed).</li><li><code>U</code> : Global LVs (normed).</li><li><code>W</code> : Block weights (normed).</li><li><code>Tb</code> : Block LVs (in the metric scale), returned <strong>grouped by LV</strong>.</li><li><code>Tbl</code> : Block LVs (in the original scale), returned <strong>grouped by block</strong>.</li><li><code>Vbl</code> : Block loadings (normed).</li><li><code>lb</code> : Block specific weights (&#39;lambda&#39;) for the global LVs.</li><li><code>mu</code> : Sum of the block specific weights (= eigen values of the global PCA).</li></ul><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total X inertia (squared Frobenious norm) explained by the global LVs.</li><li><code>explxbl</code> : Proportion of the inertia of each block (= Xbl[k]) explained by the global LVs.</li><li><code>contrxbl2t</code> : Contribution of each block to the global LVs (= lb proportions).  </li><li><code>rvxbl2t</code> : RV coefficients between each block and the global LVs.</li><li><code>rdxbl2t</code> : Rd coefficients between each block and the global LVs.</li><li><code>cortbl2t</code> : Correlations between the block LVs (= Tbl[k]) and the global LVs.</li><li><code>corx2t</code> : Correlation between the X-variables and the global LVs.  </li></ul><p><strong>References</strong></p><p>Mangamana, E.T., Cariou, V., Vigneau, E., Glèlè Kakaï, R.L., Qannari, E.M., 2019. Unsupervised multiblock data  analysis: A unified approach and extensions. Chemometrics and Intelligent Laboratory Systems 194, 103856.  https://doi.org/10.1016/j.chemolab.2019.103856</p><p>Smilde, A.K., Westerhuis, J.A., de Jong, S., 2003. A framework for sequential multiblock component methods.  Journal of Chemometrics 17, 323–337. https://doi.org/10.1002/cem.811</p><p>Westerhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis of multiblock and hierarchical PCA and PLS models. Journal  of Chemometrics 12, 301–321. https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5&lt;301::AID-CEM515&gt;3.0.CO;2-S</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
@names dat 
X = dat.X
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X[1:6, :], listbl)
Xblnew = mblock(X[7:8, :], listbl)
n = nro(Xbl[1]) 

nlv = 3
bscal = :frob
#bscal = :none
scal = false
#scal = true
model = mbpca(; nlv, bscal, scal, tol = 1e-15)
fit!(model, Xbl)
@names model 
fitm = model.fitm ;
@names fitm

## Global scores 
@head transf(model, Xbl)
@head fitm.T

transf(model, Xblnew)

## Blocks scores
i = 1
@head transfbl(model, Xbl)[i]
@head fitm.Tbl[i]

## Summary
res = summary(model, Xbl) ;
@names res 
res.explvarx
res.explxbl   # = fitm.lb if bscal = :frob
rowsum(Matrix(res.explxbl))
res.contrxbl2t
res.rvxbl2t
res.rdxbl2t
res.cortbl2t
res.corx2t 

#### This MBPCA can also be implemented with function &#39;pip&#39;

model1 = blockscal(; bscal, centr = true) ;
model2 = mbconcat()
model3 = pcasvd(; nlv) ;
model = pip(model1, model2, model3)
fit!(model, Xbl)

mod3 = model.model[3] ;
typeof(mod3) 
@names mod3 
@names mod3.fitm

@head transf(model, Xbl)
@head mod3.fitm.T 

transf(model, Xblnew)

#### And a MB sparse PCA as follows

meth = :soft ; nvar = 2
model1 = blockscal(; bscal, centr = true) ;
model2 = mbconcat()
model3 = spca(; nlv, meth, nvar) ;
model = pip(model1, model2, model3)
fit!(model, Xbl)

mod3 = model.model[3] ;
@names mod3 
typeof(mod3) 
@names mod3.fitm

mod3.fitm.sellv
mod3.fitm.sel

@head transf(model, Xbl)
@head mod3.fitm.T 

transf(model, Xblnew)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbpca.jl#L1-L139">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbplskdeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbplskdeda-Tuple{}"><code>Jchemo.mbplskdeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbplskdeda(; kwargs...)
mbplskdeda(Xbl, y; kwargs...)
mbplskdeda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Multiblock PLS-KDEDA.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth definition) can also be specified here.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling) in the MBPLS computation.</li></ul><p>Same as function <code>mbplsqda</code> except that the class densities are estimated from <code>dmkern</code> instead of <code>dmnorm</code>.</p><p>See function <code>mbplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplskdeda.jl#L1-L23">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbplslda-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbplslda-Tuple{}"><code>Jchemo.mbplslda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbplslda(; kwargs...)
mbplslda(Xbl, y; kwargs...)
mbplslda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Multiblock PLS-LDA.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling) in the MBPLS computation.</li></ul><p>The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>A multivariate MBPLSR (MBPLSR2) is run on the data {<code>X</code>, Ydummy}, returning   a score matrix <code>T</code>.</li><li>A LDA is done on {<code>T</code>, <code>y</code>}, returning estimates of posterior probabilities  (∊ [0, 1]) of class membership.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
wlst = names(X)
wl = parse.(Float64, wlst)
#plotsp(X, wl; nsamp = 20).f
##
listbl = [1:350, 351:700]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 

nlv = 15
scal = false
#scal = true
bscal = :none
#bscal = :frob
model = mbplslda(; nlv, bscal, scal)
#model = mbplsqda(; nlv, bscal, alpha = .5, scal)
#model = mbplskdeda(; nlv, bscal, prior = :unif, scal)
fit!(model, Xbltrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm

fitm.lev
fitm.ni
fitm.priors

fitm_emb = fitm.fitm_emb ;
typeof(fitm_emb)
@names fitm_emb 

@head transf(model, Xbltrain)
@head fitm_emb.fitm.T

@head transf(model, Xbltest)
@head transf(model, Xbltest; nlv = 3)

fitm_da = fitm.fitm_da ;
typeof(fitm_da)

res = predict(model, Xbltest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xbltest; nlv = 1:2).pred

summary(fitm_emb, Xbltrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplslda.jl#L1-L109">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbplsqda-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbplsqda-Tuple{}"><code>Jchemo.mbplsqda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbplsqda(; kwargs...)
mbplsqda(Xbl, y; kwargs...)
mbplsqda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Multiblock PLS-QDA.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling) in the MBPLS computation.</li></ul><p>The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>A multivariate MBPLSR (MBPLSR2) is run on the data {<code>X</code>, Ydummy}, returning a score matrix <code>T</code>.</li><li>A QDA (possibly with continuum) is done on {<code>T</code>, <code>y</code>}, returning estimates of posterior probabilities (∊ [0, 1]) of   class membership.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p>See function <code>mbplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsqda.jl#L1-L42">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbplsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbplsr-Tuple{}"><code>Jchemo.mbplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbplsr(; kwargs...)
mbplsr(Xbl, Y; kwargs...)
mbplsr(Xbl, Y, weights::Weight; kwargs...)
mbplsr!(Xbl::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Multiblock PLSR (MBPLSR).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. global latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This function runs a PLSR on {X, <code>Y</code>} where X is the horizontal concatenation of the blocks in <code>Xbl</code>. The function  returns the same global LVs and predictions as function <code>mbplswest</code>, but is much faster.</p><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total X inertia (squared Frobenious norm) explained by the global LVs.</li><li><code>rvxbl2t</code> : RV coefficients between each block and the global LVs.</li><li><code>rdxbl2t</code> : Rd coefficients between each block (= Xbl[k]) and the global LVs.</li><li><code>corx2t</code> : Correlation between the X-variables and the global LVs.  </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
@names dat 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
ytrain = y[s]
Xbltest = mblock(rmrow(X, s), listbl)
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 3
bscal = :frob
model = mbplsr(; nlv, bscal)
fit!(model, Xbltrain, ytrain)
@names model 
fitm = model.fitm ;
@names fitm 
typeof(fitm.fitm)
@names fitm.fitm

@head transf(model, Xbltrain)
@head fitm.fitm.T

transf(model, Xbltest)

res = predict(model, Xbltest)
res.pred 
rmsep(res.pred, ytest)

res = summary(model, Xbltrain) ;
@names res 
res.explvarx
res.rvxbl2t
res.rdxbl2t
res.corx2t 

## This MBPLSR can also be implemented with function &#39;pip&#39;

model1 = blockscal(; bscal, centr = true) ;
model2 = mbconcat()
model3 = plskern(; nlv) ;
model = pip(model1, model2, model3)
fit!(model, Xbltrain, ytrain)

mod3 = model.model[3] ;
typeof(mod3) 
@names mod3 
@names mod3.fitm

@head transf(model, Xbltrain)
@head mod3.fitm.T 

transf(model, Xbltest)

predict(model, Xbltest).pred 

## And a MB sparse PLSR as follows

meth = :soft ; nvar = 2
model1 = blockscal(; bscal, centr = true) ;
model2 = mbconcat()
model3 = splsr(; nlv, meth, nvar) ;
model = pip(model1, model2, model3)
fit!(model, Xbltrain, ytrain)

mod3 = model.model[3] ;
typeof(mod3) 
@names mod3 
@names mod3.fitm

mod3.fitm.sellv
mod3.fitm.sel

@head transf(model, Xbltrain)
@head mod3.fitm.T 

transf(model, Xbltest)

predict(model, Xbltest).pred </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsr.jl#L1-L117">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbplsrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbplsrda-Tuple{}"><code>Jchemo.mbplsrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbplsrda(; kwargs...)
mbplsrda(Xbl, y; kwargs...)
mbplsrda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on multiblock partial least squares regression (MBPLSR-DA).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing   nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1) variable. </li><li>Then, a multivariate MBPLSR (MBPLSR2) is run on the data {<code>X</code>, Ydummy}, returning predictions of the dummy variables   (= object <code>posterior</code> returned by function <code>predict</code>).  These predictions can be considered as unbounded estimates   (i.e. eventually outside of [0, 1]) of the class membership probabilities.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
wlst = names(X)
wl = parse.(Float64, wlst)
#plotsp(X, wl; nsamp = 20).f
##
listbl = [1:350, 351:700]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 

nlv = 15
scal = false
#scal = true
bscal = :none
#bscal = :frob
model = mbplsrda(; nlv, bscal, scal)
fit!(model, Xbltrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors
aggsumv(fitm.fitm.weights.w, ytrain)

@head transf(model, Xbltrain)
@head fitm.fitm.fitm.T

@head transf(model, Xbltest)
@head transf(model, Xbltest; nlv = 3)

res = predict(model, Xbltest) ; 
@head res.pred 
@show errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xbltest; nlv = 1:2).pred

summary(fitm.fitm, Xbltrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsrda.jl#L1-L100">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mbplswest-Tuple{}"><a class="docstring-binding" href="#Jchemo.mbplswest-Tuple{}"><code>Jchemo.mbplswest</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mbplswest(; kwargs...)
mbplswest(Xbl, Y; kwargs...)
mbplswest(Xbl, Y, weights::Weight; kwargs...)
mbplswest!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock PLSR (MBPLSR) - Nipals algorithm.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. global latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code> for possible values.</li><li><code>tol</code> : Tolerance value for convergence (Nipals).</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This functions implements the MBPLSR Nipals algorithm such as in Westerhuis et al. 1998. The function gives the same  global scores and predictions as function <code>mbplsr</code>.</p><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total X inertia (squared Frobenious norm) explained by the global LVs.</li><li><code>rvxbl2t</code> : RV coefficients between each block and the global LVs.</li><li><code>rdxbl2t</code> : Rd coefficients between each block (= Xbl[k]) and the global LVs.</li><li><code>cortbl2t</code> : Correlations between the block LVs (= Tbl[k]) and the global LVs.</li><li><code>corx2t</code> : Correlation between the X-variables and the global LVs.  </li></ul><p><strong>References</strong></p><p>Westerhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis of multiblock and hierarchical PCA and PLS models.  Journal of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5&lt;301::AID-CEM515&gt;3.0.CO;2-S</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
@names dat 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
ytrain = y[s]
Xbltest = mblock(rmrow(X, s), listbl)
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 3
bscal = :frob
model = mbplswest(; nlv, bscal)
fit!(model, Xbltrain, ytrain)
@names model 
fitm = model.fitm ;
@names fitm 

@head transf(model, Xbltrain)
@head fitm.T

transf(model, Xbltest)

res = predict(model, Xbltest)
res.pred 
rmsep(res.pred, ytest)

res = summary(model, Xbltrain) ;
@names res 
res.explvarx
res.rvxbl2t
res.rdxbl2t
res.corx2t </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplswest.jl#L1-L79">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.meanv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.meanv-Tuple{Any}"><code>Jchemo.meanv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">meanv(x)
meanv(x, weights::Weight)</code></pre><p>Compute the mean of a vector. </p><ul><li><code>x</code> : A vector (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 100
x = rand(n)
w = mweight(rand(n)) 

meanv(x)
meanv(x, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L32-L51">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.merrp-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.merrp-Tuple{Any, Any}"><code>Jchemo.merrp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">merrp(pred, y)</code></pre><p>Compute the mean intra-class classification error rate.</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data (class membership).</li></ul><p>ERRP (see function <code>errp</code>) is computed for each class. Function <code>merrp</code> returns the average of these intra-class ERRPs.   </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

model = plsrda(; nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
merrp(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L557-L580">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlev-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.mlev-Tuple{Any}"><code>Jchemo.mlev</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlev(x)</code></pre><p>Return the sorted levels of a vector or a dataset. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = rand([&quot;a&quot;;&quot;b&quot;;&quot;c&quot;], 20)
lev = mlev(x)
nlev = length(lev)

X = reshape(x, 5, 4)
mlev(X)

df = DataFrame(g1 = rand(1:2, n), g2 = rand([&quot;a&quot;; &quot;c&quot;], n))
mlev(df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L335-L354">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlr-Tuple{}"><a class="docstring-binding" href="#Jchemo.mlr-Tuple{}"><code>Jchemo.mlr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlr(; kwargs...)
mlr(X, Y; kwargs...)
mlr(X, Y, weights::Weight; kwargs...)
mlr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Compute a mutiple linear regression model (MLR) by using the QR algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>noint</code> : Boolean. Define if the model is computed with an intercept or not.</li></ul><p>Safe but can be little slower than other methods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
@names dat
@head dat.X
X = dat.X[:, 2:4]
y = dat.X[:, 1]
ntot = nro(X)
ntest = 30
s = samprand(ntot, ntest) 
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]

model = mlr()
#model = mlrchol()
#model = mlrpinv()
#model = mlrpinvn() 
fit!(model, Xtrain, ytrain) 
@names model
fitm = model.fitm ;
@names fitm

coef(model) 
fitm.B
fitm.int 

res = predict(model, Xtest)
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f    

model = mlr(noint = true)
fit!(model, Xtrain, ytrain) 
coef(model) 

model = mlrvec()
fit!(model, Xtrain[:, 1], ytrain) 
coef(model)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L1-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlrchol-Tuple{}"><a class="docstring-binding" href="#Jchemo.mlrchol-Tuple{}"><code>Jchemo.mlrchol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlrchol()
mlrchol(X, Y)
mlrchol(X, Y, weights::Weight)
mlrchol!mlrchol!(X::Matrix, Y::Matrix, weights::Weight)</code></pre><p>Compute a mutiple linear regression model (MLR) using the Normal equations and a Choleski factorization.</p><ul><li><code>X</code> : X-data, with nb. columns &gt;= 2 (required by function cholesky).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Only compute a model with intercept.</p><p>Faster but can be less accurate (based on squared element X&#39;X).</p><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L96-L111">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.mlrda-Tuple{}"><code>Jchemo.mlrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlrda(; kwargs...)
mlrda(X, y; kwargs...)
mlrda(X, y, weights::Weight)</code></pre><p>Discrimination based on multple linear regression (MLR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li></ul><p>The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>Then, a multiple linear regression (MLR) is run on the data {<code>X</code>, Ydummy}, returning predictions of the dummy variables   (= object <code>posterior</code> returned by fuction <code>predict</code>).  These predictions can be considered as unbounded   estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

model = mlrda()
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlrda.jl#L1-L77">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlrpinv-Tuple{}"><a class="docstring-binding" href="#Jchemo.mlrpinv-Tuple{}"><code>Jchemo.mlrpinv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlrpinv()
mlrpinv(X, Y; kwargs...)
mlrpinv(X, Y, weights::Weight; kwargs...)
mlrpinv!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Compute a mutiple linear regression model (MLR)  by using      a pseudo-inverse. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments:</p><ul><li><code>noint</code> : Boolean. Define if the model is computed with an intercept or not.</li></ul><p>Safe but can be slower.  </p><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L138-L154">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlrpinvn-Tuple{}"><a class="docstring-binding" href="#Jchemo.mlrpinvn-Tuple{}"><code>Jchemo.mlrpinvn</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlrpinvn() 
mlrpinvn(X, Y)
mlrpinvn(X, Y, weights::Weight)
mlrpinvn!mlrchol!(X::Matrix, Y::Matrix, weights::Weight)</code></pre><p>Compute a mutiple linear regression model (MLR) by using the Normal equations and a pseudo-inverse.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Safe and fast for p not too large.</p><p>Only compute a model with intercept.</p><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L191-L206">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mlrvec-Tuple{}"><a class="docstring-binding" href="#Jchemo.mlrvec-Tuple{}"><code>Jchemo.mlrvec</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mlrvec(; kwargs...)
mlrvec(X, Y; kwargs...)
mlrvec(X, Y, weights::Weight; kwargs...)
mlrvec!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Compute a simple (univariate x) linear regression model.</p><ul><li><code>x</code> : Univariate X-data (n).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments:</p><ul><li><code>noint</code> : Boolean. Define if the model is computed with an intercept or not.</li></ul><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L234-L247">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mpar"><a class="docstring-binding" href="#Jchemo.mpar"><code>Jchemo.mpar</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mpar(; kwargs...)</code></pre><p>Return a tuple with all the combinations of the parameter values defined in kwargs. Keyword arguments:</p><ul><li><code>kwargs</code> : Vector(s) of the parameter(s) values.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
nlvdis = 25 ; metric = [:mah] 
h = [1 ; 2 ; Inf] ; k = [500 ; 1000] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) 
length(pars[1])
reduce(hcat, pars)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mpar.jl#L1-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mse-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.mse-Tuple{Any, Any}"><code>Jchemo.mse</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mse(pred, Y; digits = 3)</code></pre><p>Summary of model performance for regression.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
mse(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
mse(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L454-L481">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.msep-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.msep-Tuple{Any, Any}"><code>Jchemo.msep</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">msep(pred, Y)</code></pre><p>Compute the mean of the squared prediction errors (MSEP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
msep(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
msep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L67-L94">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mweight-Tuple{Vector}"><a class="docstring-binding" href="#Jchemo.mweight-Tuple{Vector}"><code>Jchemo.mweight</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mweight(x::Vector)</code></pre><p>Return an object of type <code>Weight</code> containing vector <code>w = x / sum(x)</code> (if ad&#39;hoc building, <code>w</code> must sum to 1).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = rand(10)
w = mweight(x)
sum(w.w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_weighting.jl#L3-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.mweightcla-Tuple{AbstractVector}"><a class="docstring-binding" href="#Jchemo.mweightcla-Tuple{AbstractVector}"><code>Jchemo.mweightcla</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mweightcla(y::AbstractVector; prior::Union{Symbol, Vector} = :prop)
mweightcla(Q::DataType, y::Vector; prior::Union{Symbol, Vector} = :prop)</code></pre><p>Compute observation weights for a categorical variable, given specified sub-total weights for the classes.</p><ul><li><code>y</code> : A categorical variable (n) (class membership).</li><li><code>Q</code> : A data type (e.g. <code>Float32</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li></ul><p>Return an object of type <code>Weight</code> (see function <code>mweight</code>) containing a vector <code>w</code> (n) that sums to 1.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

y = vcat(rand([&quot;a&quot; ; &quot;c&quot;], 900), repeat([&quot;b&quot;], 100))
tab(y)
weights = mweightcla(y)
#weights = mweightcla(y; prior = :prop)
#weights = mweightcla(y; prior = [.1, .7, .2])
res = aggstat(weights.w, y; algo = sum)
[res.lev res.X]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_weighting.jl#L26-L52">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.nco-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.nco-Tuple{Any}"><code>Jchemo.nco</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">nco(X)</code></pre><p>Return the nb. columns of <code>X</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L380-L384">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.nipals-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.nipals-Tuple{Any}"><code>Jchemo.nipals</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">nipals(X; kwargs...)
nipals(X, UUt, VVt; kwargs...)</code></pre><p>Nipals to compute the first score and loading vectors of a matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>UUt</code> : Matrix (n, n) for Gram-Schmidt orthogonalization.</li><li><code>VVt</code> : Matrix (p, p) for Gram-Schmidt orthogonalization.</li></ul><p>Keyword arguments:</p><ul><li><code>tol</code> : Tolerance value for stopping    the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li></ul><p>The function finds:</p><ul><li>{u, v, sv} = argmin(||X - u * sv * v&#39;||)</li></ul><p>with the constraints:</p><ul><li>||u|| = ||v|| = 1</li></ul><p>using the alternating least squares algorithm to compute SVD (Gabriel &amp; Zalir 1979).</p><p>At the end, X ~ u * sv * v&#39;, where:</p><ul><li>u : left singular vector (u * sv = scores)</li><li>v : right singular vector (loadings)</li><li>sv : singular value.</li></ul><p>When NIPALS is used on sequentially deflated matrices, vectors u and v can loose orthogonality due to accumulation  of rounding errors. Orthogonality can be rebuilt from the Gram-Schmidt method (arguments <code>UUt</code> and <code>VVt</code>). </p><p><strong>References</strong></p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, LinearAlgebra

X = rand(5, 3)

res = nipals(X)
res.niter
res.sv
svd(X).S[1] 
res.v
svd(X).V[:, 1] 
res.u
svd(X).U[:, 1] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/nipals.jl#L1-L46">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.nipalsmiss-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.nipalsmiss-Tuple{Any}"><code>Jchemo.nipalsmiss</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">nipalsmiss(X; kwargs...)
nipalsmiss(X, UUt, VVt; kwargs...)</code></pre><p>Nipals to compute the first score and loading vectors of a matrix with missing data.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>UUt</code> : Matrix (n, n) for Gram-Schmidt orthogonalization.</li><li><code>VVt</code> : Matrix (p, p) for Gram-Schmidt orthogonalization.</li></ul><p>Keyword arguments:</p><ul><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li></ul><p>See function <code>nipals</code>. </p><p><strong>References</strong></p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.</p><p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

X = [1. 2 missing 4 ; 4 missing 6 7 ; 
    missing 5 6 13 ; missing 18 7 6 ; 
    12 missing 28 7] 

res = nipalsmiss(X)
res.niter
res.sv
res.v
res.u</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/nipalsmiss.jl#L1-L35">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.normv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.normv-Tuple{Any}"><code>Jchemo.normv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">normv(x)
normv(x, weights::Weight)</code></pre><p>Compute the norm of a vector.</p><ul><li><code>x</code> : A vector (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>The norm of vector <code>x</code> is computed by:</p><ul><li>sqrt(x&#39; * x)</li></ul><p>The weighted norm of vector <code>x</code> is computed by:</p><ul><li>sqrt(x&#39; * D * x), where D is the diagonal matrix of vector <code>weights.w</code>.</li></ul><p><strong>References</strong></p><p>@gdkrmr, https://discourse.julialang.org/t/julian-way-to-write-this-code/119348/17</p><p>@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 1000
x = rand(n)
w = mweight(ones(n))

normv(x)
sqrt(n) * normv(x, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L140-L173">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.nro-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.nro-Tuple{Any}"><code>Jchemo.nro</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">nro(X)</code></pre><p>Return the nb. rows of <code>X</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L386-L390">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.occknn-Tuple{}"><a class="docstring-binding" href="#Jchemo.occknn-Tuple{}"><code>Jchemo.occknn</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">occknn(; kwargs...)
occknn(X; kwargs...)</code></pre><p>One-class classification using kNN distance-based outlierness.</p><ul><li><code>X</code> : Training X-data (n, p) assumed to represent the reference class.</li></ul><p>Keyword arguments:</p><ul><li><code>nsamp</code> : Nb. of observations (<code>X</code>-rows) sampled in the training data and for which are computed    the outliernesses (stimated outlierness distribution of the reference class).</li><li><code>metric</code> : Metric used to compute the distances. See function <code>getknn</code>.</li><li><code>k</code> : Nb. nearest neighbors to consider.</li><li><code>algo</code> : Function summarizing the <code>k</code> distances to the neighbors.</li><li><code>cut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>See functions:</p><ul><li><code>outknn</code> for details on the outlierness computation method,</li><li>and <code>occsd</code> for details on the the cutoff computation and the outputs.</li></ul><p>For <strong>predictions</strong> (<code>predict</code>), the outlierness of each new observation is compared to the outlierness  distribution estimated from the <code>nsamp</code> sampled observations. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
@names dat
X = dat.X    
Y = dat.Y
model = savgol(npoint = 21, deriv = 2, degree = 3)
fit!(model, X) 
Xp = transf(model, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Build the example data
## - cla_train is the reference class (= &#39;in&#39;), &quot;EHH&quot; 
cla_train = &quot;EHH&quot;
s = Ytrain.typ .== cla_train
Xtrain_fin = Xtrain[s, :]    
ntrain = nro(Xtrain_fin)
## cla_test contains the observations to be predicted (i.e. to be &#39;in&#39; or &#39;out&#39; of cla_train), 
## a mix of &quot;EEH&quot; and &quot;PEE&quot; 
cla_test1 = &quot;EHH&quot;   # should be predicted &#39;in&#39;
s = Ytest.typ .== cla_test1
Xtest_fin1 = Xtest[s, :] 
ntest1 = nro(Xtest_fin1)
##
cla_test2 = &quot;PEE&quot;   # should be predicted &#39;out&#39;
s = Ytest.typ .== cla_test2
Xtest_fin2 = Xtest[s, :] 
ntest2 = nro(Xtest_fin2)
##
Xtest_fin = vcat(Xtest_fin1, Xtest_fin2)
## Only used to compute error rates
ytrain_fin = repeat([&quot;in&quot;], ntrain)
ytest_fin = [repeat([&quot;in&quot;], ntest1); repeat([&quot;out&quot;], ntest2)]
y_fin = vcat(ytrain_fin, ytest_fin)
## 
ntot = ntrain + ntest1 + ntest2
(ntot = ntot, ntrain, ntest1, ntest2)

## Data description
nlv = 10
model = pcasvd(; nlv) 
fit!(model, Xtrain_fin) 
Ttrain = model.fitm.T
Ttest = transf(model, Xtest_fin)
T = vcat(Ttrain, Ttest)
i = 1
group = vcat(repeat([&quot;Train-EHH&quot;], ntrain), repeat([&quot;Test-EHH&quot;], ntest1), repeat([&quot;Test-PEE&quot;], ntest2))
color = [:red, :blue, (:green, .5)]
plotxy(T[:, i], T[:, i + 1], group; color = color, leg_title = &quot;Type of obs.&quot;, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Training
nsamp = 150 ; k = 5 ; cri = 2.5
model = occknn(; nsamp, k, cri)
#model = occlknn(; nsamp, k = 10, cri)
fit!(model, Xtrain_fin) 
@names model 
fitm = model.fitm ;
@names fitm 
@head dtrain = fitm.d
fitm.cutoff
d = dtrain.dstand
f, ax = plotxy(1:length(d), d; color = (:green, .5), size = (500, 300), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f
## Prediction of Test
res = predict(model, Xtest_fin) 
@names res
@head pred = res.pred
@head dtest = res.d
tab(pred)
errp(pred, ytest_fin)
conf(pred, ytest_fin).cnt
##
d = vcat(dtrain.dstand, dtest.dstand)
group = vcat(repeat([&quot;Train&quot;], nsamp), repeat([&quot;Test-EHH&quot;], ntest1), repeat([&quot;Test-PEE&quot;], ntest2))
color = [:red, :blue, (:green, .5)]
f, ax = plotxy(1:length(d), d, group; color = color, size = (500, 300), leg_title = &quot;Type of obs.&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occknn.jl#L1-L115">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.occlknn-Tuple{}"><a class="docstring-binding" href="#Jchemo.occlknn-Tuple{}"><code>Jchemo.occlknn</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">occlknn(; kwargs...)
occlknn(X; kwargs...)</code></pre><p>One-class classification using local kNN distance-based outlierness.</p><ul><li><code>X</code> : Training X-data (n, p) assumed to represent the reference class.</li></ul><p>Keyword arguments:</p><ul><li><code>nsamp</code> : Nb. of observations (<code>X</code>-rows) sampled in the training data and for which are computed    the outliernesses (stimated outlierness distribution of the reference class).</li><li><code>metric</code> : Metric used to compute the distances. See function <code>getknn</code>.</li><li><code>k</code> : Nb. nearest neighbors to consider.</li><li><code>algo</code> : Function summarizing the <code>k</code> distances to the neighbors.</li><li><code>cut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>See functions:</p><ul><li><code>occknn</code> for examples,</li><li><code>outlknn</code> for details on the outlierness computation method,</li><li>and <code>occsd</code> for details on the the cutoff computation and the outputs.</li></ul><p>For <strong>predictions</strong> (<code>predict</code>), the outlierness of each new observation is compared to the outlierness  distribution estimated from the <code>nsamp</code> sampled observations. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occlknn.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.occod-Tuple{}"><a class="docstring-binding" href="#Jchemo.occod-Tuple{}"><code>Jchemo.occod</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">occod(; kwargs...)
occod(fitm, X; kwargs...)</code></pre><p>One-class classification using PCA/PLS orthognal distance (OD).</p><ul><li><code>fitm</code> : The preliminary model (e.g. object <code>fitm</code> returned by function <code>pcasvd</code>) that was fitted on    the training data assumed to represent the reference class.</li><li><code>X</code> : Training X-data (n, p), on which was fitted the model <code>fitm</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>cut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of an observation is the orthogonal distance (=  &#39;X-residuals&#39;) of this  observation, ie. the Euclidean distance between the observation and its projection to the score plan defined by  the fitted (e.g. PCA) model (e.g. Hubert et al. 2005, Van Branden &amp; Hubert 2005 p. 66, Varmuza &amp; Filzmoser  2009 p. 79).</p><p>See function <code>occsd</code> for details on the cutoff computation and the outputs.</p><p><strong>References</strong></p><p>M. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p>K. Vanden Branden, M. Hubert (2005). Robuts classification in high dimension based on the SIMCA method.  Chem. Lab. Int. Syst, 79, 10-21.</p><p>K. Varmuza, V. Filzmoser (2009). Introduction to multivariate statistical analysis in chemometrics.  CRC Press, Boca Raton.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
@names dat
X = dat.X    
Y = dat.Y
model = savgol(npoint = 21, deriv = 2, degree = 3)
fit!(model, X) 
Xp = transf(model, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Build the example data
## - cla_train is the reference class (= &#39;in&#39;), &quot;EHH&quot; 
cla_train = &quot;EHH&quot;
s = Ytrain.typ .== cla_train
Xtrain_fin = Xtrain[s, :]    
ntrain = nro(Xtrain_fin)
## cla_test contains the observations to be predicted (i.e. to be &#39;in&#39; or &#39;out&#39; of cla_train), 
## a mix of &quot;EEH&quot; and &quot;PEE&quot; 
cla_test1 = &quot;EHH&quot;   # should be predicted &#39;in&#39;
s = Ytest.typ .== cla_test1
Xtest_fin1 = Xtest[s, :] 
ntest1 = nro(Xtest_fin1)
##
cla_test2 = &quot;PEE&quot;   # should be predicted &#39;out&#39;
s = Ytest.typ .== cla_test2
Xtest_fin2 = Xtest[s, :] 
ntest2 = nro(Xtest_fin2)
##
Xtest_fin = vcat(Xtest_fin1, Xtest_fin2)
## Only used to compute error rates
ytrain_fin = repeat([&quot;in&quot;], ntrain)
ytest_fin = [repeat([&quot;in&quot;], ntest1); repeat([&quot;out&quot;], ntest2)]
y_fin = vcat(ytrain_fin, ytest_fin)
## 
ntot = ntrain + ntest1 + ntest2
(ntot = ntot, ntrain, ntest1, ntest2)

#### Preliminary PCA fitted model
nlv = 15
model0 = pcasvd(; nlv) 
#model0 = pcaout(; nlv) 
fit!(model0, Xtrain_fin) 
res = summary(model0, Xtrain_fin).explvarx 
plotgrid(res.nlv, res.pvar; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;% Variance explained&quot;).f
Ttrain = model0.fitm.T
Ttest = transf(model0, Xtest_fin)
T = vcat(Ttrain, Ttest)
i = 1
group = vcat(repeat([&quot;Train-EHH&quot;], ntrain), repeat([&quot;Test-EHH&quot;], ntest1), repeat([&quot;Test-PEE&quot;], ntest2))
color = [:red, :blue, (:green, .5)]
plotxy(T[:, i], T[:, i + 1], group; color = color, leg_title = &quot;Type of obs.&quot;, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Training
model = occod(; cri = 2.5)
#model = occod(cut = :mad, cri = 4)
#model = occod(cut = :q, risk = .01)
#model = occsdod(; cri = 2.5)
fit!(model, model0.fitm, Xtrain_fin) 
@names model 
fitm = model.fitm ;
@names fitm 
@head dtrain = fitm.d
#fitm.cutoff
d = dtrain.dstand
f, ax = plotxy(1:length(d), d; color = (:green, .5), size = (500, 300), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f
## Prediction of Test
res = predict(model, Xtest_fin) 
@names res
@head pred = res.pred
@head dtest = res.d
tab(pred)
errp(pred, ytest_fin)
conf(pred, ytest_fin).cnt
##
d = vcat(dtrain.dstand, dtest.dstand)
color = [:red, :blue, (:green, .5)]
f, ax = plotxy(1:length(d), d, group; color = color, size = (500, 300), leg_title = &quot;Type of obs.&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occod.jl#L1-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.occsd-Tuple{}"><a class="docstring-binding" href="#Jchemo.occsd-Tuple{}"><code>Jchemo.occsd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">occsd(; kwargs...)
occsd(fitm; kwargs...)</code></pre><p>One-class classification using PCA/PLS score distance (SD).</p><ul><li><code>fitm</code> : The preliminary model (e.g. object <code>fitm</code> returned by function <code>pcasvd</code>) that was fitted on    the training data assumed to represent the reference class.</li></ul><p>Keyword arguments:</p><ul><li><code>cut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of an observation is defined by its score distance (SD), ie. the Mahalanobis  distance between the projection of the observation on the score plan defined by the fitted (e.g. PCA) model and the  &quot;center&quot; (always defined by zero) of the score plan.</p><p>If a new observation has <code>d</code> higher than a given <code>cutoff</code>, the observation is assumed to not belong to the training  (= reference) class. The <code>cutoff</code> is computed with non-parametric heuristics. Noting [d] the vector of outliernesses  computed on the training class:</p><ul><li>If <code>cut</code> = <code>:mad</code>, then <code>cutoff</code> = MED([d]) + <code>cri</code> * MAD([d]). </li><li>If <code>cut</code> = <code>:q</code>, then <code>cutoff</code> is estimated from the empirical cumulative density function    computed on [d], for a given risk-I (<code>risk</code>).</li></ul><p>Alternative approximate cutoffs have been proposed in the literature (e.g.: Nomikos &amp; MacGregor 1995, Hubert et al. 2005, Pomerantsev 2008). Typically, and whatever the approximation method used to compute the cutoff, it is recommended to tune  this cutoff depending on the detection objectives. </p><p><strong>Outputs</strong></p><ul><li><code>pval</code>: Estimate of p-value (see functions <code>pval</code>) computed from the training distribution [d]. </li><li><code>dstand</code>: standardized distance defined as <code>d</code> / <code>cutoff</code>. A value <code>dstand</code> &gt; 1 may be considered as extreme    compared to the distribution of the training data.  </li><li><code>gh</code> is the Winisi &quot;GH&quot; (usually, GH &gt; 3 is considered as extreme).</li></ul><p>Specific for function <code>predict</code>:</p><ul><li><code>pred</code>: class prediction<ul><li><code>dstand</code> &lt;= 1 ==&gt; <code>in</code>: the observation is expected to belong to the training class, </li><li><code>dstand</code> &gt; 1  ==&gt; <code>out</code>: extreme value, possibly not belonging to the same class as the training. </li></ul></li></ul><p><strong>References</strong></p><p>M. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p>Nomikos, V., MacGregor, J.F., 1995. Multivariate SPC Charts for Monitoring Batch Processes. null 37, 41-59.  https://doi.org/10.1080/00401706.1995.10485888</p><p>Pomerantsev, A.L., 2008. Acceptance areas for multivariate classification derived by projection methods.  Journal of Chemometrics 22, 601-609. https://doi.org/10.1002/cem.1147</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
@names dat
X = dat.X    
Y = dat.Y
model = savgol(npoint = 21, deriv = 2, degree = 3)
fit!(model, X) 
Xp = transf(model, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Build the example data
## - cla_train is the reference class (= &#39;in&#39;), &quot;EHH&quot; 
cla_train = &quot;EHH&quot;
s = Ytrain.typ .== cla_train
Xtrain_fin = Xtrain[s, :]    
ntrain = nro(Xtrain_fin)
## cla_test contains the observations to be predicted (i.e. to be &#39;in&#39; or &#39;out&#39; of cla_train), 
## a mix of &quot;EEH&quot; and &quot;PEE&quot; 
cla_test1 = &quot;EHH&quot;   # should be predicted &#39;in&#39;
s = Ytest.typ .== cla_test1
Xtest_fin1 = Xtest[s, :] 
ntest1 = nro(Xtest_fin1)
##
cla_test2 = &quot;PEE&quot;   # should be predicted &#39;out&#39;
s = Ytest.typ .== cla_test2
Xtest_fin2 = Xtest[s, :] 
ntest2 = nro(Xtest_fin2)
##
Xtest_fin = vcat(Xtest_fin1, Xtest_fin2)
## Only used to compute error rates
ytrain_fin = repeat([&quot;in&quot;], ntrain)
ytest_fin = [repeat([&quot;in&quot;], ntest1); repeat([&quot;out&quot;], ntest2)]
y_fin = vcat(ytrain_fin, ytest_fin)
## 
ntot = ntrain + ntest1 + ntest2
(ntot = ntot, ntrain, ntest1, ntest2)

#### Preliminary PCA fitted model
nlv = 15
model0 = pcasvd(; nlv) 
#model0 = pcaout(; nlv) 
fit!(model0, Xtrain_fin) 
res = summary(model0, Xtrain_fin).explvarx 
plotgrid(res.nlv, res.pvar; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;% Variance explained&quot;).f
Ttrain = model0.fitm.T
Ttest = transf(model0, Xtest_fin)
T = vcat(Ttrain, Ttest)
i = 1
group = vcat(repeat([&quot;Train-EHH&quot;], ntrain), repeat([&quot;Test-EHH&quot;], ntest1), repeat([&quot;Test-PEE&quot;], ntest2))
color = [:red, :blue, (:green, .5)]
plotxy(T[:, i], T[:, i + 1], group; color = color, leg_title = &quot;Type of obs.&quot;, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Training
model = occsd(; cri = 2.5)
#model = occsd(cut = :mad, cri = 4)
#model = occsd(cut = :q, risk = .01)
fit!(model, model0.fitm) 
@names model 
fitm = model.fitm ;
@names fitm 
@head dtrain = fitm.d
fitm.cutoff
d = dtrain.dstand
f, ax = plotxy(1:length(d), d; color = (:green, .5), size = (500, 300), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f
## Prediction of Test
res = predict(model, Xtest_fin) 
@names res
@head pred = res.pred
@head dtest = res.d
tab(pred)
errp(pred, ytest_fin)
conf(pred, ytest_fin).cnt
##
d = vcat(dtrain.dstand, dtest.dstand)
color = [:red, :blue, (:green, .5)]
f, ax = plotxy(1:length(d), d, group; color = color, size = (500, 300), leg_title = &quot;Type of obs.&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occsd.jl#L1-L139">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.occsdod-Tuple{}"><a class="docstring-binding" href="#Jchemo.occsdod-Tuple{}"><code>Jchemo.occsdod</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">occsdod(; kwargs...)
occsdod(object, X; kwargs...)</code></pre><p>One-class classification using a consensus between PCA/PLS score and orthogonal distances (SD and OD).</p><ul><li><code>fitm</code> : The preliminary model (e.g. object <code>fitm</code> returned by function <code>pcasvd</code>) that was fitted on    the training data assumed to represent the reference class.</li><li><code>X</code> : Training X-data (n, p), on which was fitted the model <code>fitm</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>cut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is a consensus between the score distance (SD) and the orthogonal distance (OD). The consensus is computed from the standardized distances by: </p><ul><li><code>dstand</code> = sqrt(<code>dstand_sd</code> * <code>dstand_od</code>).</li></ul><p>See functions:</p><ul><li><code>occsd</code> for details on the cutoff computation and the outputs,</li><li>and <code>occod</code> for examples.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occsdod.jl#L1-L20">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.occstah-Tuple{}"><a class="docstring-binding" href="#Jchemo.occstah-Tuple{}"><code>Jchemo.occstah</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">occstah(; kwargs...)
occstah(X; kwargs...)</code></pre><p>One-class classification using the Stahel-Donoho outlierness.</p><ul><li><code>X</code> : Training X-data (n, p) assumed to represent the reference class.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. random directions on which <code>X</code> is projected. </li><li><code>cut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled such as in function <code>outstah</code>.</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is the Stahel-Donoho outlierness (see function <code>outstah</code>).</p><p>See function <code>occsd</code> for details on the outputs.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
@names dat
X = dat.X    
Y = dat.Y
model = savgol(npoint = 21, deriv = 2, degree = 3)
fit!(model, X) 
Xp = transf(model, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Build the example data
## - cla_train is the reference class (= &#39;in&#39;), &quot;EHH&quot; 
cla_train = &quot;EHH&quot;
s = Ytrain.typ .== cla_train
Xtrain_fin = Xtrain[s, :]    
ntrain = nro(Xtrain_fin)
## cla_test contains the observations to be predicted (i.e. to be &#39;in&#39; or &#39;out&#39; of cla_train), 
## a mix of &quot;EEH&quot; and &quot;PEE&quot; 
cla_test1 = &quot;EHH&quot;   # should be predicted &#39;in&#39;
s = Ytest.typ .== cla_test1
Xtest_fin1 = Xtest[s, :] 
ntest1 = nro(Xtest_fin1)
##
cla_test2 = &quot;PEE&quot;   # should be predicted &#39;out&#39;
s = Ytest.typ .== cla_test2
Xtest_fin2 = Xtest[s, :] 
ntest2 = nro(Xtest_fin2)
##
Xtest_fin = vcat(Xtest_fin1, Xtest_fin2)
## Only used to compute error rates
ytrain_fin = repeat([&quot;in&quot;], ntrain)
ytest_fin = [repeat([&quot;in&quot;], ntest1); repeat([&quot;out&quot;], ntest2)]
y_fin = vcat(ytrain_fin, ytest_fin)
## 
ntot = ntrain + ntest1 + ntest2
(ntot = ntot, ntrain, ntest1, ntest2)

## Data description
nlv = 10
model = pcasvd(; nlv) 
fit!(model, Xtrain_fin) 
Ttrain = model.fitm.T
Ttest = transf(model, Xtest_fin)
T = vcat(Ttrain, Ttest)
i = 1
group = vcat(repeat([&quot;Train-EHH&quot;], ntrain), repeat([&quot;Test-EHH&quot;], ntest1), repeat([&quot;Test-PEE&quot;], ntest2))
color = [:red, :blue, (:green, .5)]
plotxy(T[:, i], T[:, i + 1], group; color = color, leg_title = &quot;Type of obs.&quot;, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Training
model = occstah(; nlv = 5000, cri = 2, scal = true)
fit!(model, Xtrain_fin) 
@names model 
fitm = model.fitm ;
@names fitm 
@head fitm.V  # random projection directions 
@head dtrain = fitm.d
d = dtrain.dstand
f, ax = plotxy(1:length(d), d; color = (:green, .5), size = (500, 300), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f
## Prediction of Test
res = predict(model, Xtest_fin) ;
@names res
@head dtest = res.d
@head res.pred
tab(res.pred)
errp(res.pred, ytest_fin)
conf(res.pred, ytest_fin).cnt
##
d = vcat(dtrain.dstand, dtest.dstand)
color = [:red, :blue, (:green, .5)]
f, ax = plotxy(1:length(d), d, group; color = color, size = (500, 300), leg_title = &quot;Type of obs.&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occstah.jl#L1-L105">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.out-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.out-Tuple{Any, Any}"><code>Jchemo.out</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">out(x)</code></pre><p>Return if elements of a vector are strictly outside of a given range.</p><ul><li><code>x</code> : Univariate data.</li><li><code>y</code> : Univariate data on which is computed the range (min, max).</li></ul><p>Return a BitVector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = [-200.; -100; -1; 0; 1; 200]
out(x, [-1; .2; 1])
out(x, (-1, 1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L392-L409">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outeucl-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.outeucl-Tuple{Any}"><code>Jchemo.outeucl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outeucl(X; scal = false)
outeucl!(X::Matrix; scal = false)</code></pre><p>Compute outlierness from Euclidean distances to center.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its MAD before computing the outlierness.</li></ul><p>Outlyingness is calculated by the Euclidean distance between the observation (rows of <code>X</code>) and a robust estimate  of the center of the data (in the present function, the spatial median). Such outlyingness was for instance used in the robust  PLSR algorithm of Serneels et al. 2005 (PRM). </p><p><strong>References</strong></p><p>Serneels, S., Croux, C., Filzmoser, V., Van Espen, V.J., 2005. Partial robust M-regression.  Chemometrics and Intelligent Laboratory Systems 79, 55-64. https://doi.org/10.1016/j.chemolab.2005.04.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie
n = 300 ; p = 700 ; m = 80
ntot = n + m
X1 = randn(n, p)
X2 = randn(m, p) .+ rand(1:3, p)&#39;
X = vcat(X1, X2)

scal = false
#scal = true
res = outeucl(X; scal) ;
@names res
res.d    # outlierness 
plotxy(1:ntot, res.d).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outeucl.jl#L1-L33">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outknn-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.outknn-Tuple{Any}"><code>Jchemo.outknn</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outknn(X; metric = :eucl, k, algo = sum, scal::Bool = false)
outknn!(X::Matrix; metric = :eucl, k, algo = sum, scal::Bool = false)</code></pre><p>Compute a kNN distance-based outlierness.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Metric used to compute the distances. See function <code>getknn</code>.</li><li><code>k</code> : Nb. nearest neighbors to consider.</li><li><code>algo</code> : Function summarizing the <code>k</code> distances to the neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled before computing the outlierness.</li></ul><p>For each observation (row of <code>X</code>), the outlierness is defined by a summary (e.g. by sum or maximum) of the distances  between the observation and its <code>k</code> nearest neighbors. </p><p><strong>References</strong></p><p>Angiulli, F., Pizzuti, C., 2005. Outlier mining in large high-dimensional data sets. IEEE Transactions on Knowledge  and Data Engineering 17, 203–215. https://doi.org/10.1109/TKDE.2005.31</p><p>Angiulli, F., Basta, S., Pizzuti, C., 2006. Distance-based detection and prediction of outliers. IEEE Transactions  on Knowledge and Data Engineering 18, 145–160. https://doi.org/10.1109/TKDE.2006.29</p><p>Campos, G.O., Zimek, A., Sander, J., Campello, R.J.G.B., Micenková, B., Schubert, E., Assent, I., Houle, M.E., 2016.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. Data Min Knowl  Disc 30, 891–927. https://doi.org/10.1007/s10618-015-0444-8</p><p>Ramaswamy, S., Rastogi, R., Shim, K., 2000. Efficient algorithms for mining outliers from large data sets,  in: Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, SIGMOD ’00.  Association for Computing Machinery, New York, NY, USA, pp. 427–438. https://doi.org/10.1145/342009.335437</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;)
@load db dat
X = dat.X
wlst = names(X)
wl = parse.(Float64, wlst)
n, p = size(X)
## Six of the samples (25, 26, and 36-39) contain added alcohol.
s = [25; 26; 36:39]
typ = zeros(Int, n)
typ[s] .= 1
#plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

metric = :eucl ; k = 15 ; algo = sum
#algo = maximum
res = outknn(X; metric, k, algo) ;
@names res
f, ax = plotxy(1:n, res.d, typ, xlabel = &quot;Obs. index&quot;, ylabel = &quot;Outlierness&quot;)
text!(ax, 1:n, res.d; text = string.(1:n), fontsize = 10)
f

## With a preliminary PCA
nlv = 3
model = pcasph(; nlv)
fit!(model, X)
T = model.fitm.T
metric = :eucl 
k = 15
res = outknn(T; metric, k, scal = true)
plotxy(1:n, res.d, typ, xlabel = &quot;Obs. index&quot;, ylabel = &quot;Outlierness&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outknn.jl#L1-L64">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outlknn-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.outlknn-Tuple{Any}"><code>Jchemo.outlknn</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outlknn(X; metric = :eucl, k, algo = sum, scal::Bool = false)
outlknn!(X::Matrix; metric = :eucl, k, algo = sum, scal::Bool = false)</code></pre><p>Compute a local kNN distance-based outlierness.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Metric used to compute the distances. See function <code>getknn</code>.</li><li><code>k</code> : Nb. nearest neighbors to consider.</li><li><code>algo</code> : Function summarizing the distances to the neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled before computing the outlierness.</li></ul><p>The idea is to compare the KNN-outlierness of the observation to the KNN-outlierness of its neighbors, giving a local  measure of outlierness. For each observation (row of <code>X</code>), the outlierness is defined as folloxs:</p><ul><li>A summary (e.g. by sum) of the distances between the observation and its <code>k</code> nearest neighbors   is computed, say out1.</li><li>The same summary is computed for each of the <code>k</code> nearest neighbors of the observation, and the median of    the <code>k</code> returned values is computed, say out2.</li><li>The outlierness of the observation is finally defined as the ratio out1 / out2.</li></ul><p>The approach can be seen as a simplification of the local outlier factor (LOF) method (Breunig et al. 2000), such as the Simplified-LOF method (Schubert et al 2014 p.206, Campos et al. 2016 p.896) where local density  is estimated by the inverse of the k-distance.</p><p><strong>References</strong></p><p>Campos, G.O., Zimek, A., Sander, J., Campello, R.J.G.B., Micenková, B., Schubert, E., Assent, I., Houle, M.E., 2016.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. Data Min Knowl  Disc 30, 891–927. https://doi.org/10.1007/s10618-015-0444-8</p><p>Schubert, E., Zimek, A., Kriegel, H.-P., 2014. Local outlier detection reconsidered: a generalized view on  locality with applications to spatial, video, and network outlier detection. Data Min Knowl Disc 28, 190–237.  https://doi.org/10.1007/s10618-012-0300-z</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;)
@load db dat
X = dat.X
wlst = names(X)
wl = parse.(Float64, wlst)
n, p = size(X)
## Six of the samples (25, 26, and 36-39) contain added alcohol.
s = [25; 26; 36:39]
typ = zeros(Int, n)
typ[s] .= 1
#plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

metric = :eucl ; k = 15 ; algo = sum
#algo = maximum
res = outlknn(X; metric, k, algo) ;
@names res
f, ax = plotxy(1:n, res.d, typ, xlabel = &quot;Obs. index&quot;, ylabel = &quot;Outlierness&quot;)
text!(ax, 1:n, res.d; text = string.(1:n), fontsize = 10)
f

## With a preliminary PCA
nlv = 3
model = pcasph(; nlv)
fit!(model, X)
T = model.fitm.T
metric = :eucl 
k = 15
res = outlknn(T; metric, k, scal = true)
plotxy(1:n, res.d, typ, xlabel = &quot;Obs. index&quot;, ylabel = &quot;Outlierness&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outlknn.jl#L1-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outod-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.outod-Tuple{Any, Any}"><code>Jchemo.outod</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outod(fitm, X)</code></pre><p>Compute outlierness from PCA/PLS orthogonal distance (OD).</p><ul><li><code>fitm</code> : The preliminary model (e.g. object <code>fitm</code> returned by function <code>pcasvd</code>) that was fitted on    the data.</li><li><code>X</code> : X-data (n, p) on which was fitted the model <code>fitm</code>.</li></ul><p>In this method, the outlierness <code>d</code> of an observation is the orthogonal distance (=  &#39;X-residuals&#39;) of this  observation, ie. the Euclidean distance between the observation and its projection to the score plan defined by  the fitted (e.g. PCA) model (e.g. Hubert et al. 2005, Van Branden &amp; Hubert 2005 p. 66, Varmuza &amp; Filzmoser  2009 p. 79).</p><p><strong>References</strong></p><p>M. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p>K. Vanden Branden, M. Hubert (2005). Robuts classification in high dimension based on the SIMCA method.  Chem. Lab. Int. Syst, 79, 10-21.</p><p>K. Varmuza, V. Filzmoser (2009). Introduction to multivariate statistical analysis in chemometrics.  CRC Press, Boca Raton.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;)
@load db dat
X = dat.X
wlst = names(X)
wl = parse.(Float64, wlst)
n, p = size(X)
## Six of the samples (25, 26, and 36-39) contain added alcohol.
s = [25; 26; 36:39]
typ = zeros(Int, n)
typ[s] .= 1
#plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

model = pcaout(; nlv = 3)
fit!(model, X) 
fitm = model.fitm ;
res = outsd(fitm) ;
#res = outsdod(fitm, X) ;
@names res
f, ax = plotxy(1:n, res.d, typ, xlabel = &quot;Obs. index&quot;, ylabel = &quot;Outlierness&quot;)
text!(ax, 1:n, res.d; text = string.(1:n), fontsize = 10)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outod.jl#L1-L49">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outsd-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.outsd-Tuple{Any}"><code>Jchemo.outsd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outsd(fitm)</code></pre><p>Compute outlierness from PCA/PLS score distance (SD).</p><ul><li><code>fitm</code> : The preliminary model (e.g. object <code>fitm</code> returned by function <code>pcasvd</code>) that was fitted on    the data.</li></ul><p>In this method, the outlierness <code>d</code> of an observation is defined by its score distance (SD), ie. the Mahalanobis  distance between the projection of the observation on the score plan defined by the fitted (e.g. PCA) model and the  &quot;center&quot; (always defined by zero) of the score plan.</p><p><strong>References</strong></p><p>M. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;)
@load db dat
X = dat.X
wlst = names(X)
wl = parse.(Float64, wlst)
n, p = size(X)
## Six of the samples (25, 26, and 36-39) contain added alcohol.
s = [25; 26; 36:39]
typ = zeros(Int, n)
typ[s] .= 1
#plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

model = pcaout(; nlv = 3)
fit!(model, X) 
fitm = model.fitm ;
res = outsd(fitm) ;
@names res
f, ax = plotxy(1:n, res.d, typ, xlabel = &quot;Obs. index&quot;, ylabel = &quot;Outlierness&quot;)
text!(ax, 1:n, res.d; text = string.(1:n), fontsize = 10)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outsd.jl#L1-L40">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outsdod-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.outsdod-Tuple{Any, Any}"><code>Jchemo.outsdod</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outsdod(fitm, X; cut = :mad, cri = 3, risk = .025)</code></pre><p>Compute outlierness from PCA/PLS score and orthogonal distances (SD and OD).</p><ul><li><code>fitm</code> : The preliminary model (e.g. object <code>fitm</code> returned by function <code>pcasvd</code>) that was fitted on    the data.</li><li><code>X</code> : X-data (n, p) on which was fitted the model <code>fitm</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>cut</code> : Type of cutoff to standardize SD and OD. Possible values are: <code>:mad</code>, <code>:q</code>. See Thereafter.</li><li><code>cri</code> : When <code>cut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>cut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is a consensus between the standardized score and orthogonal distances. The returned consensus is computed by: </p><ul><li><code>d</code> = sqrt(SD<em>stand * OD</em>stand)</li></ul><p>where:</p><ul><li>SD<em>stand = SD / cutoff</em>SD</li><li>OD<em>stand = OD / cutoff</em>OD</li></ul><p>The cutoff is computed with non-parametric heuristics. Noting [d] the SD- or OD-vector:</p><ul><li>If <code>cut</code> = <code>:mad</code>, then cutoff = MED([d]) + <code>cri</code> * MAD([d]). </li><li>If <code>cut</code> = <code>:q</code>, then cutoff is estimated from the empirical cumulative density function computed on [d],    for a given risk-I (<code>risk</code>).</li></ul><p>See function <code>outod</code> for examples.</p><p><strong>References</strong></p><p>M. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p>K. Vanden Branden, M. Hubert (2005). Robuts classification in high dimension based on the SIMCA method.  Chem. Lab. Int. Syst, 79, 10-21.</p><p>K. Varmuza, V. Filzmoser (2009). Introduction to multivariate statistical analysis in chemometrics.  CRC Press, Boca Raton.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outsdod.jl#L1-L35">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.outstah-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.outstah-Tuple{Any, Any}"><code>Jchemo.outstah</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">outstah(X, V; scal = false)
outstah!(X::Matrix, V::Matrix; scal = false)</code></pre><p>Compute the Stahel-Donoho outlierness.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>V</code> : A projection matrix (p, nlv) representing the directions of the projection pursuit.</li></ul><p>Keyword arguments:</p><ul><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its MAD before computing the outlierness.</li></ul><p>See Maronna and Yohai 1995 for details on the outlierness measure. </p><p>A projection-pursuit approach is used: given a projection matrix <code>V</code> (p, nlv) (in general built randomly),  the observations (rows of <code>X</code>) are projected on the <code>nlv</code> directions and the Stahel-Donoho outlierness is computed  for each observation from these projections.</p><p><strong>References</strong></p><p>Maronna, R.A., Yohai, V.J., 1995. The Behavior of the Stahel-Donoho Robust Multivariate Estimator.  Journal of the American Statistical Association 90, 330–341. https://doi.org/10.1080/01621459.1995.10476517</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie

n = 300 ; p = 700 ; m = 80
ntot = n + m
X1 = randn(n, p)
X2 = randn(m, p) .+ rand(1:3, p)&#39;
X = vcat(X1, X2)

nlv = 100
V = rand(0:1, p, nlv)
scal = false
#scal = true
res = outstah(X, V; scal) ;
@names res
res.d    # outlierness 
plotxy(1:ntot, res.d).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/outstah.jl#L1-L39">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.parsemiss-Tuple{Any, Vector{Union{Missing, String}}}"><a class="docstring-binding" href="#Jchemo.parsemiss-Tuple{Any, Vector{Union{Missing, String}}}"><code>Jchemo.parsemiss</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">parsemiss(Q, x::Vector{Union{String, Missing}})</code></pre><p>Parsing a string vector containing missing data.</p><ul><li><code>Q</code> : Type that results from the parsing of type `String&#39;. </li><li><code>x</code> : A string vector containing observations <code>missing</code> (of type <code>Missing</code>).</li></ul><p>See examples.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = [&quot;1&quot;; &quot;3.2&quot;; missing]
x_p = parsemiss(Float64, x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L204-L220">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcaeigen-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcaeigen-Tuple{}"><code>Jchemo.pcaeigen</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcaeigen(; kwargs...)
pcaeigen(X; kwargs...)
pcaeigen(X, weights::Weight; kwargs...)
pcaeigen!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by Eigen factorization.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * V&#39;||^2  in metric D, by computing an Eigen factorization of X&#39; * D * X. </p><p>See function <code>pcasvd</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcaeigen.jl#L1-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcaeigenk-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcaeigenk-Tuple{}"><code>Jchemo.pcaeigenk</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcaeigenk(; kwargs...)
pcaeigenk(X; kwargs...)
pcaeigenk(X, weights::Weight; kwargs...)
pcaeigenk!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by Eigen factorization of the kernel matrix XX&#39;.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>This is the &quot;kernel cross-product&quot; version of the PCA algorithm (e.g. Wu et al. 1997). For wide matrices (n &lt;&lt; p,  where p is the nb. columns) and n not too large, this algorithm can be much faster than the others.</p><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * V&#39;||^2  in metric D, by computing an Eigen factorization of D^(1/2) * X * X&#39; D^(1/2).</p><p>See function <code>pcasvd</code> for examples.</p><p><strong>References</strong></p><p>Wu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA algorithms for wide data. Part I: Theory and algorithms.  Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcaeigenk.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcanipals-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcanipals-Tuple{}"><code>Jchemo.pcanipals</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcanipals(; kwargs...)
pcanipals(X; kwargs...)
pcanipals(X, weights::Weight; kwargs...)
pcanipals!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by NIPALS algorithm.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt orthogonalization of the scores and loadings is done   before each X-deflation. </li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * V&#39;||^2  in metric D by NIPALS. </p><p>See function <code>pcasvd</code> for examples.</p><p><strong>References</strong></p><p>Andrecut, M., 2009. Parallel GPU Implementation of Iterative PCA Algorithms. Journal of Computational Biology 16, 1593-1599.  https://doi.org/10.1089/cmb.2008.0221</p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.</p><p>Gabriel, R. K., 2002. Le biplot - Outil d&#39;exploration de données multidimensionnelles. Journal de la Société Française  de la Statistique, 143, 5-55.</p><p>Lingen, F.J., 2000. Efficient Gram-Schmidt orthonormalisation on parallel computers. Communications in Numerical Methods  in Engineering 16, 57-66. https://doi.org/10.1002/(SICI)1099-0887(200001)16:1&lt;57::AID-CNM320&gt;3.0.CO;2-I</p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.</p><p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcanipals.jl#L1-L39">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcanipalsmiss-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcanipalsmiss-Tuple{}"><code>Jchemo.pcanipalsmiss</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcanipalsmiss(; kwargs...)
pcanipals(X; kwargs...)
pcanipals(X, weights::Weight; kwargs...)
pcanipals!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by NIPALS algorithm allowing missing data.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt orthogonalization of the scores and loadings is done   before each X-deflation. </li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p><strong>References</strong></p><p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1 2. missing 4 ; 4 missing 6 7 ; 
    missing 5 6 13 ; missing 18 7 6 ; 
    12 missing 28 7] 

nlv = 3 
tol = 1e-15
scal = false
#scal = true
gs = false
#gs = true
model = pcanipalsmiss(; nlv, tol, gs, maxit = 500, scal)
fit!(model, X)
@names model 
@names model.fitm
fitm = model.fitm ;
fitm.niter
fitm.sv
fitm.V
fitm.T
## Orthogonality 
## only if gs = true
fitm.T&#39; * fitm.T
fitm.V&#39; * fitm.V

## Impute missing data in X
model = pcanipalsmiss(; nlv = 2, gs = true) ;
fit!(model, X)
Xfit = xfit(model.fitm)
s = ismissing.(X)
X_imp = copy(X)
X_imp[s] .= Xfit[s]
X_imp</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcanipalsmiss.jl#L1-L56">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcaout-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcaout-Tuple{}"><code>Jchemo.pcaout</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcaout(; kwargs...)
pcaout(X; kwargs...)
pcaout(X, weights::Weight; kwargs...)
pcaout!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Robust PCA using outlierness.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>prm</code> : Proportion of the data removed (hard rejection of outliers) for each outlierness measure.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its MAD when computing the outlierness and by    its uncorrected standard deviation when computing weighted PCA. </li></ul><p>Robust PCA combining outlyingness measures and weighted PCA (WPCA). </p><p>The objective is to remove the effect of multivariate <code>X</code>-outliers that have potentially bad leverages.  Observations (<code>X</code>-rows) receive weights depending on two outlyingness indicators:</p><ol><li>The Stahel-Donoho outlyingness (Maronna and Yohai, 1995) is computed (function <code>outstah</code>) on <code>X</code>.   The proportion <code>prm</code> of the observations with the highest outlyingness values receive a weight w1 = 0   (the other receive a weight w1 = 1).</li><li>An outlyingness based on the Euclidean distance to center (function <code>outeucl</code>) is computed. The proportion <code>prm</code>   of the observations with the highest outlyingness values receive a weight w2 = 0 (the other receive a weight w2 = 1).</li></ol><p>The final weights of the observations are computed by weights.w * w1 * w2 that is used in a weighted PCA.</p><p>By default, the function uses <code>prm = .3</code> (such as in the ROBPCA algorithm of Hubert et al. 2005, 2009). </p><p><strong>References</strong></p><p>Hubert, M., Rousseeuw, V.J., Vanden Branden, K., 2005. ROBPCA: A New Approach to Robust Principal Component  Analysis. Technometrics 47, 64-79. https://doi.org/10.1198/004017004000000563</p><p>Hubert, M., Rousseeuw, V., Verdonck, T., 2009. Robust PCA for skewed data and its outlier map. Computational  Statistics &amp; Data Analysis 53, 2264-2274. https://doi.org/10.1016/j.csda.2008.05.027</p><p>Maronna, R.A., Yohai, V.J., 1995. The Behavior of the Stahel-Donoho Robust Multivariate Estimator. Journal of the  American Statistical Association 90, 330–341. https://doi.org/10.1080/01621459.1995.10476517</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie 
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
wlst = names(X)
wl = parse.(Float64, wlst)
n = nro(X)

nlv = 3
model = pcaout(; nlv)  
#model = pcasvd(; nlv) 
fit!(model, X)
@names model
@names model.fitm
@head T = model.fitm.T
## Same as:
transf(model, X)

i = 1
plotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1)).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcaout.jl#L1-L65">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcapp-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcapp-Tuple{}"><code>Jchemo.pcapp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcapp(; kwargs...)
pcapp(X; kwargs...)
pcapp!(X::Matrix; kwargs...)</code></pre><p>Robust PCA by projection pursuit.</p><ul><li><code>X</code> : X-data (n, p). </li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>nsim</code> : Nb. of additional (to X-rows) simulated directions for the projection pursuit.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its MAD.</li></ul><p>For <code>nsim = 0</code>, this is the Croux &amp; Ruiz-Gazen (C-R, 2005) PCA algorithm that uses a projection pursuit (PP) method.  Data <code>X</code> are robustly centered by the spatial median, and the observations are projected to the &quot;PP&quot; directions  defined by the observations (rows of <code>X</code>) after they are normed. The first PCA loading vector is the direction  (within the PP directions) that maximizes a given &#39;projection index&#39;, here the median absolute deviation (MAD).  Then, <code>X</code> is deflated to this loading vector, and the process is re-run to define the next loading vector. And so on. </p><p>A possible extension of this algorithm is to randomly simulate additionnal candidate PP directions to the n row  observations. If <code>nsim &gt; 0</code>, the function simulates <code>nsim</code> additional PP directions to the n initial ones, as  proposed in Hubert et al. (2005): random couples of observations are sampled in <code>X</code> and, for each couple, the  direction passes through the two observations of the couple (see function <code>simpphub</code>).</p><p><strong>References</strong></p><p>Croux, C., Ruiz-Gazen, A., 2005. High breakdown estimators for principal components: the projection-pursuit  approach revisited. Journal of Multivariate Analysis 95, 206–226. https://doi.org/10.1016/j.jmva.2004.08.002</p><p>Hubert, M., Rousseeuw, V.J., Vanden Branden, K., 2005. ROBPCA: A New Approach to Robust Principal  Component Analysis. Technometrics 47, 64-79. https://doi.org/10.1198/004017004000000563</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie 
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
wlst = names(X)
wl = parse.(Float64, wlst)
n = nro(X)

nlv = 3
model = pcapp(; nlv, nsim = 2000)  
#model = pcasvd(; nlv) 
fit!(model, X)
@names model
@names model.fitm
@head T = model.fitm.T
## Same as:
@head transf(model, X)

i = 1
plotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1)).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcapp.jl#L1-L56">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcasph-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcasph-Tuple{}"><code>Jchemo.pcasph</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcasph(; kwargs...)
pcasph(X; kwargs...)
pcasph(X, weights::Weight; kwargs...)
pcasph!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Spherical PCA.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Spherical PCA (Locantore et al. 1990, Maronna 2005, Daszykowski et al. 2007). Matrix <code>X</code> is centered by the spatial  median computed by function<code>Jchemo.colmedspa</code>.</p><p><strong>References</strong></p><p>Daszykowski, M., Kaczmarek, K., Vander Heyden, Y., Walczak, B., 2007. Robust statistics in data analysis - A review.  Chemometrics and Intelligent Laboratory Systems 85, 203-219. https://doi.org/10.1016/j.chemolab.2006.06.016</p><p>Locantore N., Marron J.S., Simpson D.G., Tripoli N., Zhang J.T., Cohen K.L. Robust principal component analysis for  functional data, Test 8 (1999) 1–7</p><p>Maronna, R., 2005. Principal components and orthogonal regression based on robust scales, Technometrics, 47:3, 264-273, DOI: 10.1198/004017005000000166</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie 
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
wlst = names(X)
wl = parse.(Float64, wlst)
n = nro(X)

nlv = 3
model = pcasph(; nlv)  
#model = pcasvd(; nlv) 
fit!(model, X)
@names model
@names model.fitm
@head T = model.fitm.T
## Same as:
transf(model, X)

i = 1
plotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcasph.jl#L1-L51">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcasvd-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcasvd-Tuple{}"><code>Jchemo.pcasvd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcasvd(; kwargs...)
pcasvd(X; kwargs...)
pcasvd(X, weights::Weight; kwargs...)
pcasvd!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * V&#39;||^2  in metric D, by computing a SVD factorization of sqrt(D) * X:</p><ul><li>sqrt(D) * X ~ U * S * V&#39;</li></ul><p>Outputs are:</p><ul><li><code>T</code> = D^(-1/2) * U * S</li><li><code>V</code> = V</li><li>The diagonal of S   </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
@head Xtrain = X[s.train, :]
@head Xtest = X[s.test, :]

nlv = 3
model = pcasvd(; nlv)
#model = pcaeigen(; nlv)
#model = pcaeigenk(; nlv)
#model = pcanipals(; nlv)
fit!(model, Xtrain)
@names model
@names model.fitm
@head T = model.fitm.T
## Same as:
@head transf(model, X)
T&#39; * T
@head V = model.fitm.V
V&#39; * V

@head Ttest = transf(model, Xtest)

res = summary(model, Xtrain) ;
@names res
res.explvarx
res.contr_var
res.coord_var
res.cor_circle</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcasvd.jl#L1-L61">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pcr-Tuple{}"><a class="docstring-binding" href="#Jchemo.pcr-Tuple{}"><code>Jchemo.pcr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pcr(; kwargs...)
pcr(X, Y; kwargs...)
pcr(X, Y, weights::Weight; kwargs...)
pcr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Principal component regression (PCR) with a SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
model = pcr(; nlv) ;
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
@names fitm
typeof(fitm.fitm)
@names fitm.fitm

@head transf(model, Xtrain)
@head fitm.fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

coef(model)
coef(model; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

res = predict(model, Xtest; nlv = 1:2)
@head res.pred[1]
@head res.pred[2]

res = summary(model, Xtrain) ;
@names res
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcr.jl#L1-L64">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pip-Tuple"><a class="docstring-binding" href="#Jchemo.pip-Tuple"><code>Jchemo.pip</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pip(args...)</code></pre><p>Build a pipeline of models.</p><ul><li><code>args...</code> : Succesive models, see examples.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Pipeline Snv :&gt; Savgol :&gt; Pls :&gt; Svmr

model1 = snv()
model2 = savgol(npoint = 11, deriv = 2, degree = 3)
model3 = plskern(nlv = 15)
model4 = svmr(gamma = 1e3, cost = 1000, epsilon = .1)
model = pip(model1, model2, model3, model4)
fit!(model, Xtrain, ytrain)
res = predict(model, Xtest) ; 
@head res.pred 
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,
      ylabel = &quot;Observed&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_pip.jl#L6-L46">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plotconf-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.plotconf-Tuple{Any}"><code>Jchemo.plotconf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plotconf(object; size = (500, 400), cnt = true, ptext = true, fontsize = 15, coldiag = :red, )</code></pre><p>Plot a confusion matrix.</p><ul><li><code>object</code> : Output of function <code>conf</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>cnt</code> : Boolean. If <code>true</code>, plot the occurrences, else plot the row %s.</li><li><code>ptext</code> : Boolean. If <code>true</code>, display the value in each cell.</li><li><code>fontsize</code> : Font size when <code>ptext = true</code>.</li><li><code>coldiag</code> : Font color when <code>ptext = true</code>.</li></ul><p>See examples in help page of function <code>conf</code>.</p><p>To use the function, a backend (e.g. CairoMakie) has to be specified. ```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plotconf.jl#L1-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plotgrid-Tuple{AbstractVector, Any}"><a class="docstring-binding" href="#Jchemo.plotgrid-Tuple{AbstractVector, Any}"><code>Jchemo.plotgrid</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plotgrid(indx::AbstractVector, r; size = (500, 300), step = 5, color = nothing, kwargs...)
plotgrid(indx::AbstractVector, r, group; size = (700, 350), step = 5, color = nothing, leg = true, 
    leg_title = &quot;Group&quot;, kwargs...)</code></pre><p>Plot error/performance rates of a model.</p><ul><li><code>indx</code> : A numeric variable representing the grid of model parameters, e.g. the nb. LVs if PLSR models.</li><li><code>r</code> : The error/performance rate.</li></ul><p>Keyword arguments: </p><ul><li><code>group</code> : Categorical variable defining groups. A separate line is plotted for each level of <code>group</code>.</li><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>step</code> : Step used for defining the xticks.</li><li><code>color</code> : Set color. If <code>group</code> if used, must be a vector of same length as the number of levels in <code>group</code>.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>leg_title</code> : Title of the legend.</li><li><code>kwargs</code> : Optional arguments to pass in <code>Axis</code> of CairoMakie.</li></ul><p>To use the function, a backend (e.g. CairoMakie) has to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

model = plskern() 
nlv = 0:20
res = gridscore(model, Xtrain, ytrain, Xtest, ytest; score = rmsep, nlv)
plotgrid(res.nlv, res.y1; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

model = lwplsr() 
nlvdis = 15 ; metric = [:mah]
h = [1 ; 2.5 ; 5] ; k = [50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
nlv = 0:20
res = gridscore(model, Xtrain, ytrain, Xtest, ytest; score = rmsep, pars, nlv)
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSECV&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plotgrid.jl#L1-L50">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plotlv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.plotlv-Tuple{Any}"><code>Jchemo.plotlv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plotlv(T; size = (700, 350), shape, start = 1, color = nothing, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, kwargs...)
plotlv(T, group; size = (700, 350), shape, start = 1, color = nothing, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, leg::Bool = true, leg_title = &quot;Group&quot;, 
    kwargs...)</code></pre><p>Matrix of 2-D plots of successive latent variables (PCA, PLS, etc.).</p><ul><li><code>T</code> : A matrix of (PCA, PLS, ec.) latent variables (LVs) to plot (n, A).</li><li><code>group</code> : Categorical variable defining groups (n). </li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>shape</code> : A tuple of length = 2 defining the shape of the figure: nb. rows and columns of the matrice of plots. </li><li><code>start</code> : Start of the numbering of the LVs in the plots.</li><li><code>color</code> : Set color(s). If <code>group</code> if used, <code>color</code> must be a vector of same length as the number of levels in <code>group</code>.</li><li><code>zeros</code> : Boolean. Draw horizontal and vertical axes passing through origin (0, 0).</li><li><code>xlabel</code> : Label for the x-axis.</li><li><code>ylabel</code> : Label for the y-axis.</li><li><code>zlabel</code> : Label for the z-axis.</li><li><code>title</code> : Title of the graphic.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>leg_title</code> : Title of the legend.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>scatter</code> of Makie.</li></ul><p>To use the function, a backend (e.g. CairoMakie) has to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
model = pcasvd(; nlv) 
fit!(model, Xtrain)
@head Ttrain = model.fitm.T
@head Ttest = transf(model, Xtest)
T = vcat(Ttrain, Ttest)

CairoMakie.activate!()
#GLMakie.activate!()

plotlv(Ttrain[:, 1:6]; shape = (2, 3), color = (:blue, .5), zeros = true, xlabel = &quot;PC&quot;, ylabel = &quot;PC&quot;).f
plotlv(Ttrain[:, 3:8]; shape = (2, 3), start = 3, color = (:blue, .5), zeros = true, xlabel = &quot;PC&quot;, ylabel = &quot;PC&quot;).f

group = vcat(repeat([&quot;Train&quot;], ntrain), repeat([&quot;Test&quot;], ntest))
plotlv(T[:, 1:6], group; shape = (2, 3), color = nothing, zeros = true, xlabel = &quot;PC&quot;, ylabel = &quot;PC&quot;,
    leg = true).f

group = vcat(repeat([&quot;Train&quot;], ntrain), repeat([&quot;Test&quot;], ntest))
color = [(:red, .3); (:blue, .3)]
#color = cgrad(:Dark2_5, 2; categorical = true, alpha = .5)
plotlv(1000 * T[:, 1:6], group; shape = (2, 3), color = color, zeros = true, xlabel = &quot;PC&quot;, ylabel = &quot;PC&quot;,
    leg = true).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plotlv.jl#L1-L70">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plotsp"><a class="docstring-binding" href="#Jchemo.plotsp"><code>Jchemo.plotsp</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">plotsp(X, wl = 1:nco(X); size = (500, 300), nsamp = nro(X), color = nothing, kwargs...)</code></pre><p>Plotting spectra.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>wl</code> : Column names of <code>X</code>. Must be numeric.</li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>nsamp</code> : Nb. spectra (X-rows) to plot. If <code>nothing</code>, all spectra are plotted.</li><li><code>color</code> : Set a unique color (and eventually transparency) to the spectra.</li><li><code>kwargs</code> : Optional arguments to pass in <code>Axis</code> of CairoMakie.</li></ul><p>Plot of the rows (e.g. spectrum) of <code>X</code>.</p><p>To use the function, a backend (e.g. CairoMakie) has to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
wlst = names(X)
wl = parse.(Float64, wlst) 

plotsp(X).f
plotsp(X; color = (:red, .2)).f
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

tck = collect(wl[1]:200:wl[end]) ;
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;, xticks = tck).f

f, ax = plotsp(X, wl; color = (:red, .2))
xmeans = colmean(X)
lines!(ax, wl, xmeans; color = :black, linewidth = 2)
vlines!(ax, 1200)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plotsp.jl#L1-L41">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plotxy-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.plotxy-Tuple{Any, Any}"><code>Jchemo.plotxy</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plotxy(x, y; size = (500, 300), color = nothing, ellipse::Bool = false, 
    prob = .95, circle::Bool = false, bisect::Bool = false, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, kwargs...)
plotxy(x, y, group; size = (600, 350), color = nothing, ellipse::Bool = false, 
    prob = .95, circle::Bool = false, bisect::Bool = false, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, leg::Bool = true, leg_title = &quot;Group&quot;, 
    kwargs...)</code></pre><p>2-D scatter plot of x-y data</p><ul><li><code>x</code> : A x-vector (n).</li><li><code>y</code> : A y-vector (n). </li><li><code>group</code> : Categorical variable defining groups (n). </li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>color</code> : Set color(s). If <code>group</code> if used, <code>color</code> must be a vector of same length as the number of levels in <code>group</code>.</li><li><code>ellipse</code> : Boolean. Draw an ellipse of confidence, assuming a Ch-square distribution with df = 2. If <code>group</code>    is used, one ellipse is drawn per group.</li><li><code>prob</code> : Probability for the ellipse of confidence.</li><li><code>bisect</code> : Boolean. Draw a bisector.</li><li><code>zeros</code> : Boolean. Draw horizontal and vertical axes passing    through origin (0, 0).</li><li><code>xlabel</code> : Label for the x-axis.</li><li><code>ylabel</code> : Label for the y-axis.</li><li><code>title</code> : Title of the graphic.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>leg_title</code> : Title of the legend.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>scatter</code> of Makie.</li></ul><p>To use the function, a backend (e.g. CairoMakie) has to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
lev = mlev(year)
nlev = length(lev)

model = pcasvd(nlv = 5)  
fit!(model, X) 
@head T = model.fitm.T

plotxy(T[:, 1], T[:, 2]; color = (:red, .5)).f

plotxy(T[:, 1], T[:, 2], year; ellipse = true, xlabel = &quot;PC1&quot;, ylabel = &quot;PC2&quot;).f

i = 2
colm = cgrad(:Dark2_5, nlev; categorical = true)
plotxy(T[:, i], T[:, i + 1], year; color = colm, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1), zeros = true, ellipse = true).f

plotxy(T[:, 1], T[:, 2], year).lev

plotxy(1:5, 1:5).f

y = reshape(rand(5), 5, 1)
plotxy(1:5, y).f

## Several layers can be added
## (same syntax as in Makie)
A = rand(50, 2)
f, ax = plotxy(A[:, 1], A[:, 2]; xlabel = &quot;x1&quot;, ylabel = &quot;x2&quot;)
ylims!(ax, -1, 2)
hlines!(ax, 0.5; color = :red, linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plotxy.jl#L1-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plotxyz-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#Jchemo.plotxyz-Tuple{Any, Any, Any}"><code>Jchemo.plotxyz</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plotxy(x, y, z; size = (500, 300), color = nothing, perspectiveness = .1,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, zlabel = &quot;&quot;, title = &quot;&quot;, kwargs...)
plotxy(x, y, z, group; size = (500, 300), color = nothing, perspectiveness = .1, 
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, zlabel = &quot;&quot;, title = &quot;&quot;, leg::Bool = true, leg_title = &quot;Group&quot;, 
    kwargs...)</code></pre><p>3-D scatter plot of x-y-z data.</p><ul><li><code>x</code> : A x-vector (n).</li><li><code>y</code> : A y-vector (n). </li><li><code>z</code> : A y-vector (n). </li><li><code>group</code> : Categorical variable defining groups (n). </li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>color</code> : Set color(s). If <code>group</code> if used, <code>color</code> must be a vector of same length as the number of levels in <code>group</code>.</li><li><code>xlabel</code> : Label for the x-axis.</li><li><code>ylabel</code> : Label for the y-axis.</li><li><code>zlabel</code> : Label for the z-axis.</li><li><code>title</code> : Title of the graphic.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>leg_title</code> : Title of the legend.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>scatter</code> of Makie.</li></ul><p>To use the function, a backend (e.g. CairoMakie) has to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie, GLMakie
n = 1000
x = randn(n)
y = randn(n)
z = randn(n)
group = rand([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;], n)
s = group .== &quot;B&quot;
x[s] .+= 10 ;
s = group .== &quot;C&quot;
x[s] .+= 20 ;

CairoMakie.activate!()
#GLMakie.activate!()

plotxyz(x, y, z; size = (500, 300), markersize = 10, xlabel = &quot;V1&quot;).f
plotxyz(x, y, z; size = (500, 300), color = (:red, .3), markersize = 10, xlabel = &quot;V1&quot;).f

plotxyz(x, y, z, group; size = (500, 300), markersize = 10, xlabel = &quot;V1&quot;).f
plotxyz(x, y, z, group; size = (500, 300), markersize = 10, xlabel = &quot;V1&quot;, alpha = .3).f

color = [(:red, .3); (:blue, .3); (:green, .3)]
#color = cgrad(:Dark2_5; categorical = true, alpha = .3)[1:nlev]
plotxyz(x, y, z, group; size = (500, 300), color = color, leg = true, markersize = 10, xlabel = &quot;V1&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plotxyz.jl#L1-L51">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plscan-Tuple{}"><a class="docstring-binding" href="#Jchemo.plscan-Tuple{}"><code>Jchemo.plscan</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plscan(; kwargs...)
plscan(X, Y; kwargs...)
plscan(X, Y, weights::Weight; kwargs...)
plscan!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Canonical partial least squares regression (Canonical PLS).</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are: <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks <code>X</code> and <code>Y</code> is scaled by its uncorrected standard    deviation (before the block scaling).</li></ul><p>Canonical PLS with the Nipals algorithm (Wold 1984, Tenenhaus 1998 chap.11), referred to as PLS-W2A  (i.e. Wold PLS mode A) in Wegelin 2000. The two blocks <code>X</code> and <code>Y</code> play a symmetric role.  After each  step of scores computation, X and Y are deflated by the x- and y-scores, respectively. </p><p>Function <code>summary</code> returns:</p><ul><li><code>cortx2ty</code>: Correlations between the X- and Y-LVs</li></ul><p>and for block <code>X</code>: </p><ul><li><code>explvarx</code> : Proportion of the block inertia (squared Frobenious norm) explained by the block LVs (<code>Tx</code>).</li><li><code>rvx2tx</code> : RV coefficients between the block and the block LVs.</li><li><code>rdx2tx</code> : Rd coefficients between the block and the block LVs.</li><li><code>corx2tx</code> : Correlation between the block variables and the block LVs. The same is returned for block <code>Y</code>.  </li></ul><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p>Wegelin, J.A., 2000. A Survey of Partial Least Squares (PLS) Methods, with Emphasis on the Two-Block Case  (No. 371). University of Washington, Seattle, Washington, USA.</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear Regression. The Partial  Least Squares (PLS) Approach to Generalized Inverses. SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
@names dat
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 2
bscal = :frob
model = plscan(; nlv, bscal)
fit!(model, X, Y)
@names model
@names model.fitm

fitm = model.fitm
@head fitm.Tx
@head transfbl(model, X, Y).Tx

@head fitm.Ty
@head transfbl(model, X, Y).Ty

res = summary(model, X, Y) ;
@names res
res.explvarx
res.explvary
res.cortx2ty
res.rvx2tx
res.rvy2ty
res.rdx2tx
res.rdy2ty
res.corx2tx 
res.cory2ty </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plscan.jl#L1-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plskdeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.plskdeda-Tuple{}"><code>Jchemo.plskdeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plskdeda(; kwargs...)
plskdeda(X, y; kwargs...)
plskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>KDE-DA on PLS latent variables (PLS-KDEDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Eventual keyword arguments of function <code>dmkern</code> (bandwidth definition).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>Same as function <code>plsqda</code> except that the class densities are estimated from <code>dmkern</code> instead of <code>dmnorm</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
model = plskdeda(; nlv) 
#model = plskdeda(; nlv, prior = :unif) 
#model = plskdeda(; nlv, a = .5)
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm

fitm.lev
fitm.ni
fitm.priors

fitm_emb = fitm.fitm_emb ;
typeof(fitm_emb)
@names fitm_emb 
@head transf(model, Xtrain)
@head fitm_emb.T
@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

fitm_da = fitm.fitm_da ;
typeof(fitm_da)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred
summary(fitm_emb, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plskdeda.jl#L1-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plskern-Tuple{}"><a class="docstring-binding" href="#Jchemo.plskern-Tuple{}"><code>Jchemo.plskern</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plskern(; kwargs...)
plskern(X, Y; kwargs...)
plskern(X, Y, weights::Weight; kwargs...)
plskern!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Partial least squares regression (PLSR) with the &quot;improved kernel algorithm #1&quot; (Dayal &amp; McGegor, 1997).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>About the row-weighting in PLS algorithms (<code>weights</code>): see in particular Schaal et al. 2002, Siccard &amp; Sabatier  2006, Kim et al. 2011, and Lesnoff et al. 2020. </p><p><strong>References</strong></p><p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms. Journal of Chemometrics 11, 73-85.</p><p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active pharmaceutical ingredients  content using locally weighted partial least squares and statistical wavelength selection. Int.  J. Pharm., 421, 269-274.</p><p>Lesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR Data. Journal of Chemometrics. e3209.  https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209</p><p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric statistics  for the real time robot learning. Applied Intell., 17, 49-60.</p><p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression and application to a  rainfall dataset. Comput. Stat. Data Anal., 51, 1393-1410.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
model = plskern(; nlv) ;
#model = plsnipals(; nlv) ;
#model = plswold(; nlv) ;
#model = plsrosa(; nlv) ;
#model = plssimp(; nlv) ;
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
@names fitm

@head transf(model, Xtrain)
@head fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

coef(model)
coef(model; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

res = predict(model, Xtest; nlv = 1:2)
@head res.pred[1]
@head res.pred[2]

res = summary(model, Xtrain) ;
@names res
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plskern.jl#L1-L86">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plslda-Tuple{}"><a class="docstring-binding" href="#Jchemo.plslda-Tuple{}"><code>Jchemo.plslda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plslda(; kwargs...)
plslda(X, y; kwargs...)
plslda(X, y, weights::Weight; kwargs...)</code></pre><p>LDA on PLS latent variables (PLS-LDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>LDA on PLS latent variables. The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>A multivariate PLSR (PLSR2) is run on the data {<code>X</code>, Ydummy}, returning a score matrix <code>T</code>.</li><li>A LDA is done on {<code>T</code>, <code>y</code>}, returning estimates of posterior probabilities (∊ [0, 1]) of class membership.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
model = plslda(; nlv) 
#model = plslda(; nlv, prior = :unif) 
#model = plsqda(; nlv, alpha = .1) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm

fitm.lev
fitm.ni
fitm.priors

fitm_emb = fitm.fitm_emb ;
typeof(fitm_emb)
@names fitm_emb 

@head transf(model, Xtrain)
@head fitm_emb.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

fitm_da = fitm.fitm_da ;
typeof(fitm_da)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred
summary(fitm_emb, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plslda.jl#L1-L95">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plsnipals-Tuple{}"><a class="docstring-binding" href="#Jchemo.plsnipals-Tuple{}"><code>Jchemo.plsnipals</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plsnipals(; kwargs...)
plsnipals(X, Y; kwargs...)
plsnipals(X, Y, weights::Weight; kwargs...)
plsnipals!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the Nipals algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>In this function, for PLS2 (multivariate Y), the Nipals iterations are replaced by a direct computation of the  PLS weights (w) by SVD decomposition of matrix X&#39;Y (Hoskuldsson 1988 p.213).</p><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>Hoskuldsson, A., 1988. PLS regression methods. Journal of Chemometrics 2, 211-228. https://doi.org/10.1002/cem.1180020306</p><p>Tenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique. Editions Technip, Paris, France.</p><p>Wold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression: a basic tool for chemometrics.  Chem. Int. Lab. Syst., 58, 109-130.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsnipals.jl#L1-L27">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plsqda-Tuple{}"><a class="docstring-binding" href="#Jchemo.plsqda-Tuple{}"><code>Jchemo.plsqda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plsqda(; kwargs...)
plsqda(X, y; kwargs...)
plsqda(X, y, weights::Weight; kwargs...)</code></pre><p>QDA on PLS latent variables (PLS-QDA) with continuum.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute. Must be &gt;= 1.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.</li></ul><p>QDA on PLS latent variables. The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>A multivariate PLSR (PLSR2) is run on the data {<code>X</code>, Ydummy}, returning a score matrix <code>T</code>.</li><li>A QDA (possibly with continuum) is done on {<code>T</code>, <code>y</code>}, returning estimates of posterior probabilities (∊ [0, 1])   of class membership.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p>See function <code>plslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsqda.jl#L1-L40">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plsravg-Tuple{}"><a class="docstring-binding" href="#Jchemo.plsravg-Tuple{}"><code>Jchemo.plsravg</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plsravg(; kwargs...)
plsravg(X, Y; kwargs...)
plsravg(X, Y, weights::Weight; kwargs...)
plsravg!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Averaging PLSR models with different numbers of  latent variables (PLSR-AVG).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : A range of nb. of latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>Ensemblist method where the predictions are computed by averaging the predictions of a set of models built  with different numbers of LVs.</p><p>For instance, if argument <code>nlv</code> is set to <code>nlv</code> = <code>5:10</code>, the prediction for a new observation is the simple average of the predictions returned by the models with 5 LVs, 6 LVs, ... 10 LVs, respectively.</p><p><strong>References</strong></p><p>Lesnoff, M., Andueza, D., Barotin, C., Barre, V., Bonnal, L., Fernández Pierna, J.A., Picard, F., Vermeulen, V.,  Roger, J.-M., 2022. Averaging and Stacking Partial Least Squares Regression Models to Predict the Chemical Compositions  and the Nutritive Values of Forages from Spectral Near Infrared Data. Applied Sciences 12, 7850.  https://doi.org/10.3390/app12157850</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
Y = dat.Y
@head Y
y = Y.ndf
#y = Y.dm
n = nro(X)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(y, s)
Xtest = X[s, :]
ytest = y[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)

nlv = 0:30
#nlv = 5:20
#nlv = 25
model = plsravg(; nlv) ;
fit!(model, Xtrain, ytrain)

res = predict(model, Xtest)
@head res.pred
res.predlv   # predictions for each nb. of LVs 
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f   </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsravg.jl#L1-L62">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plsrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.plsrda-Tuple{}"><code>Jchemo.plsrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plsrda(; kwargs...)
plsrda(X, y; kwargs...)
plsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on partial least squares regression (PLSR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and Ydummy is scaled by its uncorrected standard deviation.</li></ul><p>This is the usual and simplest &quot;PLSDA&quot;. The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>Then, a multivariate PLSR (PLSR2) is run on the data {<code>X</code>, Ydummy}, returning predictions of the dummy variables   (= object <code>posterior</code> returned by fuction <code>predict</code>).  These predictions can be considered as unbounded   estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
model = plsrda(; nlv) 
#model = plsrda(; nlv, prior = :unif) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors
aggsumv(fitm.fitm.weights.w, ytrain)

@head transf(model, Xtrain)
@head fitm.fitm.T

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

coef(fitm.fitm)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred

summary(fitm.fitm, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsrda.jl#L1-L93">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plsrosa-Tuple{}"><a class="docstring-binding" href="#Jchemo.plsrosa-Tuple{}"><code>Jchemo.plsrosa</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plsrosa(; kwargs...)
plsrosa(X, Y; kwargs...)
plsrosa(X, Y, weights::Weight; kwargs...)
plsrosa!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the  ROSA algorithm (Liland et al. 2016).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p><strong>Note:</strong> The function has the following differences with the original algorithm of Liland et al. (2016):</p><ul><li>Scores T (LVs) are not normed.</li><li>Multivariate Y is allowed.</li></ul><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>Liland, K.H., Næs, T., Indahl, U.G., 2016. ROSA—a fast extension of partial least squares regression for  multiblock data analysis. Journal of Chemometrics 30, 651–662. https://doi.org/10.1002/cem.2824</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsrosa.jl#L1-L23">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plsrout-Tuple{}"><a class="docstring-binding" href="#Jchemo.plsrout-Tuple{}"><code>Jchemo.plsrout</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plsrout(; kwargs...)
plsrout(X, Y; kwargs...)
plsrout(X, Y, weights::Weight; kwargs...)
pcaout!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Robust PLSR using outlierness.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>Y</code> : Y-data (n, q). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of latent variables (LVs).</li><li><code>prm</code> : Proportion of the data removed (hard rejection of outliers) for each outlierness measure.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its MAD when computing the outlierness    and by its uncorrected standard deviation when computing weighted PCA. </li></ul><p>Robust PLSR combining outlyingness measures and weighted PLSR (WPLSR). This is the same principle as function  <code>pcaout</code> (see the help page) but the final step is a weighted PLSR instead of a weighted PCA.  </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
model = plsrout(; nlv)
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm
@head model.fitm.T

coef(model)
coef(model; nlv = 3)

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,  
    ylabel = &quot;Observed&quot;).f    

res = predict(model, Xtest; nlv = 1:2)
@head res.pred[1]
@head res.pred[2]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsrout.jl#L1-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plssimp-Tuple{}"><a class="docstring-binding" href="#Jchemo.plssimp-Tuple{}"><code>Jchemo.plssimp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plssimp(; kwargs...)
plssimp(X, Y; kwargs...)
plssimp(X, Y, weights::Weight; kwargs...)
plssimp!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the SIMPLS algorithm (de Jong 1993).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected    standard deviation.</li></ul><p><strong>Note:</strong> In this function, scores T (LVs) are not normed, conversely to the original algorithm of  de Jong (2013).</p><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>de Jong, S., 1993. SIMPLS: An alternative approach to partial least squares regression. Chemometrics and Intelligent  Laboratory Systems 18, 251–263. https://doi.org/10.1016/0169-7439(93)85002-X</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plssimp.jl#L1-L23">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plstuck-Tuple{}"><a class="docstring-binding" href="#Jchemo.plstuck-Tuple{}"><code>Jchemo.plstuck</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plstuck(; kwargs...)
plstuck(X, Y; kwargs...)
plstuck(X, Y, weights::Weight; kwargs...)
plstuck!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Tucker&#39;s inter-battery method of factor analysis</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks <code>X</code> and <code>Y</code> is scaled by its uncorrected    standard deviation (before the block scaling).</li></ul><p>Inter-battery method of factor analysis (Tucker 1958, Tenenhaus 1998 chap.3). The two blocks  <code>X</code> and <code>X</code> play a symmetric role.  This method is referred to as PLS-SVD in Wegelin 2000. The method  factorizes the covariance matrix X&#39;Y by SVD. </p><p>See function <code>plscan</code> for the details on the <code>summary</code> outputs.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p>Tishler, A., Lipovetsky, S., 2000. Modelling and forecasting with robust canonical  analysis: method and application. Computers &amp; Operations Research 27, 217–232.  https://doi.org/10.1016/S0305-0548(99)00014-3</p><p>Tucker, L.R., 1958. An inter-battery method of factor analysis. Psychometrika 23, 111–136. https://doi.org/10.1007/BF02289009</p><p>Wegelin, J.A., 2000. A Survey of Partial Least Squares (PLS) Methods, with Emphasis  on the Two-Block Case (No. 371). University of Washington, Seattle, Washington, USA.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
Y = dat.Y

model = plstuck(nlv = 3)
fit!(model, X, Y) 
@names model
@names model.fitm

fitm = model.fitm
@head fitm.Tx
@head transfbl(model, X, Y).Tx

@head fitm.Ty
@head transfbl(model, X, Y).Ty

res = summary(model, X, Y) ;
@names res
res.explvarx
res.explvary
res.cortx2ty
res.rvx2tx
res.rvy2ty
res.rdx2tx
res.rdy2ty
res.corx2tx 
res.cory2ty </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plstuck.jl#L1-L70">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.plswold-Tuple{}"><a class="docstring-binding" href="#Jchemo.plswold-Tuple{}"><code>Jchemo.plswold</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">plswold(; kwargs...)
plswold(X, Y; kwargs...)
plswold(X, Y, weights::Weight; kwargs...)
plswold!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the Wold algorithm </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>tol</code> : Tolerance for the Nipals algorithm.</li><li><code>maxit</code> : Maximum number of iterations for the Nipals algorithm.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>Wold Nipals PLSR algorithm: Tenenhaus 1998 p.204.</p><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique. Editions Technip, Paris, France.</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear Regression.  The Partial Least Squares (PLS). Approach to Generalized Inverses. SIAM Journal on Scientific and Statistical  Computing 5, 735–743. https://doi.org/10.1137/0905052</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plswold.jl#L1-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Calds, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Calds, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Calds, X; kwargs...)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/calds.jl#L65-L70">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Calpds, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Calpds, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Calpds, X; kwargs...)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/calpds.jl#L86-L91">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Cglsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. iterations, or collection of nb. iterations, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/cglsr.jl#L172-L178">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Dkplsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplsr.jl#L146-L153">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Dmkern, x)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>x</code> : Data (vector) for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dmkern.jl#L144-L149">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Dmnorm, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Data (vector) for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dmnorm.jl#L158-L163">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Knnda1, X)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/knnda.jl#L89-L94">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Knnr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/knnr.jl#L104-L109">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Kplsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul><p>If nothing, it is the maximum nb. LVs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplsr.jl#L200-L207">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Krr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Krr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Krr, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters, &#39;lambda&#39; to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/krr.jl#L163-L169">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Loessr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Loessr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Loessr, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/loessr.jl#L70-L75">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwmlr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwmlr.jl#L108-L113">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwmlrda, X)</code></pre><p>Compute y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwmlrda.jl#L81-L86">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwplslda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplslda.jl#L101-L106">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwplsqda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsqda.jl#L67-L72">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwplsr, X; nlv = nothing)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsr.jl#L128-L133">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwplsravg, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwplsravg, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwplsravg, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsravg.jl#L97-L102">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Lwplsrda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lwplsrda.jl#L112-L117">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Mbplsprobda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Mbplsprobda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Mbplsprobda, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplslda.jl#L144-L150">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Mbplsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Mbplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Mbplsr, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsr.jl#L171-L177">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Mbplsrda, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsrda.jl#L130-L136">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Mbplswest, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Mbplswest, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Mbplswest, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplswest.jl#L221-L227">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Mlrda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlrda.jl#L98-L103">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Occod, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Occod, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Occod, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occod.jl#L146-L151">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Occsd, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occsd.jl#L166-L171">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Occsdod, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occsdod.jl#L39-L44">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Occstah, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/occstah.jl#L135-L140">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Pcr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Pcr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Pcr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcr.jl#L125-L131">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Plsprobda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Plsprobda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Plsprobda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plslda.jl#L130-L136">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Plsravg, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsravg.jl#L81-L86">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Plsrda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsrda.jl#L123-L129">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Rda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Rda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Qda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rda.jl#L128-L133">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Rosaplsr, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rosaplsr.jl#L229-L235">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Rr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Rr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Rr, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters, &#39;lambda&#39; to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rr.jl#L126-L132">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Rrda, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters, &quot;lambda&quot; to consider.    If nothing, it is the parameter stored in the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rrda.jl#L102-L109">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Soplsr, Xbl)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/soplsr.jl#L142-L147">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Svmda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/svmda.jl#L122-L127">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Svmr, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/svmr.jl#L126-L131">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Treeda, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Treeda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Treeda, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/treeda.jl#L99-L104">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Jchemo.Treer, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Jchemo.Treer, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Treer, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/treer.jl#L85-L90">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Union{Jchemo.Lda, Jchemo.Qda}, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Union{Jchemo.Lda, Jchemo.Qda}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Union{Lda, Qda}, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/lda.jl#L99-L104">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Union{Mlr, MlrNoArg, Rrchol}, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mlr.jl#L292-L297">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.predict-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><a class="docstring-binding" href="#Jchemo.predict-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(object::Union{Plsr, Pcr, Splsr}, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plskern.jl#L209-L215">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.pval-Tuple{Distributions.Distribution, Any}"><a class="docstring-binding" href="#Jchemo.pval-Tuple{Distributions.Distribution, Any}"><code>Jchemo.pval</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pval(d::Distribution, q)
pval(x::Array, q)
pval(e_cdf::ECDF, q)</code></pre><p>Compute p-value(s) for a distribution, an ECDF or vector.</p><ul><li><code>d</code> : A distribution computed from <code>Distribution.jl</code>.</li><li><code>x</code> : Univariate data.</li><li><code>e_cdf</code> : An ECDF computed from <code>StatsBase.jl</code>.</li><li><code>q</code> : Value(s) for which to compute the p-value(s).</li></ul><p>Compute or estimate the p-value of quantile <code>q</code>, ie. V(Q &gt; <code>q</code>) where Q is the random variable.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, Distributions, StatsBase

d = Distributions.Normal(0, 1)
q = 1.96
#q = [1.64; 1.96]
Distributions.cdf(d, q)    # cumulative density function (CDF)
Distributions.ccdf(d, q)   # complementary CDF (CCDF)
pval(d, q)                 # Distributions.ccdf

x = rand(5)
e_cdf = StatsBase.ecdf(x)
e_cdf(x)                # empirical CDF computed at each point of x (ECDF)
p_val = 1 .- e_cdf(x)   # complementary ECDF at each point of x
q = .3
#q = [.3; .5; 10]
pval(e_cdf, q)          # 1 .- e_cdf(q)
pval(x, q)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L411-L443">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.qda-Tuple{}"><a class="docstring-binding" href="#Jchemo.qda-Tuple{}"><code>Jchemo.qda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">qda(; kwargs...)
qda(X, y; kwargs...)
qda(X, y, weights::Weight; kwargs...)</code></pre><p>Quadratic discriminant analysis (QDA, with continuum towards LDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li></ul><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p>For the continuum approach, a value <code>alpha</code> &gt; 0 shrinks the QDA class covariances (Wi) toward a common LDA  covariance (&#39;within-W&#39;). This corresponds to the &#39;first regularization (Eqs.16)&#39; approach described in  Friedman 1989 (in which the present parameter <code>alpha</code> is referred to as &#39;lambda&#39;).</p><p><strong>References</strong></p><p>Friedman JH. Regularized Discriminant Analysis. Journal of the American Statistical Association. 1989; 84(405):165-175.  doi:10.1080/01621459.1989.10478752.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

model = qda()
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
@names fitm
typeof(fitm.fitm)

fitm.lev
fitm.ni
fitm.priors
aggsumv(fitm.weights.w, ytrain)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

## With regularization
alpha = 0.5
#alpha = 1 # = LDA
model = qda(; alpha)
fit!(model, Xtrain, ytrain)
model.fitm.Wi
res = predict(model, Xtest) ;
errp(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/qda.jl#L1-L83">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.r2-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.r2-Tuple{Any, Any}"><code>Jchemo.r2</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">r2(pred, Y)</code></pre><p>Compute the R2 coefficient.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>The rate R2 is calculated by:</p><ul><li>R2 = 1 - MSEP(current model) / MSEP(null model) </li></ul><p>where the &quot;null model&quot; is the overall mean. For predictions over CV or test sets, and/or for non linear models,  it can be different from the square of the correlation coefficient (<code>cor2</code>) between the true data and the predictions. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
r2(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
r2(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L337-L369">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rasvd-Tuple{}"><a class="docstring-binding" href="#Jchemo.rasvd-Tuple{}"><code>Jchemo.rasvd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rasvd(; kwargs...)
rasvd(X, Y; kwargs...)
rasvd(X, Y, weights::Weight; kwargs...)
rasvd!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Redundancy analysis (RA), a.k.a PCA on instrumental variables (PCAIV)</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are: <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks <code>X</code> and <code>Y</code> is scaled by its uncorrected standard    deviation (before the block scaling).</li></ul><p>See e.g. Bougeard et al. 2011a,b and Legendre &amp; Legendre 2012. Let Y<em>hat be the fitted values of the regression  of <code>Y</code> on <code>X</code>. The scores <code>Ty</code> are the PCA scores of Y</em>hat. The scores <code>Tx</code> are the fitted values of the  regression of <code>Ty</code> on <code>X</code>.</p><p>A continuum regularization is available.  After block centering and scaling, the covariances  matrices are computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li></ul><p>where D is the observation (row) metric. Value <code>tau</code> = 0 can generate unstability when inverting the covariance  matrices. A better alternative is generally to use an epsilon value (e.g. <code>tau</code> = 1e-8) to get similar results  as with pseudo-inverses.  </p><p><strong>References</strong></p><p>Bougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011-a. Multiblock redundancy analysis from a user&#39;s  perspective. Application in veterinary epidemiology. Electronic Journal of Applied Statistical Analysis  4, 203-214. https://doi.org/10.1285/i20705948v4n2p203</p><p>Bougeard, S., Qannari, E.M., Rose, N., 2011-b. Multiblock redundancy analysis: interpretation tools  and application in epidemiology. Journal of Chemometrics 25, 467-475. https://doi.org/10.1002/cem.1392</p><p>Legendre, V., Legendre, L., 2012. Numerical Ecology. Elsevier, Amsterdam, The Netherlands.</p><p>Tenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and Sparse Generalized Canonical Correlation  Analysis for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
@names dat
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 2
bscal = :frob ; tau = 1e-4
model = rasvd(; nlv, bscal, tau)
fit!(model, X, Y)
@names model
@names model.fitm

@head model.fitm.Tx
@head transfbl(model, X, Y).Tx

@head model.fitm.Ty
@head transfbl(model, X, Y).Ty

res = summary(model, X, Y) ;
@names res
res.explvarx
res.explvary
res.cortx2ty
res.rvx2tx
res.rvy2ty
res.rdx2tx
res.rdy2ty
res.corx2tx 
res.cory2ty </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rasvd.jl#L1-L78">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rd-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rd-Tuple{Any, Any}"><code>Jchemo.rd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rd(X, Y; typ = :cor)
rd(X, Y, weights::Weight; typ = :cor)</code></pre><p>Compute redundancy coefficients (Rd).</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>typ</code> : Possibles values are: <code>:cor</code> (correlation), <code>:cov</code> (uncorrected covariance). </li></ul><p>Returns the redundancy coefficient between <code>X</code> and each column of <code>Y</code>, i.e. for each k = 1,...,q: </p><ul><li>Mean {cor(xj, yk)^2 ;  j = 1, ..., p }</li></ul><p>Depending argument <code>typ</code>, the correlation can be replaced by the (not corrected) covariance.</p><p>See Tenenhaus 1998 section 2.2.1 p.10-11.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
X = rand(5, 10)
Y = rand(5, 3)
rd(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/angles.jl#L1-L29">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rda-Tuple{}"><a class="docstring-binding" href="#Jchemo.rda-Tuple{}"><code>Jchemo.rda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rda(; kwargs...)
rda(X, y; kwargs...)
rda(X, y, weights::Weight; kwargs...)</code></pre><p>Regularized discriminant analysis (RDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot; (&gt;= 0).</li><li><code>simpl</code> : Boolean. See function <code>dmnorm</code>. </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Let us note W the (corrected) pooled within-class covariance matrix and Wi the (corrected) within-class  covariance matrix of class i. The regularization is done by the two following successive steps (for each class i):</p><ol><li>Continuum between QDA and LDA: Wi(1) = (1 - <code>alpha</code>) * Wi + <code>alpha</code> * W       </li><li>Ridge regularization: Wi(2) = Wi(1) + <code>lb</code> * I</li></ol><p>Then the QDA algorithm is run on matrices {Wi(2)}.</p><p>Function <code>rda</code> is slightly different from the regularization expression used by Friedman 1989 (Eq.18): the choice is  to shrink the covariance matrices Wi(2) to the diagonal of the Idendity matrix (ridge regularization; e.g. Guo  et al. 2007).  </p><p>Particular cases:</p><ul><li><code>alpha</code> = 1 &amp; <code>lb</code> = 0 : LDA</li><li><code>alpha</code> = 0 &amp; <code>lb</code> = 0 : QDA</li><li><code>alpha</code> = 1 &amp; <code>lb</code> &gt; 0 : Penalized LDA (Hastie et al 1995) with diagonal regularization matrix</li></ul><p>See functions <code>lda</code> and <code>qda</code> for other details (arguments <code>weights</code>and <code>prior</code>).</p><p><strong>References</strong></p><p>Friedman JH. Regularized Discriminant Analysis. Journal of the American Statistical Association. 1989;  84(405):165-175. doi:10.1080/01621459.1989.10478752.</p><p>Guo Y, Hastie T, Tibshirani R. Regularized linear discriminant analysis and its application in microarrays.  Biostatistics. 2007; 8(1):86-100. doi:10.1093/biostatistics/kxj035.</p><p>Hastie, T., Buja, A., Tibshirani, R., 1995. Penalized Discriminant Analysis. The Annals of Statistics 23, 73–102.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

alpha = .5
lb = 1e-8
model = rda(; alpha, lb)
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm
fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rda.jl#L1-L83">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_catbydict-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.recod_catbydict-Tuple{Any, Any}"><code>Jchemo.recod_catbydict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_catbydict(x, dict)</code></pre><p>Recode a categorical variable to dictionnary levels.</p><ul><li><code>x</code> : Categorical variable (n) to replace.</li><li><code>dict</code> : Dictionary giving the correpondances between the old and new levels.</li></ul><p>See examples.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

dict = Dict(&quot;a&quot; =&gt; 1000, &quot;b&quot; =&gt; 1, &quot;c&quot; =&gt; 2)
x = [&quot;c&quot; ; &quot;c&quot; ; &quot;a&quot; ; &quot;a&quot; ; &quot;a&quot;]
recod_catbydict(x, dict)

x = [&quot;c&quot; ; &quot;c&quot; ; &quot;a&quot; ; &quot;a&quot; ; &quot;a&quot; ; &quot;e&quot;]
recod_catbydict(x, dict)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L1-L20">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_catbyind-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.recod_catbyind-Tuple{Any, Any}"><code>Jchemo.recod_catbyind</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_catbyind(x, lev)</code></pre><p>Recode a categorical variable to indexes of sorted levels.</p><ul><li><code>x</code> : Categorical variable (n) to replace.</li><li><code>lev</code> : Vector containing categorical levels. </li></ul><p>Internally in the function, the elements of vector <code>lev</code> are made unique and are sorted.</p><p>See examples.</p><p><em>Warning</em>: The levels in <code>x</code> must be contained in <code>lev</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

lev = [&quot;EHH&quot; ; &quot;FFS&quot; ; &quot;ANF&quot; ; &quot;CLZ&quot; ; &quot;CNG&quot; ; &quot;FRG&quot; ; &quot;MPW&quot; ; &quot;PEE&quot; ; &quot;SFG&quot; ; &quot;SFG&quot; ; &quot;TTS&quot;]
lev_sorted = mlev(lev)
[lev_sorted 1:length(lev_sorted)] 
x = [&quot;EHH&quot; ; &quot;TTS&quot; ; &quot;FRG&quot; ; &quot;EHH&quot;]
recod_catbyind(x, lev)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L25-L47">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_catbyint-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.recod_catbyint-Tuple{Any}"><code>Jchemo.recod_catbyint</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_catbyint(x; start = 1)</code></pre><p>Recode a categorical variable to integers.</p><ul><li><code>x</code> : Categorical variable (n) to replace.</li><li><code>start</code> : Integer labelling the first categorical level in <code>x</code>.</li></ul><p>The integers returned by the function correspond to the sorted levels of <code>x</code>, see examples.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = [&quot;b&quot;, &quot;a&quot;, &quot;b&quot;]
mlev(x)   
[x recod_catbyint(x)]
recod_catbyint(x; start = 0)

recod_catbyint([25, 1, 25])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L50-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_catbylev-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.recod_catbylev-Tuple{Any, Any}"><code>Jchemo.recod_catbylev</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_catbylev(x, lev)</code></pre><p>Recode a categorical variable to levels.</p><ul><li><code>x</code> : Variable (n) to replace.</li><li><code>lev</code> : Vector containing the categorical levels.</li></ul><p>The ith sorted level in <code>x</code> is replaced by the ith sorted level in <code>lev</code>, see examples.</p><p><em>Warning</em>: <code>x</code> and <code>lev</code> must contain the same number of levels.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = [10 ; 4 ; 3 ; 3 ; 4 ; 4]
lev = [&quot;B&quot; ; &quot;C&quot; ; &quot;AA&quot; ; &quot;AA&quot;]
mlev(x)
mlev(lev)
[x recod_catbylev(x, lev)]
xstr = string.(x)
[xstr recod_catbylev(xstr, lev)]

lev = [3; 0; 0; -1]
mlev(x)
mlev(lev)
[x recod_catbylev(x, lev)]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L78-L105">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_contbyint-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.recod_contbyint-Tuple{Any, Any}"><code>Jchemo.recod_contbyint</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_contbyint(x, q)</code></pre><p>Recode a continuous variable to integers.</p><ul><li><code>x</code> : Continuous variable (n) to replace.</li><li><code>q</code> : Numerical values (K) separating the class levels from <code>x</code>.  </li></ul><p>The function potentially returns K + 1 levels. For a given value x of vector <code>x</code> and <code>q</code> a vector  of length K: </p><ul><li>x &lt;= q[1]             : ==&gt; 1</li><li>q[1] &lt; x &lt;= q[2]      : ==&gt; 2</li><li>q[2] &lt; x &lt;= q[3]      : ==&gt; 3</li><li>etc.</li><li>q[K - 1] &lt; x &lt;= q[K]  : ==&gt; K</li><li>q[K] &lt; x              : ==&gt; K + 1 </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, Statistics
x = [collect(1:10); 8.1 ; 3.1] 

q = [3; 8]
zx = recod_contbyint(x, q)  
[x zx]
probs = [.33; .66]
q = quantile(x, probs) 
zx = recod_contbyint(x, q)  
[x zx]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L119-L147">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_indbylev-Tuple{Union{Int64, Array{Int64}}, Array}"><a class="docstring-binding" href="#Jchemo.recod_indbylev-Tuple{Union{Int64, Array{Int64}}, Array}"><code>Jchemo.recod_indbylev</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_indbylev(x::Union{Int, Array{Int}}, lev::Array)</code></pre><p>Recode an index variable to levels.</p><ul><li><code>x</code> : Index variable (n) to replace.</li><li><code>lev</code> : Vector containing the categorical levels.</li></ul><p>Assuming lev<em>sorted = &#39;sort(unique(lev))&#39;, each element <code>x[i]</code> (i = 1, ..., n) is replaced by `lev</em>sorted[x[i]]`,  see examples.</p><p><em>Warning</em>: Vector <code>x</code> must contain integers between 1 and nlev, where nlev is the number of levels in <code>lev</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = [2 ; 1 ; 2 ; 2]
lev = [&quot;B&quot; ; &quot;C&quot; ; &quot;AA&quot; ; &quot;AA&quot;]
mlev(x)
mlev(lev)
[x recod_indbylev(x, lev)]
recod_indbylev([2], lev)
recod_indbylev(2, lev)

x = [2 ; 1 ; 2]
lev = [3 ; 0 ; 0 ; -1]
mlev(x)
mlev(lev)
recod_indbylev(x, lev)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L161-L190">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recod_miss-Tuple{AbstractArray}"><a class="docstring-binding" href="#Jchemo.recod_miss-Tuple{AbstractArray}"><code>Jchemo.recod_miss</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recod_miss(X; miss = nothing)
recod_miss(df; miss = nothing)</code></pre><p>Declare data as missing in a dataset.</p><ul><li><code>X</code> : An array-dataset.</li><li><code>df</code> : A dataframe-dataset.</li><li><code>miss</code> : The value used in the dataset to identify the missing data. </li></ul><p>Each cell of <code>X</code> or <code>df</code> having the value <code>miss</code> is replaced by value <code>missing</code> of type <code>Missing</code>.</p><p>The case <code>miss = nothing</code> has the only action to allow <code>missing</code> in <code>X</code> or <code>df</code>. </p><p>See examples.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, DataFrames

X = hcat(1:5, [0, 0, 7., 10, 1.2])
X_miss = recod_miss(X; miss = 0)

df = DataFrame(i = 1:5, x = [0, 0, 7., 10, 1.2])
df_miss = recod_miss(df; miss = 0)

df = DataFrame(i = 1:5, x = [&quot;0&quot;, &quot;0&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;])
df_miss = recod_miss(df; miss = &quot;0&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_recod.jl#L228-L255">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.recovkw-Tuple{DataType, Any}"><a class="docstring-binding" href="#Jchemo.recovkw-Tuple{DataType, Any}"><code>Jchemo.recovkw</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">recovkw(ParStruct, kwargs)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L450-L452">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.residcla-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.residcla-Tuple{Any, Any}"><code>Jchemo.residcla</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">residcla(pred, y)</code></pre><p>Compute the discrimination residual vector (0 = no error, 1 = error).</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data (class membership).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

model = plsrda(; nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
residcla(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L507-L527">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.residreg-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.residreg-Tuple{Any, Any}"><code>Jchemo.residreg</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">residreg(pred, Y)</code></pre><p>Compute the regression residual vector.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
residreg(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
residreg(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L6-L31">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rfda-Tuple{}"><a class="docstring-binding" href="#Jchemo.rfda-Tuple{}"><code>Jchemo.rfda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rfda(; kwargs...)
rfda(X, y::Union{Array{Int}, Array{String}}; kwargs...)</code></pre><p>Random forest discrimination with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_trees</code> : Nb. trees built for the forest. </li><li><code>partial_sampling</code> : Proportion of sampled observations for each tree.</li><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: -1 ==&gt; sqrt(#variables)).</li><li><code>max_depth</code> : Maximum depth of the decision trees (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>mth</code> : Boolean indicating if a multi-threading is done when new data are predicted with function <code>predict</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function is a wrapper of package <code>DecisionTree.jl&#39; to fit a random forest discrimnation model. In DecisionTree.jl, &#39;y&#39; component must have type</code>Int<code>or</code>String`.</p><p><strong>References</strong></p><p>Breiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140. https://doi.org/10.1007/BF00058655</p><p>Breiman, L., 2001. Random Forests. Machine Learning 45, 5–32. https://doi.org/10.1023/A:1010933404324</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Genuer, R., 2010. Forêts aléatoires : aspects théoriques, sélection de variables et applications. PhD Thesis.  Université Paris Sud - Paris XI.</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting : trois thèmes statistiques autour de CART en  régression (These de doctorat). Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
using CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
wlst = names(X)
wl = parse.(Float64, wlst)
n, p = size(X) 

s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

n_trees = 200
n_subfeatures = p / 3 
max_depth = 10
model = rfda(; n_trees, n_subfeatures, max_depth) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni

res = predict(model, Xtest) ; 
@names res 
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

imp = fitm.fitm.featim  # variable importances
plotsp(imp&#39;, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Importance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rfda.jl#L1-L83">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rfr-Tuple{}"><a class="docstring-binding" href="#Jchemo.rfr-Tuple{}"><code>Jchemo.rfr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rfr(; kwargs...)
rfr(X, y; kwargs...)</code></pre><p>Random forest regression with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_trees</code> : Nb. trees built for the forest. </li><li><code>partial_sampling</code> : Proportion of sampled observations for each tree.</li><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: -1 ==&gt; sqrt(#variables)).</li><li><code>max_depth</code> : Maximum depth of the decision trees (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>mth</code> : Boolean indicating if a multi-threading is done when new data are predicted with function <code>predict</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function is a wrapper of package `DecisionTree.jl&#39; to fit a random forest regression model.</p><p><strong>References</strong></p><p>Breiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140. https://doi.org/10.1007/BF00058655</p><p>Breiman, L., 2001. Random Forests. Machine Learning 45, 5–32. https://doi.org/10.1023/A:1010933404324</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Genuer, R., 2010. Forêts aléatoires : aspects théoriques, sélection de variables et applications. PhD Thesis.  Université Paris Sud - Paris XI.</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting : trois thèmes statistiques autour de CART en  régression (These de doctorat). Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
wlst = names(X)
wl = parse.(Float64, wlst)
p = nco(X)

tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

n_trees = 200
n_subfeatures = p / 3
max_depth = 15
model = rfr(; n_trees, n_subfeatures, max_depth) 
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f  
    
@names fitm.fitm
imp = fitm.fitm.featim  # variable importances
plotsp(imp&#39;, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Importance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rfr.jl#L1-L72">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, BitVector, Vector}}"><a class="docstring-binding" href="#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, BitVector, Vector}}"><code>Jchemo.rmcol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rmcol(X::Union{AbstractMatrix, DataFrame}, s::Union{Vector, BitVector, UnitRange, Number})
rmcol(X::Vector, s::Union{Vector, BitVector, UnitRange, Number})</code></pre><p>Remove the columns of a matrix or the components of a vector  having indexes <code>s</code>.</p><ul><li><code>X</code> : Matrix or vector.</li><li><code>s</code> : Vector of the indexes.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = rand(5, 3) 
rmcol(X, [1, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L475-L490">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rmgap-Tuple{}"><a class="docstring-binding" href="#Jchemo.rmgap-Tuple{}"><code>Jchemo.rmgap</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rmgap(; kwargs...)
rmgap(X; kwargs...)</code></pre><p>Remove vertical gaps in spectra (e.g. for ASD).  </p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>indexcol</code> : Indexes (∈ [1, p]) of the <code>X</code>-columns where are located the gaps to remove. </li><li><code>npoint</code> : The number of <code>X</code>-columns used on the left side of each gap for fitting the linear regressions.</li></ul><p>For each spectra (row-observation of matrix <code>X</code>) and each defined gap, the correction is done by extrapolation  from a simple linear regression computed on the left side of the gap. </p><p>For instance, If two gaps are observed between column-indexes 651-652 and between column-indexes 1425-1426, respectively,  the syntax should be <code>indexcol</code> = [651 ; 1425].</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/asdgap.jld2&quot;) 
@load db dat
@names dat
X = dat.X
wlst = names(dat.X)
wl = parse.(Float64, wlst)

wl_target = [1000 ; 1800] 
indexcol = findall(in(wl_target).(wl))

f, ax = plotsp(X, wl)
vlines!(ax, wl_target; linestyle = :dot, color = (:grey, .8))
f

## Corrected data
model = rmgap(; indexcol, npoint = 5)
fit!(model, X)
Xc = transf(model, X)
f, ax = plotsp(Xc, wl)
vlines!(ax, wl_target; linestyle = :dot, color = (:grey, .8))
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rmgap.jl#L1-L42">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, BitVector, Vector}}"><a class="docstring-binding" href="#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, BitVector, Vector}}"><code>Jchemo.rmrow</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rmrow(X::Union{AbstractMatrix, DataFrame}, s::Union{Vector, BitVector, UnitRange, Number})
rmrow(X::Union{Vector, BitVector}, s::Union{Vector, BitVector, UnitRange, Number})</code></pre><p>Remove the rows of a matrix or the components of a vector having indexes <code>s</code>.</p><ul><li><code>X</code> : Matrix or vector.</li><li><code>s</code> : Vector of the indexes.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = rand(5, 2) 
rmrow(X, [1, 4])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L501-L515">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rmsep-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rmsep-Tuple{Any, Any}"><code>Jchemo.rmsep</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rmsep(pred, Y)</code></pre><p>Compute the square root of the mean of the squared prediction errors (RMSEP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
rmsep(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
rmsep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L100-L127">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rmsepstand-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rmsepstand-Tuple{Any, Any}"><code>Jchemo.rmsepstand</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rmsepstand(pred, Y)</code></pre><p>Compute the standardized square root of the mean of the squared prediction errors (RMSEP_stand).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>RMSEP is standardized to <code>Y</code>: </p><ul><li>RMSEP_stand = RMSEP ./ <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
rmsepstand(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
rmsepstand(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L130-L160">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rosaplsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.rosaplsr-Tuple{}"><code>Jchemo.rosaplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rosaplsr(; kwargs...)
rosaplsr(Xbl, Y; kwargs...)
rosaplsr(Xbl, Y, weights::Weight; kwargs...)
rosaplsr!(Xbl::Vector, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock ROSA PLSR (Liland et al. 2016).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data. Typically, output of function <code>mblock</code>    from data (n, p).  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and <code>Y</code> is scaled by its uncorrected standard    deviation (before the block scaling).</li></ul><p>The function has the following differences with the original algorithm of Liland et al. (2016):</p><ul><li>Scores T (latent variables LVs) are not normed to 1.</li><li>Multivariate <code>Y</code> is allowed. In such a case, the squared residuals are summed over the columns to find the    winning block for each global LV (therefore, Y-columns should have the same scale).</li></ul><p><strong>References</strong></p><p>Liland, K.H., Næs, T., Indahl, U.G., 2016. ROSA — a fast extension of partial least squares regression  for multiblock data analysis. Journal of Chemometrics 30, 651–662. https://doi.org/10.1002/cem.2824</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
@names dat 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
Xbltest = mblock(rmrow(X, s), listbl)
ytrain = y[s]
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 3
scal = false
#scal = true
model = rosaplsr(; nlv, scal)
fit!(model, Xbltrain, ytrain)
@names model 
@names model.fitm
@head model.fitm.T
@head transf(model, Xbltrain)
transf(model, Xbltest)

res = predict(model, Xbltest)
res.pred 
rmsep(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rosaplsr.jl#L1-L62">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rowmean-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.rowmean-Tuple{Any}"><code>Jchemo.rowmean</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rowmean(X)</code></pre><p>Compute row-wise means of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
rowmean(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_rowwise.jl#L1-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rownorm-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.rownorm-Tuple{Any}"><code>Jchemo.rownorm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rownorm(X)</code></pre><p>Compute row-wise norms of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>The norm of each row x of <code>X</code> is computed as:</p><ul><li>sqrt(x&#39; * x)</li></ul><p>Return a vector.</p><p>Note: Thanks to @mcabbott  at https://discourse.julialang.org/t/orders-of-magnitude-runtime-difference-in-row-wise-norm/96363.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)

rownorm(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_rowwise.jl#L19-L41">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rowstd-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.rowstd-Tuple{Any}"><code>Jchemo.rowstd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rowstd(X)</code></pre><p>Compute row-wise standard deviations (uncorrected) of a matrix`.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
rowstd(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_rowwise.jl#L44-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rowsum-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.rowsum-Tuple{Any}"><code>Jchemo.rowsum</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rowsum(X)</code></pre><p>Compute row-wise sums of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
 
X = rand(5, 2) 
rowsum(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_rowwise.jl#L62-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rowvar-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.rowvar-Tuple{Any}"><code>Jchemo.rowvar</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rowvar(X)</code></pre><p>Compute row-wise variances (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n, p = 5, 6
X = rand(n, p)
rowvar(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_rowwise.jl#L79-L94">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rp-Tuple{}"><a class="docstring-binding" href="#Jchemo.rp-Tuple{}"><code>Jchemo.rp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rp(; kwargs...)
rp(X; kwargs...)
rp(X, weights::Weight; kwargs...)
rp!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Make a random projection of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. dimensions on which <code>X</code> is projected.</li><li><code>meth</code> : Method of random projection. Possible values are: <code>:gauss</code>, <code>:li</code>. See the respective functions    <code>rpmatgauss</code> and <code>rpmatli</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
n, p = (5, 10)
X = rand(n, p)
nlv = 3
meth = :li ; s = sqrt(p) 
#meth = :gauss
model = rp(; nlv, meth, s)
fit!(model, X)
@names model
@names model.fitm
@head model.fitm.T 
@head model.fitm.V 
transf(model, X[1:2, :])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rp.jl#L1-L31">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rpd-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rpd-Tuple{Any, Any}"><code>Jchemo.rpd</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rpd(pred, Y)</code></pre><p>Compute the ratio &quot;deviation to model performance&quot; (RPD).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>This is the ratio of the deviation to the model performance to the deviation, defined by:</p><ul><li>RPD = Std(Y) / RMSEP</li></ul><p>where Std(Y) is the standard deviation. </p><p>Since Std(Y) = RMSEP(null model) where the null model is the simple average, this also gives:</p><ul><li>RPD = RMSEP(null model) / RMSEP </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
rpd(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
rpd(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L377-L411">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rpdr-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rpdr-Tuple{Any, Any}"><code>Jchemo.rpdr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rpdr(pred, Y)</code></pre><p>Compute a robustified RPD.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
rpdr(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
rpdr(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L417-L444">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rpmatgauss"><a class="docstring-binding" href="#Jchemo.rpmatgauss"><code>Jchemo.rpmatgauss</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">rpmatgauss(p::Int, nlv::Int, Q = Float64)</code></pre><p>Build a gaussian random projection matrix.</p><ul><li><code>p</code> : Nb. variables (attributes) to project.</li><li><code>nlv</code> : Nb. of simulated projection dimensions.</li><li><code>Q</code> : Type of components of the built projection matrix.</li></ul><p>The function returns a random projection matrix V of dimension <code>p</code> x <code>nlv</code>. The projection of a given matrix X  of size n x <code>p</code> is given by X * V.</p><p>V is simulated from i.i.d. N(0, 1) / sqrt(<code>nlv</code>).</p><p><strong>References</strong></p><p>Li, V., Hastie, T.J., Church, K.W., 2006. Very sparse random projections, in: Proceedings of the 12th ACM SIGKDD  International Conference on Knowledge Discovery and Data Mining, KDD ’06. Association for Computing Machinery,  New York, NY, USA, pp. 287–296. https://doi.org/10.1145/1150402.1150436</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
p = 10 ; nlv = 3
rpmatgauss(p, nlv)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rpmat.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rpmatli"><a class="docstring-binding" href="#Jchemo.rpmatli"><code>Jchemo.rpmatli</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">rpmatli(p::Int, nlv::Int, Q = Float64; s)</code></pre><p>Build a sparse random projection matrix (Achlioptas 2001, Li et al. 2006).</p><ul><li><code>p</code> : Nb. variables (attributes) to project.</li><li><code>nlv</code> : Nb. of simulated projection dimensions.</li><li><code>Q</code> : Type of components of the built projection matrix.</li></ul><p>Keyword arguments:</p><ul><li><code>s</code> : Coefficient defining the sparsity of the returned matrix (higher is <code>s</code>, higher is the sparsity).</li></ul><p>The function returns a random projection matrix V of dimension <code>p</code> x <code>nlv</code>. The projection of a given matrix X  of size n x <code>p</code> is given by X * V.</p><p>Matrix V is simulated from i.i.d. discrete sampling within values: </p><ul><li>1 with prob. 1/(2 * <code>s</code>)</li><li>0 with prob. 1 - 1 / <code>s</code></li><li>-1 with prob. 1/(2 * <code>s</code>)</li></ul><p>Usual values for <code>s</code> are:</p><ul><li>sqrt(<code>p</code>)       (Li et al. 2006)</li><li><code>p</code> / log(<code>p</code>)  (Li et al. 2006)</li><li>1               (Achlioptas 2001)</li><li>3               (Achlioptas 2001) </li></ul><p><strong>References</strong></p><p>Achlioptas, D., 2001. Database-friendly random projections, in: Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART  Symposium on Principles of Database Systems, PODS ’01. Association for Computing Machinery, New York, NY, USA, pp. 274–281.  https://doi.org/10.1145/375551.375608</p><p>Li, V., Hastie, T.J., Church, K.W., 2006. Very sparse random projections, in: Proceedings of the 12th ACM  SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06. Association for Computing Machinery,  New York, NY, USA, pp. 287–296. https://doi.org/10.1145/1150402.1150436</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
p = 10 ; nlv = 3
rpmatli(p, nlv)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rpmat.jl#L29-L67">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rr-Tuple{}"><a class="docstring-binding" href="#Jchemo.rr-Tuple{}"><code>Jchemo.rr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rr(; kwargs...)
rr(X, Y; kwargs...)
rr(X, Y, weights::Weight; kwargs...)
rr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Ridge regression (RR) implemented by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Ridge regularization parameter &#39;lambda&#39;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function computes a model with intercept. After <code>X</code> and y (a given column of <code>Y</code>) have been centered (an <code>X</code> eventually scaled) and weighted by sqrtw = sqrt.(<code>weights.w</code>), the function finds  b (q, 1) (the corresponding column of output <code>B</code> (p, q)) that minimizes </p><ul><li>||y - X * b||^2 + <code>lb</code>^2 * ||b||^2 </li></ul><p>where ||.|| is the Euclidean norm.</p><p><strong>References</strong></p><p>Cule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice of ridge parameter  in ridge regression. arXiv:1205.0686.</p><p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic regularization for expression arrays.  Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010</p><p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.</p><p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.  Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

lb = 1e-3
model = rr(; lb) 
#model = rrchol(; lb) 
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm

coef(model)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## !! Only for function &#39;rr&#39; (not for &#39;rrchol&#39;)
coef(model; lb = 1e-1)
res = predict(model, Xtest; lb = [.1 ; .01])
@head res.pred[1]
@head res.pred[2]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rr.jl#L1-L71">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rrchol-Tuple{}"><a class="docstring-binding" href="#Jchemo.rrchol-Tuple{}"><code>Jchemo.rrchol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rrchol(; kwargs...)
rrchol(X, Y; kwargs...)
rrchol(X, Y, weights::Weight; kwargs...)
rrchol!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Ridge regression (RR) using the Normal equations and a Cholesky factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Ridge regularization parameter &#39;lambda&#39;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>See function <code>rr</code> for details and examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rrchol.jl#L1-L15">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.rrda-Tuple{}"><code>Jchemo.rrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rrda(; kwargs...)
rrda(X, y; kwargs...)
rrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on ridge regression (RR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The approach is as follows:</p><ol><li>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy)   containing nlev columns, where nlev is the number of classes present in <code>y</code>. Each column of   Ydummy is a dummy (0/1) variable. </li><li>Then, a ridge regression (RR) is run on the data {<code>X</code>, Ydummy}, returning predictions of the dummy variables   (= object <code>posterior</code> returned by fuction <code>predict</code>).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities.</li><li>For a given observation, the final prediction is the class corresponding to the dummy variable for which   the probability estimate is the highest.</li></ol><p>The low-level function method (i.e. having argument <code>weights</code>) requires to set as input a vector of observation  weights. In that case, argument <code>prior</code> has no effect: the class prior probabilities (output <code>priors</code>) are always  computed by summing the observation weights by class.</p><p>In the high-level methods (no argument <code>weights</code>), argument <code>prior</code> defines how are preliminary computed the  observation weights (see function <code>mweightcla</code>) that are then given as input in the hidden low level method.</p><p><strong>Note:</strong> For highly unbalanced classes, it may be recommended to define equal class weights (&#39;prior = :unif&#39;), and to use a performance score such as <code>merrp</code>, instead of <code>errp</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

lb = 1e-5
model = rrda(; lb) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

coef(fitm.fitm)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; lb = [.1; .01]).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rrda.jl#L1-L83">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rrmsep-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rrmsep-Tuple{Any, Any}"><code>Jchemo.rrmsep</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rrmsep(pred, Y)</code></pre><p>Compute the relative RMSEP.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>For each variable y in <code>Y</code>, RRMSEP = RMSEP / mean(y)</p><p><strong>Examples</strong></p><p>```julia using Jchemo</p><p>Xtrain = rand(10, 5)  Ytrain = rand(10, 2) ytrain = Ytrain[:, 1] Xtest = rand(4, 5)  Ytest = rand(4, 2) ytest = Ytest[:, 1]</p><p>model = plskern(nlv = 2) fit!(model, Xtrain, Ytrain) pred = predict(model, Xtest).pred rrmsep(pred, Ytest)</p><p>fit!(model, Xtrain, ytrain) pred = predict(model, Xtest).pred rrmsep(pred, ytest)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L166-L194">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rrr-Tuple{}"><a class="docstring-binding" href="#Jchemo.rrr-Tuple{}"><code>Jchemo.rrr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rrr(; kwargs...)
rrr(X, Y; kwargs...)
rrr(X, Y, weights::Weight; kwargs...)
rr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Reduced rank regression (RRR, a.k.a RA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>tol</code> : Tolerance for the Nipals algorithm.</li><li><code>maxit</code> : Maximum number of iterations for the Nipals algorithm.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>Reduced rank regression, also referred to as redundancy analysis (RA) regression. In this function,  the RA uses the Nipals algorithm presented in Mangamana et al 2021, section 2.1.1.</p><p>A continuum regularization is available. After block centering and scaling, the covariances matrices  are computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li></ul><p>where D is the observation (row) metric. </p><p><strong>Note:</strong> Value <code>tau = 0</code> can generate unstability when inverting the covariance matrices. A better alternative is generally to  use an epsilon value (e.g. <code>tau = 1e-8</code>) to get similar results as with pseudo-inverses.  </p><p><strong>References</strong></p><p>Bougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011. Multiblock redundancy analysis from  a user’s perspective. Application in veterinary epidemiology. Electronic Journal of  Applied Statistical Analysis 4, 203-214–214. https://doi.org/10.1285/i20705948v4n2p203</p><p>Bougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy analysis: interpretation tools  and application in epidemiology. Journal of Chemometrics 25, 467–475. https://doi.org/10.1002/cem.1392 </p><p>Tchandao Mangamana, E., Glèlè Kakaï, R., Qannari, E.M., 2021. A general strategy for setting up supervised  methods of multiblock data analysis. Chemometrics and Intelligent Laboratory Systems 217, 104388.  https://doi.org/10.1016/j.chemolab.2021.104388</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 1
tau = 1e-4
model = rrr(; nlv, tau) 
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm
@head model.fitm.T

coef(model)
coef(model; nlv = 3)

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rrr.jl#L1-L77">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rv-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rv-Tuple{Any, Any}"><code>Jchemo.rv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rv(X, Y; centr = true)
rv(Xbl::Vector; centr = true)</code></pre><p>Compute RV coefficients.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>Xbl</code> : A list (vector) of matrices.</li><li><code>centr</code> : Boolean indicating if the matrices will be internally centered or not.</li></ul><p>RV is bounded within [0, 1]. </p><p>A dissimilarty measure between <code>X</code> and <code>Y</code> can be computed by d = sqrt(2 * (1 - RV)).</p><p><strong>References</strong></p><p>Escoufier, Y., 1973. Le Traitement des Variables Vectorielles. Biometrics 29, 751–760.  https://doi.org/10.2307/2529140</p><p>Josse, J., Holmes, S., 2016. Measuring multivariate association and beyond. Stat Surv 10, 132–167.  https://doi.org/10.1214/16-SS116</p><p>Josse, J., Pagès, J., Husson, F., 2008. Testing the significance of the RV coefficient. Computational Statistics  &amp; Data Analysis 53, 82–91. https://doi.org/10.1016/j.csda.2008.06.012</p><p>Kazi-Aoual, F., Hitier, S., Sabatier, R., Lebreton, J.-D., 1995. Refined approximations to permutation tests  for multivariate inference. Computational Statistics &amp; Data Analysis 20, 643–656.  https://doi.org/10.1016/0167-9473(94)00064-2</p><p>Mayer, C.-D., Lorent, J., Horgan, G.W., 2011. Exploratory Analysis of Multiple Omics Datasets Using the Adjusted  RV Coefficient. Statistical Applications in Genetics and Molecular Biology 10. https://doi.org/10.2202/1544-6115.1540</p><p>Smilde, A.K., Kiers, H.A.L., Bijlsma, S., Rubingh, C.M., van Erk, M.J., 2009. Matrix correlations for high-dimensional  data: the modified RV-coefficient. Bioinformatics 25, 401–405. https://doi.org/10.1093/bioinformatics/btn634</p><p>Robert, P., Escoufier, Y., 1976. A Unifying Tool for Linear Multivariate Statistical Methods: The RV-Coefficient.  Journal of the Royal Statistical Society: Series C (Applied Statistics) 25, 257–265. https://doi.org/10.2307/2347233</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
X = rand(5, 10)
Y = rand(5, 3)
rv(X, Y)

X = rand(5, 15) 
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl)
rv(Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/angles.jl#L46-L94">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.rweight-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.rweight-Tuple{Any, Any}"><code>Jchemo.rweight</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">rweight(X, v)
rweight!(X::AbstractMatrix, v)</code></pre><p>Weight each row of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>v</code> : A weighting vector (n).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, LinearAlgebra

X = rand(5, 2) 
w = rand(5) 
rweight(X, w)
diagm(w) * X

rweight!(X, w)
X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_weighting.jl#L78-L97">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sampcla"><a class="docstring-binding" href="#Jchemo.sampcla"><code>Jchemo.sampcla</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">sampcla(x, k::Union{Int, Vector{Int}}, y = nothing; seed::Union{Nothing, Int} = nothing)</code></pre><p>Build training vs. test sets using a stratified sampling.  </p><ul><li><code>x</code> : Class membership (n) of the observations.</li><li><code>k</code> : Nb. test observations to sample in each class. If <code>k</code> is a single value, the nb. of sampled observations    is the same for each class. Alternatively, <code>k</code> can be a vector of length equal to the nb. of classes in <code>x</code>.</li><li><code>y</code> : Quantitative variable (n) used if systematic sampling.</li></ul><p>Keyword arguments:</p><ul><li><code>seed</code> : Eventual seed for the <code>Random.MersenneTwister</code> generator.  </li></ul><p>Two outputs are returned (= row indexes of the data): </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>If <code>y</code> = <code>nothing</code>, the sampling ( within each class) of the <code>k</code> test observations is random, else it is systematic  over the sorted <code>y</code> (see the principle in function <code>sampsys</code>).</p><p><strong>References</strong></p><p>Naes, T., 1987. The design of calibration in near infra-red reflectance analysis by clustering.  Journal of Chemometrics 1, 121-134.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
x = string.(repeat(1:3, 5))
n = length(x)
tab(x)
k = 2 
res = sampcla(x, k)
res.test
x[res.test]
tab(x[res.test])

sampcla(x, k; seed = 123)

y = rand(n)
res = sampcla(x, k, y)
res.test
x[res.test]
tab(x[res.test])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/sampcla.jl#L1-L42">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sampdf"><a class="docstring-binding" href="#Jchemo.sampdf"><code>Jchemo.sampdf</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">sampdf(Y::DataFrame, k::Union{Int, Vector{Int}}, id = 1:nro(Y); meth = :rand,
    seed::Union{Nothing, Int} = nothing)</code></pre><p>Build training vs. test sets from each column of a dataframe. </p><ul><li><code>Y</code> : DataFrame (n, p). Typivally, contains a set of response variables to predict. Can contain missing values.</li><li><code>k</code> : Nb. of test observations selected for each <code>Y</code>-column. The selection is done within the non-missing observations    of the considered column. If <code>k</code> is a single value, the same nb. of     observations are selected for each column.    Alternatively, <code>k</code> can be a vector of length p. </li><li><code>id</code> : Vector (n) of IDs.</li></ul><p>Keyword arguments:</p><ul><li><code>meth</code> : Type of sampling for the test set. Possible values are: <code>:rand</code> = random sampling, <code>:sys</code> = systematic    sampling over each sorted <code>Y</code>-column (see the principle in function <code>sampsys</code>).  </li><li><code>seed</code> : When <code>meth = :rand</code>, eventual seed for the <code>Random.MersenneTwister</code> generator.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, DataFrames

Y = hcat([rand(5); missing; rand(6)], [rand(2); missing; missing; rand(7); missing])
Y = DataFrame(Y, :auto)
n = nro(Y)

k = 3
res = sampdf(Y, k) 
#res = sampdf(Y, k, string.(1:n))
@names res
res.nam
length(res.test)
res.train
res.test

sampdf(Y, k; seed = 123) 

## Replicated splitting Train/Test
rep = 10
k = 3
ids = [sampdf(Y, k) for i = 1:rep]
length(ids)
i = 1    # replication
ids[i]
ids[i].train 
ids[i].test
j = 1    # variable y  
ids[i].train[j]
ids[i].test[j]
ids[i].nam[j]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/sampdf.jl#L1-L49">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sampdp-Tuple{Any, Int64}"><a class="docstring-binding" href="#Jchemo.sampdp-Tuple{Any, Int64}"><code>Jchemo.sampdp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sampdp(X, k::Int; metric = :eucl)</code></pre><p>Build training vs. test sets by DUPLEX sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>k</code> : Nb. pairs (training/test) of observations to sample. Must be &lt;= n / 2. </li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Metric used for the distance computation. Possible values are: <code>:eucl</code> (Euclidean),    <code>:mah</code> (Mahalanobis).</li></ul><p>Three outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>k</code>), </li><li><code>test</code> (<code>k</code>),</li><li><code>remain</code> (n - 2 * <code>k</code>). </li></ul><p>Outputs <code>train</code> and <code>test</code> are built from the DUPLEX algorithm (Snee, 1977 p.421). They are expected  to cover approximately the same X-space region and have similar statistical properties. </p><p>In practice, when output <code>remain</code> is not empty (i.e. when there are remaining observations), one common strategy  is to add it to output <code>train</code>.</p><p><strong>References</strong></p><p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments. Technometrics, 11(1), 137-148.</p><p>Snee, R.D., 1977. Validation of Regression Models: Methods and Examples. Technometrics 19, 415-428.  https://doi.org/10.1080/00401706.1977.10489581</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = [0.381392  0.00175002 ; 0.1126    0.11263 ; 
    0.613296  0.152485 ; 0.726536  0.762032 ;
    0.367451  0.297398 ; 0.511332  0.320198 ; 
    0.018514  0.350678] 

k = 3
sampdp(X, k)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/sampdp.jl#L1-L39">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sampks-Tuple{Any, Int64}"><a class="docstring-binding" href="#Jchemo.sampks-Tuple{Any, Int64}"><code>Jchemo.sampks</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sampks(X, k::Int; metric = :eucl)</code></pre><p>Build training vs. test sets by Kennard-Stone sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>k</code> : Nb. test observations to sample.</li></ul><p>Keyword arguments: </p><ul><li><code>metric</code> : Metric used for the distance computation. Possible values are: <code>:eucl</code> (Euclidean),    <code>:mah</code> (Mahalanobis).</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>n</code> - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built from the Kennard-Stone (KS) algorithm (Kennard &amp; Stone, 1969). </p><p><strong>Note:</strong> By construction, the set of observations selected by KS sampling contains higher variability than the set of  the remaining observations. In the seminal article (K&amp;S, 1969), the algorithm is used to select observations that will  be used to build a calibration set. To the opposite, in the present function, KS is used to select a test set with  higher variability than the training set. </p><p><strong>References</strong></p><p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments. Technometrics, 11(1), 137-148.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat

X = dat.X 
y = dat.Y.tbc

k = 80
res = sampks(X, k)
@names res
res.train 
res.test

model = pcasvd(nlv = 15) 
fit!(model, X) 
@head T = model.fitm.T
res = sampks(T, k; metric = :mah)

#####################

n = 10
k = 25 
X = [repeat(1:n, inner = n) repeat(1:n, outer = n)] 
X = Float64.(X) 
X .= X + .1 * randn(nro(X), nco(X))
s = sampks(X, k).test
f, ax = plotxy(X[:, 1], X[:, 2])
scatter!(ax, X[s, 1], X[s, 2]; color = &quot;red&quot;) 
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/sampks.jl#L1-L58">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.samprand-Tuple{Int64, Int64}"><a class="docstring-binding" href="#Jchemo.samprand-Tuple{Int64, Int64}"><code>Jchemo.samprand</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">samprand(n::Int, k::Int; seed::Union{Nothing, Int} = nothing)
samprand(group::Vector, k::Int; seed::Union{Nothing, Int} = nothing)</code></pre><p>Build training vs. test sets by random sampling.  </p><ul><li><code>n</code> : Total nb. of observations.</li><li><code>group</code> : A vector (<code>n</code>) defining groups of observations.</li><li><code>k</code> : Nb. test observations, or nb. test groups if <code>group</code> is used, returned in each validation segment.</li></ul><p>Keyword arguments:</p><ul><li><code>seed</code> : Eventual seed for the <code>Random.MersenneTwister</code> generator. </li></ul><p>Two outputs are returned (= row indexes of the data): </p><ul><li><code>train</code> (<code>n</code> - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>If <code>group</code> is used (must be a vector of length <code>n</code>), the function samples groups of observations instead of single  observations. Such a group-sampling is required when the data are structured by groups and when the response to predict  is correlated within groups. This prevents underestimation of the generalization error.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 10
samprand(n, 4)
samprand(n, 4; seed = 123)

n = 10 
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]    # groups of the observations
tab(group)  
k = 2 
res = samprand(group, k)
group[res.test]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/samprand.jl#L1-L34">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sampsys-Tuple{Any, Int64}"><a class="docstring-binding" href="#Jchemo.sampsys-Tuple{Any, Int64}"><code>Jchemo.sampsys</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sampsys(y, k::Int)</code></pre><p>Build training vs. test sets by systematic sampling over a quantitative variable.  </p><ul><li><code>y</code> : Quantitative variable (n) to sample.</li><li><code>k</code> : Nb. test observations to sample. Must be &gt;= 2.</li></ul><p>Two outputs are returned (= row indexes of the data): </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built by systematic sampling over the rank of the <code>y</code> observations. For instance if <code>k</code> / n ~ .3,  one observation over three observations over the sorted <code>y</code> is selected. </p><p>Output <code>test</code> always contains the indexes of the minimum and maximum of <code>y</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

y = rand(7)
[y sort(y)]
res = sampsys(y, 3)
sort(y[res.test])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/sampsys.jl#L1-L25">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sampwsp-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.sampwsp-Tuple{Any, Any}"><code>Jchemo.sampwsp</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sampwsp(X, dmin; recod = false, maxit = nro(X))</code></pre><p>Build training vs. test sets by WSP sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>dmin</code> : Distance &quot;dmin&quot; (Santiago et al. 2012).</li></ul><p>Keyword arguments: </p><ul><li><code>recod</code> : Boolean indicating if <code>X</code> is recoded or not before the sampling (see below).</li><li><code>maxit</code> : Maximum number of iterations.</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>n</code> - k),</li><li><code>test</code> (k). </li></ul><p>Output <code>test</code> is built from the &quot;Wootton, Sergent, Phan-Tan-Luu&quot; (WSP) algorithm, assumed to generate samples  uniformely distributed in the <code>X</code> domain (Santiago et al. 2012).</p><p>If <code>recod = true</code>, each column x of <code>X</code> is recoded within [0, 1] and the center of the domain is the vector  <code>repeat([.5], p)</code>. Column x is recoded such as: </p><ul><li>vmin = minimum(x)</li><li>vmax = maximum(x)</li><li>vdiff = vmax - vmin</li><li>x .=  0.5 .+ (x .- (vdiff / 2 + vmin)) / vdiff</li></ul><p><strong>References</strong></p><p>Béal A. 2015. Description et sélection de données en grande dimensio. Thèse de doctorat. Laboratoire  d’Instrumentation et de sciences analytiques, Ecole doctorale des siences chimiques, Université d&#39;Aix-Marseille.</p><p>Santiago, J., Claeys-Bruno, M., Sergent, M., 2012. Construction of space-filling designs using WSP algorithm  for high dimensional spaces. Chemometrics and Intelligent Laboratory Systems, Selected Papers from  Chimiométrie 2010 113, 26–31. https://doi.org/10.1016/j.chemolab.2011.06.003</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 600 ; p = 2
X = rand(n, p)
dmin = .5
s = sampwsp(X, dmin)
@names res
@show length(s.test)
plotxy(X[s.test, 1], X[s.test, 2]).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/sampwsp.jl#L1-L45">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.savgk-Tuple{Int64, Int64, Int64}"><a class="docstring-binding" href="#Jchemo.savgk-Tuple{Int64, Int64, Int64}"><code>Jchemo.savgk</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">savgk(nhwindow::Int, degree::Int, deriv::Int)</code></pre><p>Compute the kernel of the Savitzky-Golay filter.</p><ul><li><code>nhwindow</code> : Nb. points (&gt;= 1) of the half window.</li><li><code>degree</code> : Degree of the smoothing polynom, where 1 &lt;= <code>degree</code> &lt;= 2 * nhwindow.</li><li><code>deriv</code> : Derivation order, where 0 &lt;= <code>deriv</code> &lt;= degree.</li></ul><p>The size of the kernel is odd (npoint = 2 * nhwindow + 1): </p><ul><li>x[-nhwindow], x[-nhwindow+1], ..., x[0], ...., x[nhwindow-1], x[nhwindow].</li></ul><p>If <code>deriv</code> = 0, there is no derivation (only polynomial smoothing).</p><p>The case <code>degree</code> = 0 (i.e. simple moving average) is not allowed by the funtion.</p><p><strong>References</strong></p><p>Luo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation filter for even number data. Signal  Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo
res = savgk(21, 3, 2)
@names res
res.S 
res.G 
res.kern</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L416-L444">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.savgol-Tuple{}"><a class="docstring-binding" href="#Jchemo.savgol-Tuple{}"><code>Jchemo.savgol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">savgol(; kwargs...)
savgol(X; kwargs...)</code></pre><p>Savitzky-Golay derivation and smoothing of each row of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Size of the filter (nb. points involved in the kernel). Must be odd and &gt;= 3. The half-window size    is nhwindow = (<code>npoint</code> - 1) / 2.</li><li><code>deriv</code> : Derivation order. Must be: 0 &lt;= <code>deriv</code> &lt;= <code>degree</code>.</li><li><code>degree</code> : Degree of the smoothing polynom. Must be: 1 &lt;= <code>degree</code> &lt;= <code>npoint</code> - 1.</li></ul><p>The smoothing is computed by convolution (with padding), using function imfilter of package ImageFiltering.jl.  Each returned point is located on the center of the kernel. The kernel is computed with function <code>savgk</code>.</p><p>The function returns a matrix (n, p).</p><p><strong>References</strong></p><p>Luo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation filter for  even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002</p><p>Savitzky, A., Golay, M.J.E., 2002. Smoothing and Differentiation of Data by Simplified Least Squares Procedures.  [WWW Document]. https://doi.org/10.1021/ac60214a047</p><p>Schafer, R.W., 2011. What Is a Savitzky-Golay Filter? [Lecture Notes]. IEEE Signal Processing Magazine 28, 111–117. https://doi.org/10.1109/MSP.2011.941097</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

npoint = 11 ; deriv = 2 ; degree = 2
model = savgol(; npoint, deriv, degree) 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f

####### Gaussian signal 

u = -15:.1:15
n = length(u)
x = exp.(-.5 * u.^2) / sqrt(2 * pi) + .03 * randn(n)
M = 10  # half window
N = 3   # degree
deriv = 0
#deriv = 1
model = savgol(; npoint = 2M + 1, degree = N, deriv)
fit!(model, x&#39;)
xp = transf(model, x&#39;)
f, ax = plotsp(x&#39;, u; color = :blue)
lines!(ax, u, vec(xp); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L459-L525">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.scale-Tuple{}"><a class="docstring-binding" href="#Jchemo.scale-Tuple{}"><code>Jchemo.scale</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">scale()
scale(X)
scale(X, weights::Weight)</code></pre><p>Column-wise scaling of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = scale() 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
colstd(Xptrain)
@head Xptest 
@head Xtest ./ colstd(Xtrain)&#39;
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L169-L202">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.segmkf-Tuple{Int64, Int64}"><a class="docstring-binding" href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>Jchemo.segmkf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">segmkf(n::Int, K::Int; rep = 1, seed::Union{Nothing, Int, Vector{Int}} = nothing)
segmkf(group::Vector, K::Int; rep = 1, seed::Union{Nothing, Int, Vector{Int}} = nothing)</code></pre><p>Build segments of observations for K-fold cross-validation.  </p><ul><li><code>n</code> : Total nb. of observations in the dataset. The sampling is implemented with 1:<code>n</code>.</li><li><code>group</code> : A vector (<code>n</code>) defining blocks of observations.</li><li><code>K</code> : Nb. folds (segments) splitting the <code>n</code> observations. </li></ul><p>Keyword arguments:</p><ul><li><code>rep</code> : Nb. replications of the sampling.</li><li><code>seed</code> : Eventual seed for the <code>Random.MersenneTwister</code> generator. Must be of length = <code>rep</code>. </li></ul><p>For each replication, the function splits the <code>n</code> observations to <code>K</code> segments  that can be used for K-fold cross-validation. </p><p>If <code>group</code> is used (must be a vector of length <code>n</code>), the function samples entire groups (= blocks) of observations  instead of observations. Such a block-sampling is required when data is structured by blocks and when the response to  predict is correlated within blocks. This prevents underestimation of the generalization error.</p><p>The function returns a list (vector) of <code>rep</code> elements. Each element of the list contains <code>K</code> segments (= <code>K</code> vectors).  Each segment contains the indexes (position within 1:<code>n</code>) of the sampled observations.    </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

n = 10 ; K = 3
rep = 4 
segm = segmkf(n, K; rep)
i = 1 
segm[i]
segm[i][1]

segmkf(n, K; seed = 123)
segmkf(n, K; rep, seed = collect(1:rep))

n = 10 
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]    # blocks of the observations
tab(group) 
K = 3 ; rep = 4 
segm = segmkf(group, K; rep)
i = 1 
segm[i]
segm[i][1]
group[segm[i][1]]
group[segm[i][2]]
group[segm[i][3]]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/segmkf.jl#L1-L47">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.segmts-Tuple{Int64, Int64}"><a class="docstring-binding" href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>Jchemo.segmts</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">segmts(n::Int, k::Int; rep = 1, seed::Union{Nothing, Int, Vector{Int}} = nothing)
segmts(group::Vector, k::Int; rep = 1, seed::Union{Nothing, Int, Vector{Int}} = nothing)</code></pre><p>Build segments of observations for &quot;test-set&quot; validation.</p><ul><li><code>n</code> : Total nb. of observations in the dataset. The sampling is implemented within 1:<code>n</code>.</li><li><code>group</code> : A vector (<code>n</code>) defining groups of observations.</li><li><code>k</code> : Nb. test observations, or nb. test groups if <code>group</code> is used, returned in each validation segment.</li></ul><p>Keyword arguments: </p><ul><li><code>rep</code> : Nb. replications of the sampling.</li><li><code>seed</code> : Eventual seed for the <code>Random.MersenneTwister</code> generator. Must be of length = <code>rep</code>. </li></ul><p>For each replication, the function builds a test set that can be used to validate a model. </p><p>If <code>group</code> is used (must be a vector of length <code>n</code>), the function samples groups of observations instead of single  observations. Such a group-sampling is required when the data are structured by groups and when the response to  predict is correlated within groups. This prevents underestimation of the generalization error.</p><p>The function returns a list (vector) of <code>rep</code> elements. Each element of the list is a vector of the indexes  (positions within 1:<code>n</code>) of the sampled observations.  </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo 

n = 10 ; k = 3
rep = 4 
segm = segmts(n, k; rep) 
i = 1
segm[i]
segm[i][1]

segmts(n, k; seed = 123)
segmts(n, k; rep, seed = collect(1:rep))

n = 10 
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]    # groups of the observations
tab(group)  
k = 2 ; rep = 4 
segm = segmts(group, k; rep)
i = 1 
segm[i]
segm[i][1]
group[segm[i][1]]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/segmts.jl#L1-L45">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.selwold-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.selwold-Tuple{Any, Any}"><code>Jchemo.selwold</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">selwold(indx, r; smooth = true, npoint = 5, alpha = .05, digits = 3, graph = true, 
    step = 2, xlabel = &quot;Index&quot;, ylabel = &quot;Value&quot;, title = &quot;Score&quot;)</code></pre><p>Wold&#39;s criterion to select dimensionality in LV models (e.g. PLSR).</p><ul><li><code>indx</code> : A variable representing the model parameter(s), e.g. nb. LVs if PLSR models.</li><li><code>r</code> : A vector of error rates (n), e.g. RMSECV.</li></ul><p>Keyword arguments:</p><ul><li><code>smooth</code> : Boolean. If <code>true</code>,  the selection is done after a moving-average smoothing of rate R (see function <code>mavg</code>).</li><li><code>npoint</code> : Window of the moving-average used to smooth rate R.</li><li><code>alpha</code> : Proportion alpha used as threshold for rate R.</li><li><code>digits</code> : Number of digits in the outputs.</li><li><code>graph</code> : Boolean. If <code>true</code>, outputs are plotted.</li><li><code>step</code> : Step used for defining the xticks in the graphs.</li><li><code>xlabel</code> : Horizontal label for the plots.</li><li><code>ylabel</code> : Vertical label for the plots.</li><li><code>title</code> : Title of the left plot.</li></ul><p>The slection criterion is the &quot;precision gain ratio&quot;: </p><ul><li>R = 1 - <code>r</code>(a+1) / <code>r</code>(a)</li></ul><p>where <code>r</code> is an observed error rate quantifying the model performance (e.g. RMSEP, classification error rate, etc.)  and a the model dimensionnality (= nb. LVs). <code>r</code> can also represent other indicators such as the eigenvalues of a PCA.</p><p>R is the relative gain in perforamnce efficiency after a new LV is added to the model. The iterations continue until  R becomes lower than a threshold value <code>alpha</code>. By default and only as an indication, the default <code>alpha</code>=.05 is set  in the function, but the user should set any other value depending on his data and parsimony objective.</p><p>In his original article, Wold (1978; see also Bro et al. 2008) used the ratio of cross-validated over training residual sums of squares, i.e. PRESS over SSR. Instead, function <code>selwold</code> compares values of consistent nature (the successive  values in the input vector <code>r</code>). For instance, <code>r</code> was set to PRESS values in Li et al. (2002) and Andries et al. (2011),  which is equivalent to the &quot;punish factor&quot; described in Westad &amp; Martens (2000).</p><p>The ratio R can be erratic (particulary when <code>r</code> is the error rate of a discrimination model), making difficult the  dimensionnaly selection. In such a situation, function <code>selwold</code> proposes to calculatea smoothing of R (argument <code>smooth</code>).</p><p>The function returns two outputs (in addition to eventual plots):</p><ul><li><code>opt</code> : The index corresponding to the minimum value of <code>r</code>.</li><li><code>sel</code> : The index of the selection from the R (or smoothed R) threshold.</li></ul><p><strong>References</strong></p><p>Andries, J.V.M., Vander Heyden, Y., Buydens, L.M.C., 2011. Improved variable reduction in partial least squares  modelling based on Predictive-Property-Ranked Variables and adaptation of partial least squares complexity. Analytica  Chimica Acta 705, 292-305. https://doi.org/10.1016/j.aca.2011.06.037</p><p>Bro, R., Kjeldahl, K., Smilde, A.K., Kiers, H.A.L., 2008. Cross-validation of component models: A critical look at  current methods. Anal Bioanal Chem 390, 1241-1251. https://doi.org/10.1007/s00216-007-1790-1</p><p>Li, B., Morris, J., Martin, E.B., 2002. Model selection for partial least squares regression. Chemometrics and  Intelligent Laboratory Systems 64, 79-89. https://doi.org/10.1016/S0169-7439(02)00051-5</p><p>Westad, F., Martens, H., 2000. Variable Selection in near Infrared Spectroscopy Based on Significance Testing in  Partial Least Squares Regression. J. Near Infrared Spectrosc., JNIRS 8, 117â124.</p><p>Wold S. Cross-Validatory Estimation of the Number of Components in Factor and Principal Components Models.  Technometrics. 1978;20(4):397-405</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
n = nro(Xtrain)

segm = segmts(n, 50; rep = 30)
model = plskern()
nlv = 0:20
res = gridcv(model, Xtrain, ytrain; segm, score = rmsep, nlv).res
res[res.y1 .== minimum(res.y1), :]
plotgrid(res.nlv, res.y1;xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
zres = selwold(res.nlv, res.y1; smooth = true, graph = true) ;
@show zres.opt
@show zres.sel
zres.f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/selwold.jl#L1-L86">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sep-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.sep-Tuple{Any, Any}"><code>Jchemo.sep</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sep(pred, Y)</code></pre><p>Compute the corrected SEP (&quot;SEP_c&quot;), i.e. the standard deviation of the prediction errors.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>References</strong></p><p>Bellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B., Roger, J.-M., McBratney, A., 2010. Critical review of  chemometric indicators commonly used for assessing the quality of the prediction of soil attributes by NIR  spectroscopy. TrAC Trends in Analytical Chemistry 29, 1073–1081. https://doi.org/10.1016/j.trac.2010.05.006</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
sep(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
sep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L233-L265">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.snorm-Tuple{}"><a class="docstring-binding" href="#Jchemo.snorm-Tuple{}"><code>Jchemo.snorm</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">snorm()
snorm(X)</code></pre><p>Row-wise norming of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Each row of <code>X</code> is divide by its norm.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = snorm()
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f
@head rownorm(Xptrain)
@head rownorm(Xptest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L570-L603">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.snv-Tuple{}"><a class="docstring-binding" href="#Jchemo.snv-Tuple{}"><code>Jchemo.snv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">snv(; kwargs...)
snv(X; kwargs...)</code></pre><p>Standard-normal-variate (SNV) transformation of each row of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>centr</code> : Boolean indicating if the centering in done.</li><li><code>scal</code> : Boolean indicating if the scaling in done.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(X, wl; nsamp = 20).f

model = snv() 
#model = snv(scal = false) 
fit!(model, Xtrain)
Xptrain = transf(model, Xtrain)
Xptest = transf(model, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f
@head rowmean(Xptrain)
@head rowstd(Xptrain)
@head rowmean(Xptest)
@head rowstd(Xptest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L627-L664">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.softmax-Tuple{AbstractVector}"><a class="docstring-binding" href="#Jchemo.softmax-Tuple{AbstractVector}"><code>Jchemo.softmax</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">softmax(x::AbstractVector)
softmax(X::Union{Matrix, DataFrame})</code></pre><p>Softmax function.</p><ul><li><code>x</code> : A vector to transform.</li><li><code>X</code> : A matrix whose rows are transformed.</li></ul><p>Let v be a vector:</p><ul><li>&#39;softmax&#39;(v) = exp.(v) / sum(exp.(v))</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = 1:3
softmax(x)

X = rand(5, 3)
softmax(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L526-L546">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.soplsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.soplsr-Tuple{}"><code>Jchemo.soplsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">soplsr(; kwargs...)
soplsr(Xbl, Y; kwargs...)
soplsr(Xbl, Y, weights::Weight; kwargs...)
soplsr!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock sequentially orthogonalized PLSR (SO-PLSR).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data Typically, output of function <code>mblock</code> from data (n, p).  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs; = scores) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and <code>Y</code> is scaled by its uncorrected    standard deviation.</li></ul><p><strong>References</strong></p><p>Biancolillo et al. , 2015. Combining SO-PLS and linear discriminant analysis for multi-block classification.  Chemometrics and Intelligent Laboratory Systems, 141, 58-67.</p><p>Biancolillo, A. 2016. Method development in the area of multi-block analysis focused on food analysis. PhD.  University of copenhagen.</p><p>Menichelli et al., 2014. SO-PLS as an exploratory toolfor path modelling. Food Quality and Preference, 36, 122-134.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
@names dat 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
Xbltest = mblock(rmrow(X, s), listbl)
ytrain = y[s]
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 2
#nlv = [2, 1, 2]
#nlv = [2, 0, 1]
scal = false
#scal = true
model = soplsr(; nlv, scal)
fit!(model, Xbltrain, ytrain)
@names model 
@names model.fitm
@head model.fitm.T
@head transf(model, Xbltrain)
transf(model, Xbltest)

res = predict(model, Xbltest)
res.pred 
rmsep(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/soplsr.jl#L1-L63">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sourcedir-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.sourcedir-Tuple{Any}"><code>Jchemo.sourcedir</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sourcedir(path)</code></pre><p>Include all the files contained in a directory.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L562-L565">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.spca-Tuple{}"><a class="docstring-binding" href="#Jchemo.spca-Tuple{}"><code>Jchemo.spca</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">spca(; kwargs...)
spca(X; kwargs...)
spca(X, weights::Weight; kwargs...)
spca!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Sparse PCA by regularized low rank matrix approximation (sPCA-rSVD, Shen &amp; Huang 2008).</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>meth</code> : Method used for the thresholding of the loadings. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>defl</code> : Type of <code>X</code>-matrix deflation, see below.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) kept to build the PCs (non-zero loadings). Can be a single integer    (i.e. same nb. of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>tol</code> : Tolerance value for stopping the Nipals iterations.</li><li><code>maxit</code> : Maximum nb. of Nipals iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>sPCA-rSVD algorithm (regularized low rank matrix approximation) of Shen &amp; Huang 2008.  The method approximates matrix <code>X</code> by T * V&#39;, where T is a matrix of scores (PCs) and V is a matrix of  normed regularized loadings. </p><p>The algorithm extracts the PCs one by one (deflation approach). Each loadings vector v is computed iteratively,  by alternating least squares regressions (Nipals) that includes a thresholding of the computed values  (sparsity step). Function <code>spca</code> provides the thresholding methods &#39;1&#39; and &#39;2&#39; reported in Shen &amp; Huang 2008  Lemma 2 (<code>:soft</code> and <code>:hard</code>):</p><ul><li>In Shen &amp; Huang 2008 (see p.2020) the tuning parameter is a cardinality constraint defined by the    number of zero elements in the loadings vector, referred to as degree of sparsity. Conversely, the present    function <code>spca</code> uses the number of non-zero elements <code>nvar</code> (equal to p - degree of sparsity).</li><li>See the code of function <code>snipals_shen</code> for the details on how is computed (given a <code>nvar</code> value) the cutoff    &#39;lambda&#39; used inside the soft thresholding function (Shen &amp; Huang 2008). Discrepancies with other softwares    may occur when the loadings vector contains tied values (in such cases, results can depend on the method    that computes the quantiles).</li></ul><p>Function <code>spca</code> allows two types of deflation of matrix <code>X</code>:</p><ul><li><code>defl = :v</code> : Matrix <code>X</code> is deflated by regression of the <code>X&#39;</code>-columns on the loadings vector v. This is    the method used by Shen &amp; Huang 2008 (see p.1033 in Theorem A.2).</li><li><code>defl = :t</code> : Matrix <code>X</code> is deflated by regression of the <code>X</code>-columns on the score vector t. This is the method    used in function <code>spca</code> of the R package <code>mixOmics</code> (Le Cao et al. 2016).</li></ul><p>The computation of the % of variance explained in <code>X</code> by each PC (returned by function <code>summary</code>) depends on  the type of deflation chosen (see the code).    </p><p><strong>References</strong></p><p>Guerra-Urzola, R., Van Deun, K., Vera, J.C., Sijtsma, K., 2021. A Guide for Sparse PCA: Model Comparison  and Applications. Psychometrika 86, 893–919. https://doi.org/10.1007/s11336-021-09773-2</p><p>Kim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien Dejean with key contributors Benoit Gautier, Francois  Bartolo, contributions from Pierre Monget, Jeff Coquery, FangZou Yao and Benoit Liquet. (2016). mixOmics: Omics Data  Integration Project. R package version 6.1.1.  https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Shen, H., Huang, J.Z., 2008. Sparse principal component analysis via regularized low rank matrix approximation.  Journal of Multivariate Analysis 99, 1015–1034. https://doi.org/10.1016/j.jmva.2007.06.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
@names dat
@head dat.X
X = dat.X[:, 1:4]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
Xtest = X[s.test, :]

nlv = 3 
meth = :soft
#meth = :hard
nvar = 2
model = spca(; nlv, meth, nvar, defl = :t) ;
fit!(model, Xtrain) 
@names model
fitm = model.fitm ;
typeof(fitm) 
@names fitm

fitm.niter

fitm.sellv 
fitm.sel

V = fitm.V

@head transf(model, Xtrain)
@head fitm.T

@head transf(fitm, Xtest)

res = summary(model, Xtrain) ;
@names res 
res.explvarx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/spca.jl#L1-L98">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.spcr-Tuple{}"><a class="docstring-binding" href="#Jchemo.spcr-Tuple{}"><code>Jchemo.spcr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">spcr(; kwargs...)
spcr(X, Y; kwargs...)
spcr(X, Y, weights::Weight; kwargs...)
spcr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Sparse principal component regression (sPCR). </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>meth</code> : Method used for the thresholding of the loadings. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>defl</code> : Type of <code>X</code>-matrix deflation, see below.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) kept to build the PCs (non-zero loadings). Can be a single integer    (i.e. same nb. of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>tol</code> : Tolerance value for stopping the Nipals iterations.</li><li><code>maxit</code> : Maximum nb. of Nipals iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Regression (MLR) on scores computed from a sparse PCA (sPCA-rSVD algorithm of Shen &amp; Huang 2008 ).  See function <code>spca</code> for details.</p><p><strong>References</strong></p><p>Shen, H., Huang, J.Z., 2008. Sparse principal component analysis via regularized low rank matrix approximation.  Journal of Multivariate Analysis 99, 1015–1034. https://doi.org/10.1016/j.jmva.2007.06.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
meth = :soft
#meth = :hard
nvar = 10 
model = spcr(; nlv, meth, nvar, defl = :t) ;
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
@names fitm
typeof(fitm.fitm)
@names fitm.fitm

fitm.fitm.niter

fitm.fitm.sellv
fitm.fitm.sel

@head transf(model, Xtrain)
@head fitm.fitm.T

@head fitm.fitm.V

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

res = summary(model, Xtrain) ;
@names res
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/spcr.jl#L1-L80">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.splskdeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.splskdeda-Tuple{}"><code>Jchemo.splskdeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">splskdeda(; kwargs...)
splskdeda(X, y; kwargs...)
splskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLS-KDE-DA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code> : Method used for the sparse thresholding. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each LV. Can be a single integer (i.e. same nb.    of variables for each LV), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li>Eventual keyword arguments of function <code>dmkern</code> for bandwidth definition.</li><li><code>tol</code> : Only when q &gt; 1; tolerance used in function <code>snipals_shen</code>. </li><li><code>maxit</code> : Only when q &gt; 1; maximum nb. of iterations used in function <code>snipals_shen</code>.    </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation. </li></ul><p>Same as function <code>plskdeda</code> (PLS-KDEDA) except that a sparse PLSR (function <code>splsr</code>), instead of a PLSR,  is run on the Y-dummy table. </p><p>See function <code>splslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/splskdeda.jl#L1-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.splslda-Tuple{}"><a class="docstring-binding" href="#Jchemo.splslda-Tuple{}"><code>Jchemo.splslda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">splslda(; kwargs...)
splslda(X, y; kwargs...)
splslda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code> : Method used for the sparse thresholding. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each LV. Can be a single integer (i.e. same nb.    of variables for each LV), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>tol</code> : Only when q &gt; 1; tolerance used in function <code>snipals_shen</code>. </li><li><code>maxit</code> : Only when q &gt; 1; maximum nb. of iterations used in function <code>snipals_shen</code>.    </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.    </li></ul><p>Same as function <code>plslda</code> (PLSR-LDA) except that a sparse PLSR (function <code>splsr</code>), instead of a PLSR,  is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
meth = :soft ; nvar = 10
model = splslda(; nlv, meth, nvar) 
#model = splsqda(; nlv, meth, nvar, alpha = .1) 
#model = splskdeda(; nlv, meth, nvar) 
#model = splskdeda(; nlv, meth, nvar, a = .8) 
fit!(model, Xtrain, ytrain)
@names model
@names fitm = model.fitm

fitm.lev
fitm.ni
fitm.priors

fitm_emb = fitm.fitm_emb ;
typeof(fitm_emb)
@names fitm_emb  
fitm_emb.sellv
fitm_emb.sel
@head fitm_emb.T
@head transf(model, Xtrain)
@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

coef(fitm_emb)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred
summary(fitm_emb, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/splslda.jl#L1-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.splsqda-Tuple{}"><a class="docstring-binding" href="#Jchemo.splsqda-Tuple{}"><code>Jchemo.splsqda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">splsqda(; kwargs...)
splsqda(X, y; kwargs...)
splsqda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLS-QDA (with continuum).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code> : Method used for the sparse thresholding. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each LV. Can be a single integer (i.e. same nb.    of variables for each LV), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>tol</code> : Only when q &gt; 1; tolerance used in function <code>snipals_shen</code>. </li><li><code>maxit</code> : Only when q &gt; 1; maximum nb. of iterations used in function <code>snipals_shen</code>.    </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.    </li></ul><p>Same as function <code>plsqda</code> (PLSR-QDA) except that a sparse PLSR (function <code>splsr</code>), instead of a PLSR,  is run on the Y-dummy table.</p><p>See function <code>splslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/splsqda.jl#L1-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.splsr-Tuple{}"><a class="docstring-binding" href="#Jchemo.splsr-Tuple{}"><code>Jchemo.splsr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">splsr(; kwargs...)
splsr(X, Y; kwargs...)
splsr(X, Y, weights::Weight; kwargs...)
splsr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)</code></pre><p>Sparse partial least squares regression (Lê Cao et al. 2008)</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code> : Method used for the sparse thresholding. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each LV. Can be a single integer (i.e. same nb.    of variables for each LV), or a vector of length <code>nlv</code>.   </li><li><code>tol</code> : Only when q &gt; 1; tolerance used in function <code>snipals_shen</code>. </li><li><code>maxit</code> : Only when q &gt; 1; maximum nb. of iterations used in function <code>snipals_shen</code>.    </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.    </li></ul><p>Sparse partial least squares regression algorihm of Lê Cao et al. 2008, but with the fast  &quot;improved kernel algorithm #1&quot; of Dayal &amp; McGregor (1997) used instead Nipals (results are the same). </p><p>In the present version of <code>splsr</code>, only the <code>X</code>-loading weights (not the <code>Y</code>-loading weights) are penalized.  The function provides two thresholding methods: <code>:soft</code> and <code>:hard</code>, see function <code>spca</code> for description. </p><p>In brief, to penalize the <code>X</code>-loading weights, the trick of Lê Cao et al. 2008 algorithm is to apply the  sPCA-rSVD algorithm (Shen &amp; Huang 2008) on matrix <code>Y&#39;X</code> (instead of <code>X</code> in sparse PCA).</p><p>When <code>meth = :soft</code> the function returns the same results as function <code>spls</code> of the R package mixOmics  (Lê Cao et al.) when regression mode and no sparseness on <code>Y</code> are specified.</p><p>The case <code>nvar = 1</code> corresponds to the Covsel regression method described in Roger et al 2011 (see also  Höskuldsson 1992).</p><p><strong>References</strong></p><p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms. Journal of Chemometrics 11, 73-85.</p><p>Höskuldsson, A., 1992. The H-principle in modelling with applications to chemometrics. Chemometrics  and Intelligent Laboratory Systems, Proceedings of the 2nd Scandinavian Symposium on Chemometrics 14,  139–153. https://doi.org/10.1016/0169-7439(92)80099-P</p><p>Lê Cao, K.-A., Rossouw, D., Robert-Granié, C., Besse, P., 2008. A Sparse PLS for Variable Selection  when Integrating Omics Data. Statistical Applications in Genetics and Molecular Biology 7.  https://doi.org/10.2202/1544-6115.1390</p><p>Kim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien Dejean with key contributors Benoit Gautier, Francois  Bartolo, contributions from Pierre Monget, Jeff Coquery, FangZou Yao and Benoit Liquet. (2016). mixOmics: Omics Data  Integration Project. R package version 6.1.1.  https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Package mixOmics on Bioconductor: https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Roger, J.M., Palagos, B., Bertrand, D., Fernandez-Ahumada, E., 2011. Covsel: Variable selection for highly  multivariate and multi-response calibration: Application to IR spectroscopy.  Chem. Lab. Int. Syst. 106, 216-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
meth = :soft
#meth = :hard
nvar = 20
model = splsr(; nlv, meth, nvar) ;
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
@names fitm
@head fitm.T
@head fitm.W

fitm.niter

fitm.sellv
fitm.sel

coef(model)
coef(model; nlv = 3)

@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

res = summary(model, Xtrain) ;
@names res
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/splsr.jl#L1-L108">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.splsrda-Tuple{}"><a class="docstring-binding" href="#Jchemo.splsrda-Tuple{}"><code>Jchemo.splsrda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">splsrda(; kwargs...)
splsrda(X, y; kwargs...)
splsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLSR-DA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code> : Method used for the sparse thresholding. Possible values are: <code>:soft</code>, <code>:hard</code>. See thereafter.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each LV. Can be a single integer (i.e. same nb.    of variables for each LV), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class membership. Possible values are: <code>:prop</code> (proportionnal),    <code>:unif</code> (uniform), or a vector (of length equal to the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as <code>mlev(y)</code>).</li><li><code>tol</code> : Only when q &gt; 1; tolerance used in function <code>snipals_shen</code>. </li><li><code>maxit</code> : Only when q &gt; 1; maximum nb. of iterations used in function <code>snipals_shen</code>.    </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.    </li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that a sparse PLSR (function <code>splsr</code>), instead of a PLSR,  is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
meth = :soft ; nvar = 10
#meth = :soft ; nvar = 1
model = splsrda(; nlv, meth, nvar) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

fitm.fitm.sellv
fitm.fitm.sel

@head fitm.fitm.T
@head transf(model, Xtrain)
@head transf(model, Xtest)
@head transf(model, Xtest; nlv = 3)

coef(fitm.fitm)

res = predict(model, Xtest) ;
@names res
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(model, Xtest; nlv = 1:2).pred
summary(fitm.fitm, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/splsrda.jl#L1-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.ssr-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.ssr-Tuple{Any, Any}"><code>Jchemo.ssr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ssr(pred, Y)</code></pre><p>Compute the sum of squared prediction errors (SSR).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

model = plskern(nlv = 2)
fit!(model, Xtrain, Ytrain)
pred = predict(model, Xtest).pred
ssr(pred, Ytest)

model = plskern(nlv = 2)
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
ssr(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/scores.jl#L34-L61">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.stdv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.stdv-Tuple{Any}"><code>Jchemo.stdv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">stdv(x)
stdv(x, weights::Weight)</code></pre><p>Compute the uncorrected standard deviation of a vector.</p><ul><li><code>x</code> : A vector (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 1000
x = rand(n)
w = mweight(rand(n))

stdv(x)
stdv(x, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L55-L74">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.summ-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.summ-Tuple{Any}"><code>Jchemo.summ</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">summ(X; digits = 3)
summ(X, y; digits = 3)</code></pre><p>Summarize a dataset (or a variable).</p><ul><li><code>X</code> : A dataset (n, p).</li><li><code>y</code> : A categorical variable (n) (class membership).</li><li><code>digits</code> : Nb. digits in the outputs.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 50
X = rand(n, 3) 
y = rand(1:3, n)
res = summ(X)
@names res
summ(X[:, 2]).res

summ(X, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L573-L594">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.sumv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.sumv-Tuple{Any}"><code>Jchemo.sumv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sumv(x)
sumv(x, weights::Weight)</code></pre><p>Compute the sum of a vector. </p><ul><li><code>x</code> : A vector (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 100
x = rand(n)
w = mweight(rand(n)) 

sumv(x)
sumv(x, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L3-L22">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.svmda-Tuple{}"><a class="docstring-binding" href="#Jchemo.svmda-Tuple{}"><code>Jchemo.svmda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">svmda(; kwargs...)
svmda(X, y; kwargs...)</code></pre><p>Support vector machine for discrimination &quot;C-SVC&quot; (SVM-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>kern</code> : Type of kernel used to compute the Gram matrices.Possible values are: <code>:krbf</code>, <code>:kpol</code>,    <code>:klin</code>, <code>:ktanh</code>. See below.  </li><li><code>gamma</code> : <code>kern</code> parameter, see below.</li><li><code>degree</code> : <code>kern</code> parameter, see below.</li><li><code>coef0</code> : <code>kern</code> parameter, see below.</li><li><code>cost</code> : Cost of constraints violation C parameter.</li><li><code>epsilon</code> : Epsilon parameter in the loss function.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Kernel types: </p><ul><li>:krbf – radial basis function: exp(-gamma * ||x - y||^2)</li><li>:kpol – polynomial: (gamma * x&#39; * y + coef0)^degree</li><li>&quot;klin – linear: x&#39; * y</li><li>:ktan – sigmoid: tanh(gamma * x&#39; * y + coef0)</li></ul><p>The function is a wrapper to package LIBSVM.jl (that is an interface to library LIBSVM of Chang &amp; Li 2001) to fit a C-SVC discriminiation model.</p><p><strong>References</strong></p><p>Chang, C.-C. &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation (algorithms, formulae, ...) can be found  in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</p><p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support vector machines. ACM Transactions on Intelligent  Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</p><p>LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl</p><p>Schölkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization,  and beyond. Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

kern = :krbf ; gamma = 1e4
cost = 1000 ; epsilon = .5
model = svmda(; kern, gamma, cost, epsilon) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ; 
@names res 
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/svmda.jl#L1-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.svmr-Tuple{}"><a class="docstring-binding" href="#Jchemo.svmr-Tuple{}"><code>Jchemo.svmr</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">svmr(; kwargs...)
svmr(X, y; kwargs...)</code></pre><p>Support vector machine for regression (Epsilon-SVR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>kern</code> : Type of kernel used to compute the Gram matrices. Possible values are: <code>:krbf</code>, <code>:kpol</code>,    <code>:klin</code>, <code>:ktanh</code>. See below.  </li><li><code>gamma</code> : <code>kern</code> parameter, see below.</li><li><code>coef0</code> : <code>kern</code> parameter, see below.</li><li><code>degree</code> : <code>kern</code> parameter, see below.</li><li><code>cost</code> : Cost of constraints violation C parameter.</li><li><code>epsilon</code> : Epsilon parameter in the loss function.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>Kernel types: </p><ul><li>:krbf – radial basis function: exp(-gamma * ||x - y||^2)</li><li>:kpol – polynomial: (gamma * x&#39; * y + coef0)^degree</li><li>&quot;klin – linear: x&#39; * y</li><li>:ktan – sigmoid: tanh(gamma * x&#39; * y + coef0)</li></ul><p>The function is a wrapper to package LIBSVM.jl (that is an interface to library LIBSVM of Chang &amp; Li 2001) to fit a SVM regression model.</p><p><strong>References</strong></p><p>Chang, C.-C. &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation (algorithms, formulae, ...) can be found  in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</p><p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support vector machines. ACM Transactions on Intelligent  Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</p><p>LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl</p><p>Schölkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization, optimization,  and beyond. Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
p = nco(X)

kern = :krbf ; gamma = .1
cost = 1000 ; epsilon = 1
model = svmr(; kern, gamma, cost, epsilon) 
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
kern = :krbf ; gamma = .1
model = svmr(; kern, gamma) 
fit!(model, x, y)
pred = predict(model, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/svmr.jl#L1-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.tab-Tuple{AbstractArray}"><a class="docstring-binding" href="#Jchemo.tab-Tuple{AbstractArray}"><code>Jchemo.tab</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">tab(X::AbstractArray)
tab(X::DataFrame; group = nothing)</code></pre><p>Tabulation of categorical variables.</p><ul><li><code>x</code> : Categorical variable or dataset containing categorical variable(s).</li></ul><p>Specific for a dataset:</p><ul><li><code>group</code> : Vector of the names of the group variables to consider in <code>X</code> (by default: all the columns of <code>X</code>).</li></ul><p>The function returns sorted levels. It does not support inputs of type <code>Any</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, DataFrames

x = rand(1:3, 20)

res = tab(x)
res.keys
res.vals

n = 20
X = hcat(rand([&quot;1&quot;; &quot;2&quot;], n), rand([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], n))
df = DataFrame(X, [:v1, :v2])

tab(X[:, 2])
tab(X)

tab(df)
tab(df; group = [:v1, :v2])
tab(df; group = :v2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_table.jl#L1-L32">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.tabcont-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.tabcont-Tuple{Any, Any}"><code>Jchemo.tabcont</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">tabcont(x, q)</code></pre><p>Tabulate a continuous variable.</p><ul><li><code>x</code> : Continuous variable (n).</li><li><code>q</code> : Numerical values (K) separating the class levels from <code>x</code>.  </li></ul><p>The function returns K + 1 levels. For a given value x of vector <code>x</code> and <code>q</code> a vector  of length K: </p><ul><li>x &lt;= q[1]             : ==&gt; 1 </li><li>q[1] &lt; x &lt;= q[2]      : ==&gt; 2 </li><li>etc.</li><li>q[K - 1] &lt; x &lt;= q[K]  : ==&gt; K </li><li>q[K] &lt; x              : ==&gt; K + 1 </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = rand(100)
q = [.01; .5; .500001; .9; 1.1]

res = tabcont(x, q)
sum(res.n)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_table.jl#L68-L92">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.tabdupl-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.tabdupl-Tuple{Any}"><code>Jchemo.tabdupl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">tabdupl(x)</code></pre><p>Tabulate duplicated values in a vector.</p><ul><li><code>x</code> : Categorical variable.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

x = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;]
tab(x)
res = tabdupl(x)
res.keys
res.vals</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_table.jl#L45-L60">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.thresh_hard-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.thresh_hard-Tuple{Any, Any}"><code>Jchemo.thresh_hard</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">thresh_hard(x::Real, delta)</code></pre><p>Hard thresholding function.</p><ul><li><code>x</code> : Value (scalar) to transform.</li><li><code>delta</code> : Range for the thresholding.</li></ul><p>The returned value is:</p><ul><li>abs(<code>x</code>) &gt; <code>delta</code> ? <code>x</code> : 0</li></ul><p>where delta &gt;= 0.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie 

delta = .7
thresh_hard(3, delta)

x = LinRange(-2, 2, 500)
y = thresh_hard.(x, delta)
lines(x, y; axis = (xlabel = &quot;x&quot;, ylabel = &quot;f(x)&quot;))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L619-L640">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.thresh_soft-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.thresh_soft-Tuple{Any, Any}"><code>Jchemo.thresh_soft</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">thresh_soft(x::Real, delta)</code></pre><p>Soft thresholding function.</p><ul><li><code>x</code> : Value (scalar) to transform.</li><li><code>delta</code> : Range for the thresholding.</li></ul><p>The returned value is:</p><ul><li>sign(<code>x</code>) * max(0, abs(<code>x</code>) - <code>delta</code>)</li></ul><p>where delta &gt;= 0.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie 

delta = .7
thresh_soft(3, delta)

x = LinRange(-2, 2, 100)
y = thresh_soft.(x, delta)
lines(x, y; axis = (xlabel = &quot;x&quot;, ylabel = &quot;f(x)&quot;))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L647-L668">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Blockscal, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Blockscal, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Blockscal, Xbl)
transf!(object::Blockscal, Xbl)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_mb.jl#L127-L134">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Center, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Center, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Center, X)
transf!(object::Center, X::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L152-L159">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Comdim, Xbl; nlv = nothing)
transfbl(object::Comdim, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/comdim.jl#L198-L206">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Covsel, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Covsel, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Covsel, X; nlv = nothing)</code></pre><p>Compute selected variables from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which the selected variables are computed.</li><li><code>nlv</code> : Nb. variables to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/covsel.jl#L138-L145">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Cscale, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Cscale, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Cscale, X)
transf!(object::Cscale, X::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L282-L289">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.DetrendAirpls, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.DetrendAirpls, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::DetrendAirpls, X)
transf!(object::DetrendAirpls, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/detrend_airpls.jl#L62-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.DetrendArpls, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.DetrendArpls, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::DetrendArpls, X)
transf!(object::DetrendArpls, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/detrend_arpls.jl#L58-L65">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.DetrendAsls, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.DetrendAsls, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::DetrendAsls, X)
transf!(object::DetrendAsls, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/detrend_asls.jl#L64-L71">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.DetrendLo, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.DetrendLo, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::DetrendLo, X)
transf!(object::DetrendLo, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L57-L64">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.DetrendPol, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.DetrendPol, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::DetrendPol, X)
transf!(object::DetrendPol, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L138-L145">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Dkplsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/dkplsr.jl#L123-L130">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Fdif, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Fdif, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Fdif, X)
transf!(object::Fdif, X::Matrix, M::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li><li><code>M</code> : Pre-allocated output matrix (n, p - npoint + 1).</li></ul><p>The in-place function stores the output in <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L212-L221">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Interpl, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Interpl, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Interpl, X)
transf!(object::Interpl, X::Matrix, M::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li><li><code>M</code> : Pre-allocated output matrix (n, p).</li></ul><p>The in-place function stores the output in <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L290-L299">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Kpca, X; nlv = nothing)</code></pre><p>Compute PCs (scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kpca.jl#L100-L107">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Kplsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/kplsr.jl#L166-L173">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mavg, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mavg, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mavg, X)
transf!(object::Mavg, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L388-L395">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mbconcat, Xbl)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_mb.jl#L224-L230">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mbpca, Xbl; nlv = nothing)
transfbl(object::Mbpca, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbpca.jl#L229-L237">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mbplsprobda, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mbplsprobda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mbplsprobda, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplslda.jl#L133-L140">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mbplsr, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mbplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mbplsr, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsr.jl#L157-L164">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mbplsrda, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplsrda.jl#L119-L126">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Mbplswest, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Mbplswest, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Mbplswest, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/mbplswest.jl#L207-L214">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Plsprobda, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Plsprobda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Plsprobda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plslda.jl#L119-L126">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Plsrda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plsrda.jl#L112-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Rmgap, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Rmgap, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Rmgap, X)
transf!(object::Rmgap, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rmgap.jl#L50-L57">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Rosaplsr, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rosaplsr.jl#L196-L204">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Rp, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Rp, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Rp, X; nlv = nothing)</code></pre><p>Compute scores T from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which scores T are computed.</li><li><code>nlv</code> : Nb. scores to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rp.jl#L66-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Savgol, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Savgol, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Savgol, X)
transf!(object::Savgol, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L533-L540">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Scale, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Scale, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Scale, X)
transf!(object::Scale, X::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/center_scale.jl#L215-L222">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Snorm, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Snorm, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Snorm, X)
transf!(object::Snorm, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L610-L617">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Snv, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Snv, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Snv, X)
transf!(object::Snv, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/preprocessing.jl#L672-L679">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Soplsr, Xbl)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices) of X-data for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/soplsr.jl#L123-L129">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Spca, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Spca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Spca, X; nlv = nothing)
Compute principal components (PCs = scores T) from a fitted model and X-data.</code></pre><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/spca.jl#L167-L174">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Jchemo.Umap, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Jchemo.Umap, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Umap, X)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/umap.jl#L124-L130">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Union{Pca, Fda}, X; nlv = nothing)</code></pre><p>Compute principal components (PCs = scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcasvd.jl#L99-L106">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Union{Pcr, Spcr}, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/pcr.jl#L92-L99">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><a class="docstring-binding" href="#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transf(object::Union{Plsr, Splsr}, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plskern.jl#L172-L179">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}"><a class="docstring-binding" href="#Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transfbl(object::Cca, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/cca.jl#L165-L173">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}"><a class="docstring-binding" href="#Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transfbl(object::Ccawold, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/ccawold.jl#L226-L234">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}"><a class="docstring-binding" href="#Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transfbl(object::Plscan, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plscan.jl#L177-L185">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}"><a class="docstring-binding" href="#Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transfbl(object::Plstuck, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/plstuck.jl#L125-L133">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}"><a class="docstring-binding" href="#Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">transfbl(object::Rasvd, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs; = scores) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/rasvd.jl#L156-L164">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.treeda-Tuple{}"><a class="docstring-binding" href="#Jchemo.treeda-Tuple{}"><code>Jchemo.treeda</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">treeda(; kwargs...)
treeda(X, y; kwargs...)</code></pre><p>Discrimination tree (CART) with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: 0 ==&gt; keep all).</li><li><code>max_depth</code> : Maximum depth of the decision tree (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function is a wrapper of package `DecisionTree.jl&#39; to fit a single discrimnation tree (CART).</p><p><strong>References</strong></p><p>Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. Classification And Regression Trees.  Chapman &amp; Hall, 1984.</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting : trois thèmes statistiques autour de CART  en régression (These de doctorat). Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
@names dat
X = dat.X
Y = dat.Y
n, p = size(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

n_subfeatures = p / 3 
max_depth = 10
model = treeda(; n_subfeatures, max_depth) 
fit!(model, Xtrain, ytrain)
@names model
fitm = model.fitm ;
typeof(fitm)
@names fitm
typeof(fitm.fitm) 
@names fitm.fitm

fitm.lev
fitm.ni
fitm.priors

res = predict(model, Xtest) ; 
@names res 
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/treeda.jl#L1-L68">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.treer-Tuple{}"><a class="docstring-binding" href="#Jchemo.treer-Tuple{}"><code>Jchemo.treer</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">treer(; kwargs...)
treer(X, y; kwargs...)</code></pre><p>Regression tree (CART) with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: 0 ==&gt; keep all).</li><li><code>max_depth</code> : Maximum depth of the decision tree (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function is a wrapper of package `DecisionTree.jl&#39; to fit a single regression tree (CART).</p><p><strong>References</strong></p><p>Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. Classification And Regression Trees.  Chapman &amp; Hall, 1984.</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting : trois thèmes statistiques autour de CART  en régression (These de doctorat). Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
p = nco(X)

n_subfeatures = p / 3 
max_depth = 15
model = treer(; n_subfeatures, max_depth) 
fit!(model, Xtrain, ytrain)
@names model
@names model.fitm

res = predict(model, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/treer.jl#L1-L57">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.umap-Tuple{}"><a class="docstring-binding" href="#Jchemo.umap-Tuple{}"><code>Jchemo.umap</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">umap(; kwargs...)
umap(X; kwargs...)</code></pre><p>UMAP: Uniform manifold approximation and projection for dimension reduction</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>psamp</code> : Proportion of sampling in <code>X</code> for training.</li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>metric</code> : Distance metric used. This can be any subtype of the <code>SemiMetric</code> type from    the <code>Distances.jl</code> package, including user-defined types. Default is <code>Distances.Euclidean()</code>.</li><li><code>n_neighbors</code> : Nb. approximate neighbors used to construct the initial high-dimensional graph.</li><li><code>min_dist</code> : Minimum distance between points in low-dimensional space.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p>The function fits a UMAP dimension reduction using package `UMAP.jl&#39;.</p><p>If <code>psamp &lt; 1</code>, only a proportion <code>psamp</code> of the observations (rows of <code>X</code>) are used to build the model (systematic  sampling over the first score of the PCA of <code>X</code>). Can be used to decrease computation times when n is large.</p><p><strong>References</strong></p><p>https://github.com/dillondaudert/UMAP.jl</p><p>McInnes, L, Healy, J, Melville, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.  ArXiV 1802.03426, 2018. https://arxiv.org/abs/1802.03426</p><p>https://umap-learn.readthedocs.io/en/latest/how<em>umap</em>works.html</p><p>https://pair-code.github.io/understanding-umap/ </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JLD2, DataFrames, GLMakie
using Distances

using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;challenge2018.jld2&quot;) 
@load db dat
@names dat
X = dat.X 
Y = dat.Y
wlst = names(X)
wl = parse.(Float64, wlst)
ntot = nro(X)
summ(Y)
y = Y.conc
ycla = Y.typ
test = Y.test
## Preprocessing
model1 = snv() 
model2 = savgol(npoint = 21, deriv = 2, degree = 3)
model = pip(model1, model2)
fit!(model, X)
@head Xp = transf(model, X)
plotsp(Xp, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;, nsamp = 20).f
## Tot =&gt; Train + Test
s = Bool.(test)
Xtrain = rmrow(Xp, s)
ytrain = rmrow(y, s)
yclatrain = rmrow(ycla, s)
Xtest = Xp[s, :]
ytest = y[s]
yclatest = ycla[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)
tab(string.(ycla, &quot;-&quot;, Y.label))
##### End

psamp = .2  # to decrease the computation time for the example
#psamp = 1  # all samples
nlv = 3
metric = Distances.Euclidean()
#metric = Distances.CosineDist()
#metric = Jchemo.SamDist()
n_neighbors = 20 ; min_dist = .4 
model = umap(; psamp, nlv, metric, n_neighbors, min_dist)  
fit!(model, Xtrain)
fitm = model.fitm ;
@names fitm 
@head T = fitm.T
@head T_all = transf(model, Xtrain)   # full training scores after refitting 
@head Ttest = transf(model, Xtest)

s = fitm.s
zycla = yclatrain[s]
lev = mlev(zycla)
nlev = length(lev)
colm = cgrad(:tab10, nlev; categorical = true, alpha = .5)
i = 1
f, ax = plotxyz(T[:, i], T[:, i + 1], T[:, i + 2], zycla; size = (700, 500), color = colm, markersize = 10, 
    title = &quot;Umap score space&quot;, xlabel = string(&quot;LV&quot;, i), ylabel = string(&quot;LV&quot;, i + 1), zlabel = string(&quot;LV&quot;, i + 2))
scatter!(ax, Ttest[:, i], Ttest[:, i + 1], Ttest[:, i + 2], color = :black, colormap = :tab20, markersize = 7)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/umap.jl#L1-L97">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.varv-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.varv-Tuple{Any}"><code>Jchemo.varv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">varv(x)
varv(x, weights::Weight)</code></pre><p>Compute the uncorrected variance of a vector.</p><ul><li><code>x</code> : A vector (n).</li><li><code>weights</code> : Weights (n) of the observations. Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

n = 1000
x = rand(n)
w = mweight(rand(n))

varv(x)
varv(x, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util_stat.jl#L82-L101">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.vcatdf-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.vcatdf-Tuple{Any}"><code>Jchemo.vcatdf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">vcatdf(dat; cols = :intersect)</code></pre><p>Vertical concatenation of a list of dataframes.</p><ul><li><code>dat</code> : List (vector) of dataframes.</li><li><code>cols</code> : Determines the columns of the returned dataframe. See ?DataFrames.vcat.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, DataFrames

dat1 = DataFrame(rand(5, 2), [:v3, :v1]) 
dat2 = DataFrame(100 * rand(2, 2), [:v3, :v1])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)

dat2 = DataFrame(100 * rand(2, 2), [:v1, :v3])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)

dat2 = DataFrame(100 * rand(2, 3), [:v3, :v1, :a])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)
Jchemo.vcatdf(dat; cols = :union)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L675-L699">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.vcol-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.vcol-Tuple{Any, Any}"><code>Jchemo.vcol</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">vcol(X::AbstractMatrix, j)
vcol(X::DataFrame, j)
vcol(x::Vector, j)</code></pre><p>View of the j-th column(s) of a matrix <code>X</code>, or of the j-th element(s) of vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L713-L718">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.vip-Tuple{Union{Jchemo.Mbplsr, Jchemo.Pcr, Jchemo.Plsr, Jchemo.Spcr, Jchemo.Splsr}}"><a class="docstring-binding" href="#Jchemo.vip-Tuple{Union{Jchemo.Mbplsr, Jchemo.Pcr, Jchemo.Plsr, Jchemo.Spcr, Jchemo.Splsr}}"><code>Jchemo.vip</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">vip(object::Union{Pcr, Plsr, Spcr, Splsr, Mbplsr}; nlv = nothing)
vip(object::Union{Pcr, Plsr, Spcr, Splsr, Mbplsr}, Y; nlv = nothing)</code></pre><p>Variable importance on Projections (VIP).</p><ul><li><code>object</code> : The fitted model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to consider. If <code>nothing</code>, the maximal model is considered.</li></ul><p>For a PLS model  (or PCR, etc.) fitted on (X, Y) with a number of A latent variables, and for variable  xj (column j of X): </p><ul><li>VIP(xj) = Sum.a(1,...,A) R2(Yc, ta) waj^2 / Sum.a(1,...,A) R2(Yc, ta) (1 / p) </li></ul><p>where:</p><ul><li>Yc is the centered Y, </li><li>ta is the a-th X-score, </li><li>R2(Yc, ta) is the proportion of Yc-variance explained by ta, i.e. ||Yc.hat||^2 / ||Yc||^2 (where Yc.hat    is the LS estimate of Yc by ta).  </li></ul><p>When <code>Y</code> is used, R2(Yc, ta) is replaced by the redundancy Rd(Yc, ta) (see function <code>rd</code>), such as in  Tenenhaus 1998 p.139. </p><p><strong>References</strong></p><p>Chong, I.-G., Jun, C.-H., 2005. Performance of some variable selection methods when  multicollinearity is present. Chemometrics and Intelligent Laboratory Systems 78, 103–112.  https://doi.org/10.1016/j.chemolab.2004.12.011</p><p>Mehmood, T., Sæbø, S., Liland, K.H., 2020. Comparison of variable selection methods in partial least  squares regression. Journal of Chemometrics 34, e3226. https://doi.org/10.1002/cem.3226</p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 27 18 7 6 ; 12 11 28 7] 
Y = [10. 11 13; 120 131 27; 8 12 4; 1 200 8; 100 10 89] 
y = Y[:, 1] 
ycla = [1; 1; 1; 2; 2]

nlv = 3
model = plskern(; nlv)
fit!(model, X, y)
res = vip(model.fitm)
@names res
res.imp

fit!(model, X, Y)
vip(model.fitm).imp
vip(model.fitm, Y).imp

## For PLSDA

model = plsrda(; nlv) 
fit!(model, X, ycla)
@names model.fitm
fitm = model.fitm.fitm ;  # fitted PLS model
vip(fitm).imp
Ydummy = dummy(ycla).Y
vip(fitm, Ydummy).imp

model = plslda(; nlv) 
fit!(model, X, ycla)
@names model.fitm.fitm
fitm = model.fitm.fitm_emb ;  # fitted PLS model
vip(fitm).imp
vip(fitm, Ydummy).imp</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/vip.jl#L1-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.viperm!-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#Jchemo.viperm!-Tuple{Any, Any, Any}"><code>Jchemo.viperm!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">viperm!(model, X, Y; score = rmsep, psamp = .3, rep = 50)</code></pre><p>Variable importance by direct permutations.</p><ul><li><code>model</code> : Model to evaluate.</li><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).  </li></ul><p>Keyword arguments:</p><ul><li><code>score</code> : Function computing the prediction score (an error rate).</li><li><code>psamp</code> : Proportion of data used as validation set to compute the <code>score</code>.</li><li><code>rep</code> : Number of replications of the splitting calibration/validation. </li></ul><p>The principle is as follows:</p><ul><li>Data (X, Y) are splitted randomly to a calibration set (Xcal, Ycal) and a validation set (Xval, Yval).</li><li>The model is fitted on (Xcal, Ycal) and used to compute the predictions from Xval. The error rate (the <code>score</code>)    is computed by comparing these predictions to Yval, giving the reference error rate.</li><li>Consider Xval and a given variable (feature = column) j.<ul><li>a) The rows of variable j are permutated randomly (the other columns of Xval are unchanged), generating a new   matrix &quot;Xval.perm.j&quot;.</li><li>b) The model is used to compute the predictions from Xval.perm.j, and the error rate is computed by comparing    these predictions to Yval. The variable importance (for variable j) is the difference between this error rate and    the reference error rate.</li></ul></li><li>This process is run for each variable j, separately.</li></ul><p>The overall process above is replicated <code>rep</code> times. The outputs provided by the function are the average  results (i.e. over the <code>rep</code> replications;<code>imp</code>) and the results per replication (<code>res_rep</code>).</p><p>In general, this method returns similar results as the out-of-bag permutation method (such as the one used in random  forests; Breiman, 2001).</p><p><strong>References</strong></p><p>Breiman, L., 2001. Random Forests. Machine Learning 45, 5–32. https://doi.org/10.1023/A:1010933404324</p><p>Nørgaard, L., Saudland, A., Wagner, J., Nielsen, J.V., Munck, L., Engelsen, S.B., 2000. Interval Partial  Least-Squares Regression (iPLS): A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419. https://doi.org/10.1366/0003702001949500</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;tecator.jld2&quot;) 
@load db dat
@names dat
X = dat.X
Y = dat.Y 
wl_str = names(X)
wl = parse.(Float64, wl_str) 
ntot, p = size(X)
typ = Y.typ
namy = names(Y)[1:3]
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f
s = typ .== &quot;train&quot;
Xtrain = X[s, :]
Ytrain = Y[s, namy]
Xtest = rmrow(X, s)
Ytest = rmrow(Y[:, namy], s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Work on the j-th y-variable 
j = 2
nam = namy[j]
ytrain = Ytrain[:, nam]
ytest = Ytest[:, nam]

model = plskern(nlv = 9)
res = viperm!(model, Xtrain, ytrain; score = rmsep, rep = 50) ;
z = vec(res.imp)
f = Figure(size = (500, 400))
ax = Axis(f[1, 1]; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Importance&quot;)
scatter!(ax, wl, vec(z); color = (:red, .5))
u = [910; 950]
vlines!(ax, u; color = :grey, linewidth = 1)
f

model = rfr(n_trees = 10, max_depth = 2000, min_samples_leaf = 5)
res = viperm!(model, Xtrain, ytrain; rep = 50)
z = vec(res.imp)
f = Figure(size = (500, 400))
ax = Axis(f[1, 1]; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Importance&quot;)
scatter!(ax, wl, vec(z); color = (:red, .5))
u = [910; 950]
vlines!(ax, u; color = :grey, linewidth = 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/viperm.jl#L1-L88">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.vrow-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.vrow-Tuple{Any, Any}"><code>Jchemo.vrow</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">vrow(X::AbstractMatrix, i)
vrow(X::DataFrame, i)
vrow(x::Vector, i)</code></pre><p>View of the i-th row(s) of a matrix <code>X</code>, or of the i-th element(s) of vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L723-L728">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.wdis-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.wdis-Tuple{Any}"><code>Jchemo.wdis</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">wdis(d; typw = :bisquare, alpha = 0)</code></pre><p>Different functions to compute weights from distances.</p><ul><li><code>d</code> : Vector of distances.</li></ul><p>Keyword arguments:</p><ul><li><code>typw</code> : Define the weight function.</li><li><code>alpha</code> : Parameter of the weight function, see below.</li></ul><p>The returned weight vector is: </p><ul><li>w = f(<code>d</code> / q) where f is the weight function and q the 1-<code>alpha</code> quantile of <code>d</code> (Cleveland &amp; Grosse 1991).</li></ul><p>Possible values for <code>typw</code> are: </p><ul><li>:bisquare: w = (1 - d^2)^2 </li><li>:cauchy: w = 1 / (1 + d^2) </li><li>:epan: w = 1 - d^2 </li><li>:fair: w =  1 / (1 + d)^2 </li><li>:invexp: w = exp(-d) </li><li>:invexp2: w = exp(-d / 2) </li><li>:gauss: w = exp(-d^2)</li><li>:trian: w = 1 - d  </li><li>:tricube: w = (1 - d^3)^3  </li></ul><p><strong>References</strong></p><p>Cleveland, W.S., Grosse, E., 1991. Computational methods for local regression. Stat Comput 1, 47–62.  https://doi.org/10.1007/BF01890836</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie, Distributions

d = sort(sqrt.(rand(Chi(1), 1000)))
colm = cgrad(:tab10, collect(1:9)) ;
alpha = 0
f = Figure(size = (600, 500))
ax = Axis(f, xlabel = &quot;d&quot;, ylabel = &quot;Weight&quot;)
typw = :bisquare
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[1])
typw = :cauchy
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[2])
typw = :epan
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[3])
typw = :fair
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[4])
typw = :gauss
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[5])
typw = :trian
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[6])
typw = :invexp
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[7])
typw = :invexp2
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[8])
typw = :tricube
w = wdis(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = colm[9])
axislegend(&quot;Function&quot;, position = :lb)
f[1, 1] = ax
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/wdis.jl#L1-L68">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.winvs-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.winvs-Tuple{Any}"><code>Jchemo.winvs</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">winvs(d; h = 2, criw = 4, squared = false)
winvs!(d; h = 2, criw = 4, squared = false)</code></pre><p>Compute weights from distances using an inverse scaled exponential function.</p><ul><li><code>d</code> : A vector of distances.</li></ul><p>Keyword arguments:</p><ul><li><code>h</code> : A scaling positive scalar defining the shape of the weight function. </li><li><code>criw</code> : A positive scalar defining outliers in the distances vector <code>d</code>.</li><li><code>squared</code>: If <code>true</code>, distances are replaced by the squared distances; the weight function is then a Gaussian    (RBF) kernel function.</li></ul><p>Weights are computed by: </p><ul><li>exp(-<code>d</code> / (<code>h</code> * MAD(<code>d</code>)))</li></ul><p>or are set to 0 for extreme (potentially outlier) distances such as <code>d</code> &gt; Median(<code>d</code>) + criw * MAD(<code>d</code>). Finally,  weights are standardized to their maximal value. This is an adaptation of the weight function presented in Kim et al. 2011.</p><p>The weights decrease when distances increase. Lower is h, sharper is the decreasing function.</p><p><strong>References</strong></p><p>Kim S, Kano M, Nakagawa H, Hasebe S. Estimation of active pharmaceutical ingredients content using locally weighted  partial least squares and statistical wavelength selection. Int J Pharm. 2011; 421(2):269-274.  https://doi.org/10.1016/j.ijpharm.2011.10.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo, CairoMakie, Distributions

x1 = rand(Chisq(10), 100) ;
x2 = rand(Chisq(40), 10) ;
d = [sqrt.(x1) ; sqrt.(x2)]
h = 2 ; criw = 3
w = winvs(d; h, criw) ;
f = Figure(size = (600, 300))
ax1 = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Nb. observations&quot;)
hist!(ax1, d, bins = 30)
ax2 = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
scatter!(ax2, d, w)
f[1, 1] = ax1 
f[1, 2] = ax2 
f

d = collect(0:.5:15) ;
h = [.5, 1, 1.5, 2.5, 5, 10, Inf] 
#h = [1, 2, 5, Inf] 
w = winvs(d; h = h[1]) 
f = Figure(size = (500, 400))
ax = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
lines!(ax, d, w, label = string(&quot;h = &quot;, h[1]))
for i = 2:length(h)
    w = winvs(d; h = h[i])
    lines!(ax, d, w, label = string(&quot;h = &quot;, h[i]))
end
axislegend(&quot;Values of h&quot;; position = :lb)
f[1, 1] = ax
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/winvs.jl#L1-L58">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.wtal-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.wtal-Tuple{Any}"><code>Jchemo.wtal</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">wtal(d; a = 1)</code></pre><p>Compute weights from distances using the &#39;talworth&#39; distribution.</p><ul><li><code>d</code> : Vector of distances.</li></ul><p>Keyword arguments:</p><ul><li><code>a</code> : Parameter of the weight function, see below.</li></ul><p>The returned weight vector w has component w[i] = 1 if |<code>d</code>[i]| &lt;= <code>a</code>,  and w[i] = 0 if |<code>d</code>[i]| &gt; <code>a</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">d = rand(10)
wtal(d; a = .8)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/wtal.jl#L1-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.xfit-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.xfit-Tuple{Any}"><code>Jchemo.xfit</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">xfit(object)
xfit(object, X; nlv = nothing)
xfit!(object, X::Matrix; nlv = nothing)</code></pre><p>Matrix fitting from a bilinear model (e.g. PCA).</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : New X-data to be approximated from the model. Must be in the same scale as the X-data used to fit   the model <code>object</code>, i.e. before centering and eventual scaling.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. components (PCs or LVs) to consider. If <code>nothing</code>, it is the maximum nb. of components.</li></ul><p>Compute an approximate of matrix <code>X</code> from a bilinear model (e.g. PCA or PLS) fitted on <code>X</code>. The fitted X is  returned in the original scale of the X-data used to fit the model <code>object</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

X = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 
    27 18 7 6; 12 11 28 7] 
Y = [10. 11 13; 120 131 27; 8 12 4; 
    1 200 8; 100 10 89] 
n, p = size(X)
Xnew = X[1:3, :]
Ynew = Y[1:3, :]
y = Y[:, 1]
ynew = Ynew[:, 1]
weights = mweight(rand(n))

nlv = 2 
scal = false
#scal = true
model = pcasvd(; nlv, scal) ;
fit!(model, X)
fitm = model.fitm ;
@head xfit(fitm)
xfit(fitm, Xnew)
xfit(fitm, Xnew; nlv = 0)
xfit(fitm, Xnew; nlv = 1)
fitm.xmeans

@head X
@head xfit(fitm) + xresid(fitm, X)
@head xfit(fitm, X; nlv = 1) + xresid(fitm, X; nlv = 1)

@head Xnew
@head xfit(fitm, Xnew) + xresid(fitm, Xnew)

model = pcasvd(; nlv = min(n, p), scal) 
fit!(model, X)
fitm = model.fitm ;
@head xfit(fitm) 
@head xfit(fitm, X)
@head xresid(fitm, X)

nlv = 3
scal = false
#scal = true
model = plskern(; nlv, scal)
fit!(model, X, Y, weights) 
fitm = model.fitm ;
@head xfit(fitm)
xfit(fitm, Xnew)
xfit(fitm, Xnew, nlv = 0)
xfit(fitm, Xnew, nlv = 1)

@head X
@head xfit(fitm) + xresid(fitm, X)
@head xfit(fitm, X; nlv = 1) + xresid(fitm, X; nlv = 1)

@head Xnew
@head xfit(fitm, Xnew) + xresid(fitm, Xnew)

model = plskern(; nlv = min(n, p), scal) 
fit!(model, X, Y, weights) 
fitm = model.fitm ;
@head xfit(fitm) 
@head xfit(fitm, Xnew)
@head xresid(fitm, Xnew)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/xfit.jl#L1-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.xresid-Tuple{Any, Any}"><a class="docstring-binding" href="#Jchemo.xresid-Tuple{Any, Any}"><code>Jchemo.xresid</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">xresid(object, X; nlv = nothing)
xresid!(object, X::Matrix; nlv = nothing)</code></pre><p>Residual matrix from a bilinear model (e.g. PCA).</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : New X-data to be approximated from the model.Must be in the same scale as the X-data used to fit   the model <code>object</code>, i.e. before centering and eventual scaling.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. components (PCs or LVs) to consider. If <code>nothing</code>, it is the maximum nb. of components.</li></ul><p>Compute the residual matrix:</p><ul><li>E = <code>X</code> - X_fit</li></ul><p>where X_fit is the fitted X returned by function <code>xfit</code>. See <code>xfit</code> for examples.  ```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/xresid.jl#L1-L15">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.@names-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.@names-Tuple{Any}"><code>Jchemo.@names</code></a> — <span class="docstring-category">Macro</span></summary><section><div><pre><code class="language-julia hljs">@names x</code></pre><p>Return the names of the sub-objects contained in a object.</p><ul><li><code>x</code>: An object.</li></ul><p>Shortcut for function <code>propertynames</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L749-L755">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.@namvar-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.@namvar-Tuple{Any}"><code>Jchemo.@namvar</code></a> — <span class="docstring-category">Macro</span></summary><section><div><pre><code class="language-julia hljs">@namvar x</code></pre><p>Return the name of a variable.</p><ul><li><code>x</code> : A variable or function.</li></ul><p>Thanks to:  https://stackoverflow.com/questions/38986764/save-variable-name-as-string-in-julia</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

z = 1:5
Jchemo.@namvar z</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L356-L371">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.@pars-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.@pars-Tuple{Any}"><code>Jchemo.@pars</code></a> — <span class="docstring-category">Macro</span></summary><section><div><pre><code class="language-julia hljs">@pars fun</code></pre><p>Display the keyword arguments (with their default values) of a function.</p><ul><li><code>fun</code> : The name of a function.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Jchemo

@pars krr</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L759-L770">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.@plist-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.@plist-Tuple{Any}"><code>Jchemo.@plist</code></a> — <span class="docstring-category">Macro</span></summary><section><div><pre><code class="language-julia hljs">@plist x</code></pre><p>Display each element of a named list.</p><ul><li><code>x</code> : A list.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L775-L780">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.@pmod-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.@pmod-Tuple{Any}"><code>Jchemo.@pmod</code></a> — <span class="docstring-category">Macro</span></summary><section><div><pre><code class="language-julia hljs">@pmod fun</code></pre><p>Shortcut for function <code>parentmodule</code>.</p><ul><li><code>fun</code> : The name of a function.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">@pmod rand</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L735-L745">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jchemo.@type-Tuple{Any}"><a class="docstring-binding" href="#Jchemo.@type-Tuple{Any}"><code>Jchemo.@type</code></a> — <span class="docstring-category">Macro</span></summary><section><div><pre><code class="language-julia hljs">@type x</code></pre><p>Display the type and size of a dataset.</p><ul><li><code>x</code> : A dataset.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/09aaa4c95b7768ae5a8be2e536702c21bc44809d/src/_util.jl#L794-L799">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../domains/">« Available methods</a><a class="docs-footer-nextpage" href="../news/">News »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 2 February 2026 15:12">Monday 2 February 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
