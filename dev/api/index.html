<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Index of functions · Jchemo.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://mlesnoff.github.io/Jchemo.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Jchemo.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../domains/">Available methods</a></li><li class="is-active"><a class="tocitem" href>Index of functions</a></li><li><a class="tocitem" href="../news/">News</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Index of functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Index of functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mlesnoff/Jchemo.jl/blob/master/docs/src/api.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Index-of-functions"><a class="docs-heading-anchor" href="#Index-of-functions">Index of functions</a><a id="Index-of-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Index-of-functions" title="Permalink"></a></h1><p>Here is a list of all exported functions from Jchemo.jl. </p><p>For more details, click on the link and you&#39;ll be directed to the function help.</p><ul><li><a href="#Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Comdim, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Ccawold, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Plstuck, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Cca, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Rasvd, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Mbplsr, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Mbplswest, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Mbpca, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Fda}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Pca, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Pcr, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Plscan, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Kpca}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Spca, Any}"><code>Base.summary</code></a></li><li><a href="#Jchemo.aggstat-Tuple{Any, Any}"><code>Jchemo.aggstat</code></a></li><li><a href="#Jchemo.aggsum-Tuple{Vector, Vector}"><code>Jchemo.aggsum</code></a></li><li><a href="#Jchemo.aicplsr-Tuple{Any, Any}"><code>Jchemo.aicplsr</code></a></li><li><a href="#Jchemo.aov1-Tuple{Any, Any}"><code>Jchemo.aov1</code></a></li><li><a href="#Jchemo.bias-Tuple{Any, Any}"><code>Jchemo.bias</code></a></li><li><a href="#Jchemo.blockscal-Tuple{Any}"><code>Jchemo.blockscal</code></a></li><li><a href="#Jchemo.calds-Tuple{Any, Any}"><code>Jchemo.calds</code></a></li><li><a href="#Jchemo.calpds-Tuple{Any, Any}"><code>Jchemo.calpds</code></a></li><li><a href="#Jchemo.cca-Tuple{Any, Any}"><code>Jchemo.cca</code></a></li><li><a href="#Jchemo.ccawold-Tuple{Any, Any}"><code>Jchemo.ccawold</code></a></li><li><a href="#Jchemo.center-Tuple{Any}"><code>Jchemo.center</code></a></li><li><a href="#Jchemo.cglsr-Tuple{Any, Any}"><code>Jchemo.cglsr</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Rr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Krr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Cglsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Dkplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg}}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Kplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.colmad-Tuple{Any}"><code>Jchemo.colmad</code></a></li><li><a href="#Jchemo.colmean-Tuple{Any}"><code>Jchemo.colmean</code></a></li><li><a href="#Jchemo.colmed-Tuple{Any}"><code>Jchemo.colmed</code></a></li><li><a href="#Jchemo.colnorm-Tuple{Any}"><code>Jchemo.colnorm</code></a></li><li><a href="#Jchemo.colstd-Tuple{Any}"><code>Jchemo.colstd</code></a></li><li><a href="#Jchemo.colsum-Tuple{Any}"><code>Jchemo.colsum</code></a></li><li><a href="#Jchemo.colvar-Tuple{Any}"><code>Jchemo.colvar</code></a></li><li><a href="#Jchemo.comdim-Tuple{Any}"><code>Jchemo.comdim</code></a></li><li><a href="#Jchemo.conf-Tuple{Any, Any}"><code>Jchemo.conf</code></a></li><li><a href="#Jchemo.cor2-Tuple{Any, Any}"><code>Jchemo.cor2</code></a></li><li><a href="#Jchemo.corm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.corm</code></a></li><li><a href="#Jchemo.cosm-Tuple{Any}"><code>Jchemo.cosm</code></a></li><li><a href="#Jchemo.cosv-Tuple{Any, Any}"><code>Jchemo.cosv</code></a></li><li><a href="#Jchemo.covm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.covm</code></a></li><li><a href="#Jchemo.cscale-Tuple{Any}"><code>Jchemo.cscale</code></a></li><li><a href="#Jchemo.detrend-Tuple{Any}"><code>Jchemo.detrend</code></a></li><li><a href="#Jchemo.dfplsr_cg-Tuple{Any, Any}"><code>Jchemo.dfplsr_cg</code></a></li><li><a href="#Jchemo.difmean-Tuple{Any, Any}"><code>Jchemo.difmean</code></a></li><li><a href="#Jchemo.dkplskdeda-Tuple{Any, Any}"><code>Jchemo.dkplskdeda</code></a></li><li><a href="#Jchemo.dkplslda-Tuple{Any, Any}"><code>Jchemo.dkplslda</code></a></li><li><a href="#Jchemo.dkplsqda-Tuple{Any, Any}"><code>Jchemo.dkplsqda</code></a></li><li><a href="#Jchemo.dkplsr-Tuple{Any, Any}"><code>Jchemo.dkplsr</code></a></li><li><a href="#Jchemo.dkplsrda-Tuple{Any, Any}"><code>Jchemo.dkplsrda</code></a></li><li><a href="#Jchemo.dmkern-Tuple{Any}"><code>Jchemo.dmkern</code></a></li><li><a href="#Jchemo.dmnorm"><code>Jchemo.dmnorm</code></a></li><li><a href="#Jchemo.dmnormlog"><code>Jchemo.dmnormlog</code></a></li><li><a href="#Jchemo.dummy"><code>Jchemo.dummy</code></a></li><li><a href="#Jchemo.dupl-Tuple{Any}"><code>Jchemo.dupl</code></a></li><li><a href="#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><code>Jchemo.ensure_df</code></a></li><li><a href="#Jchemo.ensure_mat-Tuple{AbstractMatrix}"><code>Jchemo.ensure_mat</code></a></li><li><a href="#Jchemo.eposvd-Tuple{Any}"><code>Jchemo.eposvd</code></a></li><li><a href="#Jchemo.errp-Tuple{Any, Any}"><code>Jchemo.errp</code></a></li><li><a href="#Jchemo.euclsq-Tuple{Any, Any}"><code>Jchemo.euclsq</code></a></li><li><a href="#Jchemo.fblockscal-Tuple{Any, Any}"><code>Jchemo.fblockscal</code></a></li><li><a href="#Jchemo.fcenter-Tuple{Any, Any}"><code>Jchemo.fcenter</code></a></li><li><a href="#Jchemo.fcscale-Tuple{Any, Any, Any}"><code>Jchemo.fcscale</code></a></li><li><a href="#Jchemo.fda-Tuple{Any, Any}"><code>Jchemo.fda</code></a></li><li><a href="#Jchemo.fdasvd-Tuple{Any, Any}"><code>Jchemo.fdasvd</code></a></li><li><a href="#Jchemo.fdif-Tuple{Any}"><code>Jchemo.fdif</code></a></li><li><a href="#Jchemo.findindex-Tuple{Any, Any}"><code>Jchemo.findindex</code></a></li><li><a href="#Jchemo.findmax_cla-Tuple{Any}"><code>Jchemo.findmax_cla</code></a></li><li><a href="#Jchemo.frob-Tuple{Any}"><code>Jchemo.frob</code></a></li><li><a href="#Jchemo.fscale-Tuple{Any, Any}"><code>Jchemo.fscale</code></a></li><li><a href="#Jchemo.fweight-Tuple{Any}"><code>Jchemo.fweight</code></a></li><li><a href="#Jchemo.getknn-Tuple{Any, Any}"><code>Jchemo.getknn</code></a></li><li><a href="#Jchemo.gridcv-Tuple{Any, Any, Any}"><code>Jchemo.gridcv</code></a></li><li><a href="#Jchemo.gridcv_br-Tuple{Any, Any}"><code>Jchemo.gridcv_br</code></a></li><li><a href="#Jchemo.gridcv_lb-Tuple{Any, Any}"><code>Jchemo.gridcv_lb</code></a></li><li><a href="#Jchemo.gridcv_lv-Tuple{Any, Any}"><code>Jchemo.gridcv_lv</code></a></li><li><a href="#Jchemo.gridscore-NTuple{5, Any}"><code>Jchemo.gridscore</code></a></li><li><a href="#Jchemo.gridscore-Tuple{Jchemo.Pipeline, Any, Any, Any, Any}"><code>Jchemo.gridscore</code></a></li><li><a href="#Jchemo.gridscore_br-NTuple{4, Any}"><code>Jchemo.gridscore_br</code></a></li><li><a href="#Jchemo.gridscore_lb-NTuple{4, Any}"><code>Jchemo.gridscore_lb</code></a></li><li><a href="#Jchemo.gridscore_lv-NTuple{4, Any}"><code>Jchemo.gridscore_lv</code></a></li><li><a href="#Jchemo.head-Tuple{Any}"><code>Jchemo.head</code></a></li><li><a href="#Jchemo.interpl-Tuple{Any}"><code>Jchemo.interpl</code></a></li><li><a href="#Jchemo.isel!"><code>Jchemo.isel!</code></a></li><li><a href="#Jchemo.kdeda-Tuple{Any, Any}"><code>Jchemo.kdeda</code></a></li><li><a href="#Jchemo.knnda-Tuple{Any, Any}"><code>Jchemo.knnda</code></a></li><li><a href="#Jchemo.knnr-Tuple{Any, Any}"><code>Jchemo.knnr</code></a></li><li><a href="#Jchemo.kpca-Tuple{Any}"><code>Jchemo.kpca</code></a></li><li><a href="#Jchemo.kplskdeda-Tuple{Any, Any}"><code>Jchemo.kplskdeda</code></a></li><li><a href="#Jchemo.kplslda-Tuple{Any, Any}"><code>Jchemo.kplslda</code></a></li><li><a href="#Jchemo.kplsqda-Tuple{Any, Any}"><code>Jchemo.kplsqda</code></a></li><li><a href="#Jchemo.kplsr-Tuple{Any, Any}"><code>Jchemo.kplsr</code></a></li><li><a href="#Jchemo.kplsrda-Tuple{Any, Any}"><code>Jchemo.kplsrda</code></a></li><li><a href="#Jchemo.kpol-Tuple{Any, Any}"><code>Jchemo.kpol</code></a></li><li><a href="#Jchemo.krbf-Tuple{Any, Any}"><code>Jchemo.krbf</code></a></li><li><a href="#Jchemo.krr-Tuple{Any, Any}"><code>Jchemo.krr</code></a></li><li><a href="#Jchemo.krrda-Tuple{Any, Any}"><code>Jchemo.krrda</code></a></li><li><a href="#Jchemo.lda-Tuple{Any, Any}"><code>Jchemo.lda</code></a></li><li><a href="#Jchemo.lg-Tuple{Any, Any}"><code>Jchemo.lg</code></a></li><li><a href="#Jchemo.list-Tuple{Any, Integer}"><code>Jchemo.list</code></a></li><li><a href="#Jchemo.list-Tuple{Integer}"><code>Jchemo.list</code></a></li><li><a href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>Jchemo.locw</code></a></li><li><a href="#Jchemo.locwlv-Tuple{Any, Any, Any}"><code>Jchemo.locwlv</code></a></li><li><a href="#Jchemo.lwmlr-Tuple{Any, Any}"><code>Jchemo.lwmlr</code></a></li><li><a href="#Jchemo.lwmlrda-Tuple{Any, Any}"><code>Jchemo.lwmlrda</code></a></li><li><a href="#Jchemo.lwplslda-Tuple{Any, Any}"><code>Jchemo.lwplslda</code></a></li><li><a href="#Jchemo.lwplsqda-Tuple{Any, Any}"><code>Jchemo.lwplsqda</code></a></li><li><a href="#Jchemo.lwplsr-Tuple{Any, Any}"><code>Jchemo.lwplsr</code></a></li><li><a href="#Jchemo.lwplsravg-Tuple{Any, Any}"><code>Jchemo.lwplsravg</code></a></li><li><a href="#Jchemo.lwplsrda-Tuple{Any, Any}"><code>Jchemo.lwplsrda</code></a></li><li><a href="#Jchemo.mahsq-Tuple{Any, Any}"><code>Jchemo.mahsq</code></a></li><li><a href="#Jchemo.mahsqchol-Tuple{Any, Any}"><code>Jchemo.mahsqchol</code></a></li><li><a href="#Jchemo.matB"><code>Jchemo.matB</code></a></li><li><a href="#Jchemo.matW"><code>Jchemo.matW</code></a></li><li><a href="#Jchemo.mavg-Tuple{Any}"><code>Jchemo.mavg</code></a></li><li><a href="#Jchemo.mbconcat-Tuple{Any}"><code>Jchemo.mbconcat</code></a></li><li><a href="#Jchemo.mblock-Tuple{Any, Any}"><code>Jchemo.mblock</code></a></li><li><a href="#Jchemo.mbpca-Tuple{Any}"><code>Jchemo.mbpca</code></a></li><li><a href="#Jchemo.mbplskdeda-Tuple{Any, Any}"><code>Jchemo.mbplskdeda</code></a></li><li><a href="#Jchemo.mbplslda-Tuple{Any, Any}"><code>Jchemo.mbplslda</code></a></li><li><a href="#Jchemo.mbplsqda-Tuple{Any, Any}"><code>Jchemo.mbplsqda</code></a></li><li><a href="#Jchemo.mbplsr-Tuple{Any, Any}"><code>Jchemo.mbplsr</code></a></li><li><a href="#Jchemo.mbplsrda-Tuple{Any, Any}"><code>Jchemo.mbplsrda</code></a></li><li><a href="#Jchemo.mbplswest-Tuple{Any, Any}"><code>Jchemo.mbplswest</code></a></li><li><a href="#Jchemo.merrp-Tuple{Any, Any}"><code>Jchemo.merrp</code></a></li><li><a href="#Jchemo.miss-Tuple{Any}"><code>Jchemo.miss</code></a></li><li><a href="#Jchemo.mlev-Tuple{Any}"><code>Jchemo.mlev</code></a></li><li><a href="#Jchemo.mlr-Tuple{Any, Any}"><code>Jchemo.mlr</code></a></li><li><a href="#Jchemo.mlrchol-Tuple{Any, Any}"><code>Jchemo.mlrchol</code></a></li><li><a href="#Jchemo.mlrda-Tuple{Any, Any}"><code>Jchemo.mlrda</code></a></li><li><a href="#Jchemo.mlrpinv-Tuple{Any, Any}"><code>Jchemo.mlrpinv</code></a></li><li><a href="#Jchemo.mlrpinvn-Tuple{Any, Any}"><code>Jchemo.mlrpinvn</code></a></li><li><a href="#Jchemo.mlrvec-Tuple{Any, Any}"><code>Jchemo.mlrvec</code></a></li><li><a href="#Jchemo.model-Tuple{Function}"><code>Jchemo.model</code></a></li><li><a href="#Jchemo.mpar"><code>Jchemo.mpar</code></a></li><li><a href="#Jchemo.mse-Tuple{Any, Any}"><code>Jchemo.mse</code></a></li><li><a href="#Jchemo.msep-Tuple{Any, Any}"><code>Jchemo.msep</code></a></li><li><a href="#Jchemo.mweight-Tuple{Vector}"><code>Jchemo.mweight</code></a></li><li><a href="#Jchemo.mweightcla-Tuple{Vector}"><code>Jchemo.mweightcla</code></a></li><li><a href="#Jchemo.nco-Tuple{Any}"><code>Jchemo.nco</code></a></li><li><a href="#Jchemo.nipals-Tuple{Any}"><code>Jchemo.nipals</code></a></li><li><a href="#Jchemo.nipalsmiss-Tuple{Any}"><code>Jchemo.nipalsmiss</code></a></li><li><a href="#Jchemo.normw-Tuple{Any, Jchemo.Weight}"><code>Jchemo.normw</code></a></li><li><a href="#Jchemo.nro-Tuple{Any}"><code>Jchemo.nro</code></a></li><li><a href="#Jchemo.occod-Tuple{Any, Any}"><code>Jchemo.occod</code></a></li><li><a href="#Jchemo.occsd-Tuple{Any}"><code>Jchemo.occsd</code></a></li><li><a href="#Jchemo.occsdod-Tuple{Any, Any}"><code>Jchemo.occsdod</code></a></li><li><a href="#Jchemo.occstah-Tuple{Any}"><code>Jchemo.occstah</code></a></li><li><a href="#Jchemo.out-Tuple{Any, Any}"><code>Jchemo.out</code></a></li><li><a href="#Jchemo.outeucl-Tuple{Any}"><code>Jchemo.outeucl</code></a></li><li><a href="#Jchemo.outstah-Tuple{Any, Any}"><code>Jchemo.outstah</code></a></li><li><a href="#Jchemo.pcaeigen-Tuple{Any}"><code>Jchemo.pcaeigen</code></a></li><li><a href="#Jchemo.pcaeigenk-Tuple{Any}"><code>Jchemo.pcaeigenk</code></a></li><li><a href="#Jchemo.pcanipals-Tuple{Any}"><code>Jchemo.pcanipals</code></a></li><li><a href="#Jchemo.pcanipalsmiss-Tuple{Any}"><code>Jchemo.pcanipalsmiss</code></a></li><li><a href="#Jchemo.pcasph-Tuple{Any}"><code>Jchemo.pcasph</code></a></li><li><a href="#Jchemo.pcasvd-Tuple{Any}"><code>Jchemo.pcasvd</code></a></li><li><a href="#Jchemo.pcr-Tuple{Any, Any}"><code>Jchemo.pcr</code></a></li><li><a href="#Jchemo.pip-Tuple"><code>Jchemo.pip</code></a></li><li><a href="#Jchemo.plist-Tuple{Any}"><code>Jchemo.plist</code></a></li><li><a href="#Jchemo.plotconf-Tuple{Any}"><code>Jchemo.plotconf</code></a></li><li><a href="#Jchemo.plotgrid-Tuple{AbstractVector, Any}"><code>Jchemo.plotgrid</code></a></li><li><a href="#Jchemo.plotsp"><code>Jchemo.plotsp</code></a></li><li><a href="#Jchemo.plotxy-Tuple{Any, Any}"><code>Jchemo.plotxy</code></a></li><li><a href="#Jchemo.plscan-Tuple{Any, Any}"><code>Jchemo.plscan</code></a></li><li><a href="#Jchemo.plskdeda-Tuple{Any, Any}"><code>Jchemo.plskdeda</code></a></li><li><a href="#Jchemo.plskern-Tuple{Any, Any}"><code>Jchemo.plskern</code></a></li><li><a href="#Jchemo.plslda-Tuple{Any, Any}"><code>Jchemo.plslda</code></a></li><li><a href="#Jchemo.plsnipals-Tuple{Any, Any}"><code>Jchemo.plsnipals</code></a></li><li><a href="#Jchemo.plsqda-Tuple{Any, Any}"><code>Jchemo.plsqda</code></a></li><li><a href="#Jchemo.plsravg-Tuple{Any, Any}"><code>Jchemo.plsravg</code></a></li><li><a href="#Jchemo.plsrda-Tuple{Any, Any}"><code>Jchemo.plsrda</code></a></li><li><a href="#Jchemo.plsrosa-Tuple{Any, Any}"><code>Jchemo.plsrosa</code></a></li><li><a href="#Jchemo.plssimp-Tuple{Any, Any}"><code>Jchemo.plssimp</code></a></li><li><a href="#Jchemo.plstuck-Tuple{Any, Any}"><code>Jchemo.plstuck</code></a></li><li><a href="#Jchemo.plswold-Tuple{Any, Any}"><code>Jchemo.plswold</code></a></li><li><a href="#Jchemo.pmod-Tuple{Any}"><code>Jchemo.pmod</code></a></li><li><a href="#Jchemo.pnames-Tuple{Any}"><code>Jchemo.pnames</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.TreerDt, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.CalPds, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Krr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Qda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.CalDs, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mbplslda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occod, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.psize-Tuple{Any}"><code>Jchemo.psize</code></a></li><li><a href="#Jchemo.pval-Tuple{Distributions.Distribution, Any}"><code>Jchemo.pval</code></a></li><li><a href="#Jchemo.qda-Tuple{Any, Any}"><code>Jchemo.qda</code></a></li><li><a href="#Jchemo.r2-Tuple{Any, Any}"><code>Jchemo.r2</code></a></li><li><a href="#Jchemo.rasvd-Tuple{Any, Any}"><code>Jchemo.rasvd</code></a></li><li><a href="#Jchemo.rd-Tuple{Any, Any}"><code>Jchemo.rd</code></a></li><li><a href="#Jchemo.rda-Tuple{Any, Any}"><code>Jchemo.rda</code></a></li><li><a href="#Jchemo.recodcat2int-Tuple{Any}"><code>Jchemo.recodcat2int</code></a></li><li><a href="#Jchemo.recodnum2int-Tuple{Any, Any}"><code>Jchemo.recodnum2int</code></a></li><li><a href="#Jchemo.recovkwargs-Tuple{DataType, Any}"><code>Jchemo.recovkwargs</code></a></li><li><a href="#Jchemo.replacebylev-Tuple{Any, Any}"><code>Jchemo.replacebylev</code></a></li><li><a href="#Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}"><code>Jchemo.replacebylev2</code></a></li><li><a href="#Jchemo.replacedict-Tuple{Any, Any}"><code>Jchemo.replacedict</code></a></li><li><a href="#Jchemo.residcla-Tuple{Any, Any}"><code>Jchemo.residcla</code></a></li><li><a href="#Jchemo.residreg-Tuple{Any, Any}"><code>Jchemo.residreg</code></a></li><li><a href="#Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}"><code>Jchemo.rfda_dt</code></a></li><li><a href="#Jchemo.rfr_dt-Tuple{Any, Any}"><code>Jchemo.rfr_dt</code></a></li><li><a href="#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}"><code>Jchemo.rmcol</code></a></li><li><a href="#Jchemo.rmgap-Tuple{Any}"><code>Jchemo.rmgap</code></a></li><li><a href="#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}"><code>Jchemo.rmrow</code></a></li><li><a href="#Jchemo.rmsep-Tuple{Any, Any}"><code>Jchemo.rmsep</code></a></li><li><a href="#Jchemo.rmsepstand-Tuple{Any, Any}"><code>Jchemo.rmsepstand</code></a></li><li><a href="#Jchemo.rosaplsr-Tuple{Any, Any}"><code>Jchemo.rosaplsr</code></a></li><li><a href="#Jchemo.rowmean-Tuple{Any}"><code>Jchemo.rowmean</code></a></li><li><a href="#Jchemo.rownorm-Tuple{Any}"><code>Jchemo.rownorm</code></a></li><li><a href="#Jchemo.rowstd-Tuple{Any}"><code>Jchemo.rowstd</code></a></li><li><a href="#Jchemo.rowsum-Tuple{Any}"><code>Jchemo.rowsum</code></a></li><li><a href="#Jchemo.rowvar-Tuple{Any}"><code>Jchemo.rowvar</code></a></li><li><a href="#Jchemo.rp-Tuple{Any}"><code>Jchemo.rp</code></a></li><li><a href="#Jchemo.rpd-Tuple{Any, Any}"><code>Jchemo.rpd</code></a></li><li><a href="#Jchemo.rpdr-Tuple{Any, Any}"><code>Jchemo.rpdr</code></a></li><li><a href="#Jchemo.rpmatgauss"><code>Jchemo.rpmatgauss</code></a></li><li><a href="#Jchemo.rpmatli"><code>Jchemo.rpmatli</code></a></li><li><a href="#Jchemo.rr-Tuple{Any, Any}"><code>Jchemo.rr</code></a></li><li><a href="#Jchemo.rrchol-Tuple{Any, Any}"><code>Jchemo.rrchol</code></a></li><li><a href="#Jchemo.rrda-Tuple{Any, Any}"><code>Jchemo.rrda</code></a></li><li><a href="#Jchemo.rrr-Tuple{Any, Any}"><code>Jchemo.rrr</code></a></li><li><a href="#Jchemo.rv-Tuple{Any, Any}"><code>Jchemo.rv</code></a></li><li><a href="#Jchemo.sampcla"><code>Jchemo.sampcla</code></a></li><li><a href="#Jchemo.sampdf"><code>Jchemo.sampdf</code></a></li><li><a href="#Jchemo.sampdp-Tuple{Any, Int64}"><code>Jchemo.sampdp</code></a></li><li><a href="#Jchemo.sampks-Tuple{Any, Int64}"><code>Jchemo.sampks</code></a></li><li><a href="#Jchemo.samprand-Tuple{Int64, Int64}"><code>Jchemo.samprand</code></a></li><li><a href="#Jchemo.sampsys-Tuple{Any, Int64}"><code>Jchemo.sampsys</code></a></li><li><a href="#Jchemo.sampwsp-Tuple{Any, Any}"><code>Jchemo.sampwsp</code></a></li><li><a href="#Jchemo.savgk-Tuple{Int64, Int64, Int64}"><code>Jchemo.savgk</code></a></li><li><a href="#Jchemo.savgol-Tuple{Any}"><code>Jchemo.savgol</code></a></li><li><a href="#Jchemo.scale-Tuple{Any}"><code>Jchemo.scale</code></a></li><li><a href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>Jchemo.segmkf</code></a></li><li><a href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>Jchemo.segmts</code></a></li><li><a href="#Jchemo.selwold-Tuple{Any, Any}"><code>Jchemo.selwold</code></a></li><li><a href="#Jchemo.sep-Tuple{Any, Any}"><code>Jchemo.sep</code></a></li><li><a href="#Jchemo.snorm-Tuple{Any}"><code>Jchemo.snorm</code></a></li><li><a href="#Jchemo.snv-Tuple{Any}"><code>Jchemo.snv</code></a></li><li><a href="#Jchemo.soft-Tuple{Any, Any}"><code>Jchemo.soft</code></a></li><li><a href="#Jchemo.softmax-Tuple{AbstractVector}"><code>Jchemo.softmax</code></a></li><li><a href="#Jchemo.soplsr-Tuple{Any, Any}"><code>Jchemo.soplsr</code></a></li><li><a href="#Jchemo.sourcedir-Tuple{Any}"><code>Jchemo.sourcedir</code></a></li><li><a href="#Jchemo.spca-Tuple{Any}"><code>Jchemo.spca</code></a></li><li><a href="#Jchemo.splskdeda-Tuple{Any, Any}"><code>Jchemo.splskdeda</code></a></li><li><a href="#Jchemo.splskern-Tuple{Any, Any}"><code>Jchemo.splskern</code></a></li><li><a href="#Jchemo.splslda-Tuple{Any, Any}"><code>Jchemo.splslda</code></a></li><li><a href="#Jchemo.splsqda-Tuple{Any, Any}"><code>Jchemo.splsqda</code></a></li><li><a href="#Jchemo.splsrda-Tuple{Any, Any}"><code>Jchemo.splsrda</code></a></li><li><a href="#Jchemo.ssq-Tuple{Any}"><code>Jchemo.ssq</code></a></li><li><a href="#Jchemo.ssr-Tuple{Any, Any}"><code>Jchemo.ssr</code></a></li><li><a href="#Jchemo.summ-Tuple{Any}"><code>Jchemo.summ</code></a></li><li><a href="#Jchemo.svmda-Tuple{Any, Any}"><code>Jchemo.svmda</code></a></li><li><a href="#Jchemo.svmr-Tuple{Any, Any}"><code>Jchemo.svmr</code></a></li><li><a href="#Jchemo.tab-Tuple{Any}"><code>Jchemo.tab</code></a></li><li><a href="#Jchemo.tabdf-Tuple{Any}"><code>Jchemo.tabdf</code></a></li><li><a href="#Jchemo.tabdupl-Tuple{Any}"><code>Jchemo.tabdupl</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Spca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Center, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Interpl, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Snorm, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbplslda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mavg, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Scale, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Pcr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Cscale, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Fdif, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Detrend, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rmgap, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Savgol, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Blockscal, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rp, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Snv, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}"><code>Jchemo.transfbl</code></a></li><li><a href="#Jchemo.treer_dt-Tuple{Any, Any}"><code>Jchemo.treer_dt</code></a></li><li><a href="#Jchemo.vcatdf-Tuple{Any}"><code>Jchemo.vcatdf</code></a></li><li><a href="#Jchemo.vcol-Tuple{Any, Any}"><code>Jchemo.vcol</code></a></li><li><a href="#Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}"><code>Jchemo.vip</code></a></li><li><a href="#Jchemo.viperm-Tuple{Any, Any, Any}"><code>Jchemo.viperm</code></a></li><li><a href="#Jchemo.vrow-Tuple{Any, Any}"><code>Jchemo.vrow</code></a></li><li><a href="#Jchemo.wdist-Tuple{Any}"><code>Jchemo.wdist</code></a></li><li><a href="#Jchemo.xfit-Tuple{Any}"><code>Jchemo.xfit</code></a></li><li><a href="#Jchemo.xresid-Tuple{Any, Any}"><code>Jchemo.xresid</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Cca, Any, Any}" href="#Base.summary-Tuple{Jchemo.Cca, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Cca, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/cca.jl#LL192-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Ccawold, Any, Any}" href="#Base.summary-Tuple{Jchemo.Ccawold, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Ccawold, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/ccawold.jl#LL255-L261">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Comdim, Any}" href="#Base.summary-Tuple{Jchemo.Comdim, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Comdim, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to    fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/comdim.jl#LL263-L269">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Fda}" href="#Base.summary-Tuple{Jchemo.Fda}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Fda)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/fda.jl#LL144-L149">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Kpca}" href="#Base.summary-Tuple{Jchemo.Kpca}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Kpca)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kpca.jl#LL127-L131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Mbpca, Any}" href="#Base.summary-Tuple{Jchemo.Mbpca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Mbpca, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to    fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbpca.jl#LL238-L244">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Mbplsr, Any}" href="#Base.summary-Tuple{Jchemo.Mbplsr, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Mbplsr, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to    fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsr.jl#LL145-L151">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Mbplswest, Any}" href="#Base.summary-Tuple{Jchemo.Mbplswest, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Mbplswest, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to    fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplswest.jl#LL199-L205">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Pca, Any}" href="#Base.summary-Tuple{Jchemo.Pca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Pca, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcasvd.jl#LL115-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Pcr, Any}" href="#Base.summary-Tuple{Jchemo.Pcr, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Pcr, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcr.jl#LL101-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Plscan, Any, Any}" href="#Base.summary-Tuple{Jchemo.Plscan, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Plscan, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plscan.jl#LL196-L202">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Plstuck, Any, Any}" href="#Base.summary-Tuple{Jchemo.Plstuck, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Plstuck, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plstuck.jl#LL135-L141">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Rasvd, Any, Any}" href="#Base.summary-Tuple{Jchemo.Rasvd, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Rasvd, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rasvd.jl#LL188-L194">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Spca, Any}" href="#Base.summary-Tuple{Jchemo.Spca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Spca, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/spca.jl#LL212-L217">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}" href="#Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Union{Plsr, Splsr}, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to    fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plskern.jl#LL234-L240">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aggstat-Tuple{Any, Any}" href="#Jchemo.aggstat-Tuple{Any, Any}"><code>Jchemo.aggstat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aggstat(X, y; fun = mean)
aggstat(X::DataFrame; vars, groups, fun = mean)</code></pre><p>Compute column-wise statistics by class in a dataset.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>y</code> : A categorical variable (n) (class membership).</li><li><code>fun</code> : Function to compute (default = mean).</li></ul><p>Specific for dataframes:</p><ul><li><code>vars</code> : Vector of the ames of the variables to summarize.</li><li><code>groups</code> : Vector of the names of the categorical variables to consider   for computations by class.</li></ul><p>Variables defined in <code>vars</code> and <code>groups</code> must be columns of <code>X</code>.</p><p>Return a matrix or, if only argument <code>X::DataFrame</code> is used, a dataframe.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrames, Statistics

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, :auto)
y = rand(1:3, n)
res = aggstat(X, y; fun = sum)
res.X
aggstat(df, y; fun = sum).X

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, string.(&quot;v&quot;, 1:p))
df.gr1 = rand(1:2, n)
df.gr2 = rand([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], n)
df
aggstat(df; vars = [:v1, :v2], groups = [:gr1, :gr2], fun = var)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aggsum-Tuple{Vector, Vector}" href="#Jchemo.aggsum-Tuple{Vector, Vector}"><code>Jchemo.aggsum</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aggsum(x::Vector, y::Vector)</code></pre><p>Compute sub-total sums by class of a categorical variable.</p><ul><li><code>x</code> : A quantitative variable to sum (n) </li><li><code>y</code> : A categorical variable (n) (class membership).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(1000)
y = vcat(rand([&quot;a&quot; ; &quot;c&quot;], 900), repeat([&quot;b&quot;], 100))
aggsum(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL58-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aicplsr-Tuple{Any, Any}" href="#Jchemo.aicplsr-Tuple{Any, Any}"><code>Jchemo.aicplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aicplsr(X, y; alpha = 2, kwargs...)</code></pre><p>Compute Akaike&#39;s (AIC) and Mallows&#39;s (Cp) criteria for univariate PLSR models.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate Y-data.</li></ul><p>Keyword arguments:</p><ul><li>Same arguments as those of function <code>cglsr</code>.</li><li><code>alpha</code> : Coefficient multiplicating   the model complexity (df) to compute AIC. </li></ul><p>The function uses function <code>dfplsr_cg</code>. </p><p><strong>References</strong></p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9</p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 40
res = aicplsr(X, y; nlv) ;
res.crit
res.opt
res.delta

zaic = res.crit.aic
f, ax = plotgrid(0:nlv, zaic; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;AIC&quot;)
scatter!(ax, 0:nlv, zaic)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/aicplsr.jl#LL1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aov1-Tuple{Any, Any}" href="#Jchemo.aov1-Tuple{Any, Any}"><code>Jchemo.aov1</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aov1(x, Y)
One-factor ANOVA test.</code></pre><ul><li><code>x</code> : Univariate categorical (factor) data (n).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
@head dat.X
x = dat.X[:, 5]
Y = dat.X[:, 1:4]
tab(x) 

res = aov1(x, Y) ;
pnames(res)
res.SSF
res.SSR 
res.F 
res.pval</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/aov1.jl#LL1-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.bias-Tuple{Any, Any}" href="#Jchemo.bias-Tuple{Any, Any}"><code>Jchemo.bias</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">bias(pred, Y)</code></pre><p>Compute the prediction bias, i.e. the opposite of the mean prediction error.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
bias(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
bias(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL6-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.blockscal-Tuple{Any}" href="#Jchemo.blockscal-Tuple{Any}"><code>Jchemo.blockscal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">blockscal(Xbl; kwargs...)
blockscal(Xbl, weights::Weight; kwargs...)</code></pre><p>Scale multiblock X-data.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>weights</code> : Weights (n) of the observations (rows    of the blocks). Must be of type <code>Weight</code> (see e.g.    function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>, <code>:mfa</code>, <code>:ncol</code>, <code>:sd</code>. See thereafter.</li><li><code>centr</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    is centered (before the block scaling).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>Types of block scaling:</p><ul><li><code>:none</code> : No block scaling. </li><li><code>:frob</code> : Let D be the diagonal matrix    of vector <code>weights.w</code>. Each block X is divided by    its Frobenius norm  = sqrt(tr(X&#39; * D * X)). After    this scaling, tr(X&#39; * D * X) = 1.</li><li><code>mfa</code> : Each block X is divided by sv, where sv is the    dominant singular value of X (this is the &quot;MFA&quot; approach).</li><li><code>ncol</code> : Each block X is divided by the nb.    of columns of the block.</li><li><code>sd</code> : Each block X is divided by    sqrt(sum(weighted variances of the block-columns)). After    this scaling, sum(weighted variances of the block-columns)    = 1.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5 ; m = 3 ; p = 10 
X = rand(n, p) 
Xnew = rand(m, p)
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl) 
Xblnew = mblock(Xnew, listbl) 
@head Xbl[3]

centr = true ; scal = true
bscal = :frob
mod = model(blockscal; centr, scal, bscal)
fit!(mod, Xbl)
zXbl = transf(mod, Xbl) ; 
@head zXbl[3]

zXblnew = transf(mod, Xblnew) ; 
zXblnew[3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/blockscal.jl#LL1-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.calds-Tuple{Any, Any}" href="#Jchemo.calds-Tuple{Any, Any}"><code>Jchemo.calds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calds(X1, X2; kwargs...)</code></pre><p>Direct standardization (DS) for calibration transfer of spectral data.</p><ul><li><code>X1</code> : Spectra (n, p) to transfer to the target.</li><li><code>X2</code> : Target spectra (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>fun</code> : Function used as transfer model.  </li><li>Other optional arguments for function <code>fun</code>.</li></ul><p><code>X1</code> and <code>X2</code> must represent the same n samples (&quot;standards&quot;).</p><p>The objective is to transform spectra <code>X1</code> to new spectra as close  as possible as the target <code>X2</code>. Method DS fits a model  (defined in <code>fun</code>) that predicts <code>X2</code> from <code>X1</code>.</p><p><strong>References</strong></p><p>Y. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
pnames(dat)
## Objects X1 and X2 are spectra collected 
## on the same samples. 
## X2 represents the target space. 
## We want to transfer X1 in the same space
## as X2.
## Data to transfer
X1cal = dat.X1cal
X1val = dat.X1val
n = nro(X1cal)
m = nro(X1val)
## Target space
X2cal = dat.X2cal
X2val = dat.X2val

## Fitting the model
mod = model(calds; fun = plskern, nlv = 10) 
#mod = model(calds; fun = mlrpinv)   # less robust
fit!(mod, X1cal, X2cal)

## Transfer of new spectra X1val 
## expected to be close to X2val
pred = predict(mod, X1val).pred

i = 1
f = Figure(size = (500, 300))
ax = Axis(f[1, 1])
lines!(X2val[i, :]; label = &quot;x2&quot;)
lines!(ax, X1val[i, :]; label = &quot;x1&quot;)
lines!(pred[i, :]; linestyle = :dash, label = &quot;x1_corrected&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/calds.jl#LL1-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.calpds-Tuple{Any, Any}" href="#Jchemo.calpds-Tuple{Any, Any}"><code>Jchemo.calpds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calpds(X1, X2; npoint = 5, fun = plskern, kwargs...)</code></pre><p>Piecewise direct standardization (PDS) for calibration transfer of spectral data.</p><ul><li><code>X1</code> : Spectra (n, p) to transfer to the target.</li><li><code>X2</code> : Target spectra (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Half-window size (nb. points left or right    to the given wavelength). </li><li><code>fun</code> : Function used as transfer model.  </li><li><code>kwargs</code> : Optional arguments for <code>fun</code>.</li></ul><p><code>X1</code> and <code>X2</code> must represent the same n standard samples.</p><p>The objective is to transform spectra <code>X1</code> to new spectra as close  as possible as the target <code>X2</code>. Method PDS fits models  (defined in <code>fun</code>) that predict <code>X2</code> from <code>X1</code>.</p><p>The window used in <code>X1</code> to predict wavelength &quot;i&quot; in <code>X2</code> is:</p><ul><li>i - <code>npoint</code>, i - <code>npoint</code> + 1, ..., i, ..., i + <code>npoint</code> - 1, i + <code>npoint</code></li></ul><p><strong>References</strong></p><p>Bouveresse, E., Massart, D.L., 1996. Improvement of the piecewise direct targetisation procedure  for the transfer of NIR spectra for multivariate calibration. Chemometrics and Intelligent Laboratory  Systems 32, 201–213. https://doi.org/10.1016/0169-7439(95)00074-7</p><p>Y. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.</p><p>Wülfert, F., Kok, W.Th., Noord, O.E. de, Smilde, A.K., 2000. Correction of Temperature-Induced  Spectral Variation by Continuous Piecewise Direct Standardization. Anal. Chem. 72, 1639–1644. https://doi.org/10.1021/ac9906835</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
pnames(dat)
## Objects X1 and X2 are spectra collected 
## on the same samples. 
## X2 represents the target space. 
## We want to transfer X1 in the same space
## as X2.
## Data to transfer
X1cal = dat.X1cal
X1val = dat.X1val
n = nro(X1cal)
m = nro(X1val)
## Target space
X2cal = dat.X2cal
X2val = dat.X2val

## Fitting the model
mod = model(calpds; npoint = 2, fun = plskern, nlv = 2) 
fit!(mod, X1cal, X2cal)

## Transfer of new spectra X1val 
## expected to be close to X2val
pred = predict(mod, X1val).pred

i = 1
f = Figure(size = (500, 300))
ax = Axis(f[1, 1])
lines!(X2val[i, :]; label = &quot;x2&quot;)
lines!(ax, X1val[i, :]; label = &quot;x1&quot;)
lines!(pred[i, :]; linestyle = :dash, label = &quot;x1_corrected&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/calpds.jl#LL1-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cca-Tuple{Any, Any}" href="#Jchemo.cca-Tuple{Any, Any}"><code>Jchemo.cca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cca(X, Y; kwargs...)
cca(X, Y, weights::Weight; kwargs...)
cca!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Canonical correlation Analysis (CCA, RCCA).</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This function implements a CCA algorithm using SVD decompositions and  presented in Weenink 2003 section 2. </p><p>A continuum regularization is available (parameter <code>tau</code>).  After block centering and scaling, the function returns  block scores (Tx and Ty) that are proportionnal to the  eigenvectors of Projx * Projy and Projy * Projx, respectively,  defined as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li><li>Cy = (1 - <code>tau</code>) * Y&#39;DY + <code>tau</code> * Iy</li><li>Cxy = X&#39;DY </li><li>Projx = sqrt(D) * X * invCx * X&#39; * sqrt(D)</li><li>Projy = sqrt(D) * Y * invCx * Y&#39; * sqrt(D)</li></ul><p>where D is the observation (row) metric.  Value <code>tau</code> = 0 can generate unstability when inverting  the covariance matrices. Often, a better alternative is  to use an epsilon value (e.g. <code>tau</code> = 1e-8) to get similar  results as with pseudo-inverses.  </p><p>The normed scores returned by the function are expected  (using uniform <code>weights</code>) to be the same as those  returned by functions <code>rcc</code> of the R packages <code>CCA</code> (González et al.)  and <code>mixOmics</code> (Lê Cao et al.) whith their parameters lambda1  and lambda2 set to:</p><ul><li>lambda1 = lambda2 = <code>tau</code> / (1 - <code>tau</code>) * n / (n - 1) </li></ul><p><strong>References</strong></p><p>González, I., Déjean, S., Martin, P.G.P., Baccini, A., 2008.  CCA: An R Package to Extend Canonical Correlation Analysis.  Journal of Statistical Software 23, 1-14.  https://doi.org/10.18637/jss.v023.i12</p><p>Hotelling, H. (1936): “Relations between two sets of variates”,  Biometrika 28: pp. 321–377.</p><p>Lê Cao, K.-A., Rohart, F., Gonzalez, I., Dejean, S., Abadi, A.J.,  Gautier, B., Bartolo, F., Monget, P., Coquery, J., Yao, F.,  Liquet, B., 2022. mixOmics: Omics Data Integration Project.  https://doi.org/10.18129/B9.bioc.mixOmics</p><p>Weenink, D. 2003. Canonical Correlation Analysis, Institute of  Phonetic Sciences, Univ. of Amsterdam, Proceedings 25, 81-99.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 3
bscal = :frob ; tau = 1e-8
mod = model(cca; nlv, bscal, tau)
fit!(mod, X, Y)
pnames(mod)
pnames(mod.fm)

@head mod.fm.Tx
@head transfbl(mod, X, Y).Tx

@head mod.fm.Ty
@head transfbl(mod, X, Y).Ty

res = summary(mod, X, Y) ;
pnames(res)
res.cort2t 
res.rdx
res.rdy
res.corx2t 
res.cory2t </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/cca.jl#LL1-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ccawold-Tuple{Any, Any}" href="#Jchemo.ccawold-Tuple{Any, Any}"><code>Jchemo.ccawold</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ccawold(X, Y; kwargs...)
ccawold(X, Y, weights::Weight; kwargs...)
ccawold!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Canonical correlation analysis (CCA, RCCA) - Wold      Nipals algorithm.</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>tol</code> : Tolerance value for convergence (Nipals).</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This function implements the Nipals ccawold algorithm  presented by Tenenhaus 1998 p.204 (related to Wold et al. 1984). </p><p>In this implementation, after each step of LVs computation,  X and Y are deflated relatively to their respective scores  (tx and ty). </p><p>A continuum regularization is available (parameter <code>tau</code>).  After block centering and scaling, the covariances matrices  are computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li><li>Cy = (1 - <code>tau</code>) * Y&#39;DY + <code>tau</code> * Iy</li></ul><p>where D is the observation (row) metric.  Value <code>tau</code> = 0 can generate unstability when inverting  the covariance matrices. Often, a better alternative is  to use an epsilon value (e.g. <code>tau</code> = 1e-8) to get similar  results as with pseudo-inverses.   </p><p>The normed scores returned by the function are expected  (using uniform <code>weights</code>) to be the same as those  returned by function <code>rgcca</code> of the R package <code>RGCCA</code>  (Tenenhaus &amp; Guillemot 2017, Tenenhaus et al. 2017). </p><p><strong>References</strong></p><p>Tenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and  Sparse Generalized Canonical Correlation Analysis for  Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html </p><p>Tenenhaus, M., 1998. La régression PLS: théorie et  pratique. Editions Technip, Paris.</p><p>Tenenhaus, M., Tenenhaus, A., Groenen, P.J.F., 2017.  Regularized Generalized Canonical Correlation Analysis:  A Framework for Sequential Multiblock Component Methods.  Psychometrika 82, 737–777.  https://doi.org/10.1007/s11336-017-9573-x</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984.  The Collinearity Problem in Linear Regression. The Partial  Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5,  735–743. https://doi.org/10.1137/0905052</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 2
bscal = :frob ; tau = 1e-4
mod = model(ccawold; nlv, bscal, tau, tol = 1e-10)
fit!(mod, X, Y)
pnames(mod)
pnames(mod.fm)

@head mod.fm.Tx
@head transfbl(mod, X, Y).Tx

@head mod.fm.Ty
@head transfbl(mod, X, Y).Ty

res = summary(mod, X, Y) ;
pnames(res)
res.explvarx
res.explvary
res.cort2t 
res.rdx
res.rdy
res.corx2t 
res.cory2t </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/ccawold.jl#LL1-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.center-Tuple{Any}" href="#Jchemo.center-Tuple{Any}"><code>Jchemo.center</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">center(X)
center(X, weights::Weight)</code></pre><p>Column-wise centering of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(center) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
colmean(Xptrain)
@head Xptest 
@head Xtest .- colmean(Xtrain)&#39;
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scale.jl#LL1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cglsr-Tuple{Any, Any}" href="#Jchemo.cglsr-Tuple{Any, Any}"><code>Jchemo.cglsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cglsr(X, y; kwargs...)
cglsr!(X::Matrix, y::Matrix; kwargs...)</code></pre><p>Conjugate gradient algorithm for the normal equations      (CGLS; Björck 1996).</p><ul><li><code>X</code> : X-data  (n, p).</li><li><code>y</code> : Univariate Y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. CG iterations.</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt    orthogonalization of the normal equation    residual vectors is done.</li><li><code>filt</code> : Boolean. If <code>true</code>, CG filter factors    are computed (output <code>F</code>). Default = <code>false</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    and <code>y</code> are scaled by its uncorrected standard    deviation (default = <code>false</code>).</li></ul><p><code>X</code> and <code>y</code> are internally centered. </p><p>CGLS algorithm &quot;7.4.1&quot; Bjorck 1996, p.289. The part of the  code computing the re-orthogonalization (Hansen 1998) and  filter factors (Vogel 1987, Hansen 1998) is a transcription  (with few adaptations) of the Matlab function <code>cgls</code> (Saunders et al.  https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).</p><p><strong>References</strong></p><p>Björck, A., 1996. Numerical Methods for Least Squares Problems,  Other Titles in Applied Mathematics. Society for Industrial and Applied  Mathematics. https://doi.org/10.1137/1.9781611971484</p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems,  Mathematical Modeling and Computation. Society for Industrial and Applied  Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9</p><p>Manne R. Analysis of two partial-least-squares algorithms for  multivariate calibration. Chemometrics Intell. Lab. Syst. 1987, 2: 187–197.</p><p>Phatak A, De Hoog F. Exploiting the connection between PLS,  Lanczos methods and conjugate gradients: alternative proofs  of some properties of PLS. J. Chemometrics 2002; 16: 361–367.</p><p>Vogel, C. R.,  &quot;Solving ill-conditioned linear systems using  the conjugate gradient method&quot;, Report, Dept. of Mathematical  Sciences, Montana State University, 1987.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 5 ; scal = true
mod = model(cglsr; nlv, scal) ;
fit!(mod, Xtrain, ytrain)
pnames(mod.fm) 
@head mod.fm.B
coef(mod.fm).B
coef(mod.fm).int

pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f   </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/cglsr.jl#LL1-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Cglsr}" href="#Jchemo.coef-Tuple{Jchemo.Cglsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Cglsr)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/cglsr.jl#LL171-L175">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Dkplsr}" href="#Jchemo.coef-Tuple{Jchemo.Dkplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Dkplsr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplsr.jl#LL135-L141">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Kplsr}" href="#Jchemo.coef-Tuple{Jchemo.Kplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Kplsr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplsr.jl#LL188-L194">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Krr}" href="#Jchemo.coef-Tuple{Jchemo.Krr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Krr; lb = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>lb</code> : Ridge regularization parameter    &quot;lambda&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/krr.jl#LL156-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Rosaplsr}" href="#Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Rosaplsr; nlv = nothing)</code></pre><p>Compute the X b-coefficients of a model fitted with <code>nlv</code> LVs.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rosaplsr.jl#LL216-L221">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Rr}" href="#Jchemo.coef-Tuple{Jchemo.Rr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Rr; lb = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>lb</code> : Ridge regularization parameter    &quot;lambda&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rr.jl#LL103-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg}}" href="#Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg}}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Mlr)</code></pre><p>Compute the coefficients of the fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL261-L265">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}}" href="#Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Union{Plsr, Pcr, Splsr}; nlv = nothing)</code></pre><p>Compute the b-coefficients of a LV model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul><p>For a model fitted from X(n, p) and Y(n, q), the returned  object <code>B</code> is a matrix (p, q). If <code>nlv</code> = 0, <code>B</code> is a matrix  of zeros. The returned object <code>int</code> is the intercept.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plskern.jl#LL191-L200">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colmad-Tuple{Any}" href="#Jchemo.colmad-Tuple{Any}"><code>Jchemo.colmad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colmad(X)</code></pre><p>Compute column-wise median absolute deviations (MAD) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)

colmad(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colmean-Tuple{Any}" href="#Jchemo.colmean-Tuple{Any}"><code>Jchemo.colmean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colmean(X)
colmean(X, weights::Weight)</code></pre><p>Compute column-wise means of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colmean(X)
colmean(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL18-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colmed-Tuple{Any}" href="#Jchemo.colmed-Tuple{Any}"><code>Jchemo.colmed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colmed(X)</code></pre><p>Compute column-wise medians of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)

colmed(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL42-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colnorm-Tuple{Any}" href="#Jchemo.colnorm-Tuple{Any}"><code>Jchemo.colnorm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colnorm(X)
colnorm(X, weights::Weight)</code></pre><p>Compute column-wise norms of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>The norm computed for a column x of <code>X</code> is:</p><ul><li>sqrt(x&#39; * x)</li></ul><p>The weighted norm is:</p><ul><li>sqrt(x&#39; * D * x), where D is the diagonal matrix of <code>weights.w</code>.</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colnorm(X)
colnorm(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL59-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colstd-Tuple{Any}" href="#Jchemo.colstd-Tuple{Any}"><code>Jchemo.colstd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colstd(X)
colstd(X, weights::Weight)</code></pre><p>Compute column-wise standard deviations (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colstd(X)
colstd(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL89-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colsum-Tuple{Any}" href="#Jchemo.colsum-Tuple{Any}"><code>Jchemo.colsum</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colsum(X)
colsum(X, weights::Weight)</code></pre><p>Compute column-wise sums of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colsum(X)
colsum(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL113-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colvar-Tuple{Any}" href="#Jchemo.colvar-Tuple{Any}"><code>Jchemo.colvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colvar(X)
colvar(X, weights::Weight)</code></pre><p>Compute column-wise variances (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(rand(n))

colvar(X)
colvar(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_colwise.jl#LL137-L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.comdim-Tuple{Any}" href="#Jchemo.comdim-Tuple{Any}"><code>Jchemo.comdim</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">comdim(Xbl; kwargs...)
comdim(Xbl, weights::Weight; kwargs...)
comdim!(Xbl::Matrix, weights::Weight; kwargs...)</code></pre><p>Common components and specific weights analysis (ComDim, <em>aka</em> CCSWA).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data.    Typically, output of function <code>mblock</code>.  </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>       for possible values.</li><li><code>tol</code> : Tolerance value for convergence (Nipals).</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>&quot;SVD&quot; algorithm of Hannafi &amp; Qannari 2008 p.84.</p><p>The function returns several objects, in particular:</p><ul><li><code>T</code> : The non normed global scores.</li><li><code>U</code> : The normed global scores.</li><li><code>W</code> : The global loadings.</li><li><code>Tbl</code> : The block scores (grouped by blocks, in the original scale).</li><li><code>Tb</code> : The block scores (grouped by LV, in the metric scale).</li><li><code>Wbl</code> : The block loadings.</li><li><code>lb</code> : The specific weights (saliences) &quot;lambda&quot;.</li><li><code>mu</code> : The sum of the squared saliences.</li></ul><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total inertia of X    (sum of the squared norms of the    blocks) explained by each global score.</li><li><code>explvarxx</code> : Proportion of the XX&#39; total inertia    (sum of the squared norms of the products X<em>k * X</em>k&#39;)    explained by each global score (= indicator &quot;V&quot; in Qannari    et al. 2000, Hanafi et al. 2008).</li><li><code>sal2</code> : Proportion of the squared saliences   of each block within each global score. </li><li><code>contr_block</code> : Contribution of each block    to the global scores (= proportions of the saliences    &quot;lambda&quot; within each score).<ul><li><code>explX</code> : Proportion of the inertia of the blocks </li></ul>explained by each global score.</li><li><code>corx2t</code> : Correlation between the global scores    and the original variables.  </li><li><code>cortb2t</code> : Correlation between the global scores    and the block scores.</li><li><code>rv</code> : RV coefficient. </li><li><code>lg</code> : Lg coefficient. </li></ul><p><strong>References</strong></p><p>Cariou, V., Qannari, E.M., Rutledge, D.N., Vigneau, E., 2018.  ComDim: From multiblock data analysis to path modeling. Food  Quality and Preference, Sensometrics 2016: Sensometrics-by-the-Sea  67, 27–34. https://doi.org/10.1016/j.foodqual.2017.02.012</p><p>Cariou, V., Jouan-Rimbaud Bouveresse, D., Qannari, E.M.,  Rutledge, D.N., 2019. Chapter 7 - ComDim Methods for the Analysis  of Multiblock Data in a Data Fusion Perspective, in: Cocchi, M. (Ed.),  Data Handling in Science and Technology,  Data Fusion Methodology and Applications. Elsevier, pp. 179–204.  https://doi.org/10.1016/B978-0-444-63984-4.00007-7</p><p>Ghaziri, A.E., Cariou, V., Rutledge, D.N., Qannari, E.M., 2016.  Analysis of multiblock datasets using ComDim: Overview and extension  to the analysis of (K + 1) datasets. Journal of Chemometrics 30,  420–429. https://doi.org/10.1002/cem.2810</p><p>Hanafi, M., 2008. Nouvelles propriétés de l’analyse en composantes  communes et poids spécifiques. Journal de la société française  de statistique 149, 75–97.</p><p>Qannari, E.M., Wakeling, I., Courcoux, P., MacFie, H.J.H., 2000.  Defining the underlying sensory dimensions. Food Quality and  Preference 11, 151–154.  https://doi.org/10.1016/S0950-3293(99)00069-5</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
pnames(dat) 
X = dat.X
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X[1:6, :], listbl)
Xblnew = mblock(X[7:8, :], listbl)
n = nro(Xbl[1]) 

nlv = 3
bscal = :frob
scal = false
#scal = true
mod = model(comdim; nlv, bscal, scal)
fit!(mod, Xbl)
pnames(mod) 
pnames(mod.fm)
## Global scores 
@head mod.fm.T
@head transf(mod, Xbl)
transf(mod, Xblnew)
## Blocks scores
i = 1
@head mod.fm.Tbl[i]
@head transfbl(mod, Xbl)[i]

res = summary(mod, Xbl) ;
pnames(res) 
res.explvarx
res.explvarxx
res.sal2 
res.contr_block
res.explX   # = mod.fm.lb if bscal = :frob
rowsum(Matrix(res.explX))
res.corx2t 
res.cortb2t
res.rv</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/comdim.jl#LL1-L124">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.conf-Tuple{Any, Any}" href="#Jchemo.conf-Tuple{Any, Any}"><code>Jchemo.conf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">conf(pred, y; digits = 1)</code></pre><p>Confusion matrix.</p><ul><li><code>pred</code> : Univariate predictions.</li><li><code>y</code> : Univariate observed data.</li></ul><p>Keyword arguments:</p><ul><li><code>digits</code> : Nb. digits used to round percentages.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CairoMakie

y = [&quot;d&quot;; &quot;c&quot;; &quot;b&quot;; &quot;c&quot;; &quot;a&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; 
    &quot;b&quot;; &quot;b&quot;; &quot;a&quot;; &quot;a&quot;; &quot;c&quot;; &quot;d&quot;; &quot;d&quot;]
pred = [&quot;a&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; 
    &quot;b&quot;; &quot;b&quot;; &quot;a&quot;; &quot;a&quot;; &quot;d&quot;; &quot;d&quot;; &quot;d&quot;]
#y = rand(1:10, 200); pred = rand(1:10, 200)

res = conf(pred, y) ;
pnames(res)
res.cnt       # Counts (dataframe built from `A`) 
res.pct       # Row %  (dataframe built from `Apct`))
res.A         
res.Apct
res.diagpct
res.accpct    # Accuracy (% classification successes)
res.lev       # Levels

plotconf(res).f

plotconf(res; cnt = false, ptext = false).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/conf.jl#LL1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cor2-Tuple{Any, Any}" href="#Jchemo.cor2-Tuple{Any, Any}"><code>Jchemo.cor2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cor2(pred, Y)</code></pre><p>Compute the squared linear correlation between data and predictions.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
cor2(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
cor2(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL37-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.corm-Tuple{Any, Jchemo.Weight}" href="#Jchemo.corm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.corm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">corm(X, weights::Weight)
corm(X, Y, weights::Weight)</code></pre><p>Compute a weighted correlation matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.   Object of type <code>Weight</code> (e.g. generated by    function <code>mweight</code>).</li></ul><p>Uncorrected correlation matrix </p><ul><li>of <code>X</code>-columns :  ==&gt; (p, p) matrix </li><li>or between <code>X</code>-columns and <code>Y</code>-columns :  ==&gt; (p, q) matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)
w = mweight(rand(n))

corm(X, w)
corm(X, Y, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL83-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cosm-Tuple{Any}" href="#Jchemo.cosm-Tuple{Any}"><code>Jchemo.cosm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cosm(X)
cosm(X, Y)</code></pre><p>Compute a cosinus matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li></ul><p>The function computes the cosinus matrix: </p><ul><li>of the columns of <code>X</code>:  ==&gt; (p, p) matrix </li><li>or between columns of <code>X</code> and <code>Y</code> :  ==&gt; (p, q) matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)

cosm(X)
cosm(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL132-L152">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cosv-Tuple{Any, Any}" href="#Jchemo.cosv-Tuple{Any, Any}"><code>Jchemo.cosv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cosv(x, y)</code></pre><p>Compute cosinus between two vectors.</p><ul><li><code>x</code> : vector (n).</li><li><code>y</code> : vector (n).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5
x = rand(n)
y = rand(n)

cosv(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL170-L184">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.covm-Tuple{Any, Jchemo.Weight}" href="#Jchemo.covm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.covm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">covm(X, weights::Weight)
covm(X, Y, weights::Weight)</code></pre><p>Compute a weighted covariance matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.   Object of type <code>Weight</code> (e.g. generated by    function <code>mweight</code>).</li></ul><p>The function computes the uncorrected weighted covariance  matrix: </p><ul><li>of the columns of <code>X</code>:  ==&gt; (p, p) matrix </li><li>or between columns of <code>X</code> and <code>Y</code> :  ==&gt; (p, q) matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)
w = mweight(rand(n))

covm(X, w)
covm(X, Y, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL188-L213">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cscale-Tuple{Any}" href="#Jchemo.cscale-Tuple{Any}"><code>Jchemo.cscale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cscale()
cscale(X)
cscale(X, weights::Weight)</code></pre><p>Column-wise centering and scaling of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))

db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(cscale) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
colmean(Xptrain)
colstd(Xptrain)
@head Xptest 
@head (Xtest .- colmean(Xtrain)&#39;) ./ colstd(Xtrain)&#39;
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scale.jl#LL117-L152">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.detrend-Tuple{Any}" href="#Jchemo.detrend-Tuple{Any}"><code>Jchemo.detrend</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">detrend(X; kwargs...)</code></pre><p>De-trend transformation of each row of X-data. </p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>degree</code> : Polynom degree.</li></ul><p>The function fits a polynomial regression to each observation and returns the residuals.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(detrend; degree = 2)
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain, wl).f
plotsp(Xptest, wl).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL1-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dfplsr_cg-Tuple{Any, Any}" href="#Jchemo.dfplsr_cg-Tuple{Any, Any}"><code>Jchemo.dfplsr_cg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dfplsr_cg(X, y; kwargs...)</code></pre><p>Compute the model complexity (df) of PLSR models with the CGLS algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate Y-data.</li></ul><p>Keyword arguments:</p><ul><li>Same as function <code>cglsr</code>.</li></ul><p>The number of degrees of freedom (<code>df</code>) of the PLSR model  is returned for 0, 1, ..., <code>nlv</code> LVs.</p><p><strong>References</strong></p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems,  Mathematical Modeling and Computation. Society for Industrial and  Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9</p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods  for estimating Mallows’s Cp and AIC criteria for PLSR models. Illustration  on agronomic spectroscopic NIR data. Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">## The example below reproduces the numerical illustration
## given by Kramer &amp; Sugiyama 2011 on the Ozone data 
## (Fig. 1, center).
## Function &quot;pls.model&quot; used for df calculations
## in the R package &quot;plsdof&quot; v0.2-9 (Kramer &amp; Braun 2019)
## automatically scales the X matrix before PLS.
## The example scales X for consistency with plsdof.

using JchemoData, JLD2, DataFrames, CairoMakie 
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ozone.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
dropmissing!(X) 
zX = rmcol(Matrix(X), 4) 
y = X[:, 4] 
## For consistency with plsdof
xstds = colstd(zX)
zXs = fscale(zX, xstds)
## End

nlv = 12 ; gs = true
res = dfplsr_cg(zXs, y; nlv, gs) ;
res.df 
df_kramer = [1.000000, 3.712373, 6.456417, 11.633565, 
    12.156760, 11.715101, 12.349716,
    12.192682, 13.000000, 13.000000, 
    13.000000, 13.000000, 13.000000]
f, ax = plotgrid(0:nlv, df_kramer; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;df&quot;)
scatter!(ax, 0:nlv, res.df; color = &quot;red&quot;)
ablines!(ax, 1, 1; color = :grey, linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dfplsr_cg.jl#LL1-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.difmean-Tuple{Any, Any}" href="#Jchemo.difmean-Tuple{Any, Any}"><code>Jchemo.difmean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">difmean(X1, X2; normx::Bool = false)</code></pre><p>Compute a 1-D detrimental matrix by difference of      the column-means of two X-datas.</p><ul><li><code>X1</code> : Spectra (n1, p).</li><li><code>X2</code> : Spectra (n2, p).</li></ul><p>Keyword arguments:</p><ul><li><code>normx</code> : Boolean. If <code>true</code>, the column-means vectors    of <code>X1</code> and <code>X2</code> are normed before computing their difference.</li></ul><p>The function returns a matrix <code>D</code> (1, p) computed by the difference  between two mean-spectra, i.e. the column-means of <code>X1</code> and <code>X2</code>. </p><p><code>D</code> is assumed to contain the detrimental information that can  be removed (by orthogonalization) from <code>X1</code> and <code>X2</code>  for  calibration transfer. For instance, <code>D</code> can be used as input of  function <code>eposvd</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
pnames(dat)
X1cal = dat.X1cal
X1val = dat.X1val
X2cal = dat.X2cal
X2val = dat.X2val

## The objective is to remove a detrimental 
## information (here, D) from spaces X1 and X2
D = difmean(X1cal, X2cal).D
res = eposvd(D; nlv = 1)
## Corrected Val matrices
X1val_c = X1val * res.M
X2val_c = X2val * res.M

i = 1
f = Figure(size = (800, 300))
ax1 = Axis(f[1, 1])
ax2 = Axis(f[1, 2])
lines!(ax1, X1val[i, :]; label = &quot;x1&quot;)
lines!(ax1, X2val[i, :]; label = &quot;x2&quot;)
axislegend(ax1, position = :cb, framevisible = false)
lines!(ax2, X1val_c[i, :]; label = &quot;x1_correct&quot;)
lines!(ax2, X2val_c[i, :]; label = &quot;x2_correct&quot;)
axislegend(ax2, position = :cb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/difmean.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplskdeda-Tuple{Any, Any}" href="#Jchemo.dkplskdeda-Tuple{Any, Any}"><code>Jchemo.dkplskdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dkplskdeda(X, y; kwargs...)
dkplskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>DKPLS-KDEDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plskdeda</code> (PLS-KDEDA) except that  a direct kernel PLSR (function <code>dkplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>dkplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplskdeda.jl#LL1-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplslda-Tuple{Any, Any}" href="#Jchemo.dkplslda-Tuple{Any, Any}"><code>Jchemo.dkplslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dkplslda(X, y; kwargs...)
dkplslda(X, y, weights::Weight; kwargs...)</code></pre><p>DKPLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plslda</code> (PLS-LDA) except that  a direct kernel PLSR (function <code>dkplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
gamma = .1
mod = model(dkplslda; nlv, gamma) 
#mod = model(dkplslda; nlv, gamma, prior = :prop) 
#mod = model(dkplsqda; nlv, gamma, alpha = .5) 
#mod = model(dkplskdeda; nlv, gamma, a_kde = .5) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

fmpls = fm.fm.fmpls ;
@head fmpls.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fmpls)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplslda.jl#LL1-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplsqda-Tuple{Any, Any}" href="#Jchemo.dkplsqda-Tuple{Any, Any}"><code>Jchemo.dkplsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dkplsqda(X, y; kwargs...)
dkplsqda(X, y, weights::Weight; kwargs...)</code></pre><p>DKPLS-QDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsqda</code> (PLS-QDA) except that  a direct kernel PLSR (function <code>dkplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>dkplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplsqda.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplsr-Tuple{Any, Any}" href="#Jchemo.dkplsr-Tuple{Any, Any}"><code>Jchemo.dkplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dkplsr(X, Y; kwargs...)
dkplsr(X, Y, weights::Weight; kwargs...)
dkplsr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Direct kernel partial least squares regression (DKPLSR)      (Bennett &amp; Embrechts 2003).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The method builds kernel Gram matrices and then runs a usual  PLSR algorithm on them. This is faster (but not equivalent) to the  &quot;true&quot; Nipals KPLSR algorithm (function <code>kplsr</code>) described in  Rosipal &amp; Trejo (2001).</p><p><strong>References</strong></p><p>Bennett, K.P., Embrechts, M.J., 2003. An optimization  perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and Applications,  NATO Science Series III: Computer &amp; Systems Sciences.  IOS Press Amsterdam, pp. 227-250.</p><p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least  Squares Regression in Reproducing Kernel Hilbert Space.  Journal of Machine Learning Research 2, 97-123.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
kern = :krbf ; gamma = 1e-1 ; scal = false
#gamma = 1e-4 ; scal = true
mod = model(dkplsr; nlv, kern, gamma, scal) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T

coef(mod)
coef(mod; nlv = 3)

@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f  

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
nlv = 2
gamma = 1 / 3
mod = model(dkplsr; nlv, gamma) ;
fit!(mod, x, y)
pred = predict(mod, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplsr.jl#LL1-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplsrda-Tuple{Any, Any}" href="#Jchemo.dkplsrda-Tuple{Any, Any}"><code>Jchemo.dkplsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dkplsrda(X, y; kwargs...)
dkplsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on direct kernel partial least squares      regression (KPLSR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that  a direct kernel PLSR (function <code>dkplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
kern = :krbf ; gamma = .001 
scal = true
mod = model(dkplsrda; nlv, kern, gamma, scal) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

@head fm.fm.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fm.fm)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplsrda.jl#LL1-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dmkern-Tuple{Any}" href="#Jchemo.dmkern-Tuple{Any}"><code>Jchemo.dmkern</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dmkern(X; kwargs...)</code></pre><p>Gaussian kernel density estimation (KDE).</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>h_kde</code> : Define the bandwith, see examples.</li><li><code>a_kde</code> : Constant for the Scott&#39;s rule    (default bandwith), see thereafter.</li></ul><p>Estimation of the probability density of <code>X</code> (column space) by  non parametric Gaussian kernels. </p><p>Data <code>X</code> can be univariate (p = 1) or multivariate (p &gt; 1).  In the last case, function <code>dmkern</code> computes a multiplicative  kernel such as in Scott &amp; Sain 2005 Eq.19, and the internal bandwidth  matrix <code>H</code> is diagonal (see the code). </p><p><strong>Note:</strong>  <code>H</code> in the <code>dmkern</code> code is often noted &quot;H^(1/2)&quot; in the  litterature (e.g. Wikipedia).</p><p>The default bandwith is computed by:</p><ul><li><code>h_kde</code> = <code>a_kde</code> * n^(-1 / (p + 4)) * colstd(<code>X</code>)</li></ul><p>(<code>a_kde</code> = 1 in Scott &amp; Sain 2005).</p><p><strong>References</strong></p><p>Scott, D.W., Sain, S.R., 2005. 9 - Multidimensional Density  Estimation, in: Rao, C.R., Wegman, E.J., Solka, J.L. (Eds.),  Handbook of Statistics, Data Mining and Data Visualization.  Elsevier, pp. 229–261.  https://doi.org/10.1016/S0169-7161(04)24009-3</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

mod0 = model(fda; nlv = 2)
fit!(mod0, X, y)
@head T = mod0.fm.T
p = nco(T)

#### Probability density in the FDA 
#### score space (2D)

mod = model(dmkern)
fit!(mod, T) 
pnames(mod.fm)
mod.fm.H
u = [1; 4; 150]
predict(mod, T[u, :]).pred

h_kde = .3
mod = model(dmkern; h_kde)
fit!(mod, T) 
mod.fm.H
u = [1; 4; 150]
predict(mod, T[u, :]).pred

h_kde = [.3; .1]
mod = model(dmkern; h_kde)
fit!(mod, T) 
mod.fm.H
u = [1; 4; 150]
predict(mod, T[u, :]).pred

## Bivariate distribution
npoints = 2^7
nlv = 2
lims = [(minimum(T[:, j]), maximum(T[:, j])) for j = 1:nlv]
x1 = LinRange(lims[1][1], lims[1][2], npoints)
x2 = LinRange(lims[2][1], lims[2][2], npoints)
z = mpar(x1 = x1, x2 = x2)
grid = reduce(hcat, z)
m = nro(grid)
mod = model(dmkern) 
#mod = model(dmkern; a_kde = .5) 
#mod = model(dmkern; h_kde = .3) 
fit!(mod, T) 

res = predict(mod, grid) ;
pred_grid = vec(res.pred)
f = Figure(size = (600, 400))
ax = Axis(f[1, 1];  title = &quot;Density for FDA scores (Iris)&quot;, xlabel = &quot;Score 1&quot;, 
    ylabel = &quot;Score 2&quot;)
co = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10, labels = true)
scatter!(ax, T[:, 1], T[:, 2], color = :red, markersize = 5)
#xlims!(ax, -15, 15) ;ylims!(ax, -15, 15)
f

## Univariate distribution
x = T[:, 1]
mod = model(dmkern) 
#mod = model(dmkern; a_kde = .5) 
#mod = model(dmkern; h_kde = .3) 
fit!(mod, x) 
pred = predict(mod, x).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
scatter!(ax, x, vec(pred); color = :red)
f

x = T[:, 1]
npoints = 2^8
lims = [minimum(x), maximum(x)]
#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]
grid = LinRange(lims[1], lims[2], npoints)
mod = model(dmkern) 
#mod = model(dmkern; a_kde = .5) 
#mod = model(dmkern; h_kde = .3) 
fit!(mod, x) 
pred_grid = predict(mod, grid).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
lines!(ax, grid, vec(pred_grid); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dmkern.jl#LL1-L126">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dmnorm" href="#Jchemo.dmnorm"><code>Jchemo.dmnorm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dmnorm(X = nothing; mu = nothing, S = nothing, simpl::Bool = false)
dmnorm!(X = nothing; mu = nothing, S = nothing, simpl::Bool = false)</code></pre><p>Normal probability density estimation.</p><ul><li><code>X</code> : X-data (n, p) used to estimate the mean and    the covariance matrix. If <code>nothing</code>, <code>mu</code> and <code>S</code>    must be provided.</li></ul><p>Keyword arguments:</p><ul><li><code>mu</code> : Mean vector of the normal distribution.    If <code>nothing</code>, <code>mu</code> is computed by the column-means   of <code>X</code>.</li><li><code>S</code> : Covariance matrix of the normal distribution.   If <code>nothing</code>, <code>S</code> is computed by cov(<code>X</code>; corrected = true).</li><li><code>simpl</code> : Boolean. If <code>true</code>, the constant term and    the determinant in the density formula are set to 1.</li></ul><p>Data <code>X</code> can be univariate (p = 1) or multivariate (p &gt; 1). See examples.</p><p>When <code>simple</code> = <code>true</code>, the determinant of the covariance matrix  (object <code>detS</code>) and the constant (2 * pi)^(-p / 2) (object <code>cst</code>)  in the density formula are set to 1. The function returns a pseudo  density that resumes to exp(-d / 2), where d is the squared Mahalanobis  distance to the fcenter. This can for instance be useful when the number  of columns (p) of <code>X</code> becomes too large and when consequently:</p><ul><li><code>detS</code> tends to 0 or, conversely, to infinity</li><li><code>cst</code> tends to 0</li></ul><p>which makes impossible to compute the true density. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

mod0 = model(fda; nlv = 2)
fit!(mod0, X, y)
@head T = mod0.fm.T
n, p = size(T)

#### Probability density in the FDA score space (2D)
#### Example of class Setosa 
s = y .== &quot;setosa&quot;
zT = T[s, :]

## Bivariate distribution
mod = model(dmnorm)
fit!(mod, zT)
fm = mod.fm
pnames(fm)
fm.Uinv 
fm.detS
pred = predict(mod, zT).pred
@head pred

mu = colmean(zT)
S = covm(zT, mweight(ones(nro(zT))))
## Direct syntax
dmnorm(; mu = mu, S = S).Uinv
dmnorm(; mu = mu, S = S).detS

npoints = 2^7
lims = [(minimum(zT[:, j]), maximum(zT[:, j])) for j = 1:nlv]
x1 = LinRange(lims[1][1], lims[1][2], npoints)
x2 = LinRange(lims[2][1], lims[2][2], npoints)
z = mpar(x1 = x1, x2 = x2)
grid = reduce(hcat, z)
mod = model(dmnorm)
fit!(mod, zT)
res = predict(mod, grid) ;
pred_grid = vec(res.pred)
f = Figure(size = (600, 400))
ax = Axis(f[1, 1];  title = &quot;Density for FDA scores (Iris - Setosa)&quot;, 
    xlabel = &quot;Score 1&quot;, ylabel = &quot;Score 2&quot;)
co = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10, labels = true)
scatter!(ax, T[:, 1], T[:, 2], color = :red, markersize = 5)
scatter!(ax, zT[:, 1], zT[:, 2], color = :blue, markersize = 5)
#xlims!(ax, -12, 12) ;ylims!(ax, -12, 12)
f

## Univariate distribution
j = 1
x = zT[:, j]
mod = model(dmnorm)
fit!(mod, x)
pred = predict(mod, x).pred 
f = Figure()
ax = Axis(f[1, 1]; xlabel = string(&quot;FDA-score &quot;, j))
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
scatter!(ax, x, vec(pred); color = :red)
f

x = zT[:, j]
npoints = 2^8
lims = [minimum(x), maximum(x)]
#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]
grid = LinRange(lims[1], lims[2], npoints)
mod = model(dmnorm)
fit!(mod, x)
pred_grid = predict(mod, grid).pred 
f = Figure()
ax = Axis(f[1, 1]; xlabel = string(&quot;FDA-score &quot;, j))
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
lines!(ax, grid, vec(pred_grid); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dmnorm.jl#LL1-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dmnormlog" href="#Jchemo.dmnormlog"><code>Jchemo.dmnormlog</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dmnormlog(X = nothing; mu = nothing, S = nothing, simpl::Bool = false)
dmnormlog!(X = nothing; mu = nothing, S = nothing, simpl::Bool = false)</code></pre><p>Logarithm of the normal probability density estimation.</p><ul><li><code>X</code> : X-data (n, p) used to estimate the mean and    the covariance matrix. If <code>nothing</code>, <code>mu</code> and <code>S</code>    must be provided.</li></ul><p>Keyword arguments:     * <code>mu</code> : Mean vector of the normal distribution.          If <code>nothing</code>, <code>mu</code> is computed by the column-means         of <code>X</code>.     * <code>S</code> : Covariance matrix of the normal distribution.         If <code>nothing</code>, <code>S</code> is computed by cov(<code>X</code>; corrected = true).     * <code>simpl</code> : Boolean. If <code>true</code>, the constant term and          the determinant in the density formula are set to 1.</p><p>See the help of function <code>dmnorm</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie
using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

## Example of class Setosa 
s = y .== &quot;setosa&quot;
zX = X[s, :]

mod = model(dmnormlog)
fit!(mod, zX)
fm = mod.fm
pnames(fm)
fm.Uinv 
fm.logdetS
pred = predict(mod, zX).pred
@head pred 

mod0 = model(dmnorm)
fit!(mod0, zX)
pred0 = predict(mod0, zX).pred
@head log.(pred0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dmnormlog.jl#LL1-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dummy" href="#Jchemo.dummy"><code>Jchemo.dummy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dummy(y, T = Float64)</code></pre><p>Compute dummy table from a categorical variable.</p><ul><li><code>y</code> : A categorical variable.</li><li><code>T</code> : Type of the output dummy table <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">y = [&quot;d&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;b&quot;, &quot;c&quot;]
#y =  rand(1:3, 7)
res = dummy(y)
pnames(res)
res.Y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL229-L243">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dupl-Tuple{Any}" href="#Jchemo.dupl-Tuple{Any}"><code>Jchemo.dupl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dupl(X; digits = 3)</code></pre><p>Find duplicated rows in a dataset.</p><ul><li><code>X</code> : A dataset.</li><li><code>digits</code> : Nb. digits used to round <code>X</code>   before checking.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Z = vcat(X, X[1:3, :], X[1:1, :])
dupl(X)
dupl(Z)

M = hcat(X, fill(missing, 5))
Z = vcat(M, M[1:3, :])
dupl(M)
dupl(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL268-L287">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ensure_df-Tuple{DataFrames.DataFrame}" href="#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><code>Jchemo.ensure_df</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ensure_df(X)</code></pre><p>Reshape <code>X</code> to a dataframe if necessary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL313-L316">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ensure_mat-Tuple{AbstractMatrix}" href="#Jchemo.ensure_mat-Tuple{AbstractMatrix}"><code>Jchemo.ensure_mat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ensure_mat(X)</code></pre><p>Reshape <code>X</code> to a matrix if necessary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL321-L324">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.eposvd-Tuple{Any}" href="#Jchemo.eposvd-Tuple{Any}"><code>Jchemo.eposvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eposvd(D; nlv = 1)</code></pre><p>Compute an orthogonalization matrix for calibration transfer      of spectral data.</p><ul><li><code>D</code> : Data (m, p) containing the detrimental information    on which spectra (rows of a matrix X) have to be orthogonalized.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of first loadings vectors of <code>D</code> considered for the    orthogonalization.</li></ul><p>The objective is to remove some detrimental information  (e.g. humidity patterns in signals, multiple spectrometers, etc.)  from a X-dataset (n, p).  The detrimental information is defined  by the main row-directions computed from a matrix <code>D</code> (m, p). </p><p>Function <code>eposvd</code> returns two objects:</p><ul><li><code>P</code> (p, <code>nlv</code>) : The matrix of the <code>nlv</code> first    loading vectors of the SVD decomposition (non centered PCA)    of <code>D</code>. </li><li><code>M</code> (p, p) : The orthogonalization matrix, used    to orthogonolize a given matrix X to directions    contained in <code>P</code>.</li></ul><p>Any matrix X can then be corrected from <code>D</code> by:</p><ul><li>X_corrected = X * <code>M</code>.</li></ul><p>Matrix <code>D</code> can be built from many methods. For instance,      two common methods are:</p><ul><li>EPO (Roger et al. 2003, 2018): <code>D</code> is built from a set of    differences between spectra collected under different    conditions. </li><li>TOP (Andrew &amp; Fearn 2004): Each row of <code>D</code> is the mean spectrum    computed for a given spectrometer instrument.</li></ul><p>A particular situation is the following. Assume that <code>D</code> is  built from some differences between matrices X1 and X2, and that  a bilinear model (e.g. PLSR) is fitted on the data {X1<em>corrected, Y} where X1</em>corrected = X1 * <code>M</code>. To predict new data X2<em>new with  the fitted model, there is no need to correct X2</em>new.</p><p><strong>References</strong></p><p>Andrew, A., Fearn, T., 2004. Transfer by orthogonal projection:  making near-infrared calibrations robust to between-instrument  variation. Chemometrics and Intelligent Laboratory Systems 72,  51–56. https://doi.org/10.1016/j.chemolab.2004.02.004</p><p>Roger, J.-M., Chauchard, F., Bellon-Maurel, V., 2003. EPO-PLS  external parameter orthogonalisation of PLS application to  temperature-independent measurement of sugar content of intact  fruits. Chemometrics and Intelligent Laboratory Systems 66,  191-204. https://doi.org/10.1016/S0169-7439(03)00051-0</p><p>Roger, J.-M., Boulet, J.-C., 2018. A review of orthogonal  projections for calibration. Journal of Chemometrics 32, e3045.  https://doi.org/10.1002/cem.3045</p><p>Zeaiter, M., Roger, J.M., Bellon-Maurel, V., 2006. Dynamic  orthogonal projection. A new method to maintain the on-line  robustness of multivariate calibrations. Application to NIR-based  monitoring of wine fermentations. Chemometrics and Intelligent  Laboratory Systems, 80, 227–235.  https://doi.org/10.1016/j.chemolab.2005.06.011</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;)
@load db dat
pnames(dat)
X1cal = dat.X1cal
X1val = dat.X1val
X2cal = dat.X2cal
X2val = dat.X2val

## The objective is to remove a detrimental 
## information (here, D) from spaces X1 and X2
D = X1cal - X2cal
nlv = 2
res = eposvd(D; nlv)
res.M # orthogonalization matrix
res.P # detrimental directions (columns of matrix P = loadings of D)

## Corrected Val matrices
X1val_c = X1val * res.M
X2val_c = X2val * res.M

i = 1
f = Figure(size = (800, 300))
ax1 = Axis(f[1, 1])
ax2 = Axis(f[1, 2])
lines!(ax1, X1val[i, :]; label = &quot;x1&quot;)
lines!(ax1, X2val[i, :]; label = &quot;x2&quot;)
axislegend(ax1, position = :cb, framevisible = false)
lines!(ax2, X1val_c[i, :]; label = &quot;x1_correct&quot;)
lines!(ax2, X2val_c[i, :]; label = &quot;x2_correct&quot;)
axislegend(ax2, position = :cb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/eposvd.jl#LL2-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.errp-Tuple{Any, Any}" href="#Jchemo.errp-Tuple{Any, Any}"><code>Jchemo.errp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">errp(pred, y)</code></pre><p>Compute the classification error rate (ERRP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data (class membership).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

mod = model(plsrda; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
errp(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL451-L469">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.euclsq-Tuple{Any, Any}" href="#Jchemo.euclsq-Tuple{Any, Any}"><code>Jchemo.euclsq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">euclsq(X, Y)</code></pre><p>Squared Euclidean distances between the rows of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (m, p).</li></ul><p>For <code>X</code>(n, p) and <code>Y</code> (m, p), the function returns  an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)

euclsq(X, Y)

euclsq(X[1:1, :], Y[1:1, :])

euclsq(X[:, 1], 4)
euclsq(1, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/distances.jl#LL1-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fblockscal-Tuple{Any, Any}" href="#Jchemo.fblockscal-Tuple{Any, Any}"><code>Jchemo.fblockscal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fblockscal(Xbl, bscales)
fblockscal!(Xbl::Vector, bscales::Vector)</code></pre><p>Scale multiblock X-data.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>bscales</code> : A vector (of length equal to the nb.    of blocks) of the scalars diving the blocks.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5 ; m = 3 ; p = 10 
X = rand(n, p) 
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl) 

bscales = 10 * ones(3)
zXbl = fblockscal(Xbl, bscales) ;
@head zXbl[3]
@head Xbl[3]

fblockscal!(Xbl, bscales) ;
@head Xbl[3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_mb.jl#LL33-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fcenter-Tuple{Any, Any}" href="#Jchemo.fcenter-Tuple{Any, Any}"><code>Jchemo.fcenter</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fcenter(X, v)
fcenter!(X::AbstractMatrix, v)</code></pre><p>Center each column of <code>X</code>.</p><ul><li><code>X</code> : Data.</li><li><code>v</code> : Centering vector.</li></ul><p><strong>examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
xmeans = colmean(X)
fcenter(X, xmeans)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_scale.jl#LL1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fcscale-Tuple{Any, Any, Any}" href="#Jchemo.fcscale-Tuple{Any, Any, Any}"><code>Jchemo.fcscale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fcscale(X, u, v)
fcscale!(X, u, v)</code></pre><p>Center and fscale each column of <code>X</code>.</p><ul><li><code>X</code> : Data.</li><li><code>u</code> : Centering vector.</li><li><code>v</code> : Scaling vector.</li></ul><p><strong>examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
xmeans = colmean(X)
xstds = colstd(X)
fcscale(X, xmeans, xstds)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_scale.jl#LL37-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fda-Tuple{Any, Any}" href="#Jchemo.fda-Tuple{Any, Any}"><code>Jchemo.fda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fda(X, y; kwargs...)
fda(X, y, weights; kwargs...)
fda!(X::Matrix, y, weights; kwargs...)</code></pre><p>Factorial discriminant analysis (FDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : y-data (n) (class membership).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of discriminant components.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.   Can be used when <code>X</code> has collinearities. </li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>FDA by eigen factorization of Inverse(W) * B, where W is  the &quot;Within&quot;-covariance matrix (pooled over the classes),  and B the &quot;Between&quot;-covariance matrix.</p><p>The function maximizes the compromise:</p><ul><li>p&#39;Bp / p&#39;Wp </li></ul><p>i.e. max p&#39;Bp with constraint p&#39;Wp = 1. Vectors p (columns of <code>P</code>)  are the linear discrimant coefficients often referred to as &quot;LD&quot;.</p><p>If <code>X</code> is ill-conditionned, a ridge regularization can be used:</p><ul><li>If <code>lb</code> &gt; 0, W is replaced by W + <code>lb</code> * I,    where I is the Idendity matrix.</li></ul><p>In these <code>fda</code> functions, observation weights (argument <code>weights</code>) are used  to compute matrices W and B. </p><p>In the high-level version, the observation weights are automatically  defined by the given priors (argument <code>prior</code>): the sub-total weights by  class are set equal to the prior probabilities. For other choices,  use the low-level versions.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
tab(ytrain)
tab(ytest)

nlv = 2
mod = model(fda; nlv)
#mod = model(fdasvd; nlv)
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
lev = fm.lev
nlev = length(lev)
aggsum(fm.weights.w, ytrain)

@head fm.T 
@head transf(mod, Xtrain)
@head transf(mod, Xtest)

## X-loadings matrix
## = coefficients of the linear discriminant function
## = &quot;LD&quot; of function lda of the R package MASS
fm.P
fm.P&#39; * fm.P

## Explained variance computed by weighted PCA 
## of the class centers in transformed scale
summary(mod).explvarx

## Projections of the class centers 
## to the score space
ct = fm.Tcenters 
f, ax = plotxy(fm.T[:, 1], fm.T[:, 2], ytrain; ellipse = true, title = &quot;FDA&quot;,
    xlabel = &quot;Score-1&quot;, ylabel = &quot;Score-2&quot;)
scatter!(ax, ct[:, 1], ct[:, 2], marker = :star5, markersize = 15, color = :red)  # see available_marker_symbols()
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/fda.jl#LL1-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fdasvd-Tuple{Any, Any}" href="#Jchemo.fdasvd-Tuple{Any, Any}"><code>Jchemo.fdasvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fdasvd(X, y, weights; kwargs...)
fdasvd!(X::Matrix, y, weights; kwargs...)</code></pre><p>Factorial discriminant analysis (FDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : y-data (n) (class membership).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of discriminant components.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.   Can be used when <code>X</code> has collinearities. </li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>FDA by a weighted SVD factorization of the matrix of the  class centers (after spherical transformaton).  The function gives the same results as function <code>fda</code>.</p><p>See function <code>fda</code> for details and examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/fdasvd.jl#LL1-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fdif-Tuple{Any}" href="#Jchemo.fdif-Tuple{Any}"><code>Jchemo.fdif</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fdif(X; kwargs...)</code></pre><p>Finite differences (discrete derivates) for each row of X-data. </p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Nb. points involved in the window for the    finite differences. The range of the window    (= nb. intervals of two successive colums) is npoint - 1.</li></ul><p>The method reduces the column-dimension: </p><ul><li>(n, p) –&gt; (n, p - npoint + 1). </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(fdif; npoint = 2) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL71-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.findindex-Tuple{Any, Any}" href="#Jchemo.findindex-Tuple{Any, Any}"><code>Jchemo.findindex</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">findindex(x, lev)</code></pre><p>Replace a vector containg levels by the indexes of a set of levels.</p><ul><li><code>x</code> : Vector (n) of levels to replace.</li><li><code>lev</code> : Vector (nlev) containing the levels.</li></ul><p><em>Warning</em>: The levels in <code>x</code> must be contained in <code>lev</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">lev = [&quot;EHH&quot; ; &quot;FFS&quot; ; &quot;ANF&quot; ; &quot;CLZ&quot; ; &quot;CNG&quot; ; &quot;FRG&quot; ; &quot;MPW&quot; ; &quot;PEE&quot; ; &quot;SFG&quot; ; &quot;TTS&quot;]
x = [&quot;EHH&quot; ; &quot;TTS&quot; ; &quot;FRG&quot;]
findindex(x, lev)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL334-L348">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.findmax_cla-Tuple{Any}" href="#Jchemo.findmax_cla-Tuple{Any}"><code>Jchemo.findmax_cla</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">findmax_cla(x)
findmax_cla(x, weights::Weight)</code></pre><p>Find the most occurent level in <code>x</code>.</p><ul><li><code>x</code> : A categorical variable.</li><li><code>weights</code> : Weights (n) of the observations.   Object of type <code>Weight</code> (e.g. generated by    function <code>mweight</code>).</li></ul><p>If ex-aequos, the function returns the first.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(1:3, 10)
tab(x)
findmax_cla(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL359-L376">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.frob-Tuple{Any}" href="#Jchemo.frob-Tuple{Any}"><code>Jchemo.frob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">frob(X)
frob(X, weights::Weight)</code></pre><p>Frobenius norm of a matrix.</p><ul><li><code>X</code> : A matrix (n, p).</li><li><code>weights</code> : Weights (n) of the observations.   Object of type <code>Weight</code> (e.g. generated by    function <code>mweight</code>).</li></ul><p>The Frobenius norm of <code>X</code> is:</p><ul><li>sqrt(tr(X&#39; * X)).</li></ul><p>The Frobenius weighted norm is:</p><ul><li>sqrt(tr(X&#39; * D * X)), where D is the diagonal matrix of vector <code>w</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL388-L403">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fscale-Tuple{Any, Any}" href="#Jchemo.fscale-Tuple{Any, Any}"><code>Jchemo.fscale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fscale(X, v)
fscale!(X::AbstractMatrix, v)</code></pre><p>Scale each column of <code>X</code>.</p><ul><li><code>X</code> : Data.</li><li><code>v</code> : Scaling vector.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
fscale(X, colstd(X))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_scale.jl#LL20-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fweight-Tuple{Any}" href="#Jchemo.fweight-Tuple{Any}"><code>Jchemo.fweight</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fweight(d; typw = :bisquare, alpha = 0)</code></pre><p>Computation of weights from distances.</p><ul><li><code>d</code> : Vector of distances.</li></ul><p>Keyword arguments:</p><ul><li><code>typw</code> : Define the weight function.</li><li><code>alpha</code> : Parameter of the weight function,    see below.</li></ul><p>The returned weight vector is: </p><ul><li>w = f(<code>d</code> / q) where f is the weight function    and q the 1-<code>alpha</code> quantile of <code>d</code>    (Cleveland &amp; Grosse 1991).</li></ul><p>Possible values for <code>typw</code> are: </p><ul><li>:bisquare: w = (1 - x^2)^2 </li><li>:cauchy: w = 1 / (1 + x^2) </li><li>:epan: w = 1 - x^2 </li><li>:fair: w =  1 / (1 + x)^2 </li><li>:invexp: w = exp(-x) </li><li>:invexp2: w = exp(-x / 2) </li><li>:gauss: w = exp(-x^2)</li><li>:trian: w = 1 - x  </li><li>:tricube: w = (1 - x^3)^3  </li></ul><p><strong>References</strong></p><p>Cleveland, W.S., Grosse, E., 1991. Computational methods for local regression.  Stat Comput 1, 47–62. https://doi.org/10.1007/BF01890836</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CairoMakie, Distributions

d = sort(sqrt.(rand(Chi(1), 1000)))
cols = cgrad(:tab10, collect(1:9)) ;
alpha = 0
f = Figure(size = (600, 500))
ax = Axis(f, xlabel = &quot;d&quot;, ylabel = &quot;Weight&quot;)
typw = :bisquare
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[1])
typw = :cauchy
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[2])
typw = :epan
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[3])
typw = :fair
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[4])
typw = :gauss
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[5])
typw = :trian
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[6])
typw = :invexp
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[7])
typw = :invexp2
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[8])
typw = :tricube
w = fweight(d; typw, alpha)
lines!(ax, d, w, label = String(typw), color = cols[9])
axislegend(&quot;Function&quot;, position = :lb)
f[1, 1] = ax
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/fweight.jl#LL1-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.getknn-Tuple{Any, Any}" href="#Jchemo.getknn-Tuple{Any, Any}"><code>Jchemo.getknn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">getknn(Xtrain, X; metric = :eucl, k = 1)</code></pre><p>Return the k nearest neighbors in <code>Xtrain</code> of each row of the query <code>X</code>.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>X</code> : Query X-data.</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of distance used for the query.    Possible values are <code>:eucl</code> (Euclidean),   <code>:mah</code> (Mahalanobis), <code>:sam</code> (spectral angular distance),   <code>:cor</code> (correlation distance).</li><li><code>k</code> : Number of neighbors to return.</li></ul><p>The distances (not squared) are also returned.</p><p>Spectral angular and correlation distances between two vectors x and y:</p><ul><li>Spectral angular distance (x, y) = acos(x&#39;y / norm(x)norm(y)) / pi</li><li>Correlation distance (x, y) = sqrt((1 - cor(x, y)) / 2)</li></ul><p>Both distances are bounded within 0 (y = x) and 1 (y = -x).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(5, 3)
X = rand(2, 3)
x = X[1:1, :]

k = 3
res = getknn(Xtrain, X; k)
res.ind  # indexes
res.d    # distances

res = getknn(Xtrain, x; k)
res.ind

res = getknn(Xtrain, X; metric = :mah, k)
res.ind</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/getknn.jl#LL1-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcv-Tuple{Any, Any, Any}" href="#Jchemo.gridcv-Tuple{Any, Any, Any}"><code>Jchemo.gridcv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcv(mod, X, Y; segm, score, pars = nothing, nlv = nothing, lb = nothing, 
    verbose = false)</code></pre><p>Cross-validation (CV) of a model over a grid of parameters.</p><ul><li><code>mod</code> : Model to evaluate.</li><li><code>X</code> : Training X-data (n, p).</li><li><code>Y</code> : Training Y-data (n, q).</li></ul><p>Keyword arguments: </p><ul><li><code>segm</code> : Segments of observations used for    the CV (output of functions <a href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>segmts</code></a>,    <a href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>segmkf</code></a>, etc.).</li><li><code>score</code> : Function computing the prediction    score (e.g. <code>rmsep</code>).</li><li><code>pars</code> : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If <code>true</code>, fitting information are printed.</li><li><code>nlv</code> : Value, or vector of values, of the nb. of latent   variables (LVs).</li><li><code>lb</code> : Value, or vector of values, of the ridge    regularization parameter &quot;lambda&quot;.</li></ul><p>The function is used for grid-search: it computed a prediction score  (= error rate) for model <code>mod</code> over the combinations of parameters  defined in <code>pars</code>. </p><p>For models based on LV or ridge regularization, using arguments <code>nlv</code>  and <code>lb</code> allow faster computations than including these parameters in  argument `pars. See the examples.   </p><p>The function returns two outputs: </p><ul><li><code>res</code> : mean results</li><li><code>res_p</code> : results per replication.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">######## Regression

using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
mod = model(savgol; npoint = 21, deriv = 2, degree = 2)
fit!(mod, X)
Xp = transf(mod, X)
s = year .&lt;= 2012
Xtrain = Xp[s, :]
ytrain = y[s]
Xtest = rmrow(Xp, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Replicated K-fold CV 
K = 3 ; rep = 10
segm = segmkf(ntrain, K; rep)
## Replicated test-set validation
#m = Int(round(ntrain / 3)) ; rep = 30
#segm = segmts(ntrain, m; rep)

####-- Plsr
mod = model(plskern)
nlv = 0:30
rescv = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, nlv) ;
pnames(rescv)
res = rescv.res 
plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(plskern; nlv = res.nlv[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## Adding pars 
pars = mpar(scal = [false; true])
rescv = gridcv(mod, Xtrain, ytrain; segm,  score = rmsep, pars, nlv) ;
res = rescv.res 
typ = res.scal
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(plskern; nlv = res.nlv[u], scal = res.scal[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####-- Rr 
lb = (10).^(-8:.1:3)
mod = model(rr) 
rescv = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, lb) ;
res = rescv.res 
loglb = log.(10, res.lb)
plotgrid(loglb, res.y1; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(rr; lb = res.lb[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f     
    
## Adding pars 
pars = mpar(scal = [false; true])
rescv = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, pars, lb) ;
res = rescv.res 
loglb = log.(10, res.lb)
typ = string.(res.scal)
plotgrid(loglb, res.y1, typ; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(rr; lb = res.lb[u], scal = res.scal[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####-- Kplsr 
mod = model(kplsr)
nlv = 0:30
gamma = (10).^(-5:1.:5)
pars = mpar(gamma = gamma)
rescv = gridcv(mod, Xtrain, ytrain; segm,  score = rmsep, pars, nlv) ;
res = rescv.res 
loggamma = round.(log.(10, res.gamma), digits = 1)
plotgrid(res.nlv, res.y1, loggamma; step = 2, xlabel = &quot;Nb. LVs&quot;,  ylabel = &quot;RMSEP&quot;, 
    leg_title = &quot;Log(gamma)&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(kplsr; nlv = res.nlv[u], gamma = res.gamma[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####-- Knnr 
nlvdis = [15, 25] ; metric = [:mah]
h = [1, 2.5, 5]
k = [1; 5; 10; 20; 50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
mod = model(knnr)
rescv = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, pars, verbose = true) ;
res = rescv.res 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(knnr; nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####-- Lwplsr 
nlvdis = 15 ; metric = [:mah]
h = [1, 2.5, 5] ; k = [50, 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
nlv = 0:20
mod = model(lwplsr)
rescv = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, pars, nlv, verbose = true) ;
res = rescv.res 
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(lwplsr; nlvdis = res.nlvdis[u], metric = res.metric[u], 
    h = res.h[u], k = res.k[u], nlv = res.nlv[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####-- LwplsrAvg 
nlvdis = 15 ; metric = [:mah]
h = [1, 2.5, 5] ; k = [50, 100]
nlv = [0:15, 0:20, 5:20]  
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k, nlv = nlv)
length(pars[1]) 
mod = model(lwplsravg)
rescv = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, pars, verbose = true) ;
res = rescv.res 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(lwplsravg; nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u], nlv = res.nlv[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f     

######## Discrimination
## The principle is the same as for regression

using JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Replicated K-fold CV 
K = 3 ; rep = 10
segm = segmkf(ntrain, K; rep)
## Replicated test-set validation
#m = Int(round(ntrain / 3)) ; rep = 30
#segm = segmts(ntrain, m; rep)

####-- Plslda
mod = model(plslda)
nlv = 1:30
prior = [:unif; :prop]
pars = mpar(prior = prior)
rescv = gridcv(mod, Xtrain, ytrain; segm, score = errp, pars, nlv)
res = rescv.res
typ = res.prior
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;ERR&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(plslda; nlv = res.nlv[u], prior = res.prior[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show errp(pred, ytest)
conf(pred, ytest).pct</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridcv.jl#LL1-L253">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcv_br-Tuple{Any, Any}" href="#Jchemo.gridcv_br-Tuple{Any, Any}"><code>Jchemo.gridcv_br</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcv_br(X, Y; segm, fun, score, pars, verbose = false)</code></pre><p>Working function for <code>gridcv</code>.</p><p>See function <code>gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridcv_br.jl#LL1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcv_lb-Tuple{Any, Any}" href="#Jchemo.gridcv_lb-Tuple{Any, Any}"><code>Jchemo.gridcv_lb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcv_lb(X, Y; segm, fun, score, pars = nothing, lb, verbose = false)</code></pre><p>Working function for <code>gridcv</code>.</p><p>Specific and faster than <code>gridcv_br</code> for models  using ridge regularization (e.g. RR). Argument <code>pars</code>  must not contain <code>nlv</code>.</p><p>See function <code>gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridcv_lb.jl#LL1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcv_lv-Tuple{Any, Any}" href="#Jchemo.gridcv_lv-Tuple{Any, Any}"><code>Jchemo.gridcv_lv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcv_lv((X, Y; segm, fun, score, pars = nothing, nlv, verbose = false)</code></pre><p>Working function for <code>gridcv</code>.</p><p>Specific and faster than <code>gridcv_br</code> for models  using latent variables (e.g. PLSR). Argument <code>pars</code>  must not contain <code>nlv</code>.</p><p>See function <code>gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridcv_lv.jl#LL1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscore-NTuple{5, Any}" href="#Jchemo.gridscore-NTuple{5, Any}"><code>Jchemo.gridscore</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscore(mod, Xtrain, Ytrain, X, Y; score, pars = nothing, nlv = nothing, 
    lb = nothing, verbose = false)</code></pre><p>Test-set validation of a model over a grid of parameters.</p><ul><li><code>mod</code> : Model to evaluate.</li><li><code>Xtrain</code> : Training X-data (n, p).</li><li><code>Ytrain</code> : Training Y-data (n, q).</li><li><code>X</code> : Validation X-data (m, p).</li><li><code>Y</code> : Validation Y-data (m, q).</li></ul><p>Keyword arguments: </p><ul><li><code>score</code> : Function computing the prediction    score (e.g. <code>rmsep</code>).</li><li><code>pars</code> : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If <code>true</code>, fitting information are printed.</li><li><code>nlv</code> : Value, or vector of values, of the nb. of latent   variables (LVs).</li><li><code>lb</code> : Value, or vector of values, of the ridge    regularization parameter &quot;lambda&quot;.</li></ul><p>The function is used for grid-search: it computed a prediction score  (= error rate) for model <code>mod</code> over the combinations of parameters  defined in <code>pars</code>. The score is computed over sets {<code>X,</code>Y`}. </p><p>For models based on LV or ridge regularization, using arguments <code>nlv</code>  and <code>lb</code> allow faster computations than including these parameters in  argument `pars. See the examples.   </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">######## Regression 

using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
mod = model(savgol; npoint = 21, deriv = 2, degree = 2)
fit!(mod, X)
Xp = transf(mod, X)
s = year .&lt;= 2012
Xtrain = Xp[s, :]
ytrain = y[s]
Xtest = rmrow(Xp, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
## Building Cal and Val 
## within Train
nval = Int(round(.3 * ntrain))
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]

####-- Plsr
mod = model(plskern)
nlv = 0:30
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, nlv)
plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(plskern; nlv = res.nlv[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## Adding pars 
pars = mpar(scal = [false; true])
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv)
typ = res.scal
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(plskern; nlv = res.nlv[u], scal = res.scal[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####-- Rr 
lb = (10).^(-8:.1:3)
mod = model(rr) 
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, lb)
loglb = log.(10, res.lb)
plotgrid(loglb, res.y1; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(rr; lb = res.lb[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    
    
## Adding pars 
pars = mpar(scal = [false; true])
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars, lb)
loglb = log.(10, res.lb)
typ = string.(res.scal)
plotgrid(loglb, res.y1, typ; step = 2, xlabel = &quot;log(lambda)&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(rr; lb = res.lb[u], scal = res.scal[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####-- Kplsr 
mod = model(kplsr)
nlv = 0:30
gamma = (10).^(-5:1.:5)
pars = mpar(gamma = gamma)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv)
loggamma = round.(log.(10, res.gamma), digits = 1)
plotgrid(res.nlv, res.y1, loggamma; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;,
    leg_title = &quot;Log(gamma)&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(kplsr; nlv = res.nlv[u], gamma = res.gamma[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####-- Knnr 
nlvdis = [15; 25] ; metric = [:mah]
h = [1, 2.5, 5]
k = [1, 5, 10, 20, 50, 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
mod = model(knnr)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(knnr; nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####-- Lwplsr 
nlvdis = 15 ; metric = [:mah]
h = [1, 2.5, 5] ; k = [50, 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
nlv = 0:20
mod = model(lwplsr)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv, verbose = true)
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group; xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(lwplsr; nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u], nlv = res.nlv[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####-- LwplsrAvg 
nlvdis = 15 ; metric = [:mah]
h = [1, 2.5, 5] ; k = [50, 100]
nlv = [0:15, 0:20, 5:20] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k, nlv = nlv)
length(pars[1]) 
mod = model(lwplsravg)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(lwplsravg; nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u], nlv = res.nlv[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   

####-- Mbplsr
listbl = [1:525, 526:1050]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 
Xbl_cal = mblock(Xcal, listbl) 
Xbl_val = mblock(Xval, listbl) 

mod = model(mbplsr)
bscal = [:none, :frob]
pars = mpar(bscal = bscal) 
nlv = 0:30
res = gridscore(mod, Xbl_cal, ycal, Xbl_val, yval; score = rmsep, pars, nlv)
group = res.bscal 
plotgrid(res.nlv, res.y1, group; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(mbplsr; bscal = res.bscal[u], nlv = res.nlv[u])
fit!(mod, Xbltrain, ytrain)
pred = predict(mod, Xbltest).pred
@show rmsep(pred, ytest)
plotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    
    
######## Discrimination
## The principle is the same as for regression

using JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
## Building Cal and Val 
## within Train
nval = Int(round(.3 * ntrain))
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]

####-- Plslda
mod = model(plslda)
nlv = 1:30
prior = [:unif, :prop]
pars = mpar(prior = prior)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = errp, pars, nlv)
typ = res.prior
plotgrid(res.nlv, res.y1, typ; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod = model(plslda; nlv = res.nlv[u], prior = res.prior[u])
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
@show errp(pred, ytest)
conf(pred, ytest).pct</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridscore.jl#LL1-L263">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscore-Tuple{Jchemo.Pipeline, Any, Any, Any, Any}" href="#Jchemo.gridscore-Tuple{Jchemo.Pipeline, Any, Any, Any, Any}"><code>Jchemo.gridscore</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscore(mod::Pipeline, Xtrain, Ytrain, X, Y; score, pars = nothing, 
    nlv = nothing, lb = nothing, verbose = false)</code></pre><p>Test-set validation of a model pipeline over a grid of parameters.</p><ul><li><code>mod</code> : A pipeline of models to evaluate.</li><li><code>Xtrain</code> : Training X-data (n, p).</li><li><code>Ytrain</code> : Training Y-data (n, q).</li><li><code>X</code> : Validation X-data (m, p).</li><li><code>Y</code> : Validation Y-data (m, q).</li></ul><p>Keyword arguments: </p><ul><li><code>score</code> : Function computing the prediction    score (e.g. <code>rmsep</code>).</li><li><code>pars</code> : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If <code>true</code>, fitting information are printed.</li><li><code>nlv</code> : Value, or vector of values, of the nb. of latent   variables (LVs).</li><li><code>lb</code> : Value, or vector of values, of the ridge    regularization parameter &quot;lambda&quot;.</li></ul><p>In the present version of the function, only the last model  of the pipeline (= the final predictor) is validated.</p><p>For other details, see function <code>gridscore</code> for simple models. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
## Building Cal and Val 
## within Train
nval = Int(round(.3 * ntrain))
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]

####-- Pipeline Snv :&gt; Savgol :&gt; Plsr
## Only the last model is validated
## mod1
centr = true ; scal = false
mod1 = model(snv; centr, scal)
## mod2 
npoint = 11 ; deriv = 2 ; degree = 3
mod2 = model(savgol; npoint, deriv, degree)
## mod3
nlv = 0:30
mod3 = model(plskern)
##
mod = pip(mod1, mod2, mod3)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, nlv) ;
plotgrid(res.nlv, res.y1; step = 2, xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod3 = model(plskern; nlv = res.nlv[u])
mod = pip(mod1, mod2, mod3)
fit!(mod, Xtrain, ytrain)
res = predict(mod, Xtest) ; 
@head res.pred 
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,
      ylabel = &quot;Observed&quot;).f

####-- Pipeline Pca :&gt; Svmr
## Only the last model is validated
## mod1
nlv = 15 ; scal = true
mod1 = model(pcasvd; nlv, scal)
## mod2
kern = [:krbf]
gamma = (10).^(-5:1.:5)
cost = (10).^(1:3)
epsilon = [.1, .2, .5]
pars = mpar(kern = kern, gamma = gamma, cost = cost, epsilon = epsilon)
mod2 = model(svmr)
##
mod = pip(mod1, mod2)
res = gridscore(mod, Xcal, ycal, Xval, yval; score = rmsep, pars)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mod2 = model(svmr; kern = res.kern[u], gamma = res.gamma[u], cost = res.cost[u],
    epsilon = res.epsilon[u])
mod = pip(mod1, mod2) ;
fit!(mod, Xtrain, ytrain)
res = predict(mod, Xtest) ; 
@head res.pred 
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,
      ylabel = &quot;Observed&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridscore_pip.jl#LL1-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscore_br-NTuple{4, Any}" href="#Jchemo.gridscore_br-NTuple{4, Any}"><code>Jchemo.gridscore_br</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscore_br(Xtrain, Ytrain, X, Y; fun, score, pars, 
    verbose = false)</code></pre><p>Working function for <code>gridscore</code>.</p><p>See function <code>gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridscore_br.jl#LL1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscore_lb-NTuple{4, Any}" href="#Jchemo.gridscore_lb-NTuple{4, Any}"><code>Jchemo.gridscore_lb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscore_lb(Xtrain, Ytrain, X, Y; fun, score, pars = nothing, 
    lb, verbose = false)</code></pre><p>Working function for <code>gridscore</code>.</p><p>Specific and faster than <code>gridscore_br</code> for models  using ridge regularization (e.g. RR). Argument <code>pars</code>  must not contain <code>lb</code>.</p><p>See function <code>gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridscore_lb.jl#LL1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscore_lv-NTuple{4, Any}" href="#Jchemo.gridscore_lv-NTuple{4, Any}"><code>Jchemo.gridscore_lv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscore_lv(Xtrain, Ytrain, X, Y; fun, score, pars = nothing, 
    nlv, verbose = false)</code></pre><p>Working function for <code>gridscore</code>.</p><p>Specific and faster than <code>gridscore_br</code> for models  using latent variables (e.g. PLSR). Argument <code>pars</code>  must not contain <code>nlv</code>.</p><p>See function <code>gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/gridscore_lv.jl#LL1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.head-Tuple{Any}" href="#Jchemo.head-Tuple{Any}"><code>Jchemo.head</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">head(X)</code></pre><p>Display the first rows of a dataset.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(100, 5)
head(X)
@head X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL407-L417">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.interpl-Tuple{Any}" href="#Jchemo.interpl-Tuple{Any}"><code>Jchemo.interpl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">interpl(X; kwargs...)</code></pre><p>Sampling spectra by interpolation.</p><ul><li><code>X</code> : Matrix (n, p) of spectra (rows).</li></ul><p>Keyword arguments:</p><ul><li><code>wl</code> : Values representing the column &quot;names&quot; of <code>X</code>.    Must be a numeric vector of length p, or an AbstractRange.</li><li><code>wlfin</code> : Final values (within the range of <code>wl</code>) where to interpolate   the spectrum. Must be a numeric vector, or an AbstractRange.</li></ul><p>The function implements a cubic spline interpolation using  package DataInterpolations.jl.</p><p><strong>References</strong></p><p>Package DAtaInterpolations.jl https://github.com/PumasAI/DataInterpolations.jl https://htmlpreview.github.io/?https://github.com/PumasAI/DataInterpolations.jl/blob/v2.0.0/example/DataInterpolations.html</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

wlfin = range(500, 2400, length = 10)
#wlfin = collect(range(500, 2400, length = 10))
mod = model(interpl; wl, wlfin)
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL139-L183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.isel!" href="#Jchemo.isel!"><code>Jchemo.isel!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isel!(mod, X, Y, wl = 1:nco(X); rep = 1, nint = 5, psamp = .3, score = rmsep)</code></pre><p>Interval variable selection.</p><ul><li><code>mod</code> : Model to evaluate.</li><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>wl</code> : Optional numeric labels (p, 1) of the X-columns.</li></ul><p>Keyword arguments:  </p><ul><li><code>rep</code> : Number of replications of the splitting   training/test. </li><li><code>nint</code> : Nb. intervals. </li><li><code>psamp</code> : Proportion of data used as test set    to compute the <code>score</code>.</li><li><code>score</code> : Function computing the prediction score.</li></ul><p>The principle is as follows:</p><ul><li>Data (X, Y) are splitted randomly to a training and a test set.</li><li>Range 1:p in <code>X</code> is segmented to <code>nint</code> intervals, when possible    of equal size. </li><li>The model is fitted on the training set and the score (error rate)    on the test set, firtsly accounting for all the p variables    (reference) and secondly for each of the <code>nint</code> intervals. </li><li>This process is replicated <code>rep</code> times. Average results are provided    in the outputs, as well the results per replication. </li></ul><p><strong>References</strong></p><ul><li>Nørgaard, L., Saudland, A., Wagner, J., Nielsen, J.P., Munck, L., </li></ul><p>Engelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419.  https://doi.org/10.1366/0003702001949500</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrames, JLD2, CairoMakie
using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;tecator.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y 
wl_str = names(X)
wl = parse.(Float64, wl_str) 
ntot, p = size(X)
typ = Y.typ
namy = names(Y)[1:3]
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

s = typ .== &quot;train&quot;
Xtrain = X[s, :]
Ytrain = Y[s, namy]
Xtest = rmrow(X, s)
Ytest = rmrow(Y[:, namy], s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Work on the j-th 
## y-variable 
j = 2
nam = namy[j]
ytrain = Ytrain[:, nam]
ytest = Ytest[:, nam]

mod = model(plskern; nlv = 5)
nint = 10
res = isel!(mod, Xtrain, ytrain, wl; rep = 30, nint) ;
res.res_rep
res.res0_rep
zres = res.res
zres0 = res.res0
f = Figure(size = (650, 300))
ax = Axis(f[1, 1], xlabel = &quot;Wawelength (nm)&quot;, ylabel = &quot;RMSEP_Val&quot;,
    xticks = zres.lo)
scatter!(ax, zres.mid, zres.y1; color = (:red, .5))
vlines!(ax, zres.lo; color = :grey, linestyle = :dash, linewidth = 1)
hlines!(ax, zres0.y1, linestyle = :dash)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/isel.jl#LL1-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kdeda-Tuple{Any, Any}" href="#Jchemo.kdeda-Tuple{Any, Any}"><code>Jchemo.kdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kdeda(X, y; kwargs...)</code></pre><p>Discriminant analysis using non-parametric kernel Gaussian      density estimation (KDE-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li></ul><p>The principle is the same as functions <code>lda</code> and <code>qda</code>  except that densities are estimated from function <code>dmkern</code>  instead of function <code>dmnorm</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

prior = :unif
#prior = :prop
mod = model(kdeda; prior)
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

mod = model(kdeda; prior, a_kde = .5) 
#mod = model(kdeda; prior, h_kde = .1) 
fit!(mod, Xtrain, ytrain)
mod.fm.fm[1].H</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kdeda.jl#LL1-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.knnda-Tuple{Any, Any}" href="#Jchemo.knnda-Tuple{Any, Any}"><code>Jchemo.knnda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">knnda(X, y; kwargs...)</code></pre><p>k-Nearest-Neighbours weighted discrimination (KNN-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation   for the global dimension reduction.</li></ul><p>This function has the same principle as function  <code>knnr</code>except that a discrimination is done instead of a  regression. A weighted vote is done over the neighborhood,  and the prediction corresponds to the most frequent class.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 10
mod = model(knnda; nlvdis, metric, h, k) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/knnda.jl#LL1-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.knnr-Tuple{Any, Any}" href="#Jchemo.knnr-Tuple{Any, Any}"><code>Jchemo.knnr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">knnr(X, Y; kwargs...)</code></pre><p>k-Nearest-Neighbours weighted regression (KNNR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation   for the global dimension reduction.</li></ul><p>The general principle of this function is as  follows (many other variants of kNNR pipelines  can be built):</p><p>For each new observation to predict, the prediction is the  weighted mean over a selected neighborhood (in <code>X</code>) of  size <code>k</code>. Within the selected neighborhood, the weights  are defined from the dissimilarities between the new  observation and the neighborhood, and are computed from  function &#39;wdist&#39;.</p><p>In general, for high dimensional X-data, using the  Mahalanobis distance requires preliminary dimensionality  reduction of the data. In function <code>knnr&#39;, the  preliminary reduction (argument</code>nlvdis<code>) is done by PLS on {</code>X<code>,</code>Y`}.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 5 ; metric = :mah 
#nlvdis = 0 ; metric = :eucl 
h = 1 ; k = 5 
mod = model(knnr; nlvdis, metric, h, k) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
mod = model(knnr; k = 15, h = 5) 
fit!(mod, x, y)
pred = predict(mod, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/knnr.jl#LL1-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kpca-Tuple{Any}" href="#Jchemo.kpca-Tuple{Any}"><code>Jchemo.kpca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kpca(X; kwargs...)
kpca(X, weights::Weight; kwargs...)</code></pre><p>Kernel PCA  (Scholkopf et al. 1997, Scholkopf &amp; Smola 2002, Tipping 2001).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. principal components (PCs) to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>The method is implemented by SVD factorization of the weighted  Gram matrix: </p><ul><li>D^(1/2) * Phi(X) * Phi(X)&#39; * D^(1/2)</li></ul><p>where X is the cenetred matrix and D is a diagonal matrix  of weights (<code>weights.w</code>) of the observations (rows of X).</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A., MÃ¼ller, K.-R., 1997. Kernel principal  component analysis, in: Gerstner, W., Germond, A., Hasler,  M., Nicoud, J.-D. (Eds.), Artificial Neural Networks,  ICANN 97, Lecture Notes in Computer Science. Springer, Berlin,  Heidelberg, pp. 583-588. https://doi.org/10.1007/BFb0020217</p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support  vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.</p><p>Tipping, M.E., 2001. Sparse kernel principal component analysis.  Advances in neural information processing systems, MIT Press.  http://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
Xtest = X[s.test, :]

nlv = 3
kern = :krbf ; gamma = 1e-4
mod = model(kpca; nlv, kern, gamma) ;
fit!(mod, Xtrain)
pnames(mod.fm)
@head T = mod.fm.T
T&#39; * T
mod.fm.P&#39; * mod.fm.P

@head Ttest = transf(mod, Xtest)

res = summary(mod) ;
pnames(res)
res.explvarx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kpca.jl#LL1-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplskdeda-Tuple{Any, Any}" href="#Jchemo.kplskdeda-Tuple{Any, Any}"><code>Jchemo.kplskdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kplskdeda(X, y; kwargs...)
kplskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>KPLS-KDEDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plskdeda</code> (PLS-KDEDA) except that  a kernel PLSR (function <code>kplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>kplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplskdeda.jl#LL1-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplslda-Tuple{Any, Any}" href="#Jchemo.kplslda-Tuple{Any, Any}"><code>Jchemo.kplslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kplslda(X, y; kwargs...)
kplslda(X, y, weights::Weight; kwargs...)</code></pre><p>KPLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plslda</code> (PLS-LDA) except that  a kernel PLSR (function <code>kplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
gamma = .1
mod = model(kplslda; nlv, gamma) 
#mod = model(kplslda; nlv, gamma, prior = :prop) 
#mod = model(kplsqda; nlv, gamma, alpha = .5) 
#mod = model(kplskdeda; nlv, gamma, a_kde = .5) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

fmpls = fm.fm.fmpls ;
@head fmpls.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fmpls)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplslda.jl#LL1-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplsqda-Tuple{Any, Any}" href="#Jchemo.kplsqda-Tuple{Any, Any}"><code>Jchemo.kplsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kplsqda(X, y; kwargs...)
kplsqda(X, y, weights::Weight; kwargs...)</code></pre><p>KPLS-QDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsqda</code> (PLS-QDA) except that  a kernel PLSR (function <code>kplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>kplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplsqda.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplsr-Tuple{Any, Any}" href="#Jchemo.kplsr-Tuple{Any, Any}"><code>Jchemo.kplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kplsr(X, Y; kwargs...)
kplsr(X, Y, weights::Weight; kwargs...)
kplsr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Kernel partial least squares regression (KPLSR) implemented with a Nipals      algorithm (Rosipal &amp; Trejo, 2001).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>This algorithm becomes slow for n &gt; 1000.  Use function <code>dkplsr</code> instead.</p><p><strong>References</strong></p><p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least  Squares Regression in Reproducing Kernel Hilbert Space.  Journal of Machine Learning Research 2, 97-123.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
kern = :krbf ; gamma = 1e-1
mod = model(kplsr; nlv, kern, gamma) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T

coef(mod)
coef(mod; nlv = 3)

@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
nlv = 2
kern = :krbf ; gamma = 1 / 3
mod = model(kplsr; nlv, kern, gamma) 
fit!(mod, x, y)
pred = predict(mod, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplsr.jl#LL1-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplsrda-Tuple{Any, Any}" href="#Jchemo.kplsrda-Tuple{Any, Any}"><code>Jchemo.kplsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kplsrda(X, y; kwargs...)
kplsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on kernel partial least squares     regression (KPLSR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that  a kernel PLSR (function <code>kplsr</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
kern = :krbf ; gamma = .001 
scal = true
mod = model(kplsrda; nlv, kern, gamma, scal) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

@head fm.fm.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fm.fm)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplsrda.jl#LL1-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kpol-Tuple{Any, Any}" href="#Jchemo.kpol-Tuple{Any, Any}"><code>Jchemo.kpol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kpol(X, Y; kwargs...)</code></pre><p>Compute a polynomial kernel Gram matrix. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (m, p).</li></ul><p>Keyword arguments:</p><ul><li><code>degree</code> : Degree of the polynom.</li><li><code>gamma</code> : Scale of the polynom.</li><li><code>coef0</code> : Offset of the polynom.</li></ul><p>Given matrices <code>X</code> and <code>Y</code>of sizes (n, p) and (m, p),  respectively, the function returns the (n, m) Gram matrix:</p><ul><li>K(X, Y) = Phi(X) * Phi(Y)&#39;.</li></ul><p>The polynomial kernel between two vectors x and y is  computed by (<code>gamma</code> * (x&#39; * y) + <code>coef0</code>)^<code>degree</code>.</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support  vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)
kpol(X, Y; degree = 3, gamma = .1, cost = 10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kernels.jl#LL37-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.krbf-Tuple{Any, Any}" href="#Jchemo.krbf-Tuple{Any, Any}"><code>Jchemo.krbf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">krbf(X, Y; kwargs...)</code></pre><p>Compute a Radial-Basis-Function (RBF) kernel Gram matrix. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (m, p).</li></ul><p>Keyword arguments:</p><ul><li><code>gamma</code> : Scale parameter.</li></ul><p>Given matrices <code>X</code> and <code>Y</code>of sizes (n, p) and (m, p),  respectively, the function returns the (n, m) Gram matrix:</p><ul><li>K(X, Y) = Phi(X) * Phi(Y)&#39;.</li></ul><p>The RBF kernel between two vectors x and y is computed by  exp(-<code>gamma</code> * ||x - y||^2).</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support  vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)
krbf(X, Y; gamma = .1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kernels.jl#LL1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.krr-Tuple{Any, Any}" href="#Jchemo.krr-Tuple{Any, Any}"><code>Jchemo.krr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">krr(X, Y; kwargs...)
krr(X, Y, weights::Weight; kwargs...)
krr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Kernel ridge regression (KRR) implemented by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of `X    is scaled by its uncorrected standard deviation.</li></ul><p>KRR is also referred to as least squared SVM regression  (LS-SVMR). The method is close to the particular case of  SVM regression where there is no marge excluding the  observations (epsilon coefficient set to zero). The difference  is that a L2-norm optimization is done, instead of L1 in SVM.</p><p><strong>References</strong></p><p>Bennett, K.P., Embrechts, M.J., 2003. An optimization  perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and  Applications, NATO Science Series III: Computer &amp; Systems  Sciences. IOS Press Amsterdam, pp. 227-250.</p><p>Cawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel  Ridge Regression. Neural Processing Letters 16, 293-302.  https://doi.org/10.1023/A:1021798002258</p><p>Krell, M.M., 2018. Generalizing, Decoding, and Optimizing  Support Vector Machine Classification. arXiv:1801.04929.</p><p>Saunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression  Learning Algorithm in Dual Variables, in: In Proceedings of the  15th International Conference on Machine Learning. Morgan  Kaufmann, pp. 515-521.</p><p>Suykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse  approximation using least squares support vector machines. 2000 IEEE  International Symposium on Circuits and Systems. Emerging Technologies  for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439</p><p>Welling, M., n.d. Kernel ridge regression. Department of  Computer Science, University of Toronto, Toronto, Canada.  https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

lb = 1e-3
kern = :krbf ; gamma = 1e-1
mod = model(krr; lb, kern, gamma) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

coef(mod)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

coef(mod; lb = 1e-1)
res = predict(mod, Xtest; lb = [.1 ; .01])
@head res.pred[1]
@head res.pred[2]

lb = 1e-3
kern = :kpol ; degree = 1
mod = model(krr; lb, kern, degree) 
fit!(mod, Xtrain, ytrain)
res = predict(mod, Xtest)
rmsep(res.pred, ytest)

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
lb = 1e-1
kern = :krbf ; gamma = 1 / 3
mod = model(krr; lb, kern, gamma) 
fit!(mod, x, y)
pred = predict(mod, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/krr.jl#LL1-L115">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.krrda-Tuple{Any, Any}" href="#Jchemo.krrda-Tuple{Any, Any}"><code>Jchemo.krrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">krrda(X, y; kwargs...)
krrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on kernel ridge regression (KRR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>. See respective    functions <code>krbf</code> and <code>kpol</code> for their keyword arguments.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>rrda</code> (RR-DA) except that a kernel  RR (function <code>krr</code>), instead of a RR (function <code>rr</code>),  is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

lb = 1e-5
kern = :krbf ; gamma = .001 
scal = true
mod = model(krrda; lb, kern, gamma, scal) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

coef(fm.fm)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; lb = [.1, .001]).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/krrda.jl#LL1-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lda-Tuple{Any, Any}" href="#Jchemo.lda-Tuple{Any, Any}"><code>Jchemo.lda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lda(; kwargs...)
lda(X, y; kwargs...)
lda(X, y, weights::Weight; kwargs...)</code></pre><p>Linear discriminant analysis (LDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li></ul><p>In these <code>lda</code> functions, observation weights (argument <code>weights</code>) are used  to compute the intra-class (= &quot;within&quot;) covariance matrix. Argument <code>prior</code>  is used to define the usual prior class probabilities. </p><p>In the high-level version, the observation weights are automatically  defined by the given priors (<code>prior</code>): the sub-total weights by class are set  equal to the prior probabilities. For other choices, use the low-level  version.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

mod = lda()
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni
aggsum(fm.weights.w, ytrain)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lda.jl#LL1-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lg-Tuple{Any, Any}" href="#Jchemo.lg-Tuple{Any, Any}"><code>Jchemo.lg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lg(X, Y; centr = true)
lg(Xbl; centr = true)</code></pre><p>Compute the Lg coefficient between matrices.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>Xbl</code> : A list (vector) of matrices.</li></ul><p>Keyword arguments:</p><ul><li><code>centr</code> : Boolean indicating if the matrices will    be internally centered or not.</li></ul><p>Lg(X, Y) = Sum.(j=1..p) Sum.(k= 1..q) cov(xj, yk)^2</p><p>RV(X, Y) = Lg(X, Y) / sqrt(Lg(X, X), Lg(Y, Y))</p><p><strong>References</strong></p><p>Escofier, B. &amp; Pagès, J. 1984. L’analyse factorielle multiple.  Cahiers du Bureau universitaire de recherche opérationnelle.  Série Recherche, tome 42, p. 3-68</p><p>Escofier, B. &amp; Pagès, J. (2008). Analyses Factorielles Simples  et Multiples : Objectifs, Méthodes et Interprétation. Dunod,  4e édition.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
Y = rand(5, 3)
lg(X, Y)

X = rand(5, 15) 
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl)
lg(Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/angles.jl#LL1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.list-Tuple{Any, Integer}" href="#Jchemo.list-Tuple{Any, Integer}"><code>Jchemo.list</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">list(Q, n::Integer)</code></pre><p>Create a Vector{Q}(undef, n).</p><p><code>isassigned(object, i)</code> can be used to check if cell i is empty.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">list(Float64, 5)
list(Array{Float64}, 5)
list(Matrix{Int}, 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL462-L474">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.list-Tuple{Integer}" href="#Jchemo.list-Tuple{Integer}"><code>Jchemo.list</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">list(n::Integer)</code></pre><p>Create a Vector{Any}(nothing, n).</p><p><code>isnothing(object, i)</code> can be used to check if cell i is empty.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">list(5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL449-L459">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.locw-Tuple{Any, Any, Any}" href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>Jchemo.locw</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">locw(Xtrain, Ytrain, X; listnn, listw = nothing, fun, verbose = false, 
    kwargs...)</code></pre><p>Compute predictions for a given kNN model.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>Ytrain</code> : Training Y-data.</li><li><code>X</code> : X-data (m observations) to predict.</li></ul><p>Keyword arguments:</p><ul><li><code>listnn</code> : List (vector) of m vectors of indexes.</li><li><code>listw</code> : List (vector) of m vectors of weights.</li><li><code>fun</code> : Function computing the model on    the m neighborhoods.</li><li><code>verbose</code> : Boolean. If <code>true</code>, fitting information   are printed.</li><li><code>kwargs</code> : Keywords arguments to pass in function <code>fun</code>.   Each argument must have length = 1 (not be a collection).</li></ul><p>Each component i of <code>listnn</code> and <code>listw</code> contains the indexes  and weights, respectively, of the nearest neighbors of x_i in Xtrain.  The sizes of the neighborhood for i = 1,...,m can be different.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/locw.jl#LL1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.locwlv-Tuple{Any, Any, Any}" href="#Jchemo.locwlv-Tuple{Any, Any, Any}"><code>Jchemo.locwlv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">locwlv(Xtrain, Ytrain, X; listnn, listw = nothing, fun, nlv, verbose = true, 
    kwargs...)</code></pre><p>Compute predictions for a given kNN model.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>Ytrain</code> : Training Y-data.</li><li><code>X</code> : X-data (m observations) to predict.</li></ul><p>Keyword arguments:</p><ul><li><code>listnn</code> : List (vector) of m vectors of indexes.</li><li><code>listw</code> : List (vector) of m vectors of weights.</li><li><code>fun</code> : Function computing the model on    the m neighborhoods.</li><li><code>nlv</code> : Nb. or collection of nb. of latent variables (LVs).</li><li><code>verbose</code> : Boolean. If <code>true</code>, fitting information    are printed.</li><li><code>kwargs</code> : Keywords arguments to pass in function <code>fun</code>.   Each argument must have length = 1 (not be a collection).</li></ul><p>Same as <a href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>locw</code></a> but specific and much faster  for LV-based models (e.g. PLSR).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/locwlv.jl#LL1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwmlr-Tuple{Any, Any}" href="#Jchemo.lwmlr-Tuple{Any, Any}"><code>Jchemo.lwmlr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwmlr(X, Y; kwargs...)</code></pre><p>k-Nearest-Neighbours locally weighted multiple linear      regression (kNN-LWMLR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li></ul><p>This is the same principle as function <code>lwplsr</code> except  that MLR models are fitted on the neighborhoods, instead of  PLSR models.  The neighborhoods are computed directly on <code>X</code>  (there is no preliminary dimension reduction).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
mod0 = model(pcasvd; nlv) ;
fit!(mod0, Xtrain) 
@head Ttrain = mod0.fm.T 
@head Ttest = transf(mod0, Xtest)

metric = :eucl 
h = 2 ; k = 100 
mod = model(lwmlr; metric, h, k) 
fit!(mod, Ttrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Ttest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
mod = model(lwmlr; metric = :eucl, h = 1.5, k = 20) ;
fit!(mod, x, y)
pred = predict(mod, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwmlr.jl#LL1-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwmlrda-Tuple{Any, Any}" href="#Jchemo.lwmlrda-Tuple{Any, Any}"><code>Jchemo.lwmlrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwmlrda(X, y; kwargs...)</code></pre><p>k-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation   for the global dimension reduction.</li></ul><p>This is the same principle as function <code>lwmlr</code> except  that MLR-DA models, instead of MLR models, are fitted  on the neighborhoods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

metric = :mah
h = 2 ; k = 10
mod = model(lwmlrda; metric, h, k) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwmlrda.jl#LL1-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplslda-Tuple{Any, Any}" href="#Jchemo.lwplslda-Tuple{Any, Any}"><code>Jchemo.lwplslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplslda(X, y; kwargs...)</code></pre><p>kNN-LWPLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation   for the global dimension reduction and the local   models.</li></ul><p>This is the same principle as function <code>lwplsr</code> except  that PLS-LDA models, instead of PLSR models, are fitted  on the neighborhoods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 1 ; k = 100
mod = model(lwplslda; nlvdis, metric, h, k, prior = :prop) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplslda.jl#LL1-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsqda-Tuple{Any, Any}" href="#Jchemo.lwplsqda-Tuple{Any, Any}"><code>Jchemo.lwplsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsqda(X, y; kwargs...)</code></pre><p>kNN-LWPLS-QDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation   for the global dimension reduction and the local   models.</li></ul><p>This is the same principle as function <code>lwplsr</code> except  that PLS-QDA models, instead of PLSR models, are fitted  on the neighborhoods.</p><ul><li><strong>Warning:</strong> The present version of this function suffers from </li></ul><p>frequent stops due to non positive definite matrices when doing QDA on neighborhoods, since some classes within the neighborhood can  have very few observations. It is recommended to select  a sufficiantly large number of neighbors or/and to use a  regularized QDA (<code>alpha &gt; 0</code>).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 1 ; k = 200
mod = model(lwplsqda; nlvdis, metric, h, k, prior = :prop, alpha = .5) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsqda.jl#LL1-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsr-Tuple{Any, Any}" href="#Jchemo.lwplsr-Tuple{Any, Any}"><code>Jchemo.lwplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsr(X, Y; kwargs...)</code></pre><p>k-Nearest-Neighbours locally weighted partial least squares regression      (kNN-LWPLSR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation   for the global dimension reduction and the local   models.</li></ul><p>Function <code>lwplsr</code> fits kNN-LWPLSR models such as in  Lesnoff et al. 2020. The general principle of the pipeline  is as follows (many other variants of pipelines can  be built):</p><p>LWPLSR is a particular case of weighted PLSR (WPLSR)  (e.g. Schaal et al. 2002). In WPLSR, a priori weights,  different from the usual 1/n (standard PLSR), are given  to the n training observations. These weights are used for  calculating (i) the scores and loadings of the WPLS and  (ii) the regression model that fits (by weighted  least squares) the Y-response(s) to the WPLS scores.  The specificity of LWPLSR (compared to WPLSR) is that the  weights are computed from dissimilarities (e.g. distances)  between the new observation to predict and the training  observations (&quot;L&quot; in LWPLSR comes from &quot;localized&quot;). Note  that in LWPLSR the weights and therefore the fitted WPLSR  model change for each new observation to predict.</p><p>In the original LWPLSR, all the n training observations  are used for each observation to predict (e.g. Sicard &amp; Sabatier 2006,  Kim et al 2011). This can be very time consuming when n is large.  A faster (and often more efficient) strategy is to preliminary select,  in the training set, a number of <code>k</code> nearest neighbors to the  observation to predict (= &quot;weighting 1&quot;) and then to apply  LWPLSR only to this pre-selected neighborhood (= &quot;weighting 2&quot;). T his strategy corresponds to a kNN-LWPLSR and is the one  implemented in function <code>lwplsr</code>.</p><p>In <code>lwplsr</code>, the dissimilarities used for weightings 1 and 2  are computed from the raw X-data, or after a dimension reduction, depending on argument <code>nlvdis</code>. In the last case, global PLS2  scores (LVs) are computed from {<code>X</code>, <code>Y</code>} and the dissimilarities  are computed over these scores. </p><p>In general, for high dimensional X-data, using the  Mahalanobis distance requires preliminary dimensionality  reduction of the data. In function <code>knnr&#39;, the  preliminary reduction (argument</code>nlvdis<code>) is done by PLS on {</code>X<code>,</code>Y`}.</p><p><strong>References</strong></p><p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of  active pharmaceutical ingredients content using locally weighted  partial least squares and statistical wavelength selection.  Int. J. Pharm., 421, 269-274.</p><p>Lesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally  weighted PLS strategies for regression and discrimination on  agronomic NIR data. Journal of Chemometrics, e3209.  https://doi.org/10.1002/cem.3209</p><p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable  techniques from nonparametric statistics for the real time  robot learning. Applied Intell., 17, 49-60.</p><p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local  PLS1 regression and application to a rainfall data set.  Comput. Stat. Data Anal., 51, 1393-1410.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 5 ; metric = :mah 
h = 1 ; k = 200 ; nlv = 15
mod = model(lwplsr; nlvdis, metric, h, k, nlv) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsr.jl#LL1-L125">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsravg-Tuple{Any, Any}" href="#Jchemo.lwplsravg-Tuple{Any, Any}"><code>Jchemo.lwplsravg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsravg(X, Y; kwargs...)</code></pre><p>Averaging kNN-LWPLSR models with different numbers of latent variables      (kNN-LWPLSR-AVG).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : A range of nb. of latent variables (LVs)    to compute for the local (i.e. inside each neighborhood)    models.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation   for the global dimension reduction and the local   models.</li></ul><p>Ensemblist method where the predictions are computed  by averaging the predictions of a set of models built  with different numbers of LVs, such as in Lesnoff 2023. On each neighborhood, a PLSR-averaging (Lesnoff et al. </p><ol><li>is done instead of a PLSR.</li></ol><p>For instance, if argument <code>nlv</code> is set to <code>nlv</code> = <code>5:10</code>,  the prediction for a new observation is the simple average of the predictions returned by the models with 5 LVs, 6 LVs,  ... 10 LVs, respectively.</p><p><strong>References</strong></p><p>Lesnoff, M., Andueza, D., Barotin, C., Barre, P., Bonnal, L.,  Fernández Pierna, J.A., Picard, F., Vermeulen, P., Roger,  J.-M., 2022. Averaging and Stacking Partial Least Squares  Regression Models to Predict the Chemical Compositions and  the Nutritive Values of Forages from Spectral Near Infrared  Data. Applied Sciences 12, 7850.  https://doi.org/10.3390/app12157850</p><p>M. Lesnoff, Averaging a local PLSR pipeline to predict  chemical compositions and nutritive values of forages  and feed from spectral near infrared data, Chemometrics and  Intelligent Laboratory Systems. 244 (2023) 105031.  https://doi.org/10.1016/j.chemolab.2023.105031.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 5 ; metric = :mah 
h = 1 ; k = 200 ; nlv = 4:20
mod = model(lwplsravg; nlvdis, metric, h, k, nlv) ;
fit!(mod, Ttrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Ttest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsravg.jl#LL1-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsrda-Tuple{Any, Any}" href="#Jchemo.lwplsrda-Tuple{Any, Any}"><code>Jchemo.lwplsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsrda(X, y; kwargs...)</code></pre><p>kNN-LWPLSR-DA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>nlvdis</code> : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: <code>:eucl</code> (Euclidean distance), <code>:mah</code> (Mahalanobis    distance).</li><li><code>h</code> : A scalar defining the shape of the weight    function computed by function <code>wdist</code>. Lower is h,    sharper is the function. See function <code>wdist</code> for    details (keyword arguments <code>criw</code> and <code>squared</code> of    <code>wdist</code> can also be specified here).</li><li><code>k</code> : The number of nearest neighbors to select for    each observation to predict.</li><li><code>tolw</code> : For stabilization when very close neighbors.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation   for the global dimension reduction and the local   models.</li></ul><p>This is the same principle as function <code>lwplsr</code> except  that PLSR-DA models, instead of PLSR models, are fitted  on the neighborhoods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 100
mod = model(lwplsrda; nlvdis, metric, h, k) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
res.listnn
res.listd
res.listw
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsrda.jl#LL1-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mahsq-Tuple{Any, Any}" href="#Jchemo.mahsq-Tuple{Any, Any}"><code>Jchemo.mahsq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mahsq(X, Y)
mahsq(X, Y, Sinv)</code></pre><p>Squared Mahalanobis distances between      the rows of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (m, p).</li><li><code>Sinv</code> : Inverse of a covariance matrix S.   If not given, S is computed as the uncorrected    covariance matrix of <code>X</code>.</li></ul><p>When <code>X</code> and <code>Y</code> are (n, p) and (m, p), repectively, it returns  an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using StatsBase 

X = rand(5, 3)
Y = rand(2, 3)

mahsq(X, Y)

S = cov(X, corrected = false)
Sinv = inv(S)
mahsq(X, Y, Sinv)
mahsq(X[1:1, :], Y[1:1, :], Sinv)

mahsq(X[:, 1], 4)
mahsq(1, 4, 2.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/distances.jl#LL30-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mahsqchol-Tuple{Any, Any}" href="#Jchemo.mahsqchol-Tuple{Any, Any}"><code>Jchemo.mahsqchol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mahsqchol(X, Y)
mahsqchol(X, Y, Uinv)</code></pre><p>Compute the squared Mahalanobis distances (with a Cholesky factorization) between the observations (rows) of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (m, p).</li><li><code>Uinv</code> : Inverse of the upper matrix of a Cholesky factorization    of a covariance matrix S.   If not given, the factorization is done on S,    the uncorrected covariance matrix of <code>X</code>.</li></ul><p>When <code>X</code> and <code>Y</code> are (n, p) and (m, p), repectively, it returns  an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using LinearAlgebra

X = rand(5, 3)
Y = rand(2, 3)

mahsqchol(X, Y)

S = cov(X, corrected = false)
U = cholesky(Hermitian(S)).U 
Uinv = inv(U)
mahsqchol(X, Y, Uinv)

mahsqchol(X[:, 1], 4)
mahsqchol(1, 4, sqrt(2.1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/distances.jl#LL78-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.matB" href="#Jchemo.matB"><code>Jchemo.matB</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matB(X, y, weights::Weight)</code></pre><p>Between-class covariance matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : A vector (n) defining the class membership.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Compute the between-class covariance matrix (output <code>B</code>)  of <code>X</code>. This is the (non-corrected) covariance matrix of  the weighted class centers.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using StatsBase

n = 20 ; p = 3
X = rand(n, p)
y = rand(1:3, n)
tab(y) 
weights = mweight(ones(n)) 

res = matB(X, y, weights) ;
res.B
res.priors
res.ni
res.lev

res = matW(X, y, weights) ;
res.W
res.Wi

matW(X, y, weights).W + matB(X, y, weights).B
cov(X; corrected = false)

v = mweight(collect(1:n))
matW(X, y, v).priors 
matB(X, y, v).priors 
matW(X, y, v).W + matB(X, y, v).B
covm(X, v)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/matW.jl#LL1-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.matW" href="#Jchemo.matW"><code>Jchemo.matW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matW(X, y, weights::Weight)</code></pre><p>Within-class covariance matrices.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : A vector (n) defing the class membership.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Compute the (non-corrected) within-class and pooled covariance  matrices  (outputs <code>Wi</code> and <code>W</code>, respectively) of <code>X</code>. </p><p>If class i contains only one observation, Wi is computed by:</p><ul><li><code>covm(</code>X<code>,</code>weights<code>)</code>.</li></ul><p>For examples, see function <code>matB</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/matW.jl#LL61-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mavg-Tuple{Any}" href="#Jchemo.mavg-Tuple{Any}"><code>Jchemo.mavg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mavg(X; kwargs...)</code></pre><p>Smoothing by moving averages of each row of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Nb. points involved in the window. </li></ul><p>The smoothing is computed by convolution with padding,  using function imfilter of package ImageFiltering.jl.  The centered kernel is ones(<code>npoint</code>) / <code>npoint</code>.  Each returned point is located on the center of the kernel.</p><p>The function returns a matrix (n, p).</p><p><strong>References</strong></p><p>Package ImageFiltering.jl https://github.com/JuliaImages/ImageFiltering.jl</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(mavg; npoint = 10) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL222-L263">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbconcat-Tuple{Any}" href="#Jchemo.mbconcat-Tuple{Any}"><code>Jchemo.mbconcat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbconcat(Xbl)</code></pre><p>Concatenate horizontaly multiblock X-data.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5 ; m = 3 ; p = 10 
X = rand(n, p) 
Xnew = rand(m, p)
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl) 
Xblnew = mblock(Xnew, listbl) 
@head Xbl[3]

mod = model(mbconcat) 
fit!(mod, Xbl)
transf(mod, Xbl)
transf(mod, Xblnew)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbconcat.jl#LL3-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mblock-Tuple{Any, Any}" href="#Jchemo.mblock-Tuple{Any, Any}"><code>Jchemo.mblock</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mblock(X, listbl)</code></pre><p>Make blocks from a matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>listbl</code> : A vector whose each component defines    the colum numbers defining a block in <code>X</code>.   The length of <code>listbl</code> is the number of blocks.</li></ul><p>The function returns a list (vector) of blocks.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5 ; p = 10 
X = rand(n, p) 
listbl = [3:4, 1, [6; 8:10]]

Xbl = mblock(X, listbl)
Xbl[1]
Xbl[2]
Xbl[3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_mb.jl#LL1-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbpca-Tuple{Any}" href="#Jchemo.mbpca-Tuple{Any}"><code>Jchemo.mbpca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbpca(Xbl; kwargs...)
mbpca(Xbl, weights::Weight; kwargs...)
mbpca!(Xbl::Matrix, weights::Weight; kwargs...)</code></pre><p>Consensus principal components analysis (CPCA = MBPCA).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data.    Typically, output of function <code>mblock</code>.  </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>       for possible values.</li><li><code>tol</code> : Tolerance value for Nipals convergence.</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>The MBPCA global scores are equal to the scores of the PCA  of the horizontal concatenation X = [X1 X2 ... Xk].</p><p>The function returns several objects, in particular:</p><ul><li><code>T</code> : The non normed global scores.</li><li><code>U</code> : The normed global scores.</li><li><code>W</code> : The global loadings.</li><li><code>Tbl</code> : The block scores (grouped by blocks, in    original scale).</li><li><code>Tb</code> : The block scores (grouped by LV, in    the metric scale).</li><li><code>Wbl</code> : The block loadings.</li><li><code>lb</code> : The specific weights &quot;lambda&quot;.</li><li><code>mu</code> : The sum of the specific weights (= eigen value   of the global PCA).</li></ul><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total inertia of X    (sum of the squared norms of the    blocks) explained by each global score.</li><li><code>contr_block</code> : Contribution of each block    to the global scores. </li><li><code>explX</code> : Proportion of the inertia of the blocks    explained by each global score.</li><li><code>corx2t</code> : Correlation between the global scores    and the original variables.  </li><li><code>cortb2t</code> : Correlation between the global scores    and the block scores.</li><li><code>rv</code> : RV coefficient. </li><li><code>lg</code> : Lg coefficient. </li></ul><p><strong>References</strong></p><p>Mangamana, E.T., Cariou, V., Vigneau, E., Glèlè Kakaï, R.L.,  Qannari, E.M., 2019. Unsupervised multiblock data  analysis: A unified approach and extensions. Chemometrics and  Intelligent Laboratory Systems 194, 103856.  https://doi.org/10.1016/j.chemolab.2019.103856</p><p>Westerhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis  of multiblock and hierarchical PCA and PLS models. Journal  of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5&lt;301::AID-CEM515&gt;3.0.CO;2-S</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
pnames(dat) 
X = dat.X
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X[1:6, :], listbl)
Xblnew = mblock(X[7:8, :], listbl)
n = nro(Xbl[1]) 

nlv = 3
bscal = :frob
scal = false
#scal = true
mod = model(mbpca; nlv, bscal, scal)
fit!(mod, Xbl)
pnames(mod) 
pnames(mod.fm)
## Global scores 
@head mod.fm.T
@head transf(mod, Xbl)
transf(mod, Xblnew)
## Blocks scores
i = 1
@head mod.fm.Tbl[i]
@head transfbl(mod, Xbl)[i]

res = summary(mod, Xbl) ;
pnames(res) 
res.explvarx
res.contr_block
res.explX   # = mod.fm.lb if bscal = :frob
rowsum(Matrix(res.explX))
res.corx2t 
res.cortb2t
res.rv</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbpca.jl#LL1-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplskdeda-Tuple{Any, Any}" href="#Jchemo.mbplskdeda-Tuple{Any, Any}"><code>Jchemo.mbplskdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplskdeda(Xbl, y; kwargs...)
mbplskdeda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Multiblock PLS-KDEDA.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>   for possible values.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This is the same principle as function <code>plskdeda</code>, for multiblock X-data.</p><p>See function <code>mbplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplskdeda.jl#LL1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplslda-Tuple{Any, Any}" href="#Jchemo.mbplslda-Tuple{Any, Any}"><code>Jchemo.mbplslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplslda(Xbl, y; kwargs...)
mbplslda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Multiblock PLS-LDA.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>   for possible values.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This is the same principle as function <code>plslda</code>, for multiblock X-data.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
wlst = names(X)
wl = parse.(Float64, wlst)
#plotsp(X, wl; nsamp = 20).f
##
listbl = [1:350, 351:700]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 

nlv = 15
scal = false
#scal = true
bscal = :none
#bscal = :frob
mod = model(mbplslda; nlv, bscal, scal)
#mod = model(mbplsqda; nlv, bscal, alpha = .5, scal)
#mod = model(mbplskdeda; nlv, bscal, scal)
fit!(mod, Xbltrain, ytrain) 
pnames(mod) 

@head transf(mod, Xbltrain)
@head transf(mod, Xbltest)

res = predict(mod, Xbltest) ; 
@head res.pred 
@show errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xbltest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplslda.jl#LL1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplsqda-Tuple{Any, Any}" href="#Jchemo.mbplsqda-Tuple{Any, Any}"><code>Jchemo.mbplsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplsqda(Xbl, y; kwargs...)
mbplsqda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Multiblock PLS-QDA.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>   for possible values.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This is the same principle as function <code>plsqda</code>, for multiblock X-data.</p><p>See function <code>mbplslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsqda.jl#LL1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplsr-Tuple{Any, Any}" href="#Jchemo.mbplsr-Tuple{Any, Any}"><code>Jchemo.mbplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplsr(Xbl, Y; kwargs...)
mbplsr(Xbl, Y, weights::Weight; kwargs...)
mbplsr!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock PLSR (MBPLSR).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>   for possible values.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This function runs a PLSR on {X, <code>Y</code>} where X is the horizontal  concatenation of the blocks in <code>Xbl</code>. The function gives the  same results as function <code>mbplswest</code>, but is much faster.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
pnames(dat) 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
Xbltest = mblock(rmrow(X, s), listbl)
ytrain = y[s]
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 3
bscal = :frob
scal = false
#scal = true
mod = model(mbplsr; nlv, bscal, scal)
fit!(mod, Xbltrain, ytrain)
pnames(mod) 
pnames(mod.fm)
@head mod.fm.T
@head transf(mod, Xbltrain)
transf(mod, Xbltest)

res = predict(mod, Xbltest)
res.pred 
rmsep(res.pred, ytest)

res = summary(mod, Xbltrain) ;
pnames(res) 
res.explvarx
res.corx2t 
res.rdx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsr.jl#LL1-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplsrda-Tuple{Any, Any}" href="#Jchemo.mbplsrda-Tuple{Any, Any}"><code>Jchemo.mbplsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplsrda(Xbl, y; kwargs...)
mbplsrda(Xbl, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on multiblock partial least squares      regression (MBPLSR-DA).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>   for possible values.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This is the same principle as function <code>plsrda</code>, for multiblock X-data.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
tab(Y.typ)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
wlst = names(X)
wl = parse.(Float64, wlst)
#plotsp(X, wl; nsamp = 20).f
##
listbl = [1:350, 351:700]
Xbltrain = mblock(Xtrain, listbl)
Xbltest = mblock(Xtest, listbl) 

nlv = 15
scal = false
#scal = true
bscal = :none
#bscal = :frob
mod = model(mbplsrda; nlv, bscal, scal)
fit!(mod, Xbltrain, ytrain) 
pnames(mod) 

@head mod.fm.fm.T 
@head transf(mod, Xbltrain)
@head transf(mod, Xbltest)

res = predict(mod, Xbltest) ; 
@head res.pred 
@show errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xbltest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsrda.jl#LL1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplswest-Tuple{Any, Any}" href="#Jchemo.mbplswest-Tuple{Any, Any}"><code>Jchemo.mbplswest</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplswest(Xbl, Y; kwargs...)
mbplswest(Xbl, Y, weights::Weight; kwargs...)
mbplswest!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock PLSR (MBPLSR) - Nipals algorithm.</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. See function <code>blockscal</code>   for possible values.</li><li><code>tol</code> : Tolerance value for convergence (Nipals).</li><li><code>maxit</code> : Maximum number of iterations (Nipals).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This functions implements the MBPLSR Nipals algorithm such  as in Westerhuis et al. 1998. The function gives the same  results as function <code>mbplsr</code>.</p><p><strong>References</strong></p><p>Westerhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis  of multiblock and hierarchical PCA and PLS models. Journal of  Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5&lt;301::AID-CEM515&gt;3.0.CO;2-S</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
pnames(dat) 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
Xbltest = mblock(rmrow(X, s), listbl)
ytrain = y[s]
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 3
bscal = :frob
scal = false
#scal = true
mod = model(mbplswest; nlv, bscal, scal)
fit!(mod, Xbltrain, ytrain)
pnames(mod) 
pnames(mod.fm)
@head mod.fm.T
@head transf(mod, Xbltrain)
transf(mod, Xbltest)

res = predict(mod, Xbltest)
res.pred 
rmsep(res.pred, ytest)

res = summary(mod, Xbltrain) ;
pnames(res) 
res.explvarx
res.corx2t 
res.cortb2t 
res.rdx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplswest.jl#LL1-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.merrp-Tuple{Any, Any}" href="#Jchemo.merrp-Tuple{Any, Any}"><code>Jchemo.merrp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">merrp(pred, y)</code></pre><p>Compute the mean intra-class classification error rate.</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data (class membership).</li></ul><p>ERRP (see function <code>errp</code>) is computed for each class. Function <code>merrp</code> returns the average of these intra-class ERRPs.   </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

mod = model(plsrda; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
merrp(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL476-L497">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.miss-Tuple{Any}" href="#Jchemo.miss-Tuple{Any}"><code>Jchemo.miss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">miss(X)</code></pre><p>Find rows with missing data in a dataset.</p><ul><li><code>X</code> : A dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 4)
zX = hcat(rand(2, 3), fill(missing, 2))
Z = vcat(X, zX)
miss(X)
miss(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL491-L504">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlev-Tuple{Any}" href="#Jchemo.mlev-Tuple{Any}"><code>Jchemo.mlev</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlev(x)</code></pre><p>Return the sorted levels of a vector or a dataset. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand([&quot;a&quot;;&quot;b&quot;;&quot;c&quot;], 20)
lev = mlev(x)
nlev = length(lev)

X = reshape(x, 5, 4)
mlev(X)

df = DataFrame(g1 = rand(1:2, n), 
    g2 = rand([&quot;a&quot;; &quot;c&quot;], n))
mlev(df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL512-L530">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlr-Tuple{Any, Any}" href="#Jchemo.mlr-Tuple{Any, Any}"><code>Jchemo.mlr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlr(X, Y; kwargs...)
mlr(X, Y, weights::Weight; kwargs...)
mlr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Compute a mutiple linear regression model (MLR) by using the QR algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>noint</code> : Boolean. Define if the model is computed    with an intercept or not.</li></ul><p>Safe but can be little slower than other methods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 2:4]
y = dat.X[:, 1]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]

mod = model(mlr)
#mod = model(mlrchol)
#mod = model(mlrpinv)
#mod = model(mlrpinvn) 
fit!(mod, Xtrain, ytrain) 
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.B
fm.int 
coef(mod) 
res = predict(mod, Xtest)
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

mod = model(mlr; noint = true)
fit!(mod, Xtrain, ytrain) 
coef(mod) </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL1-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrchol-Tuple{Any, Any}" href="#Jchemo.mlrchol-Tuple{Any, Any}"><code>Jchemo.mlrchol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrchol(X, Y)
mlrchol(X, Y, weights::Weight)
mlrchol!mlrchol!(X::Matrix, Y::Matrix, weights::Weight)</code></pre><p>Compute a mutiple linear regression model (MLR)  using the Normal equations and a Choleski factorization.</p><ul><li><code>X</code> : X-data, with nb. columns &gt;= 2 (required by function cholesky).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Compute a model with intercept.</p><p>Faster but can be less accurate (based on squared element X&#39;X).</p><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL83-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrda-Tuple{Any, Any}" href="#Jchemo.mlrda-Tuple{Any, Any}"><code>Jchemo.mlrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrda(X, y; kwargs...)
mlrda(X, y, weights::Weight)</code></pre><p>Discrimination based on multple linear regression (MLR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li></ul><p>The training variable <code>y</code> (univariate class membership) is  transformed to a dummy table (Ydummy) containing nlev columns,  where nlev is the number of classes present in <code>y</code>. Each column of  Ydummy is a dummy (0/1) variable. Then, a multiple linear regression  (MLR) is run on {<code>X</code>, Ydummy}, returning predictions of the dummy  variables (= object <code>posterior</code> returned by fuction <code>predict</code>).   These predictions can be considered as unbounded estimates (i.e.  eventuall outside of [0, 1]) of the class membership probabilities.  For a given observation, the final prediction is the class  corresponding to the dummy variable for which the probability  estimate is the highest.</p><p>In the high-level version of the function, the observation weights used in  the MLR are defined with argument <code>prior</code>. For other choices, use the  low-level version (argument <code>weights</code>).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

mod = model(mlrda)
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlrda.jl#LL1-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrpinv-Tuple{Any, Any}" href="#Jchemo.mlrpinv-Tuple{Any, Any}"><code>Jchemo.mlrpinv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrpinv(; kwargs...)
mlrpinv(X, Y; kwargs...)
mlrpinv(X, Y, weights::Weight; kwargs...)
mlrpinv!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Compute a mutiple linear regression model (MLR)  by using      a pseudo-inverse. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments:</p><ul><li><code>noint</code> : Boolean. Define if the model is computed    with an intercept or not.</li></ul><p>Safe but can be slower.  </p><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL122-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrpinvn-Tuple{Any, Any}" href="#Jchemo.mlrpinvn-Tuple{Any, Any}"><code>Jchemo.mlrpinvn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrpinvn() 
mlrpinvn(X, Y)
mlrpinvn(X, Y, weights::Weight)
mlrpinvn!mlrchol!(X::Matrix, Y::Matrix, 
    weights::Weight)</code></pre><p>Compute a mutiple linear regression model (MLR)      by using the Normal equations and a pseudo-inverse.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Safe and fast for p not too large.</p><p>Compute a model with intercept.</p><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL173-L191">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrvec-Tuple{Any, Any}" href="#Jchemo.mlrvec-Tuple{Any, Any}"><code>Jchemo.mlrvec</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrvec(; kwargs...)
mlrvec(X, Y; kwargs...)
mlrvec(X, Y, weights::Weight; kwargs...)
mlrvec!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Compute a simple linear regression model (univariate x).</p><ul><li><code>x</code> : Univariate X-data (n).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments:</p><ul><li><code>noint</code> : Boolean. Define if the model is computed    with an intercept or not.</li></ul><p>See function <code>mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL215-L230">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.model-Tuple{Function}" href="#Jchemo.model-Tuple{Function}"><code>Jchemo.model</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">model(fun::Function; kwargs...)</code></pre><p>Build a model.</p><ul><li><code>fun</code> : The function defining the the model.</li><li><code>kwargs...</code>: Keyword arguments of <code>fun</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
y = rand(5)

mod = model(detrend)  # use the default arguments of &#39;detrend&#39;
#mod = detrend(X; degree = 2)
pnames(mod)
fit!(mod, X)
Xp = transf(mod, X)

mod = model(plskern; nlv = 3) 
fit!(mod, X, y)
pred = predict(mod, X).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/_model.jl#LL7-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mpar" href="#Jchemo.mpar"><code>Jchemo.mpar</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mpar(; kwargs...)</code></pre><p>Return a tuple with all the combinations of the parameter      values defined in kwargs. Keyword arguments:</p><ul><li><code>kwargs</code> : Vector(s) of the parameter(s) values.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">nlvdis = 25 ; metric = [:mah] 
h = [1 ; 2 ; Inf] ; k = [500 ; 1000] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) 
length(pars[1])
reduce(hcat, pars)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mpar.jl#LL1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mse-Tuple{Any, Any}" href="#Jchemo.mse-Tuple{Any, Any}"><code>Jchemo.mse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mse(pred, Y; digits = 3)</code></pre><p>Summary of model performance for regression.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
mse(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
mse(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL379-L404">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.msep-Tuple{Any, Any}" href="#Jchemo.msep-Tuple{Any, Any}"><code>Jchemo.msep</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">msep(pred, Y)</code></pre><p>Compute the mean of the squared prediction errors (MSEP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
msep(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
msep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL71-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mweight-Tuple{Vector}" href="#Jchemo.mweight-Tuple{Vector}"><code>Jchemo.mweight</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mweight(x::Vector)</code></pre><p>Return an object of type <code>Weight</code> containing vector  <code>w = x / sum(x)</code> (if ad&#39;hoc building, <code>w</code> must sum to 1).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(10)
w = mweight(x)
sum(w.w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL532-L544">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mweightcla-Tuple{Vector}" href="#Jchemo.mweightcla-Tuple{Vector}"><code>Jchemo.mweightcla</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mweightcla(x::Vector; prior::Union{Symbol, Vector} = :unif)
mweightcla(Q::DataType, x::Vector; prior::Union{Symbol, Vector} = :unif)</code></pre><p>Compute observation weights for a categorical variable,      given specified sub-total weights for the classes.</p><ul><li><code>x</code> : A categorical variable (n) (class membership).</li><li><code>Q</code> : A data type (e.g. <code>Float32</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li></ul><p>Return an object of type <code>Weight</code> (see function <code>mweight</code>) containing a vector <code>w</code> (n) that sums to 1.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = vcat(rand([&quot;a&quot; ; &quot;c&quot;], 900), repeat([&quot;b&quot;], 100))
tab(x)
weights = mweightcla(x)
#weights = mweightcla(x; prior = :prop)
#weights = mweightcla(x; prior = [.1, .7, .2])
aggstat(weights.w, x; fun = sum).X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL552-L579">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nco-Tuple{Any}" href="#Jchemo.nco-Tuple{Any}"><code>Jchemo.nco</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nco(X)</code></pre><p>Return the nb. columns of <code>X</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL603-L607">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nipals-Tuple{Any}" href="#Jchemo.nipals-Tuple{Any}"><code>Jchemo.nipals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nipals(X; kwargs...)
nipals(X, UUt, VVt; kwargs...)</code></pre><p>Nipals to compute the first score and loading vectors of a matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>UUt</code> : Matrix (n, n) for Gram-Schmidt orthogonalization.</li><li><code>VVt</code> : Matrix (p, p) for Gram-Schmidt orthogonalization.</li></ul><p>Keyword arguments:</p><ul><li><code>tol</code> : Tolerance value for stopping    the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li></ul><p>The function finds:</p><ul><li>{u, v, sv} = argmin(||X - u * sv * v&#39;||)</li></ul><p>with the constraints:</p><ul><li>||u|| = ||v|| = 1</li></ul><p>using the alternating least squares algorithm to compute  SVD (Gabriel &amp; Zalir 1979).</p><p>At the end, X ~ u * sv * v&#39;, where:</p><ul><li>u : left singular vector (u * sv = scores)</li><li>v : right singular vector (loadings)</li><li>sv : singular value.</li></ul><p>When NIPALS is used on sequentially deflated matrices,  vectors u and v can loose orthogonality due to accumulation  of rounding errors. Orthogonality can be rebuilt from the  Gram-Schmidt method (arguments <code>UUt</code> and <code>VVt</code>). </p><p><strong>References</strong></p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using LinearAlgebra

X = rand(5, 3)

res = nipals(X)
res.niter
res.sv
svd(X).S[1] 
res.v
svd(X).V[:, 1] 
res.u
svd(X).U[:, 1] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/nipals.jl#LL1-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nipalsmiss-Tuple{Any}" href="#Jchemo.nipalsmiss-Tuple{Any}"><code>Jchemo.nipalsmiss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nipalsmiss(X; kwargs...)
nipalsmiss(X, UUt, VVt; kwargs...)</code></pre><p>Nipals to compute the first score and loading vectors      of a matrix with missing data.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>UUt</code> : Matrix (n, n) for Gram-Schmidt orthogonalization.</li><li><code>VVt</code> : Matrix (p, p) for Gram-Schmidt orthogonalization.</li></ul><p>Keyword arguments:</p><ul><li><code>tol</code> : Tolerance value for stopping    the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li></ul><p>See function <code>nipals</code>. </p><p><strong>References</strong></p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of  matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.</p><p>Wright, K., 2018. Package nipals: Principal Components  Analysis using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1. 2 missing 4 ; 4 missing 6 7 ; 
    missing 5 6 13 ; missing 18 7 6 ; 
    12 missing 28 7] 

res = nipalsmiss(X)
res.niter
res.sv
res.v
res.u</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/nipalsmiss.jl#LL1-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.normw-Tuple{Any, Jchemo.Weight}" href="#Jchemo.normw-Tuple{Any, Jchemo.Weight}"><code>Jchemo.normw</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">normw(x, weights::Weight)</code></pre><p>Compute the weighted norm of a vector.</p><ul><li><code>x</code> : A vector (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>The weighted norm of vector <code>x</code> is computed by:</p><ul><li>sqrt(x&#39; * D * x), where D is the diagonal matrix of vector <code>weights.w</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL609-L619">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nro-Tuple{Any}" href="#Jchemo.nro-Tuple{Any}"><code>Jchemo.nro</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nro(X)</code></pre><p>Return the nb. rows of <code>X</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL621-L625">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occod-Tuple{Any, Any}" href="#Jchemo.occod-Tuple{Any, Any}"><code>Jchemo.occod</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occod(fm, X; kwargs...)</code></pre><p>One-class classification using PCA/PLS orthognal distance (OD).</p><ul><li><code>fm</code> : The preliminary model that (e.g. PCA) was fitted    (object <code>fm</code>) on the training data assumed to represent    the training class.</li><li><code>X</code> : Training X-data (n, p), on which was fitted    the model <code>fm</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>mcut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>.    See Thereafter.</li><li><code>cri</code> : When <code>mcut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>mcut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of an observation is the orthogonal distance (OD =  &quot;X-residuals&quot;) of this  observation, ie. the Euclidean distance between the observation  and its projection on the  score plan defined by the fitted  (e.g. PCA) model (e.g. Hubert et al. 2005, Van Branden &amp; Hubert  2005 p. 66, Varmuza &amp; Filzmoser 2009 p. 79).</p><p>See function <code>occsd</code> for details on outputs.</p><p><strong>References</strong></p><p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005).  ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p>K. Vanden Branden, M. Hubert (2005). Robuts classification  in high dimension based on the SIMCA method. Chem. Lab. Int.  Syst, 79, 10-21.</p><p>K. Varmuza, P. Filzmoser (2009). Introduction to multivariate  statistical analysis in chemometrics. CRC Press, Boca Raton.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X    
Y = dat.Y
mod = model(savgol; npoint = 21, deriv = 2, degree = 3)
fit!(mod, X) 
Xp = transf(mod, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Below, the reference class is &quot;EEH&quot;
cla1 = &quot;EHH&quot; ; cla2 = &quot;PEE&quot; ; cod = &quot;out&quot;   # here cla2 should be detected
#cla1 = &quot;EHH&quot; ; cla2 = &quot;EHH&quot; ; cod = &quot;in&quot;   # here cla2 should not be detected
s1 = Ytrain.typ .== cla1
s2 = Ytest.typ .== cla2
zXtrain = Xtrain[s1, :]    
zXtest = Xtest[s2, :] 
ntrain = nro(zXtrain)
ntest = nro(zXtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
ytrain = repeat([&quot;in&quot;], ntrain)
ytest = repeat([cod], ntest)

## Group description
mod0 = model(pcasvd; nlv = 10) 
fit!(mod, zXtrain) 
Ttrain = mod0.fm.T
Ttest = transf(mod0, zXtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat([&quot;1&quot;], ntrain), repeat([&quot;2&quot;], ntest))
i = 1
plotxy(T[:, i], T[:, i + 1], group; leg_title = &quot;Class&quot;, 
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Preliminary PCA fitted model
mod0 = model(pcasvd; nlv = 10) 
fit!(mod0, zXtrain)
fm0 = mod0.fm ;  
## Outlierness
mod = model(occod)
#mod = model(occod; mcut = :mad, cri = 4)
#mod = model(occod; mcut = :q, risk = .01) ;
#mod = model(occsdod)
fit!(mod, fm0, zXtrain) 
pnames(mod) 
pnames(mod.fm) 
@head d = mod.fm.d
d = d.dstand
f, ax = plotxy(1:length(d), d; size = (500, 300), 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f

res = predict(mod, zXtest) ;
pnames(res)
@head res.d
@head res.pred
tab(res.pred)
errp(res.pred, ytest)
conf(res.pred, ytest).cnt
d1 = mod.fm.d.dstand
d2 = res.d.dstand
d = vcat(d1, d2)
f, ax = plotxy(1:length(d), d, group; size = (500, 300), leg_title = &quot;Class&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occod.jl#LL1-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occsd-Tuple{Any}" href="#Jchemo.occsd-Tuple{Any}"><code>Jchemo.occsd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occsd(fm; kwargs...)</code></pre><p>One-class classification using PCA/PLS score distance (SD).</p><ul><li><code>fm</code> : The preliminary model that (e.g. PCA) was fitted    (object <code>fm</code>) on the training data assumed to represent    the training class.</li></ul><p>Keyword arguments:</p><ul><li><code>mcut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>.    See Thereafter.</li><li><code>cri</code> : When <code>mcut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>mcut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of an observation is  defined by its score distance (SD), ie. the Mahalanobis distance  between the projection of the observation on the score plan  defined by the fitted (e.g. PCA) model and the center of the  score plan.</p><p>If a new observation has <code>d</code> higher than a given <code>cutoff</code>, the  observation is assumed to not belong to the training (= reference)  class. The <code>cutoff</code> is computed with non-parametric heuristics.  Noting [d] the vector of outliernesses computed on the training class:</p><ul><li>If <code>mcut</code> = <code>:mad</code>, then <code>cutoff</code> = median([d]) + <code>cri</code> * mad([d]). </li><li>If <code>mcut</code> = <code>:q</code>, then <code>cutoff</code> is estimated from the empirical    cumulative density function computed on [d], for a given    risk-I (<code>risk</code>). </li></ul><p>Alternative approximate cutoffs have been proposed in the  literature (e.g.: Nomikos &amp; MacGregor 1995, Hubert et al. 2005, Pomerantsev 2008). Typically, and whatever the approximation method  used to compute the cutoff, it is recommended to tune this cutoff  depending on the detection objectives. </p><p><strong>Outputs</strong></p><ul><li><code>pval</code>: Estimate of p-value (see functions <code>pval</code>) computed    from the training distribution [d]. </li><li><code>dstand</code>: standardized distance defined as <code>d</code> / <code>cutoff</code>.    A value <code>dstand</code> &gt; 1 may be considered as extreme compared to    the distribution of the training data.  </li><li><code>gh</code> is the Winisi &quot;GH&quot; (usually, GH &gt; 3 is considered as    extreme).</li></ul><p>Specific for function <code>predict</code>:</p><ul><li><code>pred</code>: class prediction<ul><li><code>dstand</code> &lt;= 1 ==&gt; <code>in</code>: the observation is expected to    belong to the training class, </li><li><code>dstand</code> &gt; 1  ==&gt; <code>out</code>: extreme value, possibly not    belonging to the same class as the training. </li></ul></li></ul><p><strong>References</strong></p><p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005).  ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.</p><p>Nomikos, P., MacGregor, J.F., 1995. Multivariate SPC Charts for  Monitoring Batch Processes. null 37, 41-59.  https://doi.org/10.1080/00401706.1995.10485888</p><p>Pomerantsev, A.L., 2008. Acceptance areas for multivariate  classification derived by projection methods. Journal of Chemometrics  22, 601-609. https://doi.org/10.1002/cem.1147</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X    
Y = dat.Y
mod = model(savgol; npoint = 21, deriv = 2, degree = 3)
fit!(mod, X) 
Xp = transf(mod, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Below, the reference class is &quot;EEH&quot;
cla1 = &quot;EHH&quot; ; cla2 = &quot;PEE&quot; ; cod = &quot;out&quot;   # here cla2 should be detected
#cla1 = &quot;EHH&quot; ; cla2 = &quot;EHH&quot; ; cod = &quot;in&quot;   # here cla2 should not be detected
s1 = Ytrain.typ .== cla1
s2 = Ytest.typ .== cla2
zXtrain = Xtrain[s1, :]    
zXtest = Xtest[s2, :] 
ntrain = nro(zXtrain)
ntest = nro(zXtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
ytrain = repeat([&quot;in&quot;], ntrain)
ytest = repeat([cod], ntest)

## Group description
mod = model(pcasvd; nlv = 10) 
fit!(mod, zXtrain) 
Ttrain = mod.fm.T
Ttest = transf(mod, zXtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat([&quot;1&quot;], ntrain), repeat([&quot;2&quot;], ntest))
i = 1
plotxy(T[:, i], T[:, i + 1], group; leg_title = &quot;Class&quot;, 
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Preliminary PCA fitted model
mod0 = model(pcasvd; nlv = 30) 
fit!(mod0, zXtrain)
fm0 = mod0.fm ;  
## Outlierness
mod = model(occsd)
#mod = model(occsd; mcut = :mad, cri = 4)
#mod = model(occsd; mcut = :q, risk = .01)
fit!(mod, fm0) 
pnames(mod) 
pnames(mod.fm) 
@head d = mod.fm.d
d = d.dstand
f, ax = plotxy(1:length(d), d; size = (500, 300), 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f

res = predict(mod, zXtest) ;
pnames(res)
@head res.d
@head res.pred
tab(res.pred)
errp(res.pred, ytest)
conf(res.pred, ytest).cnt
d1 = mod.fm.d.dstand
d2 = res.d.dstand
d = vcat(d1, d2)
f, ax = plotxy(1:length(d), d, group; size = (500, 300), leg_title = &quot;Class&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occsd.jl#LL1-L138">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occsdod-Tuple{Any, Any}" href="#Jchemo.occsdod-Tuple{Any, Any}"><code>Jchemo.occsdod</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occsdod(object, X; kwargs...)</code></pre><p>One-class classification using a compromise between      PCA/PLS score (SD) and orthogonal (OD) distances.</p><ul><li><code>fm</code> : The preliminary model that (e.g. PCA) was fitted    (object <code>fm</code>) on the training data assumed to represent    the training class.</li><li><code>X</code> : Training X-data (n, p), on which was fitted    the model <code>fm</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>mcut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>.    See Thereafter.</li><li><code>cri</code> : When <code>mcut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>mcut</code> = <code>:q</code>, a risk-I level. See thereafter.</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is a compromise between the score distance (SD) and the orthogonal distance (OD). The compromise is computed from the  standardized distances by: </p><ul><li><code>dstand</code> = sqrt(<code>dstand_sd</code> * <code>dstand_od</code>).</li></ul><p>See functions:</p><ul><li><code>occsd</code> for details of the outputs,</li><li>and <code>occod</code> for examples.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occsdod.jl#LL1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occstah-Tuple{Any}" href="#Jchemo.occstah-Tuple{Any}"><code>Jchemo.occstah</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occstah(X; kwargs...)</code></pre><p>One-class classification using the Stahel-Donoho outlierness.</p><ul><li><code>X</code> : Training X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. dimensions on which <code>X</code> is projected. </li><li><code>mcut</code> : Type of cutoff. Possible values are: <code>:mad</code>, <code>:q</code>.    See Thereafter.</li><li><code>cri</code> : When <code>mcut</code> = <code>:mad</code>, a constant. See thereafter.</li><li><code>risk</code> : When <code>mcut</code> = <code>:q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled such as in function <code>outstah</code>.</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is the Stahel-Donoho outlierness (see <code>?outstah</code>).</p><p>See function <code>occsd</code> for details on outputs.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X    
Y = dat.Y
mod = model(savgol; npoint = 21, deriv = 2, degree = 3)
fit!(mod, X) 
Xp = transf(mod, X) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

## Below, the reference class is &quot;EEH&quot;
cla1 = &quot;EHH&quot; ; cla2 = &quot;PEE&quot; ; cod = &quot;out&quot;   # here cla2 should be detected
#cla1 = &quot;EHH&quot; ; cla2 = &quot;EHH&quot; ; cod = &quot;in&quot;   # here cla2 should not be detected
s1 = Ytrain.typ .== cla1
s2 = Ytest.typ .== cla2
zXtrain = Xtrain[s1, :]    
zXtest = Xtest[s2, :] 
ntrain = nro(zXtrain)
ntest = nro(zXtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
ytrain = repeat([&quot;in&quot;], ntrain)
ytest = repeat([cod], ntest)

## Group description
mod = model(pcasvd; nlv = 10) 
fit!(mod, zXtrain) 
Ttrain = mod.fm.T
Ttest = transf(mod, zXtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat([&quot;1&quot;], ntrain), repeat([&quot;2&quot;], ntest))
i = 1
plotxy(T[:, i], T[:, i + 1], group; leg_title = &quot;Class&quot;, 
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f

#### Occ
## Preliminary dimension 
## Not required but often more 
## efficient
nlv = 50
mod0 = model(pcasvd; nlv) ;
fit!(mod0, zXtrain)
Ttrain = mod0.fm.T
Ttest = transf(mod0, zXtest)
## Outlierness
mod = model(occstah; nlv, scal = true)
fit!(mod, Ttrain) 
pnames(mod) 
pnames(mod.fm) 
@head d = mod.fm.d
d = d.dstand
f, ax = plotxy(1:length(d), d; size = (500, 300), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f

res = predict(mod, Ttest) ;
pnames(res)
@head res.d
@head res.pred
tab(res.pred)
errp(res.pred, ytest)
conf(res.pred, ytest).cnt
d1 = mod.fm.d.dstand
d2 = res.d.dstand
d = vcat(d1, d2)
f, ax = plotxy(1:length(d), d, group; size = (500, 300), leg_title = &quot;Class&quot;, 
    xlabel = &quot;Obs. index&quot;, ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1; linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occstah.jl#LL1-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.out-Tuple{Any, Any}" href="#Jchemo.out-Tuple{Any, Any}"><code>Jchemo.out</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">out(x)</code></pre><p>Return if elements of a vector are strictly outside of a given range.</p><ul><li><code>x</code> : Univariate data.</li><li><code>y</code> : Univariate data on which is computed the range (min, max).</li></ul><p>Return a BitVector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [-200.; -100; -1; 0; 1; 200]
out(x, [-1; .2; 1])
out(x, (-1, 1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL627-L642">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.outeucl-Tuple{Any}" href="#Jchemo.outeucl-Tuple{Any}"><code>Jchemo.outeucl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">outeucl(X, P; kwargs...)
outeucl!(X::Matrix, P::Matrix; kwargs...)</code></pre><p>Compute an outlierness from Euclidean distances to center.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by MAD    before computing the outlierness.</li></ul><p>Outlyingness is calculated by the Euclidean distance between  the observation (rows of <code>X</code>) and a robust estimate of the center of the data  (in the present function, the spatial median). Such outlyingness was for  instance used in the robust PLSR algorithm of Serneels et al. 2005 (PRM). </p><p><strong>References</strong></p><p>Serneels, S., Croux, C., Filzmoser, P., Van Espen, P.J., 2005.  Partial robust M-regression.  Chemometrics and Intelligent Laboratory Systems 79, 55-64.  https://doi.org/10.1016/j.chemolab.2005.04.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 300 ; p = 700 ; m = 80
ntot = n + m
X1 = randn(n, p)
X2 = randn(m, p) .+ rand(1:3, p)&#39;
X = vcat(X1, X2)

nlv = 10
scal = false
#scal = true
res = outeucl(X; scal) ;
pnames(res)
res.d    # outlierness 
plotxy(1:ntot, res.dstand).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/outeucl.jl#LL1-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.outstah-Tuple{Any, Any}" href="#Jchemo.outstah-Tuple{Any, Any}"><code>Jchemo.outstah</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">outstah(X, P; kwargs...)
outstah!(X::Matrix, P::Matrix; kwargs...)</code></pre><p>Compute the Stahel-Donoho outlierness.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>P</code> : A projection matrix (p, nlv) representing the directions    of the projection pursuit.</li></ul><p>Keyword arguments:</p><ul><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled by MAD    before computing the outlierness.</li></ul><p>See Maronna and Yohai 1995 for details on the outlierness  measure. </p><p>A projection-pursuit approach is used: given a projection matrix <code>P</code> (p, nlv)  (in general built randomly), the observations (rows of <code>X</code>) are projected on  the <code>nlv</code> directions and the Stahel-Donoho outlierness is computed for each observation  from these projections.</p><p><strong>References</strong></p><p>Maronna, R.A., Yohai, V.J., 1995. The Behavior of the  Stahel-Donoho Robust Multivariate Estimator. Journal of the  American Statistical Association 90, 330–341.  https://doi.org/10.1080/01621459.1995.10476517</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 300 ; p = 700 ; m = 80
ntot = n + m
X1 = randn(n, p)
X2 = randn(m, p) .+ rand(1:3, p)&#39;
X = vcat(X1, X2)

nlv = 10
P = rand(0:1, p, nlv)
scal = false
#scal = true
res = outstah(X, P; scal) ;
pnames(res)
res.d    # outlierness 
plotxy(1:ntot, res.dstand).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/outstah.jl#LL1-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcaeigen-Tuple{Any}" href="#Jchemo.pcaeigen-Tuple{Any}"><code>Jchemo.pcaeigen</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcaeigen(X; kwargs...)
pcaeigen(X, weights::Weight; kwargs...)
pcaeigen!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by Eigen factorization.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D, by  computing an Eigen factorization of X&#39; * D * X. </p><p>See function <code>pcasvd</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcaeigen.jl#LL1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcaeigenk-Tuple{Any}" href="#Jchemo.pcaeigenk-Tuple{Any}"><code>Jchemo.pcaeigenk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcaeigenk(X; kwargs...)
pcaeigenk(X, weights::Weight; kwargs...)
pcaeigenk!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by Eigen factorization of the kernel matrix XX&#39;.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>This is the &quot;kernel cross-product&quot; version of the PCA  algorithm (e.g. Wu et al. 1997). For wide matrices (n &lt;&lt; p,  where p is the nb. columns) and n not too large, this algorithm  can be much faster than the others.</p><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D, by  computing an Eigen factorization of D^(1/2) * X * X&#39; D^(1/2).</p><p>See function <code>pcasvd</code> for examples.</p><p><strong>References</strong></p><p>Wu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA  algorithms for wide data. Part I: Theory and algorithms.  Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcaeigenk.jl#LL1-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcanipals-Tuple{Any}" href="#Jchemo.pcanipals-Tuple{Any}"><code>Jchemo.pcanipals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcanipals(X; kwargs...)
pcanipals(X, weights::Weight; kwargs...)
pcanipals!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by NIPALS algorithm.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt    orthogonalization of the scores and loadings is done   before each X-deflation. </li><li><code>tol</code> : Tolerance value for stopping    the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D  by NIPALS. </p><p>See function <code>pcasvd</code> for examples.</p><p><strong>References</strong></p><p>Andrecut, M., 2009. Parallel GPU Implementation of Iterative  PCA Algorithms. Journal of Computational Biology 16, 1593-1599.  https://doi.org/10.1089/cmb.2008.0221</p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices  by least squares with any choice of weights, Technometrics 21 (1979) 489–498.</p><p>Gabriel, R. K., 2002. Le biplot - Outil d&#39;exploration de données  multidimensionnelles. Journal de la Société Française de la Statistique,  143, 5-55.</p><p>Lingen, F.J., 2000. Efficient Gram-Schmidt orthonormalisation  on parallel computers. Communications in Numerical Methods in  Engineering 16, 57-66.  https://doi.org/10.1002/(SICI)1099-0887(200001)16:1&lt;57::AID-CNM320&gt;3.0.CO;2-I</p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris, France.</p><p>Wright, K., 2018. Package nipals: Principal Components Analysis  using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcanipals.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcanipalsmiss-Tuple{Any}" href="#Jchemo.pcanipalsmiss-Tuple{Any}"><code>Jchemo.pcanipalsmiss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcanipals(X; kwargs...)
pcanipals(X, weights::Weight; kwargs...)
pcanipals!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by NIPALS algorithm allowing missing data.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt    orthogonalization of the scores and loadings is done   before each X-deflation. </li><li><code>tol</code> : Tolerance value for stopping    the iterations.</li><li><code>maxit</code> : Maximum nb. of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>References</strong></p><p>Wright, K., 2018. Package nipals: Principal Components Analysis  using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1 2. missing 4 ; 4 missing 6 7 ; 
    missing 5 6 13 ; missing 18 7 6 ; 
    12 missing 28 7] 

nlv = 3 
tol = 1e-15
scal = false
#scal = true
gs = false
#gs = true
mod = model(pcanipalsmiss; nlv, tol, gs, maxit = 500, scal)
fit!(mod, X)
pnames(mod) 
pnames(mod.fm)
fm = mod.fm ;
fm.niter
fm.sv
fm.P
fm.T
## Orthogonality 
## only if gs = true
fm.T&#39; * fm.T
fm.P&#39; * fm.P

## Impute missing data in X
mod = model(pcanipalsmiss; nlv = 2, gs = true) ;
fit!(mod, X)
Xfit = xfit(mod.fm)
s = ismissing.(X)
X_imput = copy(X)
X_imput[s] .= Xfit[s]
X_imput</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcanipalsmiss.jl#LL1-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcasph-Tuple{Any}" href="#Jchemo.pcasph-Tuple{Any}"><code>Jchemo.pcasph</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcasph(X; kwargs...)
pcasph(X, weights::Weight; kwargs...)
pcasph!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Spherical PCA.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Spherical PCA (Locantore et al. 1990, Maronna 2005, Daszykowski et al. 2007).  Matrix <code>X</code> is centered by the spatial median computed by function <code>Jchemo.colmedspa</code>.</p><p><strong>References</strong></p><p>Daszykowski, M., Kaczmarek, K., Vander Heyden, Y., Walczak, B., 2007.  Robust statistics in data analysis - A review. Chemometrics and Intelligent  Laboratory Systems 85, 203-219. https://doi.org/10.1016/j.chemolab.2006.06.016</p><p>Locantore N., Marron J.S., Simpson D.G., Tripoli N., Zhang J.T., Cohen K.L. Robust principal component analysis for functional data, Test 8 (1999) 1–7</p><p>Maronna, R., 2005. Principal components and orthogonal regression based on  robust scales, Technometrics, 47:3, 264-273, DOI: 10.1198/004017005000000166</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie 
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;octane.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
wlst = names(X)
wl = parse.(Float64, wlst)
n = nro(X)

nlv = 6
mod = model(pcasph; nlv)  
#mod = model(pcasvd; nlv) 
fit!(mod, X)
pnames(mod)
pnames(mod.fm)
@head T = mod.fm.T
## Same as:
transf(mod, X)

i = 1
plotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = &quot;PC1&quot;, 
    ylabel = &quot;PC2&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcasph.jl#LL1-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcasvd-Tuple{Any}" href="#Jchemo.pcasvd-Tuple{Any}"><code>Jchemo.pcasvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcasvd(X; kwargs...)
pcasvd(X, weights::Weight; kwargs...)
pcasvd!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>PCA by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. of principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of weights (<code>weights.w</code>) and X the centered matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D, by  computing a SVD factorization of sqrt(D) * X:</p><ul><li>sqrt(D) * X ~ U * S * V&#39;</li></ul><p>Outputs are:</p><ul><li><code>T</code> = D^(-1/2) * U * S</li><li><code>P</code> = V</li><li>The diagonal of S   </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
@head Xtrain = X[s.train, :]
@head Xtest = X[s.test, :]

nlv = 3
mod = model(pcasvd; nlv)
#mod = model(pcaeigen; nlv)
#mod = model(pcaeigenk; nlv)
#mod = model(pcanipals; nlv)
fit!(mod, Xtrain)
pnames(mod)
pnames(mod.fm)
@head T = mod.fm.T
## Same as:
@head transf(mod, X)
T&#39; * T
@head P = mod.fm.P
P&#39; * P

@head Ttest = transf(mod, Xtest)

res = summary(mod, Xtrain) ;
pnames(res)
res.explvarx
res.contr_var
res.coord_var
res.cor_circle</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcasvd.jl#LL1-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcr-Tuple{Any, Any}" href="#Jchemo.pcr-Tuple{Any, Any}"><code>Jchemo.pcr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcr(X, Y; kwargs...)
pcr(X, Y, weights::Weight; kwargs...)
pcr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Principal component regression (PCR) with a SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
mod = model(pcr; nlv) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T

coef(mod)
coef(mod; nlv = 3)

@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

res = predict(mod, Xtest; nlv = 1:2)
@head res.pred[1]
@head res.pred[2]

res = summary(mod, Xtrain) ;
pnames(res)
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, 
    ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcr.jl#LL1-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pip-Tuple" href="#Jchemo.pip-Tuple"><code>Jchemo.pip</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pip(args...)</code></pre><p>Build a pipeline of models.</p><ul><li><code>args...</code> : Succesive models, see examples.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie, JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Pipeline Snv :&gt; Savgol :&gt; Pls :&gt; Svmr

mod1 = model(snv; centr = true, scal = true)
npoint = 11 ; deriv = 2 ; degree = 3
mod2 = model(savgol; npoint, deriv, degree)
mod3 = model(plskern; nlv = 15)
mod4 = model(svmr; gamma = 1e3, cost = 100, epsilon = .9)
mod = pip(mod1, mod2, mod3, mod4)
fit!(mod, Xtrain, ytrain)
res = predict(mod, Xtest) ; 
@head res.pred 
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;,
      ylabel = &quot;Observed&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/_pip.jl#LL6-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plist-Tuple{Any}" href="#Jchemo.plist-Tuple{Any}"><code>Jchemo.plist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plist(x)</code></pre><p>Print each element of a list.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL644-L648">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotconf-Tuple{Any}" href="#Jchemo.plotconf-Tuple{Any}"><code>Jchemo.plotconf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plotconf(object; size = (500, 400), cnt = true, ptext = true, 
    fontsize = 15, coldiag = :red, )</code></pre><p>Plot a conf matrix.</p><ul><li><code>object</code> : Output of function <code>conf</code>.</li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>cnt</code> : Boolean. If <code>true</code>, plot the occurrences,    else plot the row %s.</li><li><code>ptext</code> : Boolean. If <code>true</code>, display the value in each cell.</li><li><code>fontsize</code> : Font size when <code>ptext = true</code>.</li><li><code>coldiag</code> : Font color when <code>ptext = true</code>.</li></ul><p>See examples in help page of function <code>conf</code>. ```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plotconf.jl#LL1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotgrid-Tuple{AbstractVector, Any}" href="#Jchemo.plotgrid-Tuple{AbstractVector, Any}"><code>Jchemo.plotgrid</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plotgrid(indx::AbstractVector, r; 
    size = (500, 300), step = 5, color = nothing, 
    kwargs...)
plotgrid(indx::AbstractVector, r, group; 
    size = (700, 350), step = 5, color = nothing, 
    leg = true, leg_title = &quot;Group&quot;, kwargs...)</code></pre><p>Plot error/performance rates of a model.</p><ul><li><code>indx</code> : A numeric variable representing the grid of    model parameters, e.g. the nb. LVs if PLSR models.</li><li><code>r</code> : The error/performance rate.</li></ul><p>Keyword arguments: </p><ul><li><code>group</code> : Categorical variable defining groups.    A separate line is plotted for each level of <code>group</code>.</li><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>step</code> : Step used for defining the xticks.</li><li><code>color</code> : Set color. If <code>group</code> if used, must be a vector    of same length as the number of levels in <code>group</code>.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>leg_title</code> : Title of the legend.</li><li><code>kwargs</code> : Optional arguments to pass in <code>Axis</code> of CairoMakie.</li></ul><p>To use <code>plotgrid</code>, a backend (e.g. CairoMakie) has to  be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

mod = plskern() 
nlv = 0:20
res = gridscore(mod, Xtrain, ytrain, 
    Xtest, ytest; score = rmsep, nlv)
plotgrid(res.nlv, res.y1;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

mod = lwplsr() 
nlvdis = 15 ; metric = [:mah]
h = [1 ; 2.5 ; 5] ; k = [50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, 
    h = h, k = k)
nlv = 0:20
res = gridscore(mod, Xtrain, ytrain, 
    Xtest, ytest; score = rmsep, 
    pars, nlv)
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSECV&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plotgrid.jl#LL1-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotsp" href="#Jchemo.plotsp"><code>Jchemo.plotsp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">plotsp(X, wl = 1:nco(X); size = (500, 300), color = nothing, 
    nsamp = nothing, kwargs...)</code></pre><p>Plotting spectra.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>wl</code> : Column names of <code>X</code>. Must be numeric.</li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>color</code> : Set a unique color (and eventually transparency)    to the spectra.</li><li><code>nsamp</code> : Nb. spectra (X-rows) to plot. If <code>nothing</code>,    all spectra are plotted.</li><li><code>kwargs</code> : Optional arguments to pass in <code>Axis</code> of CairoMakie.</li></ul><p>The function plots the rows of <code>X</code>.</p><p>To use <code>plotxy</code>, a backend (e.g. CairoMakie) has  to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
wlst = names(X)
wl = parse.(Float64, wlst) 

plotsp(X).f
plotsp(X; color = (:red, .2)).f
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f

f, ax = plotsp(X, wl; color = (:red, .2))
xmeans = colmean(X)
lines!(ax, wl, xmeans; color = :black, linewidth = 2)
vlines!(ax, 1200)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plotsp.jl#LL1-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotxy-Tuple{Any, Any}" href="#Jchemo.plotxy-Tuple{Any, Any}"><code>Jchemo.plotxy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plotxy(x, y; size = (500, 300), color = nothing, ellipse::Bool = false, 
    prob = .95, circle::Bool = false, bisect::Bool = false, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, kwargs...)
plotxy(x, y, group; size = (600, 350), color = nothing, ellipse::Bool = false, 
    prob = .95, circle::Bool = false, bisect::Bool = false, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, leg::Bool = true, leg_title = &quot;Group&quot;, 
    kwargs...)</code></pre><p>Scatter plot of (x, y) data</p><ul><li><code>x</code> : A x-vector (n).</li><li><code>y</code> : A y-vector (n). </li><li><code>group</code> : Categorical variable defining groups (n). </li></ul><p>Keyword arguments:</p><ul><li><code>size</code> : Size (horizontal, vertical) of the figure.</li><li><code>color</code> : Set color(s). If <code>group</code> if used, <code>color</code> must be    a vector of same length as the number of levels in <code>group</code>.</li><li><code>ellipse</code> : Boolean. Draw an ellipse of confidence,    assuming a Ch-square distribution with df = 2. If <code>group</code>    is used, one ellipse is drawn per group.</li><li><code>prob</code> : Probability for the ellipse of confidence.</li><li><code>bisect</code> : Boolean. Draw a bisector.</li><li><code>zeros</code> : Boolean. Draw horizontal and vertical axes passing    through origin (0, 0).</li><li><code>xlabel</code> : Label for the x-axis.</li><li><code>ylabel</code> : Label for the y-axis.</li><li><code>title</code> : Title of the graphic.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend    or not.</li><li><code>leg_title</code> : Title of the legend.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>scatter</code>    of Makie.</li></ul><p>To use <code>plotxy</code>, a backend (e.g. CairoMakie) has  to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
lev = mlev(year)
nlev = length(lev)

mod = model(pcasvd; nlv = 5)  
fit!(mod, X) 
@head T = mod.fm.T

plotxy(T[:, 1], T[:, 2]; color = (:red, .5)).f

plotxy(T[:, 1], T[:, 2], year; ellipse = true, xlabel = &quot;PC1&quot;, 
    ylabel = &quot;PC2&quot;).f

i = 2
colm = cgrad(:Dark2_5, nlev; categorical = true)
plotxy(T[:, i], T[:, i + 1], year; color = colm, xlabel = string(&quot;PC&quot;, i), 
    ylabel = string(&quot;PC&quot;, i + 1), zeros = true, ellipse = true).f

plotxy(T[:, 1], T[:, 2], year).lev

plotxy(1:5, 1:5).f

y = reshape(rand(5), 5, 1)
plotxy(1:5, y).f

## Several layers can be added
## (same syntax as in Makie)
A = rand(50, 2)
f, ax = plotxy(A[:, 1], A[:, 2]; xlabel = &quot;x1&quot;, ylabel = &quot;x2&quot;)
ylims!(ax, -1, 2)
hlines!(ax, 0.5; color = :red, linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plotxy.jl#LL1-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plscan-Tuple{Any, Any}" href="#Jchemo.plscan-Tuple{Any, Any}"><code>Jchemo.plscan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plscan(X, Y; kwargs...)
plscan(X, Y, weights::Weight; kwargs...)
plscan!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Canonical partial least squares regression (Canonical PLS).</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>Canonical PLS with the Nipals algorithm (Wold 1984,  Tenenhaus 1998 chap.11), referred to as PLS-W2A (i.e. Wold  PLS mode A) in Wegelin 2000. The two blocks <code>X</code> and <code>X</code>  play a symmetric role.  After each step of scores computation,  X and Y are deflated by the x- and y-scores, respectively. </p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie  et pratique. Editions Technip, Paris.</p><p>Wegelin, J.A., 2000. A Survey of Partial Least  Squares (PLS) Methods, with Emphasis on the Two-Block  Case (No. 371). University of Washington, Seattle,  Washington, USA.</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984.  The Collinearity Problem in Linear Regression. The Partial  Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5,  735–743. https://doi.org/10.1137/0905052</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 2
bscal = :frob
mod = model(plscan; nlv, bscal)
fit!(mod, X, Y)
pnames(mod)
pnames(mod.fm)

@head mod.fm.Tx
@head transfbl(mod, X, Y).Tx

@head mod.fm.Ty
@head transfbl(mod, X, Y).Ty

res = summary(mod, X, Y) ;
pnames(res)
res.explvarx
res.explvary
res.cort2t 
res.rdx
res.rdy
res.corx2t 
res.cory2t </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plscan.jl#LL1-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plskdeda-Tuple{Any, Any}" href="#Jchemo.plskdeda-Tuple{Any, Any}"><code>Jchemo.plskdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plskdeda(X, y; kwargs...)
plskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>KDE-DA on PLS latent variables (PLS-KDEDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The principle is the same as functions <code>plslda</code> and  <code>plsqda</code> except that class densities are estimated from <code>dmkern</code>  instead of <code>dmnorm</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
mod = model(plskdeda; nlv) 
#mod = model(plskdeda; nlv, a_kde = .5)
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

fmpls = fm.fm.fmpls ;
@head fmpls.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fmpls)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred
summary(fmpls, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plskdeda.jl#LL1-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plskern-Tuple{Any, Any}" href="#Jchemo.plskern-Tuple{Any, Any}"><code>Jchemo.plskern</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plskern(X, Y; kwargs...)
plskern(X, Y, weights::Weight; kwargs...)
plskern!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial least squares regression (PLSR) with the &quot;improved kernel algorithm #1&quot;      (Dayal &amp; McGegor, 1997).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>About the row-weighting in PLS algorithms (<code>weights</code>): See in particular Schaal et al. 2002, Siccard &amp; Sabatier 2006,  Kim et al. 2011, and Lesnoff et al. 2020. </p><p><strong>References</strong></p><p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.</p><p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation  of active pharmaceutical ingredients content using locally  weighted partial least squares and statistical wavelength  selection. Int. J. Pharm., 421, 269-274.</p><p>Lesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally  weighted PLS strategies for regression and discrimination on  agronomic NIR Data. Journal of Chemometrics. e3209.  https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209</p><p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable  techniques from nonparametric statistics for the real time  robot learning. Applied Intell., 17, 49-60.</p><p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local  PLS1 regression and application to a rainfall data set. Comput. Stat.  Data Anal., 51, 1393-1410.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
mod = model(plskern; nlv) ;
#mod = model(plsnipals; nlv) ;
#mod = model(plswold; nlv) ;
#mod = model(plsrosa; nlv) ;
#mod = model(plssimp; nlv) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T

coef(mod)
coef(mod; nlv = 3)

@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

res = predict(mod, Xtest; nlv = 1:2)
@head res.pred[1]
@head res.pred[2]

res = summary(mod, Xtrain) ;
pnames(res)
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, 
    ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plskern.jl#LL1-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plslda-Tuple{Any, Any}" href="#Jchemo.plslda-Tuple{Any, Any}"><code>Jchemo.plslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plslda(X, y; kwargs...)
plslda(X, y, weights::Weight; kwargs...)</code></pre><p>LDA on PLS latent variables (PLS-LDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>LDA on PLS latent variables. The training variable <code>y</code>  (univariate class membership) is transformed to a dummy table  (Ydummy) containing nlev columns, where nlev is the number of  classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1)  variable. Then, a weighted PLSR2 (i.e. multivariate) is run on  {<code>X</code>, Ydummy}, returning a score matrix <code>T</code>. Finally, a LDA  is done on {<code>T</code>, <code>y</code>}. </p><p>In these <code>plslda</code> functions, observation weights (argument <code>weights</code>) are used  to compute the PLS scores and the LDA intra-class (= &quot;within&quot;) covariance matrix.  Argument <code>prior</code> is used to define the usual LDA prior class probabilities. </p><p>In the high-level version, the observation weights are automatically  defined by the given priors: the sub-total weights by class are set  equal to the prior probabilities. For other choices, use the low-level  version.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
mod = model(plslda; nlv) 
#mod = model(plslda; nlv, prior = :prop) 
#mod = model(plsqda; nlv, alpha = .1) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

fmpls = fm.fm.fmpls ;
@head fmpls.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fmpls)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred
summary(fmpls, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plslda.jl#LL1-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsnipals-Tuple{Any, Any}" href="#Jchemo.plsnipals-Tuple{Any, Any}"><code>Jchemo.plsnipals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsnipals(X, Y; kwargs...)
plsnipals(X, Y, weights::Weight; kwargs...)
plsnipals!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the Nipals algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>In this function, for PLS2 (multivariate Y), the Nipals  iterations are replaced by a direct computation of the  PLS weights (w) by SVD decomposition of matrix X&#39;Y  (Hoskuldsson 1988 p.213).</p><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>Hoskuldsson, A., 1988. PLS regression methods. Journal of  Chemometrics 2, 211-228.https://doi.org/10.1002/cem.1180020306</p><p>Tenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique.  Editions Technip, Paris, France.</p><p>Wold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression:  a basic tool for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsnipals.jl#LL1-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsqda-Tuple{Any, Any}" href="#Jchemo.plsqda-Tuple{Any, Any}"><code>Jchemo.plsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsqda(X, y; kwargs...)
plsqda(X, y, weights::Weight; kwargs...)</code></pre><p>QDA on PLS latent variables (PLS-QDA) with continuum.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>QDA on PLS latent variables. The training variable <code>y</code>  (univariate class membership) is transformed to a dummy table  (Ydummy) containing nlev columns, where nlev is the number of  classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1)  variable. Then, a PLSR2 (i.e. multivariate) is run on  {<code>X</code>, Ydummy}, returning a score matrix <code>T</code>. Finally, a QDA  (possibly with continuum) is done on {<code>T</code>, <code>y</code>}. </p><p>See functions <code>qda</code> and <code>plslda</code> for details (arguments <code>weights</code>, <code>prior</code>  and <code>alpha</code>) and examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsqda.jl#LL1-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsravg-Tuple{Any, Any}" href="#Jchemo.plsravg-Tuple{Any, Any}"><code>Jchemo.plsravg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsravg(X, Y; kwargs...)
plsravg(X, Y, weights::Weight; kwargs...)
plsravg!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Averaging PLSR models with different numbers of  latent variables (PLSR-AVG).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : A range of nb. of latent variables (LVs)    to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Ensemblist method where the predictions are computed  by averaging the predictions of a set of models built  with different numbers of LVs.</p><p>For instance, if argument <code>nlv</code> is set to <code>nlv</code> = <code>5:10</code>,  the prediction for a new observation is the simple average of the predictions returned by the models with 5 LVs, 6 LVs,  ... 10 LVs, respectively.</p><p><strong>References</strong></p><p>Lesnoff, M., Andueza, D., Barotin, C., Barre, P., Bonnal, L.,  Fernández Pierna, J.A., Picard, F., Vermeulen, P., Roger,  J.-M., 2022. Averaging and Stacking Partial Least Squares  Regression Models to Predict the Chemical Compositions and  the Nutritive Values of Forages from Spectral Near Infrared  Data. Applied Sciences 12, 7850.  https://doi.org/10.3390/app12157850</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y
@head Y
y = Y.ndf
#y = Y.dm
n = nro(X)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(y, s)
Xtest = X[s, :]
ytest = y[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)

nlv = 0:30
#nlv = 5:20
#nlv = 25
mod = model(plsravg; nlv) ;
fit!(mod, Xtrain, ytrain)

res = predict(mod, Xtest)
@head res.pred
res.predlv   # predictions for each nb. of LVs 
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsravg.jl#LL1-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsrda-Tuple{Any, Any}" href="#Jchemo.plsrda-Tuple{Any, Any}"><code>Jchemo.plsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsrda(X, y; kwargs...)
plsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on partial least squares regression (PLSR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>This is the usual &quot;PLSDA&quot; (prediction of the Y-dummy table  by a PLS2 regression). The training variable <code>y</code>  (univariate class membership) is transformed to a dummy table  (Ydummy) containing nlev columns, where nlev is the number of  classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1)  variable. Then, a weighted PLSR2 (i.e. multivariate) is run on  {<code>X</code>, Ydummy}, returning predictions of the dummy  variables (= object <code>posterior</code> returned by fuction <code>predict</code>).   These predictions can be considered as unbounded estimates (i.e.  eventuall outside of [0, 1]) of the class membership probabilities.  For a given observation, the final prediction is the class  corresponding to the dummy variable for which the probability  estimate is the highest.</p><p>In the high-level version of the function, the observation weights used in  the PLS2-R are defined with argument <code>prior</code>. For other choices, use the  low-level version (argument <code>weights</code>).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
mod = model(plsrda; nlv) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni
aggsum(fm.weights.w, ytrain)

@head fm.fm.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fm.fm)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred
summary(fm.fm, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsrda.jl#LL1-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsrosa-Tuple{Any, Any}" href="#Jchemo.plsrosa-Tuple{Any, Any}"><code>Jchemo.plsrosa</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsrosa(X, Y; kwargs...)
plsrosa(X, Y, weights::Weight; kwargs...)
plsrosa!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the  ROSA algorithm (Liland et al. 2016).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>Note:</strong> The function has the following differences with  the original algorithm of Liland et al. (2016):</p><ul><li>Scores T (LVs) are not normed.</li><li>Multivariate Y is allowed.</li></ul><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>Liland, K.H., Næs, T., Indahl, U.G., 2016. ROSA—a fast  extension of partial least squares regression for multiblock  data analysis. Journal of Chemometrics 30, 651–662.  https://doi.org/10.1002/cem.2824</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsrosa.jl#LL1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plssimp-Tuple{Any, Any}" href="#Jchemo.plssimp-Tuple{Any, Any}"><code>Jchemo.plssimp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plssimp(X, Y; kwargs...)
plssimp(X, Y, weights::Weight; kwargs...)
plssimp!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the SIMPLS algorithm (de Jong 1993).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>Note:</strong> In this function, scores T (LVs) are not normed,  conversely to the original algorithm of de Jong (2013).</p><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>de Jong, S., 1993. SIMPLS: An alternative approach to  partial least squares regression. Chemometrics and Intelligent  Laboratory Systems 18, 251–263.  https://doi.org/10.1016/0169-7439(93)85002-X</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plssimp.jl#LL1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plstuck-Tuple{Any, Any}" href="#Jchemo.plstuck-Tuple{Any, Any}"><code>Jchemo.plstuck</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plstuck(X, Y; kwargs...)
plstuck(X, Y, weights::Weight; kwargs...)
plstuck!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Tucker&#39;s inter-battery method of factor analysis</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>Inter-battery method of factor analysis (Tucker 1958,  Tenenhaus 1998 chap.3). The two blocks <code>X</code> and <code>X</code> play  a symmetric role.  This method is referred to as PLS-SVD  in Wegelin 2000. The basis of the method is to factorize  the covariance matrix X&#39;Y by SVD. </p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie  et pratique. Editions Technip, Paris.</p><p>Tishler, A., Lipovetsky, S., 2000. Modelling and forecasting  with robust canonical analysis: method and application.  Computers &amp; Operations Research 27, 217–232.  https://doi.org/10.1016/S0305-0548(99)00014-3</p><p>Tucker, L.R., 1958. An inter-battery method of factor  analysis. Psychometrika 23, 111–136. https://doi.org/10.1007/BF02289009</p><p>Wegelin, J.A., 2000. A Survey of Partial Least Squares (PLS)  Methods, with Emphasis on the Two-Block Case (No. 371).  University of Washington, Seattle, Washington, USA.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y

fm = plstuck(X, Y; nlv = 3)
pnames(fm)

fm.Tx
transf(fm, X, Y).Tx
fscale(fm.Tx, colnorm(fm.Tx))

res = summary(fm, X, Y)
pnames(res)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plstuck.jl#LL1-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plswold-Tuple{Any, Any}" href="#Jchemo.plswold-Tuple{Any, Any}"><code>Jchemo.plswold</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plswold(X, Y; kwargs...)
plswold(X, Y, weights::Weight; kwargs...)
plswold!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Partial Least Squares Regression (PLSR) with the Wold algorithm </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>tol</code> : Tolerance for the Nipals algorithm.</li><li><code>maxit</code> : Maximum number of iterations for the Nipals algorithm.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Wold Nipals PLSR algorithm: Tenenhaus 1998 p.204.</p><p>See function <code>plskern</code> for examples.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique.  Editions Technip, Paris, France.</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The  Collinearity Problem in Linear Regression. The Partial Least  Squares (PLS). Approach to Generalized Inverses. SIAM Journal on  Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plswold.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pmod-Tuple{Any}" href="#Jchemo.pmod-Tuple{Any}"><code>Jchemo.pmod</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pmod(foo)</code></pre><p>Shortcut for function <code>parentmodule</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL658-L662">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pnames-Tuple{Any}" href="#Jchemo.pnames-Tuple{Any}"><code>Jchemo.pnames</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pnames(x)</code></pre><p>Return the names of the elements of <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL664-L668">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.CalDs, Any}" href="#Jchemo.predict-Tuple{Jchemo.CalDs, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::CalDs, X; kwargs...)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/calds.jl#LL67-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.CalPds, Any}" href="#Jchemo.predict-Tuple{Jchemo.CalPds, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::CalPds, X; kwargs...)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/calpds.jl#LL90-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Cglsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Cglsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. iterations, or collection of    nb. iterations, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/cglsr.jl#LL185-L192">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dkplsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplsr.jl#LL146-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dmkern, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dmkern, x)</code></pre><p>Compute predictions from a_kde fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>x</code> : Data (vector) for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dmkern.jl#LL151-L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dmnorm, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Data (vector) for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dmnorm.jl#LL142-L147">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Knnda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Knnda1, X)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/knnda.jl#LL93-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Knnr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Knnr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/knnr.jl#LL115-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Kplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Kplsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul><p>If nothing, it is the maximum nb. LVs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplsr.jl#LL204-L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Krr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Krr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Krr, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of    regularization parameters, &quot;lambda&quot; to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/krr.jl#LL175-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lda.jl#LL103-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwmlr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwmlr.jl#LL90-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwmlrda, X)</code></pre><p>Compute y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwmlrda.jl#LL77-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplslda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplslda.jl#LL99-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplsqda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsqda.jl#LL107-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplsr, X; nlv = nothing)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsr.jl#LL144-L149">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}" href="#Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::LwplsrAvg, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsravg.jl#LL112-L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplsrda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/lwplsrda.jl#LL95-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Mbplslda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Mbplslda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Mbplslda, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplslda.jl#LL108-L115">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Mbplsrda, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsrda.jl#LL100-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Mlrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Mlrda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlrda.jl#LL86-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occod, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occod, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occod, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occod.jl#LL129-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occsd, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occsd, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occsd.jl#LL156-L161">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occsdod, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occsdod, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occsdod.jl#LL41-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occstah, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occstah, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/occstah.jl#LL123-L128">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Plslda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Plslda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plslda.jl#LL121-L127">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Plsravg, Any}" href="#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Plsravg, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsravg.jl#LL87-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Plsrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Plsrda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs,    to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsrda.jl#LL112-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Qda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Qda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Qda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/qda.jl#LL127-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Qda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rda.jl#LL144-L149">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Rosaplsr, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rosaplsr.jl#LL234-L241">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Rr, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of    regularization parameters, &quot;lambda&quot; to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rr.jl#LL122-L129">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Rrda, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters,    &quot;lambda&quot; to consider. If nothing, it is the parameter stored in the    fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rrda.jl#LL92-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Soplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Soplsr, Xbl)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/soplsr.jl#LL146-L152">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Svmda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Svmda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/svmda.jl#LL123-L128">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Svmr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Svmr, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/svmr.jl#LL131-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}" href="#Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::TreedaDt, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/treeda_dt.jl#LL103-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.TreerDt, Any}" href="#Jchemo.predict-Tuple{Jchemo.TreerDt, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::TreerDt, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/treer_dt.jl#LL92-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}" href="#Jchemo.predict-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Union{Mbplsr, Mbplswest}, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsr.jl#LL119-L126">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg}, Any}" href="#Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Mlr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mlr.jl#LL270-L275">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}, Any}" href="#Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Union{Plsr, Pcr, Splsr}, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plskern.jl#LL213-L219">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.psize-Tuple{Any}" href="#Jchemo.psize-Tuple{Any}"><code>Jchemo.psize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">psize(x)</code></pre><p>Print the type and size of <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL670-L674">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pval-Tuple{Distributions.Distribution, Any}" href="#Jchemo.pval-Tuple{Distributions.Distribution, Any}"><code>Jchemo.pval</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pval(d::Distribution, q)
pval(x::Array, q)
pval(e_cdf::ECDF, q)</code></pre><p>Compute p-value(s) for a distribution, an ECDF or vector.</p><ul><li><code>d</code> : A distribution computed from <code>Distribution.jl</code>.</li><li><code>x</code> : Univariate data.</li><li><code>e_cdf</code> : An ECDF computed from <code>StatsBase.jl</code>.</li><li><code>q</code> : Value(s) for which to compute the p-value(s).</li></ul><p>Compute or estimate the p-value of quantile <code>q</code>, ie. P(Q &gt; <code>q</code>) where Q is the random variable.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Distributions, StatsBase

d = Distributions.Normal(0, 1)
q = 1.96
#q = [1.64; 1.96]
Distributions.cdf(d, q)    # cumulative density function (CDF)
Distributions.ccdf(d, q)   # complementary CDF (CCDF)
pval(d, q)                 # Distributions.ccdf

x = rand(5)
e_cdf = StatsBase.ecdf(x)
e_cdf(x)                # empirical CDF computed at each point of x (ECDF)
p_val = 1 .- e_cdf(x)   # complementary ECDF at each point of x
q = .3
#q = [.3; .5; 10]
pval(e_cdf, q)          # 1 .- e_cdf(q)
pval(x, q)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL679-L712">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.qda-Tuple{Any, Any}" href="#Jchemo.qda-Tuple{Any, Any}"><code>Jchemo.qda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">qda(X, y; kwargs...)
qda(X, y, weights::Weight; kwargs...)</code></pre><p>Quadratic discriminant analysis (QDA, with continuum towards LDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li></ul><p>A value <code>alpha</code> &gt; 0 shrinks the class-covariances by class  (Wi) toward a common LDA covariance (&quot;within&quot; W). This corresponds to  the &quot;first regularization (Eqs.16)&quot; described in Friedman 1989 (where <code>alpha</code> is referred to as &quot;lambda&quot;).</p><p>In these <code>qda</code> functions, observation weights (argument <code>weights</code>) are used  to compute covariance matrices Wi and W. Argument <code>prior</code> is used to define  the usual prior class probabilities. </p><p>In the high-level version, the observation weights are automatically  defined by the given priors (<code>prior</code>): the sub-total weights by class are set  equal to the prior probabilities. For other choices, use the low-level  version.</p><p><strong>References</strong></p><p>Friedman JH. Regularized Discriminant Analysis. Journal  of the American Statistical Association. 1989;  84(405):165-175. doi:10.1080/01621459.1989.10478752.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

mod = model(qda)
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni
aggsum(fm.weights.w, ytrain)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

## With regularization
mod = model(qda; alpha = .5)
#mod = model(qda; alpha = 1) # = LDA
fit!(mod, Xtrain, ytrain)
mod.fm.Wi
res = predict(mod, Xtest) ;
errp(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/qda.jl#LL1-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.r2-Tuple{Any, Any}" href="#Jchemo.r2-Tuple{Any, Any}"><code>Jchemo.r2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">r2(pred, Y)</code></pre><p>Compute the R2 coefficient.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>The rate R2 is calculated by:</p><ul><li>R2 = 1 - MSEP(current model) / MSEP(null model) </li></ul><p>where the &quot;null model&quot; is the overall mean.  For predictions over CV or test sets, and/or for  non linear models, it can be different from the square  of the correlation coefficient (<code>cor2</code>) between the true  data and the predictions. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
r2(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
r2(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL102-L135">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rasvd-Tuple{Any, Any}" href="#Jchemo.rasvd-Tuple{Any, Any}"><code>Jchemo.rasvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rasvd(X, Y; kwargs...)
rasvd(X, Y, weights::Weight; kwargs...)
rasvd!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Redundancy analysis (RA), <em>aka</em> PCA on instrumental variables (PCAIV)</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling. Possible values are:   <code>:none</code>, <code>:frob</code>. See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>X</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>See e.g. Bougeard et al. 2011a,b and Legendre &amp; Legendre 2012.  Let Y<em>hat be the fitted values of the regression of <code>Y</code> on <code>X</code>.  The scores <code>Ty</code> are the PCA scores of Y</em>hat. The scores <code>Tx</code> are  the fitted values of the regression of <code>Ty</code> on <code>X</code>.</p><p>A continuum regularization is available.  After block  centering and scaling, the covariances matrices are computed  as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li></ul><p>where D is the observation (row) metric.  Value <code>tau</code> = 0 can generate unstability when inverting  the covariance matrices. Often, a better alternative is  to use an epsilon value (e.g. <code>tau</code> = 1e-8) to get similar  results as with pseudo-inverses.    </p><p><strong>References</strong></p><p>Bougeard, S., Qannari, E.M., Lupo, C., Chauvin, C.,  2011-a. Multiblock redundancy analysis from a user&#39;s  perspective. Application in veterinary epidemiology.  Electronic Journal of Applied Statistical Analysis  4, 203-214. https://doi.org/10.1285/i20705948v4n2p203</p><p>Bougeard, S., Qannari, E.M., Rose, N., 2011-b. Multiblock  redundancy analysis: interpretation tools and application  in epidemiology. Journal of Chemometrics 25,  467-475. https://doi.org/10.1002/cem.1392</p><p>Legendre, P., Legendre, L., 2012. Numerical Ecology.  Elsevier, Amsterdam, The Netherlands.</p><p>Tenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized  and Sparse Generalized Canonical Correlation Analysis  for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n, p = size(X)
q = nco(Y)

nlv = 2
bscal = :frob ; tau = 1e-4
mod = model(rasvd; nlv, bscal, tau)
fit!(mod, X, Y)
pnames(mod)
pnames(mod.fm)

@head mod.fm.Tx
@head transfbl(mod, X, Y).Tx

@head mod.fm.Ty
@head transfbl(mod, X, Y).Ty

res = summary(mod, X, Y) ;
pnames(res)
res.explvarx
res.cort2t 
res.rdx
res.rdy
res.corx2t 
res.cory2t </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rasvd.jl#LL1-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rd-Tuple{Any, Any}" href="#Jchemo.rd-Tuple{Any, Any}"><code>Jchemo.rd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rd(X, Y; typ = :cor)
rd(X, Y, weights::Weight; typ = :cor)</code></pre><p>Compute redundancy coefficients between two matrices.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>typ</code> : Possibles values are: <code>:cor</code> (correlation),    <code>:cov</code> (uncorrected covariance). </li></ul><p>Returns the redundancy coefficient between <code>X</code> and each column of <code>Y</code>, i.e.: </p><p>(1 / p) * [Sum.(j=1, .., p) cor(xj, y1)^2 ; ... ; Sum.(j=1, .., p) cor(xj, yq)^2] </p><p>See Tenenhaus 1998 section 2.2.1 p.10-11.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
Y = rand(5, 3)
rd(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/angles.jl#LL62-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rda-Tuple{Any, Any}" href="#Jchemo.rda-Tuple{Any, Any}"><code>Jchemo.rda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rda(X, y; kwargs...)
rda(X, y, weights::Weight; kwargs...)</code></pre><p>Regularized discriminant analysis (RDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot; (&gt;= 0).</li><li><code>simpl</code> : Boolean. See function <code>dmnorm</code>. </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Let us note W the (corrected) pooled within-class  covariance matrix and Wi the (corrected) within-class  covariance matrix of class i. The regularization is done  by the two following successive steps (for each class i):</p><ol><li>Continuum between QDA and LDA: Wi(1) = (1 - <code>alpha</code>) * Wi + <code>alpha</code> * W       </li><li>Ridge regularization: Wi(2) = Wi(1) + <code>lb</code> * I</li></ol><p>Then the QDA algorithm is run on matrices {Wi(2)}.</p><p>Function <code>rda</code> is slightly different from the regularization  expression used by Friedman 1989 (Eq.18). It shrinks the  covariance matrices Wi(2) to the diagonal of the Idendity  matrix (ridge regularization; e.g. Guo et al. 2007).  </p><p>Particular cases:</p><ul><li><code>alpha</code> = 1 &amp; <code>lb</code> = 0 : LDA</li><li><code>alpha</code> = 0 &amp; <code>lb</code> = 0 : QDA</li><li><code>alpha</code> = 1 &amp; <code>lb</code> &gt; 0 : Penalized LDA    (Hastie et al 1995) with diagonal regularization    matrix</li></ul><p>See functions <code>lda</code> and <code>qda</code> for other details (arguments <code>weights</code> and <code>prior</code>).</p><p><strong>References</strong></p><p>Friedman JH. Regularized Discriminant Analysis.  Journal of the American Statistical Association. 1989;  84(405):165-175. doi:10.1080/01621459.1989.10478752.</p><p>Guo Y, Hastie T, Tibshirani R. Regularized linear  discriminant analysis and its application in microarrays.  Biostatistics. 2007; 8(1):86-100.  doi:10.1093/biostatistics/kxj035.</p><p>Hastie, T., Buja, A., Tibshirani, R., 1995. Penalized  Discriminant Analysis. The Annals of Statistics 23, 73–102.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;)
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
y = dat.X[:, 5]
n = nro(X)
ntest = 30
s = samprand(n, ntest)
Xtrain = X[s.train, :]
ytrain = y[s.train]
Xtest = X[s.test, :]
ytest = y[s.test]
ntrain = n - ntest
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

alpha = .5
lb = 1e-8
mod = model(rda; alpha, lb)
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rda.jl#LL1-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.recodcat2int-Tuple{Any}" href="#Jchemo.recodcat2int-Tuple{Any}"><code>Jchemo.recodcat2int</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recodcat2int(x; start = 1)</code></pre><p>Recode a categorical variable to a integer variable.</p><ul><li><code>x</code> : Variable to recode.</li><li><code>start</code> : Integer that will be set to the first category.</li></ul><p>The integers returned by the function correspond to the  sorted levels (categories) of <code>x</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [&quot;b&quot;, &quot;a&quot;, &quot;b&quot;]   
[x recodcat2int(x)]
recodcat2int(x; start = 0)
recodcat2int([25, 1, 25])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL719-L735">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.recodnum2int-Tuple{Any, Any}" href="#Jchemo.recodnum2int-Tuple{Any, Any}"><code>Jchemo.recodnum2int</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recodnum2int(x, q)</code></pre><p>Recode a continuous variable to integer classes.</p><ul><li><code>x</code> : Variable to recode.</li><li><code>q</code> : Values separating the classes. </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Statistics
x = [collect(1:10); 8.1 ; 3.1] 
q = [3; 8]
zx = recodnum2int(x, q)  
[x zx]
probs = [.33; .66]
q = quantile(x, probs) 
zx = recodnum2int(x, q)  
[x zx]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL744-L762">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.recovkwargs-Tuple{DataType, Any}" href="#Jchemo.recovkwargs-Tuple{DataType, Any}"><code>Jchemo.recovkwargs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recovkwargs(ParamStruct, kwargs)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL776-L778">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.replacebylev-Tuple{Any, Any}" href="#Jchemo.replacebylev-Tuple{Any, Any}"><code>Jchemo.replacebylev</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replacebylev(x, lev)</code></pre><p>Replace the elements of a vector by levels of corresponding order.</p><ul><li><code>x</code> : Vector (n) of values to replace.</li><li><code>lev</code> : Vector (nlev) containing the levels.</li></ul><p><em>Warning</em>: <code>x</code> and <code>lev</code> must contain the same number (nlev) of levels.</p><p>The ith sorted level in <code>x</code> is replaced by the ith sorted level of <code>lev</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [10; 4; 3; 3; 4; 4]
lev = [&quot;B&quot;; &quot;C&quot;; &quot;AA&quot;]
sort(lev)
[x replacebylev(x, lev)]
zx = string.(x)
[zx replacebylev(zx, lev)]

lev = [3; 0; -1]
[x replacebylev(x, lev)]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL788-L810">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}" href="#Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}"><code>Jchemo.replacebylev2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replacebylev2(x::Union{Int, Array{Int}}, lev::Array)</code></pre><p>Replace the elements of an index-vector by levels.</p><ul><li><code>x</code> : Vector (n) of values to replace.</li><li><code>lev</code> : Vector (nlev) containing the levels.</li></ul><p><em>Warning</em>: Let us note nlev the number of levels in <code>lev</code>.  Vector <code>x</code> must contain integer values between 1 and nlev. </p><p>Each element <code>x</code><a href="i = 1, ..., n">i</a> is replaced by sort(<code>lev</code>)[<code>x</code>[i]].</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [2; 1; 2; 2]
lev = [&quot;B&quot;; &quot;C&quot;; &quot;AA&quot;]
sort(lev)
[x replacebylev2(x, lev)]
replacebylev2([2], lev)
replacebylev2(2, lev)

x = [2; 1; 2]
lev = [3; 0; -1]
replacebylev2(x, lev)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL825-L849">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.replacedict-Tuple{Any, Any}" href="#Jchemo.replacedict-Tuple{Any, Any}"><code>Jchemo.replacedict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replacedict(x, dict)</code></pre><p>Replace the elements of a vector by levels defined in a dictionary.</p><ul><li><code>x</code> : Vector (n) of values to replace.</li><li><code>dict</code> : A dictionary of the correpondances betwwen the old and new values.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">dict = Dict(&quot;a&quot; =&gt; 1000, &quot;b&quot; =&gt; 1, &quot;c&quot; =&gt; 2)

x = [&quot;c&quot;; &quot;c&quot;; &quot;a&quot;; &quot;a&quot;; &quot;a&quot;]
replacedict(x, dict)

x = [&quot;c&quot;; &quot;c&quot;; &quot;a&quot;; &quot;a&quot;; &quot;a&quot;; &quot;e&quot;]
replacedict(x, dict)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL861-L877">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.residcla-Tuple{Any, Any}" href="#Jchemo.residcla-Tuple{Any, Any}"><code>Jchemo.residcla</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">residcla(pred, y)</code></pre><p>Compute the discrimination residual vector (0 = no error, 1 = error).</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data (class membership).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

mod = model(plsrda; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
residcla(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL430-L448">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.residreg-Tuple{Any, Any}" href="#Jchemo.residreg-Tuple{Any, Any}"><code>Jchemo.residreg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">residreg(pred, Y)</code></pre><p>Compute the regression residual vector.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
residreg(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
residreg(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL143-L168">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}" href="#Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}"><code>Jchemo.rfda_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rfda_dt(X, y; kwargs...)</code></pre><p>Random forest discrimination with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_trees</code> : Nb. trees built for the forest. </li><li><code>partial_sampling</code> : Proportion of sampled    observations for each tree.</li><li><code>n_subfeatures</code> : Nb. variables to select at random    at each split (default: -1 ==&gt; sqrt(#variables)).</li><li><code>max_depth</code> : Maximum depth of the decision trees    (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples    each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations   in needed for a split.</li><li><code>mth</code> : Boolean indicating if a multi-threading is    done when new data are predicted with function <code>predict</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li>Do <code>dump(Par(), maxdepth = 1)</code> to print the default    values of the keyword arguments. </li></ul><p>The function fits a random forest discrimination² model using  package `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., 1996. Bagging predictors. Mach Learn 24,  123–140. https://doi.org/10.1007/BF00058655</p><p>Breiman, L., 2001. Random Forests. Machine Learning  45, 5–32. https://doi.org/10.1023/A:1010933404324</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Genuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis.  Université Paris Sud - Paris XI.</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en  régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n, p = size(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

n_trees = 200
n_subfeatures = p / 3 
max_depth = 10
mod = model(rfda_dt; n_trees, n_subfeatures, max_depth) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rfda_dt.jl#LL1-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rfr_dt-Tuple{Any, Any}" href="#Jchemo.rfr_dt-Tuple{Any, Any}"><code>Jchemo.rfr_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rfr_dt(X, y; kwargs...)</code></pre><p>Random forest regression with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_trees</code> : Nb. trees built for the forest. </li><li><code>partial_sampling</code> : Proportion of sampled    observations for each tree.</li><li><code>n_subfeatures</code> : Nb. variables to select at random    at each split (default: -1 ==&gt; sqrt(#variables)).</li><li><code>max_depth</code> : Maximum depth of the decision trees    (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples    each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations   in needed for a split.</li><li><code>mth</code> : Boolean indicating if a multi-threading is    done when new data are predicted with function <code>predict</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li>Do <code>dump(Par(), maxdepth = 1)</code> to print the default    values of the keyword arguments. </li></ul><p>The function fits a random forest regression model using  package `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., 1996. Bagging predictors. Mach Learn 24,  123–140. https://doi.org/10.1007/BF00058655</p><p>Breiman, L., 2001. Random Forests. Machine Learning  45, 5–32. https://doi.org/10.1023/A:1010933404324</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Genuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis.  Université Paris Sud - Paris XI.</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en  régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
p = nco(X)

n_trees = 200
n_subfeatures = p / 3
max_depth = 15
mod = model(rfr_dt; n_trees, n_subfeatures, max_depth) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rfr_dt.jl#LL1-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}" href="#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}"><code>Jchemo.rmcol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmcol(X, s)</code></pre><p>Remove the columns of a matrix or the components of a vector  having indexes <code>s</code>.</p><ul><li><code>X</code> : Matrix or vector.</li><li><code>s</code> : Vector of the indexes.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3) 
rmcol(X, [1, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL883-L895">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmgap-Tuple{Any}" href="#Jchemo.rmgap-Tuple{Any}"><code>Jchemo.rmgap</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmgap(X; kwargs...)</code></pre><p>Remove vertical gaps in spectra (e.g. for ASD).  </p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>indexcol</code> : Indexes (∈ [1, p]) of the <code>X</code>-columns where are    located the gaps to remove. </li><li><code>npoint</code> : The number of <code>X</code>-columns used on the left side        of each gap for fitting the linear regressions.</li></ul><p>For each spectra (row-observation of matrix <code>X</code>) and each  defined gap, the correction is done by extrapolation from  a simple linear regression computed on the left side of the gap. </p><p>For instance, If two gaps are observed between column-indexes 651-652 and between column-indexes 1425-1426, respectively, the syntax should  be <code>indexcol</code> = [651 ; 1425].</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/asdgap.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
wlst = names(dat.X)
wl = parse.(Float64, wlst)

wl_target = [1000 ; 1800] 
indexcol = findall(in(wl_target).(wl))

f, ax = plotsp(X, wl)
vlines!(ax, wl_target; linestyle = :dot, color = (:grey, .8))
f

## Corrected data
mod = model(rmgap; npoint = 5, indexcol)
fit!(mod, X)
Xc = transf(mod, X)
f, ax = plotsp(Xc, wl)
vlines!(ax, wl_target; linestyle = :dot, color = (:grey, .8))
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rmgap.jl#LL1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}" href="#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}"><code>Jchemo.rmrow</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmrow(X, s)</code></pre><p>Remove the rows of a matrix or the components of a vector  having indexes <code>s</code>.</p><ul><li><code>X</code> : Matrix or vector.</li><li><code>s</code> : Vector of the indexes.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
rmrow(X, [1, 4])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL908-L920">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmsep-Tuple{Any, Any}" href="#Jchemo.rmsep-Tuple{Any, Any}"><code>Jchemo.rmsep</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmsep(pred, Y)</code></pre><p>Compute the square root of the mean of the squared      prediction errors (RMSEP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
rmsep(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
rmsep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL171-L197">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmsepstand-Tuple{Any, Any}" href="#Jchemo.rmsepstand-Tuple{Any, Any}"><code>Jchemo.rmsepstand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmsepstand(pred, Y)</code></pre><p>Compute the standardized square root of the mean of the squared prediction errors      (RMSEP_stand).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>RMSEP is standardized to <code>Y</code>: </p><ul><li>RMSEP_stand = RMSEP ./ <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
rmsepstand(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
rmsepstand(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL200-L229">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rosaplsr-Tuple{Any, Any}" href="#Jchemo.rosaplsr-Tuple{Any, Any}"><code>Jchemo.rosaplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rosaplsr(Xbl, Y; kwargs...)
rosaplsr(Xbl, Y, weights::Weight; kwargs...)
rosaplsr!(Xbl::Vector, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock ROSA PLSR (Liland et al. 2016).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>The function has the following differences with the  original algorithm of Liland et al. (2016):</p><ul><li>Scores T are not normed to 1.</li><li>Multivariate <code>Y</code> is allowed. In such a case,    the squared residuals are summed over the columns    for finding the winning block for each global LV    (therefore Y-columns should have the same fscale).</li></ul><p><strong>References</strong></p><p>Liland, K.H., Næs, T., Indahl, U.G., 2016. ROSA — a fast  extension of partial least squares regression for multiblock  data analysis. Journal of Chemometrics 30, 651–662.  https://doi.org/10.1002/cem.2824</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
pnames(dat) 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
Xbltest = mblock(rmrow(X, s), listbl)
ytrain = y[s]
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 3
scal = false
#scal = true
mod = model(rosaplsr; nlv, scal)
fit!(mod, Xbltrain, ytrain)
pnames(mod) 
pnames(mod.fm)
@head mod.fm.T
@head transf(mod, Xbltrain)
transf(mod, Xbltest)

res = predict(mod, Xbltest)
res.pred 
rmsep(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rosaplsr.jl#LL1-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowmean-Tuple{Any}" href="#Jchemo.rowmean-Tuple{Any}"><code>Jchemo.rowmean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowmean(X)</code></pre><p>Compute row-wise means of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
rowmean(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_rowwise.jl#LL1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rownorm-Tuple{Any}" href="#Jchemo.rownorm-Tuple{Any}"><code>Jchemo.rownorm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rownorm(X)</code></pre><p>Compute row-wise norms of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>The norm computed for a row x of <code>X</code> is:</p><ul><li>sqrt(x&#39; * x)</li></ul><p>Return a vector.</p><p>Note: Thanks to @mcabbott  at https://discourse.julialang.org/t/orders-of-magnitude-runtime-difference-in-row-wise-norm/96363.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)

rownorm(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_rowwise.jl#LL17-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowstd-Tuple{Any}" href="#Jchemo.rowstd-Tuple{Any}"><code>Jchemo.rowstd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowstd(X)</code></pre><p>Compute row-wise standard deviations (uncorrected) of a matrix`.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
rowstd(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_rowwise.jl#LL40-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowsum-Tuple{Any}" href="#Jchemo.rowsum-Tuple{Any}"><code>Jchemo.rowsum</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowsum(X)</code></pre><p>Compute row-wise sums of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
rowsum(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_rowwise.jl#LL56-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowvar-Tuple{Any}" href="#Jchemo.rowvar-Tuple{Any}"><code>Jchemo.rowvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowvar(X)</code></pre><p>Compute row-wise variances (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
rowvar(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility_rowwise.jl#LL71-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rp-Tuple{Any}" href="#Jchemo.rp-Tuple{Any}"><code>Jchemo.rp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rp(X; kwargs...)
rp(X, weights::Weight; kwargs...)
rp!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Make a random projection of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. dimensions on which <code>X</code> is projected.</li><li><code>mrp</code> : Method of random projection. Possible   values are: <code>:gauss</code>, <code>:li</code>. See the respective    functions <code>rpmatgauss</code> and <code>rpmatli</code> for their    keyword arguments.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = (5, 10)
X = rand(n, p)
nlv = 3
mrp = :li ; s_li = sqrt(p) 
#mrp = :gauss
mod = model(rp; nlv, mrp, s_li)
fit!(mod, X)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T 
@head mod.fm.P 
transf(mod, X[1:2, :])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rp.jl#LL1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpd-Tuple{Any, Any}" href="#Jchemo.rpd-Tuple{Any, Any}"><code>Jchemo.rpd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rpd(pred, Y)</code></pre><p>Compute the ratio &quot;deviation to model performance&quot; (RPD).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>This is the ratio of the deviation to the model performance  to the deviation, defined by:</p><ul><li>RPD = Std(Y) / RMSEP</li></ul><p>where Std(Y) is the standard deviation. </p><p>Since Std(Y) = RMSEP(null model) where the null model is  the simple average, this also gives:</p><ul><li>RPD = RMSEP(null model) / RMSEP </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
rpd(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
rpd(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL235-L269">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpdr-Tuple{Any, Any}" href="#Jchemo.rpdr-Tuple{Any, Any}"><code>Jchemo.rpdr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rpdr(pred, Y)</code></pre><p>Compute a robustified RPD.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
rpdr(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
rpdr(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL275-L300">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpmatgauss" href="#Jchemo.rpmatgauss"><code>Jchemo.rpmatgauss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rpmatgauss(p::Int, nlv::Int, Q = Float64)</code></pre><p>Build a gaussian random projection matrix.</p><ul><li><code>p</code> : Nb. variables (attributes) to project.</li><li><code>nlv</code> : Nb. of simulated projection    dimensions.</li><li><code>Q</code> : Type of components of the built    projection matrix.</li></ul><p>The function returns a random projection matrix P of  dimension <code>p</code> x <code>nlv</code>. The projection of a given matrix X  of size n x <code>p</code> is given by X * P.</p><p>P is simulated from i.i.d. N(0, 1) / sqrt(<code>nlv</code>).</p><p><strong>References</strong></p><p>Li, P., Hastie, T.J., Church, K.W., 2006. Very sparse random  projections, in: Proceedings of the 12th ACM SIGKDD International  Conference on Knowledge Discovery and Data Mining, KDD ’06.  Association for Computing Machinery, New York, NY, USA, pp. 287–296.  https://doi.org/10.1145/1150402.1150436</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">p = 10 ; nlv = 3
rpmatgauss(p, nlv)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rpmat.jl#LL1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpmatli" href="#Jchemo.rpmatli"><code>Jchemo.rpmatli</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rpmatli(p::Int, nlv::Int, Q = Float64; s_li)</code></pre><p>Build a sparse random projection matrix (Achlioptas 2001, Li et al. 2006).</p><ul><li><code>p</code> : Nb. variables (attributes) to project.</li><li><code>nlv</code> : Nb. of simulated projection    dimensions.</li><li><code>Q</code> : Type of components of the built    projection matrix.</li></ul><p>Keyword arguments:</p><ul><li><code>s_li</code> : Coefficient defining the sparsity of the    returned matrix (higher is <code>s</code>, higher is the sparsity).</li></ul><p>The function returns a random projection matrix P of  dimension <code>p</code> x <code>nlv</code>. The projection of a given matrix X  of size n x <code>p</code> is given by X * P.</p><p>Matrix P is simulated from i.i.d. discrete  sampling within values: </p><ul><li>1 with prob. 1/(2 * <code>s</code>)</li><li>0 with prob. 1 - 1 / <code>s</code></li><li>-1 with prob. 1/(2 * <code>s</code>)</li></ul><p>Usual values for <code>s</code> are:</p><ul><li>sqrt(<code>p</code>)       (Li et al. 2006)</li><li><code>p</code> / log(<code>p</code>)  (Li et al. 2006)</li><li>1               (Achlioptas 2001)</li><li>3               (Achlioptas 2001) </li></ul><p><strong>References</strong></p><p>Achlioptas, D., 2001. Database-friendly random projections,  in: Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART  Symposium on Principles of Database Systems, PODS ’01.  Association for Computing Machinery, New York, NY, USA, pp. 274–281.  https://doi.org/10.1145/375551.375608</p><p>Li, P., Hastie, T.J., Church, K.W., 2006. Very sparse random  projections, in: Proceedings of the 12th ACM SIGKDD International  Conference on Knowledge Discovery and Data Mining, KDD ’06. Association  for Computing Machinery, New York, NY, USA, pp. 287–296.  https://doi.org/10.1145/1150402.1150436</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">p = 10 ; nlv = 3
rpmatli(p, nlv)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rpmat.jl#LL33-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rr-Tuple{Any, Any}" href="#Jchemo.rr-Tuple{Any, Any}"><code>Jchemo.rr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rr(X, Y; kwargs...)
rr(X, Y, weights::Weight; kwargs...)
rr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Ridge regression (RR) implemented by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>References</strong></p><p>Cule, E., De Iorio, M., 2012. A semi-automatic method  to guide the choice of ridge parameter in ridge regression.  arXiv:1205.0686.</p><p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic  regularization for expression arrays. Biostatistics 5, 329-340.  https://doi.org/10.1093/biostatistics/kxh010</p><p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The  elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.</p><p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased  Estimation for Nonorthogonal Problems. Technometrics 12, 55-67.  https://doi.org/10.1080/00401706.1970.10488634</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

lb = 1e-3
mod = model(rr; lb) 
#mod = model(rrchol; lb) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

coef(mod)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

## Only for function &#39;rr&#39; (not for &#39;rrchol&#39;)
coef(mod; lb = 1e-1)
res = predict(mod, Xtest; lb = [.1 ; .01])
@head res.pred[1]
@head res.pred[2]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rr.jl#LL1-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rrchol-Tuple{Any, Any}" href="#Jchemo.rrchol-Tuple{Any, Any}"><code>Jchemo.rrchol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rrchol(X, Y; kwargs...)
rrchol(X, Y, weights::Weight; kwargs...)
rrchol!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Ridge regression (RR) using the Normal equations      and a Cholesky factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>See function <code>rr</code> for examples.</p><p><strong>References</strong></p><p>Cule, E., De Iorio, M., 2012. A semi-automatic method  to guide the choice of ridge parameter in ridge regression.  arXiv:1205.0686.</p><p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic  regularization for expression arrays. Biostatistics 5, 329-340.  https://doi.org/10.1093/biostatistics/kxh010</p><p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The  elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.</p><p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased  Estimation for Nonorthogonal Problems. Technometrics 12, 55-67.  https://doi.org/10.1080/00401706.1970.10488634</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rrchol.jl#LL1-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rrda-Tuple{Any, Any}" href="#Jchemo.rrda-Tuple{Any, Any}"><code>Jchemo.rrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rrda(X, y; kwargs...)
rrda(X, y, weights::Weight; kwargs...)</code></pre><p>Discrimination based on ridge regression (RR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The training variable <code>y</code> (univariate class membership) is  transformed to a dummy table (Ydummy) containing nlev columns,  where nlev is the number of classes present in <code>y</code>. Each column of  Ydummy is a dummy (0/1) variable. Then, a ridge regression  (RR) is run on {<code>X</code>, Ydummy}, returning predictions of the dummy  variables (= object <code>posterior</code> returned by fuction <code>predict</code>).   These predictions can be considered as unbounded estimates (i.e.  eventuall outside of [0, 1]) of the class membership probabilities.  For a given observation, the final prediction is the class  corresponding to the dummy variable for which the probability  estimate is the highest.</p><p>In the high-level version of the function, the observation weights used in  the RR are defined with argument <code>prior</code>. For other choices, use the  low-level version (argument <code>weights</code>).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

lb = 1e-5
mod = model(rrda; lb) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

coef(fm.fm)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; lb = [.1; .01]).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rrda.jl#LL1-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rrr-Tuple{Any, Any}" href="#Jchemo.rrr-Tuple{Any, Any}"><code>Jchemo.rrr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rrr(X, Y; kwargs...)
rrr(X, Y, weights::Weight; kwargs...)
rr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Reduced rank regression (RRR, <em>aka</em> RA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Reduced rank regression, also referred to as redundancy  analysis (RA) regression. In this function, the RA uses  the Nipals algorithm presented in Mangamana et al 2021,  section 2.1.1.</p><p>A continuum regularization is available. After block centering  and scaling, the covariances matrices are computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li></ul><p>where D is the observation (row) metric. Value <code>tau</code> = 0  can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value  (e.g. <code>tau</code> = 1e-8) to get similar results as with pseudo-inverses.  </p><p><strong>References</strong></p><p>Bougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011.  Multiblock redundancy analysis from a user’s perspective.  Application in veterinary epidemiology. Electronic Journal of  Applied Statistical Analysis 4, 203-214–214.  https://doi.org/10.1285/i20705948v4n2p203</p><p>Bougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy  analysis: interpretation tools and application in epidemiology.  Journal of Chemometrics 25, 467–475. https://doi.org/10.1002/cem.1392 </p><p>Tchandao Mangamana, E., Glèlè Kakaï, R., Qannari, E.M., 2021.  A general strategy for setting up supervised methods of multiblock  data analysis. Chemometrics and Intelligent Laboratory Systems  217, 104388.  https://doi.org/10.1016/j.chemolab.2021.104388</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 1
tau = 1e-4
mod = model(rrr; nlv, tau) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T

coef(mod)
coef(mod; nlv = 3)

@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f   </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rrr.jl#LL1-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rv-Tuple{Any, Any}" href="#Jchemo.rv-Tuple{Any, Any}"><code>Jchemo.rv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rv(X, Y; centr = true)
rv(Xbl::Vector; centr = true)</code></pre><p>Compute the RV coefficient between matrices.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>Xbl</code> : A list (vector) of matrices.</li><li><code>centr</code> : Boolean indicating if the matrices    will be internally centered or not.</li></ul><p>RV is bounded in [0, 1]. </p><p>A dissimilarty measure between <code>X</code> and <code>Y</code> can be computed by d = sqrt(2 * (1 - RV)).</p><p><strong>References</strong></p><p>Escoufier, Y., 1973. Le Traitement des Variables Vectorielles.  Biometrics 29, 751–760. https://doi.org/10.2307/2529140</p><p>Josse, J., Holmes, S., 2016. Measuring multivariate association and beyond.  Stat Surv 10, 132–167. https://doi.org/10.1214/16-SS116</p><p>Josse, J., Pagès, J., Husson, F., 2008. Testing the significance of  the RV coefficient. Computational Statistics &amp; Data Analysis 53, 82–91.  https://doi.org/10.1016/j.csda.2008.06.012</p><p>Kazi-Aoual, F., Hitier, S., Sabatier, R., Lebreton, J.-D., 1995.  Refined approximations to permutation tests for multivariate inference.  Computational Statistics &amp; Data Analysis 20, 643–656.  https://doi.org/10.1016/0167-9473(94)00064-2</p><p>Mayer, C.-D., Lorent, J., Horgan, G.W., 2011. Exploratory Analysis  of Multiple Omics Datasets Using the Adjusted RV Coefficient. Statistical  Applications in Genetics and Molecular Biology 10. https://doi.org/10.2202/1544-6115.1540</p><p>Smilde, A.K., Kiers, H.A.L., Bijlsma, S., Rubingh, C.M., van Erk, M.J., 2009.  Matrix correlations for high-dimensional data: the modified RV-coefficient.  Bioinformatics 25, 401–405. https://doi.org/10.1093/bioinformatics/btn634</p><p>Robert, P., Escoufier, Y., 1976. A Unifying Tool for Linear Multivariate  Statistical Methods: The RV-Coefficient. Journal of the Royal Statistical Society:  Series C (Applied Statistics) 25, 257–265. https://doi.org/10.2307/2347233</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
Y = rand(5, 3)
rv(X, Y)

X = rand(5, 15) 
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl)
rv(Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/angles.jl#LL115-L169">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampcla" href="#Jchemo.sampcla"><code>Jchemo.sampcla</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sampcla(x, k::Union{Int, Vector{Int}}, y = nothing)</code></pre><p>Build training vs. test sets by stratified sampling.  </p><ul><li><code>x</code> : Class membership (n) of the observations.</li><li><code>k</code> : Nb. test observations to sample in each class.    If <code>k</code> is a single value, the nb. of sampled    observations is the same for each class. Alternatively,    <code>k</code>can be a vector of length equal to the nb. of    classes in <code>x</code>.</li><li><code>y</code> : Quantitative variable (n) used if systematic sampling.</li></ul><p>Two outputs are returned (= row indexes of the data): </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>If <code>y</code> = <code>nothing</code>, the sampling is random, else it is  systematic over the sorted <code>y</code>(see function <code>sampsys</code>).</p><p><strong>References</strong></p><p>Naes, T., 1987. The design of calibration in near infra-red reflectance  analysis by clustering. Journal of Chemometrics 1, 121-134.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = string.(repeat(1:3, 5))
n = length(x)
tab(x)
k = 2 
res = sampcla(x, k)
res.test
x[res.test]
tab(x[res.test])

y = rand(n)
res = sampcla(x, k, y)
res.test
x[res.test]
tab(x[res.test])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/sampcla.jl#LL1-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampdf" href="#Jchemo.sampdf"><code>Jchemo.sampdf</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sampdf(Y::DataFrame, k::Union{Int, Vector{Int}}, id = 1:nro(Y); msamp = :rand)</code></pre><p>Build training vs. test sets from each column of a dataframe. </p><ul><li><code>Y</code> : DataFrame (n, p) whose each column can contain missing values.</li><li><code>k</code> : Nb. of test observations selected for each <code>Y</code> column.    The selection is done within the non-missing observations    of the considered column. If <code>k</code> is a single value, the same nb.     of observations are selected for each column. Alternatively,    <code>k</code> can be a vector of length p. </li><li><code>id</code> : Vector (n) of IDs.</li></ul><p>Keyword arguments:</p><ul><li><code>msamp</code> : Type of sampling for the test set.   Possible values are: <code>:rand</code> = random sampling,    <code>:sys</code> = systematic sampling over each sorted    <code>Y</code> column (see function <code>sampsys</code>).  </li></ul><p>Typically, dataframe <code>Y</code> contains a set of response variables  to predict.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrames

Y = hcat([rand(5); missing; rand(6)],
   [rand(2); missing; missing; rand(7); missing])
Y = DataFrame(Y, :auto)
n = nro(Y)

k = 3
res = sampdf(Y, k) 
#res = sampdf(Y, k, string.(1:n))
pnames(res)
res.nam
length(res.test)
res.train
res.test

## Replicated splitting Train/Test
rep = 10
k = 3
ids = [sampdf(Y, k) for i = 1:rep]
length(ids)
i = 1    # replication
ids[i]
ids[i].train 
ids[i].test
j = 1    # variable y  
ids[i].train[j]
ids[i].test[j]
ids[i].nam[j]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/sampdf.jl#LL1-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampdp-Tuple{Any, Int64}" href="#Jchemo.sampdp-Tuple{Any, Int64}"><code>Jchemo.sampdp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampdp(X, k::Int; metric = :eucl)</code></pre><p>Build training vs. test sets by DUPLEX sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>k</code> : Nb. pairs (training/test) of observations    to sample. Must be &lt;= n / 2. </li></ul><p>Keyword arguments:</p><ul><li><code>metric</code> : Metric used for the distance computation.   Possible values are: <code>:eucl</code> (Euclidean),    <code>:mah</code> (Mahalanobis).</li></ul><p>Three outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>k</code>), </li><li><code>test</code> (<code>k</code>),</li><li><code>remain</code> (n - 2 * <code>k</code>). </li></ul><p>Outputs <code>train</code> and <code>test</code> are built from the DUPLEX algorithm  (Snee, 1977 p.421). They are expected to cover approximately the same  X-space region and have similar statistical properties. </p><p>In practice, when output <code>remain</code> is not empty (i.e. when there  are remaining observations), one common strategy is to add  it to output <code>train</code>.</p><p><strong>References</strong></p><p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.</p><p>Snee, R.D., 1977. Validation of Regression Models: Methods and Examples.  Technometrics 19, 415-428. https://doi.org/10.1080/00401706.1977.10489581</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [0.381392  0.00175002 ; 0.1126    0.11263 ; 
    0.613296  0.152485 ; 0.726536  0.762032 ;
    0.367451  0.297398 ; 0.511332  0.320198 ; 
    0.018514  0.350678] 

k = 3
sampdp(X, k)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/sampdp.jl#LL1-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampks-Tuple{Any, Int64}" href="#Jchemo.sampks-Tuple{Any, Int64}"><code>Jchemo.sampks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampks(X, k::Int; metric = :eucl)</code></pre><p>Build training vs. test sets by Kennard-Stone sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>k</code> : Nb. test observations to sample.</li></ul><p>Keyword arguments: </p><ul><li><code>metric</code> : Metric used for the distance computation.   Possible values are: <code>:eucl</code> (Euclidean),    <code>:mah</code> (Mahalanobis).</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>n</code> - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built from the Kennard-Stone (KS)  algorithm (Kennard &amp; Stone, 1969). </p><p><strong>Note:</strong> By construction, the set of observations  selected by KS sampling contains higher variability than  the set of the remaining observations. In the seminal  article (K&amp;S, 1969), the algorithm is used to select observations that will be used to build a calibration set. To the opposite, in the present function, KS is used to select a test set with  higher variability than the training set. </p><p><strong>References</strong></p><p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc

k = 80
res = sampks(X, k)
pnames(res)
res.train 
res.test

mod = model(pcasvd; nlv = 15) 
fit!(mod, X) 
@head T = mod.fm.T
res = sampks(T, k; metric = :mah)

#####################

n = 10
k = 25 
X = [repeat(1:n, inner = n) repeat(1:n, outer = n)] 
X = Float64.(X) 
X .= X + .1 * randn(nro(X), nco(X))
s = sampks(X, k).test
f, ax = plotxy(X[:, 1], X[:, 2])
scatter!(ax, X[s, 1], X[s, 2]; color = &quot;red&quot;) 
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/sampks.jl#LL1-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.samprand-Tuple{Int64, Int64}" href="#Jchemo.samprand-Tuple{Int64, Int64}"><code>Jchemo.samprand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">samprand(n::Int, k::Int; replace = false)</code></pre><p>Build training vs. test sets by random sampling.  </p><ul><li><code>n</code> : Total nb. of observations.</li><li><code>k</code> : Nb. test observations to sample.</li></ul><p>Keyword arguments:</p><ul><li><code>replace</code> : Boolean. If <code>false</code>, the sampling is    without replacement.</li></ul><p>Two outputs are returned (= row indexes of the data): </p><ul><li><code>train</code> (<code>n</code> - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built by random sampling within <code>1:n</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 10
samprand(n, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/samprand.jl#LL1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampsys-Tuple{Any, Int64}" href="#Jchemo.sampsys-Tuple{Any, Int64}"><code>Jchemo.sampsys</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampsys(y, k::Int)</code></pre><p>Build training vs. test sets by systematic sampling      over a quantitative variable.  </p><ul><li><code>y</code> : Quantitative variable (n) to sample.</li><li><code>k</code> : Nb. test observations to sample.    Must be &gt;= 2.</li></ul><p>Two outputs are returned (= row indexes of the data): </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built by systematic sampling over the rank of  the <code>y</code> observations. For instance if <code>k</code> / n ~ .3, one observation  over three observations over the sorted <code>y</code> is selected. </p><p>Output <code>test</code> always contains the indexes of the minimum and  maximum of <code>y</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">y = rand(7)
[y sort(y)]
res = sampsys(y, 3)
sort(y[res.test])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/sampsys.jl#LL1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampwsp-Tuple{Any, Any}" href="#Jchemo.sampwsp-Tuple{Any, Any}"><code>Jchemo.sampwsp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampwsp(X, dmin; recod = false, maxit = nro(X))</code></pre><p>Build training vs. test sets by WSP sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>dmin</code> : Distance &quot;dmin&quot; (Santiago et al. 2012).</li></ul><p>Keyword arguments: </p><ul><li><code>recod</code> : Boolean indicating if <code>X</code> is recoded or not    before the sampling (see below).</li><li><code>maxit</code> : Maximum number of iterations.</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>n</code> - k),</li><li><code>test</code> (k). </li></ul><p>Output <code>test</code> is built from the &quot;Wootton, Sergent, Phan-Tan-Luu&quot; (WSP)  algorithm, assumed to generate samples uniformely distributed in the <code>X</code> domain  (Santiago et al. 2012).</p><p>If <code>recod = true</code>, each column x of <code>X</code> is recoded within [0, 1] and the center of  the domain is the vector <code>repeat([.5], p)</code>. Column x is recoded such as: </p><ul><li>vmin = minimum(x)</li><li>vmax = maximum(x)</li><li>vdiff = vmax - vmin</li><li>x .=  0.5 .+ (x .- (vdiff / 2 + vmin)) / vdiff</li></ul><p><strong>References</strong></p><p>Béal A. 2015. Description et sélection de données en grande dimensio. Thèse de doctorat. Laboratoire d’Instrumentation et de sciences analytiques, Ecole doctorale des siences chimiques, Université d&#39;Aix-Marseille.</p><p>Santiago, J., Claeys-Bruno, M., Sergent, M., 2012. Construction of space-filling  designs using WSP algorithm for high dimensional spaces.  Chemometrics and Intelligent Laboratory Systems, Selected Papers from  Chimiométrie 2010 113, 26–31. https://doi.org/10.1016/j.chemolab.2011.06.003</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 600 ; p = 2
X = rand(n, p)
dmin = .5
s = sampwsp(X, dmin)
pnames(res)
@show length(s.test)
plotxy(X[s.test, 1], X[s.test, 2]).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/sampwsp.jl#LL1-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.savgk-Tuple{Int64, Int64, Int64}" href="#Jchemo.savgk-Tuple{Int64, Int64, Int64}"><code>Jchemo.savgk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">savgk(nhwindow::Int, degree::Int, deriv::Int)</code></pre><p>Compute the kernel of the Savitzky-Golay filter.</p><ul><li><code>nhwindow</code> : Nb. points (&gt;= 1) of the half window.</li><li><code>degree</code> : Degree of the smoothing polynom, where  1 &lt;= <code>degree</code> &lt;= 2 * nhwindow.</li><li><code>deriv</code> : Derivation order, where 0 &lt;= <code>deriv</code> &lt;= degree.</li></ul><p>The size of the kernel is odd (npoint = 2 * nhwindow + 1): </p><ul><li>x[-nhwindow], x[-nhwindow+1], ..., x[0], ...., x[nhwindow-1], x[nhwindow].</li></ul><p>If <code>deriv</code> = 0, there is no derivation (only polynomial smoothing).</p><p>The case <code>degree</code> = 0 (i.e. simple moving average) is not  allowed by the funtion.</p><p><strong>References</strong></p><p>Luo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation  filter for even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">res = savgk(21, 3, 2)
pnames(res)
res.S 
res.G 
res.kern</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL297-L327">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.savgol-Tuple{Any}" href="#Jchemo.savgol-Tuple{Any}"><code>Jchemo.savgol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">savgol(X; kwargs...)</code></pre><p>Savitzky-Golay derivation and smoothing of each row of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>npoint</code> : Size of the filter (nb. points involved in    the kernel). Must be odd and &gt;= 3. The half-window size is    nhwindow = (<code>npoint</code> - 1) / 2.</li><li><code>degree</code> : Degree of the smoothing polynom.   Must be: 1 &lt;= <code>degree</code> &lt;= <code>npoint</code> - 1.</li><li><code>deriv</code> : Derivation order. Must be: 0 &lt;= <code>deriv</code> &lt;= <code>degree</code>.</li></ul><p>The smoothing is computed by convolution (with padding), using  function imfilter of package ImageFiltering.jl. Each returned point is  located on the center of the kernel. The kernel is computed with  function <code>savgk</code>.</p><p>The function returns a matrix (n, p).</p><p><strong>References</strong></p><p>Luo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation filter for  even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002</p><p>Savitzky, A., Golay, M.J.E., 2002. Smoothing and Differentiation of Data by Simplified Least  Squares Procedures. [WWW Document]. https://doi.org/10.1021/ac60214a047</p><p>Schafer, R.W., 2011. What Is a Savitzky-Golay Filter? [Lecture Notes].  IEEE Signal Processing Magazine 28, 111–117. https://doi.org/10.1109/MSP.2011.941097</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

npoint = 11 ; degree = 2 ; deriv = 2
mod = model(savgol; npoint, degree, deriv) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f

####### Gaussian signal 

u = -15:.1:15
n = length(u)
x = exp.(-.5 * u.^2) / sqrt(2 * pi) + .03 * randn(n)
M = 10  # half window
N = 3   # degree
deriv = 0
#deriv = 1
mod = model(savgol; npoint = 2M + 1, degree = N, deriv)
fit!(mod, x&#39;)
xp = transf(mod, x&#39;)
f, ax = plotsp(x&#39;, u; color = :blue)
lines!(ax, u, vec(xp); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL342-L411">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.scale-Tuple{Any}" href="#Jchemo.scale-Tuple{Any}"><code>Jchemo.scale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">scale(X)
scale(X, weights::Weight)</code></pre><p>Column-wise scaling of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(scale) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
colstd(Xptrain)
@head Xptest 
@head Xtest ./ colstd(Xtrain)&#39;
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scale.jl#LL59-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.segmkf-Tuple{Int64, Int64}" href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>Jchemo.segmkf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">segmkf(n::Int, K::Int; rep = 1)
segmkf(group::Vector, K::Int; rep = 1)</code></pre><p>Build segments of observations for K-fold cross-validation.  </p><ul><li><code>n</code> : Total nb. of observations in the dataset.    The sampling is implemented with 1:n.</li><li><code>group</code> : A vector (n) defining blocks of observations.</li><li><code>K</code> : Nb. folds (segments) splitting the <code>n</code> observations. </li></ul><p>Keyword arguments:</p><ul><li><code>rep</code> : Nb. replications of the sampling.</li></ul><p>For each replication, the function splits the <code>n</code> observations  tp <code>K</code> segments  that can be used for K-fold cross-validation. </p><p>If <code>group</code> is used (must be a vector of length n), the function  samples entire groups (= blocks) of observations instead of observations.  Such a block-sampling is required when data is structured by blocks and  when the response to predict is correlated within blocks.  This prevents underestimation of the generalization error.</p><p>The function returns a list (vector) of <code>rep</code> elements.  Each element of the list contains <code>K</code> segments (= <code>K</code> vectors). Each segment contains the indexes (position within 1:<code>n</code>) of  the sampled observations.    </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 10 ; K = 3
rep = 4 
segm = segmkf(n, K; rep)
i = 1 
segm[i]
segm[i][1]

n = 10 
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]    # blocks of the observations
tab(group) 
K = 3 ; rep = 4 
segm = segmkf(group, K; rep)
i = 1 
segm[i]
segm[i][1]
group[segm[i][1]]
group[segm[i][2]]
group[segm[i][3]]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/segmkf.jl#LL1-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.segmts-Tuple{Int64, Int64}" href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>Jchemo.segmts</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">segmts(n::Int, m::Int; rep = 1, seed = nothing)
segmts(group::Vector, m::Int; rep = 1, seed = nothing)</code></pre><p>Build segments of observations for &quot;test-set&quot; validation.</p><ul><li><code>n</code> : Total nb. of observations in the dataset.    The sampling  is implemented within 1:<code>n</code>.</li><li><code>group</code> : A vector (n) defining blocks of observations.</li><li><code>m</code> : Nb. test observations, or groups if <code>group</code> is used, returned    in each segment.</li></ul><p>Keyword arguments: </p><ul><li><code>rep</code> : Nb. replications of the sampling.</li><li><code>seed</code> : Eventual seed for the <code>Random.MersenneTwister</code>    generator. Must be of length = <code>rep</code>. When <code>nothing</code>,    the seed is random at each replication.</li></ul><p>For each replication, the function builds a test set that can  be used to validate a model. </p><p>If <code>group</code> is used (must be a vector of length n), the function  samples entire groups (= blocks) of observations instead of observations.  Such a block-sampling is required when data is structured by blocks and  when the response to predict is correlated within blocks.  This prevents underestimation of the generalization error.</p><p>The function returns a list (vector) of <code>rep</code> elements.  Each element of the list is a vector of the indexes (positions  within 1:<code>n</code>) of the sampled observations.  </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 10 ; m = 3
rep = 4 
segm = segmts(n, m; rep) 
i = 1
segm[i]
segm[i][1]

n = 10 
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]    # blocks of the observations
tab(group)  
m = 2 ; rep = 4 
segm = segmts(group, m; rep)
i = 1 
segm[i]
segm[i][1]
group[segm[i][1]]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/segmts.jl#LL1-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.selwold-Tuple{Any, Any}" href="#Jchemo.selwold-Tuple{Any, Any}"><code>Jchemo.selwold</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">selwold(indx, r; smooth = true, npoint = 5, alpha = .05, digits = 3, graph = true, 
    step = 2, xlabel = &quot;Index&quot;, ylabel = &quot;Value&quot;, title = &quot;Score&quot;)</code></pre><p>Wold&#39;s criterion to select dimensionality in LV models (e.g. PLSR).</p><ul><li><code>indx</code> : A variable representing the model parameter(s), e.g. nb. LVs if PLSR models.</li><li><code>r</code> : A vector of error rates (n), e.g. RMSECV.</li></ul><p>Keyword arguments:</p><ul><li><code>smooth</code> : Boolean. If <code>true</code>,  the selection is done    after a moving-average smoothing of rate R   (see function <code>mavg</code>).</li><li><code>npoint</code> : Window of the moving-average used to    smooth rate R.</li><li><code>alpha</code> : Proportion alpha used as threshold    for rate R.</li><li><code>digits</code> : Number of digits in the outputs.</li><li><code>graph</code> : Boolean. If <code>true</code>, outputs are plotted.</li><li><code>step</code> : Step used for defining the xticks    in the graphs.</li><li><code>xlabel</code> : Horizontal label for the plots.</li><li><code>ylabel</code> : Vertical label for the plots.</li><li><code>title</code> : Title of the left plot.</li></ul><p>The slection criterion is the &quot;precision gain ratio&quot;: </p><ul><li>R = 1 - <code>r</code>(a+1) / <code>r</code>(a)</li></ul><p>where <code>r</code> is an observed error rate quantifying the model  performance (e.g. RMSEP, classification error rate, etc.)  and a the model dimensionnality (= nb. LVs). <code>r</code> can also represent  other indicators such as the eigenvalues of a PCA.</p><p>R is the relative gain in perforamnce efficiency after a new LV  is added to the model. The iterations continue until R becomes lower  than a threshold value <code>alpha</code>. By default and only as an indication,  the default <code>alpha</code>=.05 is set in the function, but the user should set  any other value depending on his data and parsimony objective.</p><p>In his original article, Wold (1978; see also Bro et al. 2008) used  the ratio of cross-validated over training residual sums of squares,  i.e. PRESS over SSR. Instead, function <code>selwold</code> compares values of  consistent nature (the successive values in the input vector <code>r</code>).  For instance, <code>r</code> was set to PRESS values in Li et al. (2002) and  Andries et al. (2011), which is equivalent to the &quot;punish factor&quot;  described in Westad &amp; Martens (2000).</p><p>The ratio R can be erratic (particulary when <code>r</code> is the error rate  of a discrimination model), making difficult the dimensionnaly  selection. In such a situation, function <code>selwold</code> proposes to calculate a smoothing of R (argument <code>smooth</code>).</p><p>The function returns two outputs (in addition to eventual plots):</p><ul><li><code>opt</code> : The index corresponding to the minimum value of <code>r</code>.</li><li><code>sel</code> : The index of the selection from the R (or smoothed R)    threshold.</li></ul><p><strong>References</strong></p><p>Andries, J.P.M., Vander Heyden, Y., Buydens, L.M.C., 2011. Improved  variable reduction in partial least squares modelling based on  Predictive-Property-Ranked Variables and adaptation of partial least  squares complexity. Analytica Chimica Acta 705, 292-305.  https://doi.org/10.1016/j.aca.2011.06.037</p><p>Bro, R., Kjeldahl, K., Smilde, A.K., Kiers, H.A.L., 2008. Cross-validation  of component models: A critical look at current methods. Anal Bioanal Chem  390, 1241-1251. https://doi.org/10.1007/s00216-007-1790-1</p><p>Li, B., Morris, J., Martin, E.B., 2002. Model selection for partial least  squares regression. Chemometrics and Intelligent Laboratory Systems 64, 79-89.  https://doi.org/10.1016/S0169-7439(02)00051-5</p><p>Westad, F., Martens, H., 2000. Variable Selection in near Infrared Spectroscopy  Based on Significance Testing in Partial Least Squares Regression. J. Near Infrared  Spectrosc., JNIRS 8, 117â124.</p><p>Wold S. Cross-Validatory Estimation of the Number of Components in Factor  and Principal Components Models. Technometrics. 1978;20(4):397-405</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
n = nro(Xtrain)

segm = segmts(n, 50; rep = 30)
mod = model(plskern)
nlv = 0:20
res = gridcv(mod, Xtrain, ytrain; segm, score = rmsep, nlv).res
res[res.y1 .== minimum(res.y1), :]
plotgrid(res.nlv, res.y1;xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f
zres = selwold(res.nlv, res.y1; smooth = true, graph = true) ;
@show zres.opt
@show zres.sel
zres.f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/selwold.jl#LL1-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sep-Tuple{Any, Any}" href="#Jchemo.sep-Tuple{Any, Any}"><code>Jchemo.sep</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sep(pred, Y)</code></pre><p>Compute the corrected SEP (&quot;SEP_c&quot;), i.e. the standard deviation of      the prediction errors.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>References</strong></p><p>Bellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B.,  Roger, J.-M., McBratney, A., 2010. Critical review of  chemometric indicators commonly used for assessing the  quality of the prediction of soil attributes by NIR  spectroscopy. TrAC Trends in Analytical Chemistry 29,  1073–1081.  https://doi.org/10.1016/j.trac.2010.05.006</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
sep(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
sep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL310-L345">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.snorm-Tuple{Any}" href="#Jchemo.snorm-Tuple{Any}"><code>Jchemo.snorm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">snorm(X)</code></pre><p>Row-wise norming of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Each row of <code>X</code> is divide by its norm.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

mod = model(snorm) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f
rownorm(Xptrain)
rownorm(Xptest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL454-L486">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.snv-Tuple{Any}" href="#Jchemo.snv-Tuple{Any}"><code>Jchemo.snv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">snv(X; kwargs...)</code></pre><p>Standard-normal-variate (SNV) transformation of each row of X-data.</p><ul><li><code>X</code> : X-data (n, p).</li></ul><p>Keyword arguments:</p><ul><li><code>centr</code> : Boolean indicating if the centering in done.</li><li><code>scal</code> : Boolean indicating if the scaling in done.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
year = dat.Y.year
s = year .&lt;= 2012
Xtrain = X[s, :]
Xtest = rmrow(X, s)
wlst = names(dat.X)
wl = parse.(Float64, wlst)
plotsp(dat.X, wl; nsamp = 20).f

centr = true ; scal = true
mod = model(snv; centr, scal) 
fit!(mod, Xtrain)
Xptrain = transf(mod, Xtrain)
Xptest = transf(mod, Xtest)
plotsp(Xptrain).f
plotsp(Xptest).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL508-L540">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.soft-Tuple{Any, Any}" href="#Jchemo.soft-Tuple{Any, Any}"><code>Jchemo.soft</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">soft(x::Real, delta)</code></pre><p>Soft thresholding function.</p><ul><li><code>x</code> : Value to transform.</li><li><code>delta</code> : Range for the thresholding.</li></ul><p>The returned value is:</p><ul><li>sign(x) * max(0, abs(x) - delta)</li></ul><p>where delta &gt;= 0.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CairoMakie 

delta = .2
soft(3, delta)

x = LinRange(-2, 2, 100)
y = soft.(x, delta)
lines(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL933-L954">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.softmax-Tuple{AbstractVector}" href="#Jchemo.softmax-Tuple{AbstractVector}"><code>Jchemo.softmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">softmax(x::AbstractVector)
softmax(X::Union{Matrix, DataFrame})</code></pre><p>Softmax function.</p><ul><li><code>x</code> : A vector to transform.</li><li><code>X</code> : A matrix whose rows are transformed.</li></ul><p>Let v be a vector:</p><ul><li>&#39;softmax&#39;(v) = exp.(v) / sum(exp.(v))</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = 1:3
softmax(x)

X = rand(5, 3)
softmax(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL960-L978">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.soplsr-Tuple{Any, Any}" href="#Jchemo.soplsr-Tuple{Any, Any}"><code>Jchemo.soplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">soplsr(Xbl, Y; kwargs...)
soplsr(Xbl, Y, weights::Weight; kwargs...)
soplsr!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Multiblock sequentially orthogonalized PLSR (SO-PLSR).</p><ul><li><code>Xbl</code> : List of blocks (vector of matrices) of X-data    Typically, output of function <code>mblock</code> from (n, p) data.  </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    and <code>Y</code> is scaled by its uncorrected standard deviation.</li></ul><p><strong>References</strong></p><p>Biancolillo et al. , 2015. Combining SO-PLS and linear  discriminant analysis for multi-block classification.  Chemometrics and Intelligent Laboratory Systems, 141, 58-67.</p><p>Biancolillo, A. 2016. Method development in the area of  multi-block analysis focused on food analysis. PhD.  University of copenhagen.</p><p>Menichelli et al., 2014. SO-PLS as an exploratory tool for path modelling. Food Quality and Preference, 36, 122-134.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;ham.jld2&quot;) 
@load db dat
pnames(dat) 
X = dat.X
Y = dat.Y
y = Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
s = 1:6
Xbltrain = mblock(X[s, :], listbl)
Xbltest = mblock(rmrow(X, s), listbl)
ytrain = y[s]
ytest = rmrow(y, s) 
ntrain = nro(ytrain) 
ntest = nro(ytest) 
ntot = ntrain + ntest 
(ntot = ntot, ntrain , ntest)

nlv = 2
#nlv = [2, 1, 2]
#nlv = [2, 0, 1]
scal = false
#scal = true
mod = model(soplsr; nlv, scal)
fit!(mod, Xbltrain, ytrain)
pnames(mod) 
pnames(mod.fm)
@head mod.fm.T
@head transf(mod, Xbltrain)
transf(mod, Xbltest)

res = predict(mod, Xbltest)
res.pred 
rmsep(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/soplsr.jl#LL1-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sourcedir-Tuple{Any}" href="#Jchemo.sourcedir-Tuple{Any}"><code>Jchemo.sourcedir</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sourcedir(path)</code></pre><p>Include all the files contained in a directory.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL994-L997">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.spca-Tuple{Any}" href="#Jchemo.spca-Tuple{Any}"><code>Jchemo.spca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">spca(X; kwargs...)
spca(X, weights::Weight; kwargs...)
spca!(X::Matrix, weights::Weight; kwargs...)</code></pre><p>Sparse PCA (Shen &amp; Huang 2008).</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>msparse</code> : Method used for the sparse thresholding.    Possible values are: <code>:soft</code>, <code>:mix</code>,    <code>:hard</code>. See thereafter.</li><li><code>delta</code> : Only used if <code>msparse = :soft</code>. Range for the    thresholding on the loadings (after they are standardized    to their maximal absolute value). Must ∈ [0, 1].   Higher is <code>delta</code>, stronger is the thresholding. </li><li><code>nvar</code> : Only used if <code>msparse = :mix</code> or <code>msparse = :hard</code>.   Nb. variables (<code>X</code>-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. of Nipals iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Sparse principal component analysis via regularized low rank  matrix approximation (Shen &amp; Huang 2008). A Nipals algorithm is used.  The Function provides three methods of thresholding to compute  the sparse loadings:</p><ul><li><p><code>msparse = :soft</code>: Soft thresholding of standardized loadings.    Let us note v a given loading vector before thresholding.    Vector abs(v) is then standardized to its maximal component    (= max{abs(v[i]), i = 1..p}). The soft-thresholding function    (see function <code>soft</code>) is applied to this standardized vector,    with the constant <code>delta</code> ∈ [0, 1]. This returns the sparse    vector <code>theta</code>. Vector v is multiplied term-by-term by this vector   <code>theta</code>, which finally gives the sparse loadings.</p></li><li><p><code>msparse = :mix</code>: Method used in function <code>spca</code> of the R    package <code>mixOmics</code> (Lê Cao et al.). For each PC, the <code>nvar</code>    <code>X</code>-variables showing the largest values in vector abs(v)    are selected. Then a soft-thresholding is applied to the    corresponding selected loadings. Range <code>delta</code> is automatically   (internally) set equal to the maximal value of the components    of abs(v) corresponding to variables removed from the selection.  </p></li><li><p><code>msparse = :hard</code>: For each PC, the <code>nvar</code> <code>X</code>-variables showing    the largest values in vector abs(v) are selected.</p></li></ul><p>The case <code>msparse = :mix</code> returns the same results as function  <code>spca</code> of the R package mixOmics.</p><p><strong>Note:</strong> The resulting sparse loadings vectors (<code>P</code>-columns)  are in general non orthogonal. Therefore, there is no a unique  decomposition of the variance of <code>X</code> such as in PCA.  Function <code>summary</code> returns the following objects:</p><ul><li><code>explvarx</code>: The proportion of variance of <code>X</code> explained    by each column t of <code>T</code>, computed by regressing <code>X</code>    on t (such as what is done in PLS).</li><li><code>explvarx_adj</code>: Adjusted explained variance proposed by    Shen &amp; Huang 2008 section 2.3.    </li></ul><p><strong>References</strong></p><p>Kim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien  Dejean with key contributors Benoit Gautier, Francois Bartolo,  contributions from Pierre Monget, Jeff Coquery, FangZou Yao  and Benoit Liquet. (2016). mixOmics: Omics Data Integration  Project. R package version 6.1.1.  https://CRAN.R-project.org/package=mixOmics</p><p>https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Shen, H., Huang, J.Z., 2008. Sparse principal component  analysis via regularized low rank matrix approximation.  Journal of Multivariate Analysis 99, 1015–1034.  https://doi.org/10.1016/j.jmva.2007.06.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
@head dat.X
X = dat.X[:, 1:4]
n = nro(X)
ntest = 30
s = samprand(n, ntest) 
Xtrain = X[s.train, :]
Xtest = X[s.test, :]

nlv = 3 
msparse = :mix ; nvar = 2
#msparse = :hard ; nvar = 2
scal = false
mod = model(spca; nlv, msparse, nvar, scal) ;
fit!(mod, Xtrain) 
fm = mod.fm ;
pnames(fm)
fm.niter
fm.sellv 
fm.sel
fm.P
fm.P&#39; * fm.P
@head T = fm.T
@head transf(mod, Xtrain)

@head Ttest = transf(fm, Xtest)

res = summary(mod, Xtrain) ;
res.explvarx
res.explvarx_adj

nlv = 3 
msparse = :soft ; delta = .4 
mod = model(spca; nlv, msparse, delta) ;
fit!(mod, Xtrain) 
mod.fm.P</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/spca.jl#LL1-L122">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splskdeda-Tuple{Any, Any}" href="#Jchemo.splskdeda-Tuple{Any, Any}"><code>Jchemo.splskdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">splskdeda(X, y; kwargs...)
splskdeda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLS-KDE-DA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>msparse</code> : Method used for the sparse thresholding.    Possible values are: <code>:soft</code>, <code>:mix</code>,    <code>:hard</code>. See thereafter.</li><li><code>delta</code> : Only used if <code>msparse = :soft</code>. Range for the    thresholding on the loadings (after they are standardized    to their maximal absolute value). Must ∈ [0, 1].   Higher is <code>delta</code>, stronger is the thresholding. </li><li><code>nvar</code> : Only used if <code>msparse = :mix</code> or <code>msparse = :hard</code>.   Nb. variables (<code>X</code>-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li>Keyword arguments of function <code>dmkern</code> (bandwidth    definition) can also be specified here.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plskdeda</code> (PLS-LDA) except that  a sparse PLSR (function <code>splskern</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>splslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/splskdeda.jl#LL1-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splskern-Tuple{Any, Any}" href="#Jchemo.splskern-Tuple{Any, Any}"><code>Jchemo.splskern</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">splskern(X, Y; kwargs...)
splskern(X, Y, weights::Weight; kwargs...)
splskern!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)</code></pre><p>Sparse partial least squares regression (Lê Cao et al. 2008)</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>).</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>msparse</code> : Method used for the sparse thresholding.    Possible values are: <code>:soft</code>, <code>:mix</code>,    <code>:hard</code>. See thereafter.</li><li><code>delta</code> : Only used if <code>msparse = :soft</code>. Range for the    thresholding on the loadings (after they are standardized    to their maximal absolute value). Must ∈ [0, 1].   Higher is <code>delta</code>, stronger is the thresholding. </li><li><code>nvar</code> : Only used if <code>msparse = :mix</code> or <code>msparse = :hard</code>.   Nb. variables (<code>X</code>-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Sparse partial least squares regression (Lê Cao et al. 2008), with  the fast &quot;improved kernel algorithm #1&quot; of Dayal &amp; McGregor (1997). </p><p>In the present version of <code>splskern</code>, the sparse correction  only concerns <code>X</code>. The function provides three methods of  thresholding to compute the sparse <code>X</code>-loading weights w,  see function <code>spca</code> for description (same principles). The case  <code>msparse = :mix</code> returns the same results as function <code>spls</code> of  the R package mixOmics with the regression mode (and without sparseness  on <code>Y</code>).</p><p>The case <code>msparse = :hard</code> (or <code>msparse = :mix</code>) and <code>nvar = 1</code> correspond  to the COVSEL regression described in Roger et al 2011 (see also Höskuldsson 1992).</p><p><strong>References</strong></p><p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.</p><p>Höskuldsson, A., 1992. The H-principle in modelling with applications  to chemometrics. Chemometrics and Intelligent Laboratory Systems,  Proceedings of the 2nd Scandinavian Symposium on Chemometrics 14,  139–153. https://doi.org/10.1016/0169-7439(92)80099-P</p><p>Lê Cao, K.-A., Rossouw, D., Robert-Granié, C., Besse, P., 2008.  A Sparse PLS for Variable Selection when Integrating Omics Data.  Statistical Applications in Genetics and Molecular Biology 7.  https://doi.org/10.2202/1544-6115.1390</p><p>Kim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien Dejean  with key contributors Benoit Gautier, Francois Bartolo, contributions  from Pierre Monget, Jeff Coquery, FangZou Yao and Benoit Liquet.  (2016). mixOmics: Omics Data Integration Project. R package  version 6.1.1. https://CRAN.R-project.org/package=mixOmics</p><p>https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Roger, J.M., Palagos, B., Bertrand, D., Fernandez-Ahumada, E., 2011.  covsel: Variable selection for highly multivariate and multi-response  calibration: Application to IR spectroscopy.  Chem. Lab. Int. Syst. 106, 216-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
msparse = :mix ; nvar = 5
#msparse = :hard ; nvar = 5
mod = model(splskern; nlv, msparse, nvar) ;
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
@head mod.fm.T
@head mod.fm.W

coef(mod)
coef(mod; nlv = 3)

@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

res = summary(mod, Xtrain) ;
pnames(res)
z = res.explvarx
plotgrid(z.nlv, z.cumpvar; step = 2, xlabel = &quot;Nb. LVs&quot;, 
    ylabel = &quot;Prop. Explained X-Variance&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/splskern.jl#LL1-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splslda-Tuple{Any, Any}" href="#Jchemo.splslda-Tuple{Any, Any}"><code>Jchemo.splslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">splslda(X, y; kwargs...)
splslda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLS-LDA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>msparse</code> : Method used for the sparse thresholding.    Possible values are: <code>:soft</code>, <code>:mix</code>,    <code>:hard</code>. See thereafter.</li><li><code>delta</code> : Only used if <code>msparse = :soft</code>. Range for the    thresholding on the loadings (after they are standardized    to their maximal absolute value). Must ∈ [0, 1].   Higher is <code>delta</code>, stronger is the thresholding. </li><li><code>nvar</code> : Only used if <code>msparse = :mix</code> or <code>msparse = :hard</code>.   Nb. variables (<code>X</code>-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plslda</code> (PLS-LDA) except that  a sparse PLSR (function <code>splskern</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
msparse = :mix ; nvar = 10
mod = model(splslda; nlv, msparse, nvar) 
#mod = model(splsqda; nlv, msparse, nvar, alpha = .1) 
#mod = model(splskdeda; nlv, msparse, nvar, a_kde = .9) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

fmpls = fm.fm.fmpls ; 
@head fmpls.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fmpls)
summary(fmpls, Xtrain)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/splslda.jl#LL1-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splsqda-Tuple{Any, Any}" href="#Jchemo.splsqda-Tuple{Any, Any}"><code>Jchemo.splsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">splsqda(X, y; kwargs...)
splsqda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLS-QDA (with continuum).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.   Must be &gt;= 1.</li><li><code>msparse</code> : Method used for the sparse thresholding.    Possible values are: <code>:soft</code>, <code>:mix</code>,    <code>:hard</code>. See thereafter.</li><li><code>delta</code> : Only used if <code>msparse = :soft</code>. Range for the    thresholding on the loadings (after they are standardized    to their maximal absolute value). Must ∈ [0, 1].   Higher is <code>delta</code>, stronger is the thresholding. </li><li><code>nvar</code> : Only used if <code>msparse = :mix</code> or <code>msparse = :hard</code>.   Nb. variables (<code>X</code>-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>) and LDA (<code>alpha = 1</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsqda</code> (PLS-LDA) except that  a sparse PLSR (function <code>splskern</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>splslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/splsqda.jl#LL1-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splsrda-Tuple{Any, Any}" href="#Jchemo.splsrda-Tuple{Any, Any}"><code>Jchemo.splsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">splsrda(X, y; kwargs...)
splsrda(X, y, weights::Weight; kwargs...)</code></pre><p>Sparse PLSR-DA.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li><li><code>weights</code> : Weights (n) of the observations.    Must be of type <code>Weight</code> (see e.g. function <code>mweight</code>). </li></ul><p>Keyword arguments: </p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>msparse</code> : Method used for the sparse thresholding.    Possible values are: <code>:soft</code>, <code>:mix</code>,    <code>:hard</code>. See thereafter.</li><li><code>delta</code> : Only used if <code>msparse = :soft</code>. Range for the    thresholding on the loadings (after they are standardized    to their maximal absolute value). Must ∈ [0, 1].   Higher is <code>delta</code>, stronger is the thresholding. </li><li><code>nvar</code> : Only used if <code>msparse = :mix</code> or <code>msparse = :hard</code>.   Nb. variables (<code>X</code>-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length <code>nlv</code>.   </li><li><code>prior</code> : Type of prior probabilities for class    membership. Possible values are: <code>:unif</code> (uniform),    <code>:prop</code> (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as <code>mlev(x)</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that  a sparse PLSR (function <code>splskern</code>), instead of a  PLSR (function <code>plskern</code>), is run on the Y-dummy table. </p><p>See function <code>plsrda</code> and <code>splskern</code> for details.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X)
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

nlv = 15
msparse = :mix ; nvar = 10
mod = model(splsrda; nlv, msparse, nvar) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

@head fm.fm.T
@head transf(mod, Xtrain)
@head transf(mod, Xtest)
@head transf(mod, Xtest; nlv = 3)

coef(fm.fm)

res = predict(mod, Xtest) ;
pnames(res)
@head res.posterior
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt

predict(mod, Xtest; nlv = 1:2).pred
summary(fm.fm, Xtrain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/splsrda.jl#LL1-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ssq-Tuple{Any}" href="#Jchemo.ssq-Tuple{Any}"><code>Jchemo.ssq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ssq(X)</code></pre><p>Compute the total inertia of a matrix.</p><ul><li><code>X</code> : Matrix.</li></ul><p>Sum of all the squared components of <code>X</code> (= <code>norm(X)^2</code>; Squared Frobenius norm). </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
ssq(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1005-L1017">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ssr-Tuple{Any, Any}" href="#Jchemo.ssr-Tuple{Any, Any}"><code>Jchemo.ssr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ssr(pred, Y)</code></pre><p>Compute the sum of squared prediction errors (SSR).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, Ytrain)
pred = predict(mod, Xtest).pred
ssr(pred, Ytest)

mod = model(plskern; nlv = 2)
fit!(mod, Xtrain, ytrain)
pred = predict(mod, Xtest).pred
ssr(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scores.jl#LL348-L373">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.summ-Tuple{Any}" href="#Jchemo.summ-Tuple{Any}"><code>Jchemo.summ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summ(X; digits = 3)
summ(X, y; digits = 3)</code></pre><p>Summarize a dataset (or a variable).</p><ul><li><code>X</code> : A dataset (n, p).</li><li><code>y</code> : A categorical variable (n) (class membership).</li><li><code>digits</code> : Nb. digits in the outputs.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 50
X = rand(n, 3) 
y = rand(1:3, n)
res = summ(X)
pnames(res)
summ(X[:, 2]).res

summ(X, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1023-L1042">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.svmda-Tuple{Any, Any}" href="#Jchemo.svmda-Tuple{Any, Any}"><code>Jchemo.svmda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">svmda(X, y; kwargs...)</code></pre><p>Support vector machine for discrimination &quot;C-SVC&quot; (SVM-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership (n).</li></ul><p>Keyword arguments:</p><ul><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>, <code>:klin</code>,    <code>:ktanh</code>. See below.  </li><li><code>gamma</code> : <code>kern</code> parameter, see below.</li><li><code>degree</code> : <code>kern</code> parameter, see below.</li><li><code>coef0</code> : <code>kern</code> parameter, see below.</li><li><code>cost</code> : Cost of constraints violation C parameter.</li><li><code>epsilon</code> : Epsilon parameter in the loss function.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Kernel types: </p><ul><li>:krbf – radial basis function: exp(-gamma * ||x - y||^2)</li><li>:kpol – polynomial: (gamma * x&#39; * y + coef0)^degree</li><li>&quot;klin – linear: x&#39; * y</li><li>:ktan – sigmoid: tanh(gamma * x&#39; * y + coef0)</li></ul><p>The function uses LIBSVM.jl (https://github.com/JuliaML/LIBSVM.jl)  that is an interface to library LIBSVM (Chang &amp; Li 2001).</p><p><strong>References</strong></p><p>Julia package LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl</p><p>Chang, C.-C. &amp; Lin, C.-J. (2001). LIBSVM: a library for support  vector machines. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation  (algorithms, formulae, ...) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</p><p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support  vector machines. ACM Transactions on Intelligent Systems and  Technology, 2:27:1–27:27, 2011. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm</p><p>Schölkopf, B., Smola, A.J., 2002. Learning with kernels:  support vector machines, regularization, optimization, and  beyond. Adaptive computation and machine learning. MIT Press,  Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y
n = nro(X) 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = n, ntrain, ntest)
tab(ytrain)
tab(ytest)

kern = :krbf ; gamma = 1e4
cost = 1000 ; epsilon = .5
mod = model(svmda; kern, gamma, cost, epsilon) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)
fm = mod.fm ;
fm.lev
fm.ni

res = predict(mod, Xtest) ; 
pnames(res) 
@head res.pred
errp(res.pred, ytest)
conf(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/svmda.jl#LL1-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.svmr-Tuple{Any, Any}" href="#Jchemo.svmr-Tuple{Any, Any}"><code>Jchemo.svmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">svmr(X, y; kwargs...)</code></pre><p>Support vector machine for regression (Epsilon-SVR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are: <code>:krbf</code>, <code>:kpol</code>, <code>:klin</code>,    <code>:ktanh</code>. See below.  </li><li><code>gamma</code> : <code>kern</code> parameter, see below.</li><li><code>degree</code> : <code>kern</code> parameter, see below.</li><li><code>coef0</code> : <code>kern</code> parameter, see below.</li><li><code>cost</code> : Cost of constraints violation C parameter.</li><li><code>epsilon</code> : Epsilon parameter in the loss function.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Kernel types: </p><ul><li>:krbf – radial basis function: exp(-gamma * ||x - y||^2)</li><li>:kpol – polynomial: (gamma * x&#39; * y + coef0)^degree</li><li>&quot;klin – linear: x&#39; * y</li><li>:ktan – sigmoid: tanh(gamma * x&#39; * y + coef0)</li></ul><p>The function uses LIBSVM.jl (https://github.com/JuliaML/LIBSVM.jl)  that is an interface to library LIBSVM (Chang &amp; Li 2001).</p><p><strong>References</strong></p><p>Julia package LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl</p><p>Chang, C.-C. &amp; Lin, C.-J. (2001). LIBSVM: a library for support  vector machines. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation  (algorithms, formulae, ...) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</p><p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support  vector machines. ACM Transactions on Intelligent Systems and  Technology, 2:27:1–27:27, 2011. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm</p><p>Schölkopf, B., Smola, A.J., 2002. Learning with kernels:  support vector machines, regularization, optimization, and  beyond. Adaptive computation and machine learning. MIT Press,  Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
p = nco(X)

kern = :krbf ; gamma = .1
cost = 1000 ; epsilon = 1
mod = model(svmr; kern, gamma, cost, epsilon) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
kern = :krbf ; gamma = .1
mod = model(svmr; kern, gamma) 
fit!(mod, x, y)
pred = predict(mod, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;Fitted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/svmr.jl#LL1-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.tab-Tuple{Any}" href="#Jchemo.tab-Tuple{Any}"><code>Jchemo.tab</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tab(x)</code></pre><p>Univariate tabulation.</p><ul><li><code>x</code> : Categorical variable.</li></ul><p>The output cointains sorted levels.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand([&quot;a&quot;;&quot;b&quot;;&quot;c&quot;], 20)
res = tab(x)
res.keys
res.vals</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1067-L1081">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.tabdf-Tuple{Any}" href="#Jchemo.tabdf-Tuple{Any}"><code>Jchemo.tabdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tabdf(X; groups = nothing)</code></pre><p>Compute the nb. occurences in categorical variables of a dataset.</p><ul><li><code>X</code> : Data.</li><li><code>groups</code> : Vector of the names of the group variables to consider    in <code>X</code> (by default: all the columns of <code>X</code>).</li></ul><p>The output (dataframe) contains sorted levels.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 20
X =  hcat(rand(1:2, n), rand([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], n))
tabdf(X)
tabdf(X[:, 2])

df = DataFrame(X, [:v1, :v2])
tabdf(df)
tabdf(df; groups = [:v1, :v2])
tabdf(df; groups = :v2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1084-L1105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.tabdupl-Tuple{Any}" href="#Jchemo.tabdupl-Tuple{Any}"><code>Jchemo.tabdupl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tabdupl(x)</code></pre><p>Tabulate duplicated values in a vector.</p><ul><li><code>x</code> : Categorical variable.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;]
tab(x)
res = tabdupl(x)
res.keys
res.vals</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1118-L1131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Blockscal, Any}" href="#Jchemo.transf-Tuple{Jchemo.Blockscal, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Blockscal, Xbl)
transf!(object::Blockscal, Xbl)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/blockscal.jl#LL89-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Center, Any}" href="#Jchemo.transf-Tuple{Jchemo.Center, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Center, X)
transf!(object::Center, X::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scale.jl#LL43-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Comdim, Any}" href="#Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Comdim, Xbl; nlv = nothing)
transfbl(object::Comdim, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from      a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/comdim.jl#LL214-L224">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Cscale, Any}" href="#Jchemo.transf-Tuple{Jchemo.Cscale, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Cscale, X)
transf!(object::Cscale, X::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scale.jl#LL164-L171">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Detrend, Any}" href="#Jchemo.transf-Tuple{Jchemo.Detrend, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Detrend, X)
transf!(object::Detrend, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL40-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Dkplsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/dkplsr.jl#LL122-L129">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Fdif, Any}" href="#Jchemo.transf-Tuple{Jchemo.Fdif, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Fdif, X)
transf!(object::Fdif, X::Matrix, M::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li><li><code>M</code> : Pre-allocated output matrix (n, p - npoint + 1).</li></ul><p>The in-place function stores the output in <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL112-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Interpl, Any}" href="#Jchemo.transf-Tuple{Jchemo.Interpl, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Interpl, X)
transf!(object::Interpl, X::Matrix, M::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li><li><code>M</code> : Pre-allocated output matrix (n, p).</li></ul><p>The in-place function stores the output in <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL188-L197">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Kpca, Any}" href="#Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Kpca, X; nlv = nothing)</code></pre><p>Compute PCs (scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kpca.jl#LL108-L115">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Kplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Kplsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/kplsr.jl#LL170-L177">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Mavg, Any}" href="#Jchemo.transf-Tuple{Jchemo.Mavg, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Mavg, X)
transf!(object::Mavg, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL269-L276">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}" href="#Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Mbconcat, Xbl)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbconcat.jl#LL29-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Mbpca, Any}" href="#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Mbpca, Xbl; nlv = nothing)
transfbl(object::Mbpca, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from      a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbpca.jl#LL190-L200">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Mbplslda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Mbplslda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Mbplslda, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from      a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplslda.jl#LL95-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Mbplsrda, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsrda.jl#LL88-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Pcr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Pcr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Pcr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcr.jl#LL90-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Plslda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Plslda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from      a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plslda.jl#LL109-L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Plsrda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Plsrda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from      a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plsrda.jl#LL100-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Rmgap, Any}" href="#Jchemo.transf-Tuple{Jchemo.Rmgap, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Rmgap, X)
transf!(object::Rmgap, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rmgap.jl#LL51-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Rosaplsr, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rosaplsr.jl#LL201-L209">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Rp, Any}" href="#Jchemo.transf-Tuple{Jchemo.Rp, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Rp, X; nlv = nothing)</code></pre><p>Compute scores T from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which scores T are computed.</li><li><code>nlv</code> : Nb. scores to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rp.jl#LL67-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Savgol, Any}" href="#Jchemo.transf-Tuple{Jchemo.Savgol, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Savgol, X)
transf!(object::Savgol, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL417-L424">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Scale, Any}" href="#Jchemo.transf-Tuple{Jchemo.Scale, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Scale, X)
transf!(object::Scale, X::Matrix)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/scale.jl#LL101-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Snorm, Any}" href="#Jchemo.transf-Tuple{Jchemo.Snorm, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Snorm, X)
transf!(object::Snorm, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL491-L498">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Snv, Any}" href="#Jchemo.transf-Tuple{Jchemo.Snv, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Snv, X)
transf!(object::Snv, X)</code></pre><p>Compute the preprocessed data from a model.</p><ul><li><code>object</code> : Model.</li><li><code>X</code> : X-data to transform.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/preprocessing.jl#LL546-L553">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Soplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Soplsr, Xbl)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/soplsr.jl#LL126-L133">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Spca, Any}" href="#Jchemo.transf-Tuple{Jchemo.Spca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Spca, X; nlv = nothing)
Compute principal components (PCs = scores T) from a 
    fitted model and X-data.</code></pre><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/spca.jl#LL188-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}" href="#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Union{Pca, Fda}, X; nlv = nothing)</code></pre><p>Compute principal components (PCs = scores T) from a      fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/pcasvd.jl#LL100-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}" href="#Jchemo.transf-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Union{Mbplsr, Mbplswest}, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list of blocks (vector of matrices)    of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/mbplsr.jl#LL104-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}" href="#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Union{Plsr, Splsr}, 
    X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from      a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plskern.jl#LL171-L180">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}" href="#Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transfbl(object::Cca, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/cca.jl#LL172-L180">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}" href="#Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transfbl(object::Ccawold, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/ccawold.jl#LL235-L243">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}" href="#Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transfbl(object::Plscan, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plscan.jl#LL176-L184">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}" href="#Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transfbl(object::Plstuck, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/plstuck.jl#LL115-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}" href="#Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}"><code>Jchemo.transfbl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transfbl(object::Rasvd, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/rasvd.jl#LL164-L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.treer_dt-Tuple{Any, Any}" href="#Jchemo.treer_dt-Tuple{Any, Any}"><code>Jchemo.treer_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">treer_dt(X, y; kwargs...)</code></pre><p>Regression tree (CART) with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate y-data (n).</li></ul><p>Keyword arguments:</p><ul><li><code>n_subfeatures</code> : Nb. variables to select at random    at each split (default: 0 ==&gt; keep all).</li><li><code>max_depth</code> : Maximum depth of the    decision tree (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples    each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations    in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The function fits a single regression tree (CART) using  package `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., Friedman, J. H., Olshen, R. A., and  Stone, C. J. Classification And Regression Trees.  Chapman &amp; Hall, 1984.</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
p = nco(X)

n_subfeatures = p / 3 
max_depth = 15
mod = model(treer_dt; n_subfeatures, max_depth) 
fit!(mod, Xtrain, ytrain)
pnames(mod)
pnames(mod.fm)

res = predict(mod, Xtest)
@head res.pred
@show rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/treer_dt.jl#LL1-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vcatdf-Tuple{Any}" href="#Jchemo.vcatdf-Tuple{Any}"><code>Jchemo.vcatdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vcatdf(dat; cols = :intersect)</code></pre><p>Vertical concatenation of a list of dataframes.</p><ul><li><code>dat</code> : List (vector) of dataframes.</li><li><code>cols</code> : Determines the columns of the returned data frame.   See ?DataFrames.vcat.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrames
dat1 = DataFrame(rand(5, 2), [:v3, :v1]) 
dat2 = DataFrame(100 * rand(2, 2), [:v3, :v1])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)

dat2 = DataFrame(100 * rand(2, 2), [:v1, :v3])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)

dat2 = DataFrame(100 * rand(2, 3), [:v3, :v1, :a])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)
Jchemo.vcatdf(dat; cols = :union)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1139-L1163">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vcol-Tuple{Any, Any}" href="#Jchemo.vcol-Tuple{Any, Any}"><code>Jchemo.vcol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vcol(X::AbstractMatrix, j)
vcol(X::DataFrame, j)
vcol(x::Vector, j)</code></pre><p>View of the j-th column(s) of a matrix <code>X</code>, or of the j-th element(s) of vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1177-L1183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}" href="#Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}"><code>Jchemo.vip</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vip(object::Union{Pcr, Plsr}; nlv = nothing)
vip(object::Union{Pcr, Plsr}, Y; nlv = nothing)</code></pre><p>Variable importance on Projections (VIP).</p><ul><li><code>object</code> : The fitted model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. latent variables (LVs) to consider.   If <code>nothing</code>, the maximal model is considered.</li></ul><p>For a PLS model  (or PCR, etc.) fitted on (X, Y) with a number  of A latent variables, and for variable xj (column j of X): </p><ul><li>VIP(xj) = Sum.a(1,...,A) R2(Yc, ta) waj^2 / Sum.a(1,...,A) R2(Yc, ta) (1 / p) </li></ul><p>where:</p><ul><li>Yc is the centered Y, </li><li>ta is the a-th X-score, </li><li>R2(Yc, ta) is the proportion of Yc-variance explained    by ta, i.e. ||Yc.hat||^2 / ||Yc||^2 (where Yc.hat is the    LS estimate of Yc by ta).  </li></ul><p>When <code>Y</code> is used, R2(Yc, ta) is replaced by the redundancy Rd(Yc, ta) (see function <code>rd</code>), such as in Tenenhaus 1998 p.139. </p><p><strong>References</strong></p><p>Chong, I.-G., Jun, C.-H., 2005. Performance of some variable selection  methods when multicollinearity is present. Chemometrics and Intelligent  Laboratory Systems 78, 103–112. https://doi.org/10.1016/j.chemolab.2004.12.011</p><p>Mehmood, T., Sæbø, S., Liland, K.H., 2020. Comparison of variable  selection methods in partial least squares regression. Journal of  Chemometrics 34, e3226. https://doi.org/10.1002/cem.3226</p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 
    27 18 7 6; 12 11 28 7] 
Y = [10. 11 13; 120 131 27; 8 12 4; 
    1 200 8; 100 10 89] 
y = Y[:, 1] 
ycla = [1; 1; 1; 2; 2]

nlv = 3
mod = model(plskern; nlv)
fit!(mod, X, y)
res = vip(mod.fm)
pnames(res)
res.imp

fit!(mod, X, Y)
vip(mod.fm).imp
vip(mod.fm, Y).imp

mod = model(plsrda; nlv) 
fit!(mod, X, ycla)
pnames(mod.fm)
fm = mod.fm.fm ;
vip(fm).imp
Ydummy = dummy(ycla).Y
vip(fm, Ydummy).imp

mod = model(plslda; nlv) 
fit!(mod, X, ycla)
pnames(mod.fm.fm)
fm = mod.fm.fm.fmpls ;
vip(fm).imp
vip(fm, Ydummy).imp</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/vip.jl#LL1-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.viperm-Tuple{Any, Any, Any}" href="#Jchemo.viperm-Tuple{Any, Any, Any}"><code>Jchemo.viperm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">viperm(mod, X, Y; rep = 50, psamp = .3, score = rmsep)</code></pre><p>Variable importance by direct permutations.</p><ul><li><code>mod</code> : Model to evaluate.</li><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).  </li></ul><p>Keyword arguments:</p><ul><li><code>rep</code> : Number of replications of the splitting   training/test. </li><li><code>psamp</code> : Proportion of data used as test set    to compute the <code>score</code>.</li><li><code>score</code> : Function computing the prediction score.</li></ul><p>The principle is as follows:</p><ul><li>Data (X, Y) are splitted randomly to a training and a test set.</li><li>The model is fitted on Xtrain, and the score (error rate)    is computed on Xtest. This gives the reference error rate.</li><li>Rows of a given variable (feature) j in Xtest are randomly    permutated (the rest of Xtest is unchanged). The score is computed    on the Xtest<em>perm</em>j (i.e. Xtest after thta the rows of variable j    were permuted). The importance of variable j is computed by the    difference between this score and the reference score.</li><li>This process is run for each variable j separately and replicated    <code>rep</code> times. Average results are provided in the outputs, as well    as the results per replication. </li></ul><p>In general, this method returns similar results as the out-of-bag permutation  method used in random forests (Breiman, 2001).</p><p><strong>References</strong></p><ul><li>Nørgaard, L., Saudland, A., Wagner, J., Nielsen, J.P., Munck, L., </li></ul><p>Engelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419.  https://doi.org/10.1366/0003702001949500</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;tecator.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X
Y = dat.Y 
wl_str = names(X)
wl = parse.(Float64, wl_str) 
ntot, p = size(X)
typ = Y.typ
namy = names(Y)[1:3]
plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Absorbance&quot;).f
s = typ .== &quot;train&quot;
Xtrain = X[s, :]
Ytrain = Y[s, namy]
Xtest = rmrow(X, s)
Ytest = rmrow(Y[:, namy], s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

## Work on the j-th y-variable 
j = 2
nam = namy[j]
ytrain = Ytrain[:, nam]
ytest = Ytest[:, nam]

mod = model(plskern; nlv = 9)
res = viperm(mod, Xtrain, ytrain; rep = 50, score = rmsep) ;
z = vec(res.imp)
f = Figure(size = (500, 400))
ax = Axis(f[1, 1]; xlabel = &quot;Wavelength (nm)&quot;, ylabel = &quot;Importance&quot;)
scatter!(ax, wl, vec(z); color = (:red, .5))
u = [910; 950]
vlines!(ax, u; color = :grey, linewidth = 1)
f

mod = model(rfr_dt; n_trees = 10, max_depth = 2000, min_samples_leaf = 5)
res = viperm(mod, Xtrain, ytrain; rep = 50)
z = vec(res.imp)
f = Figure(size = (500, 400))
ax = Axis(f[1, 1];
    xlabel = &quot;Wavelength (nm)&quot;, 
    ylabel = &quot;Importance&quot;)
scatter!(ax, wl, vec(z); color = (:red, .5))
u = [910; 950]
vlines!(ax, u; color = :grey, linewidth = 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/viperm.jl#LL1-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vrow-Tuple{Any, Any}" href="#Jchemo.vrow-Tuple{Any, Any}"><code>Jchemo.vrow</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vrow(X::AbstractMatrix, i)
vrow(X::DataFrame, i)
vrow(x::Vector, i)</code></pre><p>View of the i-th row(s) of a matrix <code>X</code>, or of the i-th element(s) of vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/utility.jl#LL1188-L1194">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.wdist-Tuple{Any}" href="#Jchemo.wdist-Tuple{Any}"><code>Jchemo.wdist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">wdist(d; h = 2, criw = 4, squared = false)
wdist!(d; h = 2, criw = 4, squared = false)</code></pre><p>Compute weights from distances using a decreasing exponential function.</p><ul><li><code>d</code> : A vector of distances.</li></ul><p>Keyword arguments:</p><ul><li><code>h</code> : A scaling positive scalar defining the shape    of the weight function. </li><li><code>criw</code> : A positive scalar defining outliers in the    distances vector <code>d</code>.</li><li><code>squared</code>: If <code>true</code>, distances are replaced by the squared    distances; the weight function is then a Gaussian (RBF)    kernel function.</li></ul><p>Weights are computed by: </p><ul><li>exp(-<code>d</code> / (<code>h</code> * MAD(<code>d</code>)))</li></ul><p>or are set to 0 for  distances &gt; Median(<code>d</code>) + criw * MAD(<code>d</code>).  This is an adaptation of the weight function presented in  Kim et al. 2011.</p><p>The weights decrease with increasing distances. Lower is h, sharper  is the decreasing function. Weights are set to 0 for outliers  (extreme distances).</p><p><strong>References</strong></p><p>Kim S, Kano M, Nakagawa H, Hasebe S. Estimation of active  pharmaceutical ingredients content using locally weighted partial  least squares and statistical wavelength selection. Int J Pharm. 2011; 421(2):269-274. https://doi.org/10.1016/j.ijpharm.2011.10.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CairoMakie, Distributions

x1 = rand(Chisq(10), 100) ;
x2 = rand(Chisq(40), 10) ;
d = [sqrt.(x1) ; sqrt.(x2)]
h = 2 ; criw = 3
w = wdist(d; h, criw) ;
f = Figure(size = (600, 300))
ax1 = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Nb. observations&quot;)
hist!(ax1, d, bins = 30)
ax2 = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
scatter!(ax2, d, w)
f[1, 1] = ax1 
f[1, 2] = ax2 
f

d = collect(0:.5:15) ;
h = [.5, 1, 1.5, 2.5, 5, 10, Inf] 
#h = [1, 2, 5, Inf] 
w = wdist(d; h = h[1]) 
f = Figure(size = (500, 400))
ax = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
lines!(ax, d, w, label = string(&quot;h = &quot;, h[1]))
for i = 2:length(h)
    w = wdist(d; h = h[i])
    lines!(ax, d, w, label = string(&quot;h = &quot;, h[i]))
end
axislegend(&quot;Values of h&quot;; position = :lb)
f[1, 1] = ax
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/wdist.jl#LL1-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.xfit-Tuple{Any}" href="#Jchemo.xfit-Tuple{Any}"><code>Jchemo.xfit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">xfit(object)
xfit(object, X; nlv = nothing)
xfit!(object, X::Matrix; nlv = nothing)</code></pre><p>Matrix fitting from a bilinear model (e.g. PCA).</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : New X-data to be approximated from the model.   Must be in the same scale as the X-data used to fit   the model <code>object</code>, i.e. before centering    and eventual scaling.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. components (PCs or LVs) to consider.    If <code>nothing</code>, it is the maximum nb. of components.</li></ul><p>Compute an approximate of matrix <code>X</code> from a bilinear  model (e.g. PCA or PLS) fitted on <code>X</code>. The fitted X is  returned in the original scale of the X-data used to fit  the model <code>object</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 
    27 18 7 6; 12 11 28 7] 
Y = [10. 11 13; 120 131 27; 8 12 4; 
    1 200 8; 100 10 89] 
n, p = size(X)
Xnew = X[1:3, :]
Ynew = Y[1:3, :]
y = Y[:, 1]
ynew = Ynew[:, 1]
weights = mweight(rand(n))

nlv = 2 
scal = false
#scal = true
mod = model(pcasvd; nlv, scal) ;
fit!(mod, X)
fm = mod.fm ;
@head xfit(fm)
xfit(fm, Xnew)
xfit(fm, Xnew; nlv = 0)
xfit(fm, Xnew; nlv = 1)
fm.xmeans

@head X
@head xfit(fm) + xresid(fm, X)
@head xfit(fm, X; nlv = 1) + xresid(fm, X; nlv = 1)

@head Xnew
@head xfit(fm, Xnew) + xresid(fm, Xnew)

mod = model(pcasvd; nlv = min(n, p), scal) 
fit!(mod, X)
fm = mod.fm ;
@head xfit(fm) 
@head xfit(fm, X)
@head xresid(fm, X)

nlv = 3
scal = false
#scal = true
mod = model(plskern; nlv, scal)
fit!(mod, X, Y, weights) 
fm = mod.fm ;
@head xfit(fm)
xfit(fm, Xnew)
xfit(fm, Xnew, nlv = 0)
xfit(fm, Xnew, nlv = 1)

@head X
@head xfit(fm) + xresid(fm, X)
@head xfit(fm, X; nlv = 1) + xresid(fm, X; nlv = 1)

@head Xnew
@head xfit(fm, Xnew) + xresid(fm, Xnew)

mod = model(plskern; nlv = min(n, p), scal) 
fit!(mod, X, Y, weights) 
fm = mod.fm ;
@head xfit(fm) 
@head xfit(fm, Xnew)
@head xresid(fm, Xnew)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/xfit.jl#LL1-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.xresid-Tuple{Any, Any}" href="#Jchemo.xresid-Tuple{Any, Any}"><code>Jchemo.xresid</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">xresid(object, X; nlv = nothing)
xresid!(object, X::Matrix; nlv = nothing)</code></pre><p>Residual matrix from a bilinear model (e.g. PCA).</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : New X-data to be approximated from the model.   Must be in the same scale as the X-data used to fit   the model <code>object</code>, i.e. before centering    and eventual scaling.</li></ul><p>Keyword arguments:</p><ul><li><code>nlv</code> : Nb. components (PCs or LVs) to consider.    If <code>nothing</code>, it is the maximum nb. of components.</li></ul><p>Compute the residual matrix:</p><ul><li>E = <code>X</code> - X_fit</li></ul><p>where X_fit is the fitted X returned by function  <code>xfit</code>. See <code>xfit</code> for examples.  ```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/056168dc934915861568f98e7f01d35d72e2d097/src/xresid.jl#LL1-L19">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../domains/">« Available methods</a><a class="docs-footer-nextpage" href="../news/">News »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 24 June 2024 08:37">Monday 24 June 2024</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
