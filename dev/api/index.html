<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Index of functions · Jchemo.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://mlesnoff.github.io/Jchemo.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Jchemo.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../domains/">Available methods</a></li><li class="is-active"><a class="tocitem" href>Index of functions</a></li><li><a class="tocitem" href="../news/">News</a></li><li><a class="tocitem" href="../see_jchemodemo/">Examples</a></li><li><a class="tocitem" href="../see_jchemodata/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Index of functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Index of functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mlesnoff/Jchemo.jl/blob/master/docs/src/api.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Index-of-functions"><a class="docs-heading-anchor" href="#Index-of-functions">Index of functions</a><a id="Index-of-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Index-of-functions" title="Permalink"></a></h1><p>Here is a list of all exported functions from Jchemo.jl. </p><p>For more details, click on the link and you&#39;ll be directed to the function help.</p><ul><li><a href="#Base.summary-Tuple{Jchemo.Pca, Union{DataFrames.DataFrame, Matrix}}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Mbplsr, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Mbpca, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Comdim, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.PlsTuck, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.MbplsWest, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.PlsCan, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Kpca}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Pcr, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.CcaWold, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Spca, Union{DataFrames.DataFrame, Matrix}}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Fda}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Cca, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Base.summary-Tuple{Jchemo.Rasvd, Any, Any}"><code>Base.summary</code></a></li><li><a href="#Jchemo.aggstat-Tuple{Any, Any}"><code>Jchemo.aggstat</code></a></li><li><a href="#Jchemo.aicplsr-Tuple{Any, Any}"><code>Jchemo.aicplsr</code></a></li><li><a href="#Jchemo.aov1-Tuple{Any, Any}"><code>Jchemo.aov1</code></a></li><li><a href="#Jchemo.bias-Tuple{Any, Any}"><code>Jchemo.bias</code></a></li><li><a href="#Jchemo.blockscal-Tuple{Any, Any}"><code>Jchemo.blockscal</code></a></li><li><a href="#Jchemo.calds-Tuple{Any, Any}"><code>Jchemo.calds</code></a></li><li><a href="#Jchemo.calpds-Tuple{Any, Any}"><code>Jchemo.calpds</code></a></li><li><a href="#Jchemo.cca-Tuple{Any, Any}"><code>Jchemo.cca</code></a></li><li><a href="#Jchemo.ccawold-Tuple{Any, Any}"><code>Jchemo.ccawold</code></a></li><li><a href="#Jchemo.center-Tuple{Any}"><code>Jchemo.center</code></a></li><li><a href="#Jchemo.cglsr-Tuple{Any, Any}"><code>Jchemo.cglsr</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Krr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Mlr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Dkplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Rr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Kplsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.coef-Tuple{Jchemo.Cglsr}"><code>Jchemo.coef</code></a></li><li><a href="#Jchemo.colmad-Tuple{Any}"><code>Jchemo.colmad</code></a></li><li><a href="#Jchemo.colmean-Tuple{Any}"><code>Jchemo.colmean</code></a></li><li><a href="#Jchemo.colnorm-Tuple{Any}"><code>Jchemo.colnorm</code></a></li><li><a href="#Jchemo.colstd-Tuple{Any}"><code>Jchemo.colstd</code></a></li><li><a href="#Jchemo.colsum-Tuple{Any}"><code>Jchemo.colsum</code></a></li><li><a href="#Jchemo.colvar-Tuple{Any}"><code>Jchemo.colvar</code></a></li><li><a href="#Jchemo.comdim-Tuple{Any}"><code>Jchemo.comdim</code></a></li><li><a href="#Jchemo.confusion-Tuple{Any, Any}"><code>Jchemo.confusion</code></a></li><li><a href="#Jchemo.cor2-Tuple{Any, Any}"><code>Jchemo.cor2</code></a></li><li><a href="#Jchemo.corm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.corm</code></a></li><li><a href="#Jchemo.cosm-Tuple{Any}"><code>Jchemo.cosm</code></a></li><li><a href="#Jchemo.cosv-Tuple{Any, Any}"><code>Jchemo.cosv</code></a></li><li><a href="#Jchemo.covm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.covm</code></a></li><li><a href="#Jchemo.cscale-Tuple{Any}"><code>Jchemo.cscale</code></a></li><li><a href="#Jchemo.dfplsr_cg-Tuple{Any, Any}"><code>Jchemo.dfplsr_cg</code></a></li><li><a href="#Jchemo.difmean-Tuple{Any, Any}"><code>Jchemo.difmean</code></a></li><li><a href="#Jchemo.dkplsr-Tuple{Any, Any}"><code>Jchemo.dkplsr</code></a></li><li><a href="#Jchemo.dkplsrda"><code>Jchemo.dkplsrda</code></a></li><li><a href="#Jchemo.dmkern-Tuple{Any}"><code>Jchemo.dmkern</code></a></li><li><a href="#Jchemo.dmnorm"><code>Jchemo.dmnorm</code></a></li><li><a href="#Jchemo.dmnormlog"><code>Jchemo.dmnormlog</code></a></li><li><a href="#Jchemo.dummy"><code>Jchemo.dummy</code></a></li><li><a href="#Jchemo.dupl-Tuple{Any}"><code>Jchemo.dupl</code></a></li><li><a href="#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><code>Jchemo.ensure_df</code></a></li><li><a href="#Jchemo.ensure_mat-Tuple{AbstractMatrix}"><code>Jchemo.ensure_mat</code></a></li><li><a href="#Jchemo.eposvd-Tuple{Any}"><code>Jchemo.eposvd</code></a></li><li><a href="#Jchemo.err-Tuple{Any, Any}"><code>Jchemo.err</code></a></li><li><a href="#Jchemo.euclsq-Tuple{Any, Any}"><code>Jchemo.euclsq</code></a></li><li><a href="#Jchemo.fcenter-Tuple{Any, Any}"><code>Jchemo.fcenter</code></a></li><li><a href="#Jchemo.fcscale-Tuple{Any, Any, Any}"><code>Jchemo.fcscale</code></a></li><li><a href="#Jchemo.fda-Tuple{Any, Any}"><code>Jchemo.fda</code></a></li><li><a href="#Jchemo.fdasvd-Tuple{Any, Any}"><code>Jchemo.fdasvd</code></a></li><li><a href="#Jchemo.fdif-Tuple{Any}"><code>Jchemo.fdif</code></a></li><li><a href="#Jchemo.findmax_cla-Tuple{Any}"><code>Jchemo.findmax_cla</code></a></li><li><a href="#Jchemo.frob-Tuple{Any}"><code>Jchemo.frob</code></a></li><li><a href="#Jchemo.fscale-Tuple{Any, Any}"><code>Jchemo.fscale</code></a></li><li><a href="#Jchemo.fweight-Tuple{Any}"><code>Jchemo.fweight</code></a></li><li><a href="#Jchemo.getknn-Tuple{Any, Any}"><code>Jchemo.getknn</code></a></li><li><a href="#Jchemo.gridcv-Tuple{Any, Any}"><code>Jchemo.gridcv</code></a></li><li><a href="#Jchemo.gridcvlb-Tuple{Any, Any}"><code>Jchemo.gridcvlb</code></a></li><li><a href="#Jchemo.gridcvlv-Tuple{Any, Any}"><code>Jchemo.gridcvlv</code></a></li><li><a href="#Jchemo.gridscore-NTuple{4, Any}"><code>Jchemo.gridscore</code></a></li><li><a href="#Jchemo.gridscorelb-NTuple{4, Any}"><code>Jchemo.gridscorelb</code></a></li><li><a href="#Jchemo.gridscorelv-NTuple{4, Any}"><code>Jchemo.gridscorelv</code></a></li><li><a href="#Jchemo.head-Tuple{Any}"><code>Jchemo.head</code></a></li><li><a href="#Jchemo.interpl-Tuple{Any}"><code>Jchemo.interpl</code></a></li><li><a href="#Jchemo.iqr-Tuple{Any}"><code>Jchemo.iqr</code></a></li><li><a href="#Jchemo.isel"><code>Jchemo.isel</code></a></li><li><a href="#Jchemo.kdeda-Tuple{Any, Any}"><code>Jchemo.kdeda</code></a></li><li><a href="#Jchemo.knnda-Tuple{Any, Any}"><code>Jchemo.knnda</code></a></li><li><a href="#Jchemo.knnr-Tuple{Any, Any}"><code>Jchemo.knnr</code></a></li><li><a href="#Jchemo.kpca-Tuple{Any}"><code>Jchemo.kpca</code></a></li><li><a href="#Jchemo.kplsr-Tuple{Any, Any}"><code>Jchemo.kplsr</code></a></li><li><a href="#Jchemo.kplsrda"><code>Jchemo.kplsrda</code></a></li><li><a href="#Jchemo.kpol-Tuple{Any, Any}"><code>Jchemo.kpol</code></a></li><li><a href="#Jchemo.krbf-Tuple{Any, Any}"><code>Jchemo.krbf</code></a></li><li><a href="#Jchemo.krr-Tuple{Any, Any}"><code>Jchemo.krr</code></a></li><li><a href="#Jchemo.krrda"><code>Jchemo.krrda</code></a></li><li><a href="#Jchemo.lda"><code>Jchemo.lda</code></a></li><li><a href="#Jchemo.lg-Tuple{Any, Any}"><code>Jchemo.lg</code></a></li><li><a href="#Jchemo.list-Tuple{Integer}"><code>Jchemo.list</code></a></li><li><a href="#Jchemo.list-Tuple{Integer, Any}"><code>Jchemo.list</code></a></li><li><a href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>Jchemo.locw</code></a></li><li><a href="#Jchemo.locwlv-Tuple{Any, Any, Any}"><code>Jchemo.locwlv</code></a></li><li><a href="#Jchemo.lwmlr-Tuple{Any, Any}"><code>Jchemo.lwmlr</code></a></li><li><a href="#Jchemo.lwmlr_s-Tuple{Any, Any}"><code>Jchemo.lwmlr_s</code></a></li><li><a href="#Jchemo.lwmlrda-Tuple{Any, Any}"><code>Jchemo.lwmlrda</code></a></li><li><a href="#Jchemo.lwmlrda_s-Tuple{Any, Any}"><code>Jchemo.lwmlrda_s</code></a></li><li><a href="#Jchemo.lwplslda-Tuple{Any, Any}"><code>Jchemo.lwplslda</code></a></li><li><a href="#Jchemo.lwplsqda-Tuple{Any, Any}"><code>Jchemo.lwplsqda</code></a></li><li><a href="#Jchemo.lwplsr-Tuple{Any, Any}"><code>Jchemo.lwplsr</code></a></li><li><a href="#Jchemo.lwplsr_s-Tuple{Any, Any}"><code>Jchemo.lwplsr_s</code></a></li><li><a href="#Jchemo.lwplsravg-Tuple{Any, Any}"><code>Jchemo.lwplsravg</code></a></li><li><a href="#Jchemo.lwplsrda-Tuple{Any, Any}"><code>Jchemo.lwplsrda</code></a></li><li><a href="#Jchemo.lwplsrda_s-Tuple{Any, Any}"><code>Jchemo.lwplsrda_s</code></a></li><li><a href="#Jchemo.mad-Tuple{Any}"><code>Jchemo.mad</code></a></li><li><a href="#Jchemo.mahsq-Tuple{Any, Any}"><code>Jchemo.mahsq</code></a></li><li><a href="#Jchemo.mahsqchol-Tuple{Any, Any}"><code>Jchemo.mahsqchol</code></a></li><li><a href="#Jchemo.matB"><code>Jchemo.matB</code></a></li><li><a href="#Jchemo.matW"><code>Jchemo.matW</code></a></li><li><a href="#Jchemo.mavg-Tuple{Any}"><code>Jchemo.mavg</code></a></li><li><a href="#Jchemo.mblock-Tuple{Any, Any}"><code>Jchemo.mblock</code></a></li><li><a href="#Jchemo.mbpca-Tuple{Any}"><code>Jchemo.mbpca</code></a></li><li><a href="#Jchemo.mbplsr-Tuple{Any, Any}"><code>Jchemo.mbplsr</code></a></li><li><a href="#Jchemo.mbplswest-Tuple{Any, Any}"><code>Jchemo.mbplswest</code></a></li><li><a href="#Jchemo.miss-Tuple{Any}"><code>Jchemo.miss</code></a></li><li><a href="#Jchemo.mlev-Tuple{Any}"><code>Jchemo.mlev</code></a></li><li><a href="#Jchemo.mlr-Tuple{Any, Any}"><code>Jchemo.mlr</code></a></li><li><a href="#Jchemo.mlrchol-Tuple{Any, Any}"><code>Jchemo.mlrchol</code></a></li><li><a href="#Jchemo.mlrda"><code>Jchemo.mlrda</code></a></li><li><a href="#Jchemo.mlrpinv-Tuple{Any, Any}"><code>Jchemo.mlrpinv</code></a></li><li><a href="#Jchemo.mlrpinvn-Tuple{Any, Any}"><code>Jchemo.mlrpinvn</code></a></li><li><a href="#Jchemo.mlrvec-Tuple{Any, Any}"><code>Jchemo.mlrvec</code></a></li><li><a href="#Jchemo.mpar"><code>Jchemo.mpar</code></a></li><li><a href="#Jchemo.mse-Tuple{Any, Any}"><code>Jchemo.mse</code></a></li><li><a href="#Jchemo.msep-Tuple{Any, Any}"><code>Jchemo.msep</code></a></li><li><a href="#Jchemo.mweight-Tuple{Vector}"><code>Jchemo.mweight</code></a></li><li><a href="#Jchemo.nco-Tuple{Any}"><code>Jchemo.nco</code></a></li><li><a href="#Jchemo.nipals-Tuple{Any}"><code>Jchemo.nipals</code></a></li><li><a href="#Jchemo.nipalsmiss-Tuple{Any}"><code>Jchemo.nipalsmiss</code></a></li><li><a href="#Jchemo.normw-Tuple{Any, Jchemo.Weight}"><code>Jchemo.normw</code></a></li><li><a href="#Jchemo.nro-Tuple{Any}"><code>Jchemo.nro</code></a></li><li><a href="#Jchemo.occknndis-Tuple{Any}"><code>Jchemo.occknndis</code></a></li><li><a href="#Jchemo.occlknndis-Tuple{Any}"><code>Jchemo.occlknndis</code></a></li><li><a href="#Jchemo.occod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}"><code>Jchemo.occod</code></a></li><li><a href="#Jchemo.occsd-Tuple{Union{Jchemo.Kpca, Jchemo.Pca, Jchemo.Plsr}}"><code>Jchemo.occsd</code></a></li><li><a href="#Jchemo.occsdod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}"><code>Jchemo.occsdod</code></a></li><li><a href="#Jchemo.occstah-Tuple{Any}"><code>Jchemo.occstah</code></a></li><li><a href="#Jchemo.out-Tuple{Any, Any}"><code>Jchemo.out</code></a></li><li><a href="#Jchemo.pcaeigen-Tuple{Any}"><code>Jchemo.pcaeigen</code></a></li><li><a href="#Jchemo.pcaeigenk-Tuple{Any}"><code>Jchemo.pcaeigenk</code></a></li><li><a href="#Jchemo.pcanipals-Tuple{Any}"><code>Jchemo.pcanipals</code></a></li><li><a href="#Jchemo.pcanipalsmiss-Tuple{Any}"><code>Jchemo.pcanipalsmiss</code></a></li><li><a href="#Jchemo.pcasph-Tuple{Any}"><code>Jchemo.pcasph</code></a></li><li><a href="#Jchemo.pcasvd-Tuple{Any}"><code>Jchemo.pcasvd</code></a></li><li><a href="#Jchemo.pcr-Tuple{Any, Any}"><code>Jchemo.pcr</code></a></li><li><a href="#Jchemo.plist-Tuple{Any}"><code>Jchemo.plist</code></a></li><li><a href="#Jchemo.plotconf-Tuple{Any}"><code>Jchemo.plotconf</code></a></li><li><a href="#Jchemo.plotgrid-Tuple{AbstractVector, Any}"><code>Jchemo.plotgrid</code></a></li><li><a href="#Jchemo.plotsp"><code>Jchemo.plotsp</code></a></li><li><a href="#Jchemo.plotxy-Tuple{Any, Any}"><code>Jchemo.plotxy</code></a></li><li><a href="#Jchemo.plscan-Tuple{Any, Any}"><code>Jchemo.plscan</code></a></li><li><a href="#Jchemo.plskdeda"><code>Jchemo.plskdeda</code></a></li><li><a href="#Jchemo.plskern-Tuple{Any, Any}"><code>Jchemo.plskern</code></a></li><li><a href="#Jchemo.plslda"><code>Jchemo.plslda</code></a></li><li><a href="#Jchemo.plsnipals-Tuple{Any, Any}"><code>Jchemo.plsnipals</code></a></li><li><a href="#Jchemo.plsqda"><code>Jchemo.plsqda</code></a></li><li><a href="#Jchemo.plsravg-Tuple{Any, Any}"><code>Jchemo.plsravg</code></a></li><li><a href="#Jchemo.plsrda"><code>Jchemo.plsrda</code></a></li><li><a href="#Jchemo.plsrosa-Tuple{Any, Any}"><code>Jchemo.plsrosa</code></a></li><li><a href="#Jchemo.plssimp-Tuple{Any, Any}"><code>Jchemo.plssimp</code></a></li><li><a href="#Jchemo.plstuck-Tuple{Any, Any}"><code>Jchemo.plstuck</code></a></li><li><a href="#Jchemo.plswold-Tuple{Any, Any}"><code>Jchemo.plswold</code></a></li><li><a href="#Jchemo.pmod-Tuple{Any}"><code>Jchemo.pmod</code></a></li><li><a href="#Jchemo.pnames-Tuple{Any}"><code>Jchemo.pnames</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.CalDs, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occlknndis, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dkplsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.CalPds, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.LwplsrS, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Krr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.LwplsrdaS, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occod, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Qda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.TreerDt, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.LwmlrdaS, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Occknndis, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.LwmlrS, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Mlr, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><code>Jchemo.predict</code></a></li><li><a href="#Jchemo.psize-Tuple{Any}"><code>Jchemo.psize</code></a></li><li><a href="#Jchemo.pval-Tuple{Distributions.Distribution, Any}"><code>Jchemo.pval</code></a></li><li><a href="#Jchemo.qda"><code>Jchemo.qda</code></a></li><li><a href="#Jchemo.r2-Tuple{Any, Any}"><code>Jchemo.r2</code></a></li><li><a href="#Jchemo.rasvd-Tuple{Any, Any}"><code>Jchemo.rasvd</code></a></li><li><a href="#Jchemo.rd-Tuple{Any, Any}"><code>Jchemo.rd</code></a></li><li><a href="#Jchemo.rda"><code>Jchemo.rda</code></a></li><li><a href="#Jchemo.recodcat2int-Tuple{Any}"><code>Jchemo.recodcat2int</code></a></li><li><a href="#Jchemo.recodnum2cla-Tuple{Any, Any}"><code>Jchemo.recodnum2cla</code></a></li><li><a href="#Jchemo.recovkwargs-Tuple{Any, Any}"><code>Jchemo.recovkwargs</code></a></li><li><a href="#Jchemo.replacebylev-Tuple{Any, Any}"><code>Jchemo.replacebylev</code></a></li><li><a href="#Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}"><code>Jchemo.replacebylev2</code></a></li><li><a href="#Jchemo.replacedict-Tuple{Any, Any}"><code>Jchemo.replacedict</code></a></li><li><a href="#Jchemo.residcla-Tuple{Any, Any}"><code>Jchemo.residcla</code></a></li><li><a href="#Jchemo.residreg-Tuple{Any, Any}"><code>Jchemo.residreg</code></a></li><li><a href="#Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}"><code>Jchemo.rfda_dt</code></a></li><li><a href="#Jchemo.rfr_dt-Tuple{Any, Any}"><code>Jchemo.rfr_dt</code></a></li><li><a href="#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}"><code>Jchemo.rmcol</code></a></li><li><a href="#Jchemo.rmgap-Tuple{Any}"><code>Jchemo.rmgap</code></a></li><li><a href="#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}"><code>Jchemo.rmrow</code></a></li><li><a href="#Jchemo.rmsep-Tuple{Any, Any}"><code>Jchemo.rmsep</code></a></li><li><a href="#Jchemo.rmsepstand-Tuple{Any, Any}"><code>Jchemo.rmsepstand</code></a></li><li><a href="#Jchemo.rosaplsr-Tuple{Any, Any}"><code>Jchemo.rosaplsr</code></a></li><li><a href="#Jchemo.rowmean-Tuple{Any}"><code>Jchemo.rowmean</code></a></li><li><a href="#Jchemo.rowstd-Tuple{Any}"><code>Jchemo.rowstd</code></a></li><li><a href="#Jchemo.rowsum-Tuple{Any}"><code>Jchemo.rowsum</code></a></li><li><a href="#Jchemo.rowvar-Tuple{Any}"><code>Jchemo.rowvar</code></a></li><li><a href="#Jchemo.rp-Tuple{Any}"><code>Jchemo.rp</code></a></li><li><a href="#Jchemo.rpd-Tuple{Any, Any}"><code>Jchemo.rpd</code></a></li><li><a href="#Jchemo.rpdr-Tuple{Any, Any}"><code>Jchemo.rpdr</code></a></li><li><a href="#Jchemo.rpmatgauss"><code>Jchemo.rpmatgauss</code></a></li><li><a href="#Jchemo.rpmatli"><code>Jchemo.rpmatli</code></a></li><li><a href="#Jchemo.rr-Tuple{Any, Any}"><code>Jchemo.rr</code></a></li><li><a href="#Jchemo.rrchol-Tuple{Any, Any}"><code>Jchemo.rrchol</code></a></li><li><a href="#Jchemo.rrda"><code>Jchemo.rrda</code></a></li><li><a href="#Jchemo.rrr-Tuple{Any, Any}"><code>Jchemo.rrr</code></a></li><li><a href="#Jchemo.rv-Tuple{Any, Any}"><code>Jchemo.rv</code></a></li><li><a href="#Jchemo.sampcla"><code>Jchemo.sampcla</code></a></li><li><a href="#Jchemo.sampdf"><code>Jchemo.sampdf</code></a></li><li><a href="#Jchemo.sampdp-Tuple{Any, Any}"><code>Jchemo.sampdp</code></a></li><li><a href="#Jchemo.sampks-Tuple{Any, Any}"><code>Jchemo.sampks</code></a></li><li><a href="#Jchemo.samprand-Tuple{Any, Any}"><code>Jchemo.samprand</code></a></li><li><a href="#Jchemo.sampsys-Tuple{Any, Any}"><code>Jchemo.sampsys</code></a></li><li><a href="#Jchemo.savgk-Tuple{Int64, Int64, Int64}"><code>Jchemo.savgk</code></a></li><li><a href="#Jchemo.savgol-Tuple{Any}"><code>Jchemo.savgol</code></a></li><li><a href="#Jchemo.scale-Tuple{Any}"><code>Jchemo.scale</code></a></li><li><a href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>Jchemo.segmkf</code></a></li><li><a href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>Jchemo.segmts</code></a></li><li><a href="#Jchemo.selwold-Tuple{Any, Any}"><code>Jchemo.selwold</code></a></li><li><a href="#Jchemo.sep-Tuple{Any, Any}"><code>Jchemo.sep</code></a></li><li><a href="#Jchemo.snv-Tuple{Any}"><code>Jchemo.snv</code></a></li><li><a href="#Jchemo.soft-Tuple{AbstractFloat, Float64}"><code>Jchemo.soft</code></a></li><li><a href="#Jchemo.softmax-Tuple{AbstractVector}"><code>Jchemo.softmax</code></a></li><li><a href="#Jchemo.soplsr-Tuple{Any, Any}"><code>Jchemo.soplsr</code></a></li><li><a href="#Jchemo.sourcedir-Tuple{Any}"><code>Jchemo.sourcedir</code></a></li><li><a href="#Jchemo.spca-Tuple{Any}"><code>Jchemo.spca</code></a></li><li><a href="#Jchemo.splskdeda"><code>Jchemo.splskdeda</code></a></li><li><a href="#Jchemo.splskern-Tuple{Any, Any}"><code>Jchemo.splskern</code></a></li><li><a href="#Jchemo.splslda"><code>Jchemo.splslda</code></a></li><li><a href="#Jchemo.splsqda"><code>Jchemo.splsqda</code></a></li><li><a href="#Jchemo.splsrda"><code>Jchemo.splsrda</code></a></li><li><a href="#Jchemo.ssq-Tuple{Any}"><code>Jchemo.ssq</code></a></li><li><a href="#Jchemo.ssr-Tuple{Any, Any}"><code>Jchemo.ssr</code></a></li><li><a href="#Jchemo.stah-Tuple{Any, Any}"><code>Jchemo.stah</code></a></li><li><a href="#Jchemo.summ-Tuple{Any}"><code>Jchemo.summ</code></a></li><li><a href="#Jchemo.svmda-Tuple{Any, Any}"><code>Jchemo.svmda</code></a></li><li><a href="#Jchemo.svmr-Tuple{Any, Any}"><code>Jchemo.svmr</code></a></li><li><a href="#Jchemo.tab-Tuple{Any}"><code>Jchemo.tab</code></a></li><li><a href="#Jchemo.tabdf-Tuple{Any}"><code>Jchemo.tabdf</code></a></li><li><a href="#Jchemo.tabdupl-Tuple{Any}"><code>Jchemo.tabdupl</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Pcr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Cca, Any, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.PlsCan, Any, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.PlsTuck, Any, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Dkplsrda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rasvd, Any, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Rp, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Spca, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.CcaWold, Any, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.transf</code></a></li><li><a href="#Jchemo.treeda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}"><code>Jchemo.treeda_dt</code></a></li><li><a href="#Jchemo.treer_dt-Tuple{Any, Any}"><code>Jchemo.treer_dt</code></a></li><li><a href="#Jchemo.vcatdf-Tuple{Any}"><code>Jchemo.vcatdf</code></a></li><li><a href="#Jchemo.vcol-Tuple{Any, Any}"><code>Jchemo.vcol</code></a></li><li><a href="#Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}"><code>Jchemo.vip</code></a></li><li><a href="#Jchemo.viperm-Tuple{Any, Any}"><code>Jchemo.viperm</code></a></li><li><a href="#Jchemo.vrow-Tuple{Any, Any}"><code>Jchemo.vrow</code></a></li><li><a href="#Jchemo.wdist-Tuple{Any}"><code>Jchemo.wdist</code></a></li><li><a href="#Jchemo.wshenk-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}, Any}"><code>Jchemo.wshenk</code></a></li><li><a href="#Jchemo.xfit-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}}"><code>Jchemo.xfit</code></a></li><li><a href="#Jchemo.xresid-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}, Any}"><code>Jchemo.xresid</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Cca, Any, Any}" href="#Base.summary-Tuple{Jchemo.Cca, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Cca, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/cca.jl#LL176-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.CcaWold, Any, Any}" href="#Base.summary-Tuple{Jchemo.CcaWold, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::CcaWold, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/ccawold.jl#LL236-L242">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Comdim, Any}" href="#Base.summary-Tuple{Jchemo.Comdim, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Comdim, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/comdim.jl#LL260-L265">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Fda}" href="#Base.summary-Tuple{Jchemo.Fda}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Fda, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/fda.jl#LL122-L127">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Kpca}" href="#Base.summary-Tuple{Jchemo.Kpca}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Kpca, X)</code></pre><p>Summarize the maximal (i.e. with maximal nb. PCs) fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kpca.jl#LL124-L129">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Mbpca, Any}" href="#Base.summary-Tuple{Jchemo.Mbpca, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Mbpca, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbpca.jl#LL240-L245">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.MbplsWest, Any}" href="#Base.summary-Tuple{Jchemo.MbplsWest, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::MbplsWest, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbplswest.jl#LL244-L249">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Mbplsr, Any}" href="#Base.summary-Tuple{Jchemo.Mbplsr, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Mbplsr, Xbl)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbplsr.jl#LL108-L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Pca, Union{DataFrames.DataFrame, Matrix}}" href="#Base.summary-Tuple{Jchemo.Pca, Union{DataFrames.DataFrame, Matrix}}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Pca, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcasvd.jl#LL112-L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Pcr, Any}" href="#Base.summary-Tuple{Jchemo.Pcr, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Pcr, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcr.jl#LL103-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.PlsCan, Any, Any}" href="#Base.summary-Tuple{Jchemo.PlsCan, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::PlsCan, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plscan.jl#LL177-L183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.PlsTuck, Any, Any}" href="#Base.summary-Tuple{Jchemo.PlsTuck, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::PlsTuck, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plstuck.jl#LL131-L137">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Rasvd, Any, Any}" href="#Base.summary-Tuple{Jchemo.Rasvd, Any, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Rasvd, X, Y)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li><li><code>Y</code> : The Y-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rasvd.jl#LL173-L179">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Jchemo.Spca, Union{DataFrames.DataFrame, Matrix}}" href="#Base.summary-Tuple{Jchemo.Spca, Union{DataFrames.DataFrame, Matrix}}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Spca, X::Union{Matrix, DataFrame})</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/spca.jl#LL206-L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}" href="#Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Base.summary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summary(object::Plsr, X)</code></pre><p>Summarize the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : The X-data that was used to fit the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plskern.jl#LL234-L239">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aggstat-Tuple{Any, Any}" href="#Jchemo.aggstat-Tuple{Any, Any}"><code>Jchemo.aggstat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aggstat(X, group; fun = mean)
aggstat(X::DataFrame; vars, groups, fun = mean)</code></pre><p>Compute column-wise statistics (e.g. mean), by group in a dataset.</p><ul><li><code>X</code> : Data.</li><li><code>group</code> : A variable defining the groups.</li><li><code>vars</code> : Names of the variables to summarize.</li><li><code>groups</code> : Names of the group variables to consider.</li><li><code>fun</code> : Function to compute.</li></ul><p>Variables defined in <code>vars</code> and <code>groups</code> must be columns of <code>X</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrame, Statistics

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, :auto)
group = rand(1:3, n)
res = aggstat(X, group; fun = sum)
res.X
aggstat(df, group; fun = sum).X

n, p = 20, 5
X = rand(n, p)
df = DataFrame(X, string.(&quot;v&quot;, 1:p))
df.gr1 = rand(1:2, n)
df.gr2 = rand([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], n)
df
aggstat(df; vars = [:v1, :v2], 
    groups = [:gr1, :gr2], fun = mean)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL1-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aicplsr-Tuple{Any, Any}" href="#Jchemo.aicplsr-Tuple{Any, Any}"><code>Jchemo.aicplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aicplsr(X, y; nlv, correct = true, bic = false, scal::Bool = false)</code></pre><p>Compute Akaike&#39;s (AIC) and Mallows&#39;s (Cp) criteria for univariate PLSR models.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate Y-data.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>correct</code> : Define if the bias correction is applied.  # Removed</li><li><code>bic</code> : Define is BIC is computed instead of AIC.  # Replaced by par.alpha_aic</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>y</code>    is scaled by its uncorrected standard deviation.</li></ul><p><code>X</code> and <code>y</code> are internally centered. </p><p>The function uses function <code>dfplsr_cg</code>. </p><p><strong>References</strong></p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9</p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc 

nlv = 25
res = aicplsr(X, y; nlv = nlv) 
pnames(res)
res.crit
res.opt
res.delta

zaic = aicplsr(X, y; nlv = nlv).crit.aic
f, ax = plotgrid(0:nlv, zaic;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;AIC&quot;)
scatter!(ax, 0:nlv, zaic)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/aicplsr.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.aov1-Tuple{Any, Any}" href="#Jchemo.aov1-Tuple{Any, Any}"><code>Jchemo.aov1</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aov1(x, Y)
Univariate anova test.</code></pre><ul><li><code>x</code> : Univariate categorical X-data.</li><li><code>Y</code> : Y-data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 100 ; p = 5
x = rand(1:3, n)
Y = randn(n, p) 

res = aov1(x, Y)
pnames(res)
res.SSF
res.SSR 
res.F 
res.pval</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/aov1.jl#LL1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.bias-Tuple{Any, Any}" href="#Jchemo.bias-Tuple{Any, Any}"><code>Jchemo.bias</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">bias(pred, Y)</code></pre><p>Compute the prediction bias, i.e. the opposite of the mean prediction error.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
bias(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
bias(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL1-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.blockscal-Tuple{Any, Any}" href="#Jchemo.blockscal-Tuple{Any, Any}"><code>Jchemo.blockscal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">blockscal(Xbl; bscales)
blockscal_frob(Xbl)
blockscal_frob(Xbl, weights = ones(nro(X[1]))
blockscal_mfa(Xbl, weights = ones(nro(X[1]))
blockscal_ncol(Xbl)
blockscal_sd(Xbl, weights = ones(nro(X[1]))</code></pre><p>Scale a list of blocks (matrices).</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data. </li><li><code>weights</code> : Weights of the observations (rows of the blocks). </li><li><code>bscales</code> : A vector (of length equal to the nb. blocks) of the scalars diving the blocks.</li></ul><p>Specificities of each function:</p><ul><li><code>blockscal</code>: Each block X is tranformed to X / `bscales&#39;.</li><li><code>blockscal_frob</code>: Let D be the diagonal matrix of vector <code>weights</code>,   standardized to sum to 1. Each block X is divided by its Frobenius norm    = sqrt(tr(X&#39; * D * X)). After this scaling, tr(X&#39; * D * X) = 1.</li><li><code>blockscal_mfa</code>: Each block X is divided by sqrt(lamda),   where lambda is the dominant eigenvalue of X (this is the &quot;MFA&quot; approach).</li><li><code>blockscal_ncol</code>: Each block X is divided by the nb. columns of the block.</li><li><code>blockscal_sd</code>: Each block X is divided by sqrt(sum(weighted variances of the block-columns)).   After this scaling, sum(weighted variances of the block-columns) = 1.</li></ul><p>The functions return the scaled blocks and the scaling values.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5 ; p = 10 
X = rand(n, p) 
Xnew = X[1:3, :]

listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl) 

Xbl[1]
Xbl[2]
Xbl[3]

bscales = ones(3)
res = blockscal(Xbl, bscales) ;
res.bscales
res.X[3]
Xbl[3]

w = ones(n)
#w = mweight(collect(1:n))
D = Diagonal(mweight(w))
res = blockscal_frob(Xbl, w) ;
res.bscales
k = 3 ; tr(Xbl[k]&#39; * D * Xbl[3])^.5
tr(res.X[k]&#39; * D * res.X[k])^.5

w = ones(n)
#w = mweight(collect(1:n))
res = blockscal_mfa(Xbl, w) ;
res.bscales
k = 3 ; pcasvd(Xbl[k], w; nlv = 1).sv[1]

res = blockscal_ncol(Xbl) ;
res.bscales
res.X[3]
Xbl[3] / size(Xbl[3], 2)

w = ones(n)
#w = mweight(collect(1:n))
res = blockscal_sd(Xbl, w) ;
res.bscales
sum(colvar(res.X[3], w))

# To concatenate the returned blocks

X_concat = reduce(hcat, res.X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/blockscal.jl#LL1-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.calds-Tuple{Any, Any}" href="#Jchemo.calds-Tuple{Any, Any}"><code>Jchemo.calds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calds(X, Xt; fun = mlrpinv, kwargs...)</code></pre><p>Direct standardization (DS) for calibration transfer of spectral data.</p><ul><li><code>X</code> : Spectra to transfer to the target (n, p).</li><li><code>Xt</code> : Target spectra (n, p).</li><li><code>fun</code> : Function used as transfer model.  </li><li><code>kwargs</code> : Optional arguments for <code>fun</code>.</li></ul><p><code>Xt</code> and <code>X</code> must represent the same n standard samples.</p><p>The objective is to transform spectra <code>X</code> to new spectra as close  as possible as the target <code>Xt</code>. Method DS fits a model  (defined in <code>fun</code>) that predicts <code>Xt</code> from <code>X</code>.</p><p><strong>References</strong></p><p>Y. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;) 
@load db dat
pnames(dat)

## To transfer
Xcal = dat.X2cal
Xval = dat.X2val
## Target
Xtcal = dat.X1cal
Xtval = dat.X1val

n = nro(Xtcal)
m = nro(Xtval)

fm = calds(Xcal, Xtcal; fun = mlrpinv) ;
#fm = calds(Xcal, Xtcal; fun = pcr, nlv = 15) ;
#fm = calds(Xcal, Xtcal; fun = plskern, nlv = 15) ;
## Transfered spectra, expected to be close to Xtval
pred = Jchemo.predict(fm, Xval).pred 

i = 1
f = Figure(resolution = (500, 300))
ax = Axis(f[1, 1])
lines!(Xtval[i, :]; label = &quot;xt&quot;)
lines!(ax, Xval[i, :]; label = &quot;x&quot;)
lines!(pred[i, :]; linestyle = :dash, label = &quot;x_transf&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/calds.jl#LL6-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.calpds-Tuple{Any, Any}" href="#Jchemo.calpds-Tuple{Any, Any}"><code>Jchemo.calpds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calpds(X, Xt; m = 5, fun = mlrpinv, kwargs...)</code></pre><p>Piecewise direct standardization (PDS) for calibration transfer of spectral data.</p><ul><li><code>X</code> : Spectra to transfer to the target (n, p).</li><li><code>Xt</code> : Target spectra (n, p).</li><li><code>m</code> : Half-window size (nb. points left/right to the target wavelength) </li><li><code>fun</code> : Function used as transfer model.  </li><li><code>kwargs</code> : Optional arguments for <code>fun</code>.</li></ul><p><code>Xt</code> and <code>X</code> must represent the same n standard samples.</p><p>The objective is to transform spectra <code>X</code> to new spectra as close  as possible as the target <code>Xt</code>. Method PDS fits models  (defined in <code>fun</code>) that predict <code>Xt</code> from <code>X</code>.</p><p>The window used in <code>X</code> to predict wavelength &quot;i&quot; in <code>Xt</code> is:</p><ul><li>i - <code>m</code>, i - <code>m</code> + 1, ..., i, ..., i + <code>m</code> - 1, i + <code>m</code></li></ul><p><strong>References</strong></p><p>Bouveresse, E., Massart, D.L., 1996. Improvement of the piecewise direct targetisation procedure  for the transfer of NIR spectra for multivariate calibration. Chemometrics and Intelligent Laboratory  Systems 32, 201–213. https://doi.org/10.1016/0169-7439(95)00074-7</p><p>Y. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.</p><p>Wülfert, F., Kok, W.Th., Noord, O.E. de, Smilde, A.K., 2000. Correction of Temperature-Induced  Spectral Variation by Continuous Piecewise Direct Standardization. Anal. Chem. 72, 1639–1644. https://doi.org/10.1021/ac9906835</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;) 
@load db dat
pnames(dat)

## To transfer
Xcal = dat.X2cal
Xval = dat.X2val
## Target
Xtcal = dat.X1cal
Xtval = dat.X1val

fm = calpds(Xcal, Xtcal;  m = 2, fun = plskern, nlv = 1) ;
## Transfered spectra, expected to be close to Xtval
pred = Jchemo.predict(fm, Xval).pred 

i = 1
f = Figure(resolution = (500, 300))
ax = Axis(f[1, 1])
lines!(Xtval[i, :]; label = &quot;xt&quot;)
lines!(ax, Xval[i, :]; label = &quot;x&quot;)
lines!(pred[i, :]; linestyle = dash, label = &quot;x_transf&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/calpds.jl#LL6-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cca-Tuple{Any, Any}" href="#Jchemo.cca-Tuple{Any, Any}"><code>Jchemo.cca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cca(X, Y, weights = ones(nro(X)); nlv, 
    bscal = :none, tau = 1e-8, scal::Bool = false)
cca!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    bscal = :none, tau = 1e-8, scal::Bool = false)</code></pre><p>Canonical correlation Analysis (CCA, RCCA).</p><ul><li><code>X</code> : First block (matrix) of data.</li><li><code>Y</code> : Second block (matrix) of data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This function implements a CCA algorithm using SVD decompositions and  presented in Weenink 2003 section 2. </p><p>A continuum regularization is available (parameter <code>tau</code>).  After block centering and scaling, the returned block scores (Tx and Ty)  are proportionnal to the eigenvectors of Projx * Projy  and Projy * Projx, respectively, defined as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li><li>Cy = (1 - <code>tau</code>) * Y&#39;DY + <code>tau</code> * Iy</li><li>Cxy = X&#39;DY </li><li>Projx = sqrt(D) * X * invCx * X&#39; * sqrt(D)</li><li>Projy = sqrt(D) * Y * invCx * Y&#39; * sqrt(D)</li></ul><p>where D is the observation (row) metric.  Value <code>tau</code> = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. <code>tau</code> = 1e-8)  to get similar results as with pseudo-inverses.  </p><p>With uniform <code>weights</code>, the normed scores returned  by the function are expected to be the same as those returned  by functions <code>rcc</code> of the R packages <code>CCA</code> (González et al.) and <code>mixOmics</code>  (Le Cao et al.) whith the parameters lambda1 and lambda2 set to:</p><ul><li>lambda1 = lambda2 = <code>tau</code> / (1 - <code>tau</code>) * n / (n - 1) </li></ul><p><strong>References</strong></p><p>González, I., Déjean, S., Martin, P.G.P., Baccini, A., 2008. CCA:  An R Package to Extend Canonical Correlation Analysis. Journal of Statistical  Software 23, 1-14. https://doi.org/10.18637/jss.v023.i12</p><p>Hotelling, H. (1936): “Relations between two sets of variates”, Biometrika 28: pp. 321–377.</p><p>Le Cao, K.-A., Rohart, F., Gonzalez, I., Dejean, S., Abadi, A.J., Gautier, B., Bartolo, F.,  Monget, P., Coquery, J., Yao, F., Liquet, B., 2022. mixOmics: Omics Data Integration Project.  https://doi.org/10.18129/B9.bioc.mixOmics</p><p>Weenink, D. 2003. Canonical Correlation Analysis, Institute of Phonetic Sciences,  Univ. of Amsterdam, Proceedings 25, 81-99.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y

tau = 1e-8
fm = cca(X, Y; nlv = 3, tau = tau)
pnames(fm)

fm.Tx
transf(fm, X, Y).Tx
fscale(fm.Tx, colnorm(fm.Tx))

res = summary(fm, X, Y)
pnames(res)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/cca.jl#LL1-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ccawold-Tuple{Any, Any}" href="#Jchemo.ccawold-Tuple{Any, Any}"><code>Jchemo.ccawold</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ccawold(X, Y, weights = ones(nro(X)); nlv,
    bscal = :none, tau = 1e-8, 
    tol = sqrt(eps(1.)), maxit = 200, scal::Bool = false)
ccawold!(X, Y, weights = ones(nro(X)); nlv,
    bscal = :none, tau = 1e-8, 
    tol = sqrt(eps(1.)), maxit = 200, scal::Bool = false)</code></pre><p>Canonical correlation analysis (ccawold, RCCA) - Wold Nipals algorithm.</p><ul><li><code>X</code> : First block (matrix) of data.</li><li><code>Y</code> : Second block (matrix) of data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>tol</code> : Tolerance for the Nipals algorithm.</li><li><code>maxit</code> : Maximum number of iterations for the Nipals algorithm.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>This function implements the Nipals ccawold algorithm presented  by Tenenhaus 1998 p.204 (related to Wold et al. 1984). </p><p>In this implementation, after each step of LVs computation, X and Y are deflated  relatively to their respective scores (tx and ty). </p><p>A continuum regularization is available (parameter <code>tau</code>).  After block centering and scaling, the covariances matrices are computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li><li>Cy = (1 - <code>tau</code>) * Y&#39;DY + <code>tau</code> * Iy</li></ul><p>where D is the observation (row) metric.   Value <code>tau</code> = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. <code>tau</code> = 1e-8)  to get similar results as with pseudo-inverses.    </p><p>With uniform <code>weights</code>, the normed scores returned  by the function are expected to be the same as those returned  by functions <code>rgcca</code> of the R package <code>RGCCA</code> (Tenenhaus &amp; Guillemot 2017,  Tenenhaus et al. 2017). </p><p><strong>References</strong></p><p>Tenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and Sparse Generalized Canonical  Correlation Analysis for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html </p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p>Tenenhaus, M., Tenenhaus, A., Groenen, P.J.F., 2017.  Regularized Generalized Canonical Correlation Analysis: A Framework for Sequential  Multiblock Component Methods. Psychometrika 82, 737–777.  https://doi.org/10.1007/s11336-017-9573-x</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear  Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y

tau = 1e-8
fm = ccawold(X, Y; nlv = 3, tau = tau)
pnames(fm)

fm.Tx
transf(fm, X, Y).Tx
fscale(fm.Tx, colnorm(fm.Tx))

res = summary(fm, X, Y)
pnames(res)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/ccawold.jl#LL1-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.center-Tuple{Any}" href="#Jchemo.center-Tuple{Any}"><code>Jchemo.center</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">center(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL406-L408">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cglsr-Tuple{Any, Any}" href="#Jchemo.cglsr-Tuple{Any, Any}"><code>Jchemo.cglsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cglsr(X, y; nlv, gs = true, filt = false, scal::Bool = false)
cglsr!(X::Matrix, y::Matrix; nlv, gs = true, filt = false, scal::Bool = false)</code></pre><p>Conjugate gradient algorithm for the normal equations (CGLS; Björck 1996).</p><ul><li><code>X</code> : X-data  (n, p).</li><li><code>y</code> : Univariate Y-data (n).</li><li><code>nlv</code> : Nb. CG iterations.</li><li><code>gs</code> : If <code>true</code>, a Gram-Schmidt gsogonalization of the normal equation    residual vectors is done.</li><li><code>filt</code> : Logical indicating if the CG filter factors are computed (output <code>F</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>y</code>    is scaled by its uncorrected standard deviation.</li></ul><p><code>X</code> and <code>y</code> are internally centered. </p><p>CGLS algorithm &quot;7.4.1&quot; Bjorck 1996, p.289. The part of the code computing the  re-orthogonalization (Hansen 1998) and filter factors (Vogel 1987, Hansen 1998)  is a transcription (with few adaptations) of the Matlab function <code>cgls</code>  (Saunders et al. https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).</p><p><strong>References</strong></p><p>Björck, A., 1996. Numerical Methods for Least Squares Problems, Other Titles in Applied Mathematics.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9781611971484</p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9</p><p>Manne R. Analysis of two partial-least-squares algorithms for multivariate calibration. Chemometrics Intell. Lab. Syst. 1987; 2: 187–197.</p><p>Phatak A, De Hoog F. Exploiting the connection between PLS, Lanczos methods and conjugate gradients: alternative proofs of some properties of PLS. J. Chemometrics 2002; 16: 361–367.</p><p>Vogel, C. R.,  &quot;Solving ill-conditioned linear systems using the conjugate gradient method&quot;,  Report, Dept. of Mathematical Sciences, Montana State University, 1987.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 12 ;
fm = cglsr(Xtrain, ytrain; nlv = nlv) ;

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.B
Jchemo.coef(fm; nlv = 7).B

res = Jchemo.predict(fm, Xtest) ;
res.pred
rmsep(ytest, res.pred)
plotxy(pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/cglsr.jl#LL1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Cglsr}" href="#Jchemo.coef-Tuple{Jchemo.Cglsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Cglsr)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/cglsr.jl#LL162-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Dkplsr}" href="#Jchemo.coef-Tuple{Jchemo.Dkplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Dkplsr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsr.jl#LL135-L141">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Kplsr}" href="#Jchemo.coef-Tuple{Jchemo.Kplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Kplsr; nlv = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kplsr.jl#LL198-L204">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Krr}" href="#Jchemo.coef-Tuple{Jchemo.Krr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Krr; lb = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.   If nothing, it is the parameter stored in the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/krr.jl#LL146-L152">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Mlr}" href="#Jchemo.coef-Tuple{Jchemo.Mlr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Mlr)</code></pre><p>Compute the coefficients of the fitted model.</p><ul><li><code>object</code> : The fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL254-L258">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Rosaplsr}" href="#Jchemo.coef-Tuple{Jchemo.Rosaplsr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Rosaplsr; nlv = nothing)</code></pre><p>Compute the X b-coefficients of a model fitted with <code>nlv</code> LVs.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rosaplsr.jl#LL212-L217">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Jchemo.Rr}" href="#Jchemo.coef-Tuple{Jchemo.Rr}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Rr; lb = nothing)</code></pre><p>Compute the b-coefficients of a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.   If nothing, it is the parameter stored in the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rr.jl#LL103-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}}" href="#Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}}"><code>Jchemo.coef</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">coef(object::Union{Plsr, Pcr}; nlv = nothing)</code></pre><p>Compute the X b-coefficients of a model fitted with <code>nlv</code> LVs.</p><ul><li><code>object</code> : The fitted model.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul><p>If X is (n, p) and Y is (n, q), the returned object <code>B</code> is a matrix (p, q).  If <code>nlv</code> = 0, <code>B</code> is a matrix of zeros. The returned object <code>int</code> is the intercept.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plskern.jl#LL189-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colmad-Tuple{Any}" href="#Jchemo.colmad-Tuple{Any}"><code>Jchemo.colmad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colmad(X)</code></pre><p>Compute the column-median absolute deviations (MAD) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)

colmad(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_colwise.jl#LL1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colmean-Tuple{Any}" href="#Jchemo.colmean-Tuple{Any}"><code>Jchemo.colmean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colmean(X)
colmean(X, weights)</code></pre><p>Compute the column-means of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(collect(1:n))

colmean(X)
colmean(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_colwise.jl#LL21-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colnorm-Tuple{Any}" href="#Jchemo.colnorm-Tuple{Any}"><code>Jchemo.colnorm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colnorm(X)
colnorm(X, w)</code></pre><p>Compute the column-norms of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>The computed norm of a column x of <code>X</code> is:</p><ul><li>sqrt(x&#39; * x)</li></ul><p>The weighted norm is:</p><ul><li>sqrt(x&#39; * D * x), where D is the diagonal matrix of <code>w</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(collect(1:n))

colnorm(X)
colnorm(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_colwise.jl#LL46-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colstd-Tuple{Any}" href="#Jchemo.colstd-Tuple{Any}"><code>Jchemo.colstd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colstd(X)
colstd(X, w)</code></pre><p>Compute the column-standard deviations (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(collect(1:n))

colstd(X)
colstd(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_colwise.jl#LL75-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colsum-Tuple{Any}" href="#Jchemo.colsum-Tuple{Any}"><code>Jchemo.colsum</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colsum(X)
colsum(X, w)</code></pre><p>Compute the column-sums of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(collect(1:n))

colsum(X)
colsum(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_colwise.jl#LL101-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.colvar-Tuple{Any}" href="#Jchemo.colvar-Tuple{Any}"><code>Jchemo.colvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">colvar(X)
colvar(X, w)</code></pre><p>Compute the column-variances (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
w = mweight(collect(1:n))

colvar(X)
colvar(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_colwise.jl#LL126-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.comdim-Tuple{Any}" href="#Jchemo.comdim-Tuple{Any}"><code>Jchemo.comdim</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">comdim(Xbl, weights = ones(nro(Xbl[1])); nlv,
    bscal = :none, tol = sqrt(eps(1.)), maxit = 200,
    scal::Bool = false)
comdim!(Xbl, weights = ones(nro(Xbl[1])); nlv,
    bscal = :none, tol = sqrt(eps(1.)), maxit = 200,
    scal::Bool = false)</code></pre><p>Common components and specific weights analysis (ComDim = CCSWA).</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>tol</code> : Tolerance value for convergence.</li><li><code>maxit</code> : Maximum number of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>&quot;SVD&quot; algorithm of Hannafi &amp; Qannari 2008 p.84.</p><p>The function returns several objects, in particular:</p><ul><li><code>T</code> : The non normed global scores.</li><li><code>U</code> : The normed global scores.</li><li><code>W</code> : The global loadings.</li><li><code>Tbl</code> : The block scores (grouped by blocks, in the original fscale).</li><li><code>Tb</code> : The block scores (grouped by LV, in the metric fscale).</li><li><code>Wbl</code> : The block loadings.</li><li><code>lb</code> : The specific weights (saliences) &quot;lambda&quot;.</li><li><code>mu</code> : The sum of the squared saliences.</li></ul><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the X total inertia (sum of the squared norms of the    blocks) explained by each global score.</li><li><code>explvarxx</code> : Proportion of the XX&#39; total inertia (sum of the squared norms of the   products X<em>k * X</em>k&#39;) explained by each global score    (= indicator &quot;V&quot; in Qannari et al. 2000, Hanafi et al. 2008).</li><li><code>sal2</code> : Proportion of the squared saliences   of each block within each global score. </li><li><code>contr_block</code> : Contribution of each block to the global scores    (= proportions of the saliences &quot;lambda&quot; within each score)</li><li><code>explX</code> : Proportion of the inertia of the blocks explained by each global score.</li><li><code>corx2t</code> : Correlation between the global scores and the original variables.  </li><li><code>cortb2t</code> : Correlation between the global scores and the block scores.</li><li><code>rv</code> : RV coefficient. </li><li><code>lg</code> : Lg coefficient. </li></ul><p><strong>References</strong></p><p>Cariou, V., Qannari, E.M., Rutledge, D.N., Vigneau, E., 2018. ComDim: From multiblock data  analysis to path modeling. Food Quality and Preference, Sensometrics 2016:  Sensometrics-by-the-Sea 67, 27–34. https://doi.org/10.1016/j.foodqual.2017.02.012</p><p>Cariou, V., Jouan-Rimbaud Bouveresse, D., Qannari, E.M., Rutledge, D.N., 2019.  Chapter 7 - ComDim Methods for the Analysis of Multiblock Data in a Data Fusion  Perspective, in: Cocchi, M. (Ed.), Data Handling in Science and Technology,  Data Fusion Methodology and Applications. Elsevier, pp. 179–204.  https://doi.org/10.1016/B978-0-444-63984-4.00007-7</p><p>Ghaziri, A.E., Cariou, V., Rutledge, D.N., Qannari, E.M., 2016. Analysis of multiblock  datasets using ComDim: Overview and extension to the analysis of (K + 1) datasets.  Journal of Chemometrics 30, 420–429. https://doi.org/10.1002/cem.2810</p><p>Hanafi, M., 2008. Nouvelles propriétés de l’analyse en composantes communes et  poids spécifiques. Journal de la société française de statistique 149, 75–97.</p><p>Qannari, E.M., Wakeling, I., Courcoux, P., MacFie, H.J.H., 2000. Defining the underlying  sensory dimensions. Food Quality and Preference 11, 151–154.  https://doi.org/10.1016/S0950-3293(99)00069-5</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ham.jld2&quot;) 
@load db dat
pnames(dat) 

X = dat.X
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X, listbl)
# &quot;New&quot; = first two rows of Xbl 
Xbl_new = mblock(X[1:2, :], listbl)

bscal = :none
#bscal = :frob
fm = comdim(Xbl; nlv = 4, bscal = bscal) ;
fm.U
fm.T
transf(fm, Xbl)
transf(fm, Xbl_new) 

res = summary(fm, Xbl) ;
fm.lb
rowsum(fm.lb)
fm.mu
res.explvarx
res.explvarxx
res.explX # = fm.lb if bscal = :frob
rowsum(Matrix(res.explX))
res.contr_block
res.sal2
colsum(Matrix(res.sal2))
res.corx2t 
res.cortb2t
res.rv</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/comdim.jl#LL1-L110">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.confusion-Tuple{Any, Any}" href="#Jchemo.confusion-Tuple{Any, Any}"><code>Jchemo.confusion</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">confusion(pred, y)</code></pre><p>Confusion matrix.</p><ul><li><code>pred</code> : Univariate predictions.</li><li><code>y</code> : Univariate observed data.</li><li><code>digits</code> : Nb. digits used to round percentages.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">y = [&quot;d&quot;; &quot;c&quot;; &quot;b&quot;; &quot;c&quot;; &quot;a&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; 
    &quot;b&quot;; &quot;b&quot;; &quot;a&quot;; &quot;a&quot;; &quot;c&quot;; &quot;d&quot;; &quot;d&quot;]
pred = [&quot;a&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; &quot;b&quot;; &quot;d&quot;; 
    &quot;b&quot;; &quot;b&quot;; &quot;a&quot;; &quot;a&quot;; &quot;d&quot;; &quot;d&quot;; &quot;d&quot;]
#y = rand(1:10, 200); pred = rand(1:10, 200)

res = confusion(pred, y) ;
pnames(res)
res.cnt       # Counts (dataframe built from `A`) 
res.pct       # Row %  (dataframe built from `Apct`))
res.A         
res.Apct
res.accuracy  # Overall accuracy (% classification successes)
res.lev       # Levels

plotconf(res).f

plotconf(res; pct = true, ptext = false).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/confusion.jl#LL1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cor2-Tuple{Any, Any}" href="#Jchemo.cor2-Tuple{Any, Any}"><code>Jchemo.cor2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cor2(pred, Y)</code></pre><p>Compute the squared linear correlation between data and predictions.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
cor2(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
cor2(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL30-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.corm-Tuple{Any, Jchemo.Weight}" href="#Jchemo.corm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.corm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">corm(X, w)
corm(X, Y, w)</code></pre><p>Compute weighted correlation matrices.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>Uncorrected correlation matrix </p><ul><li>of <code>X</code>-columns :                         ==&gt; (p, p) matrix </li><li>or between <code>X</code>-columns and <code>Y</code>-columns : ==&gt; (p, q) matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)
w = mweight(collect(1:n))

corm(X, w)
corm(X, Y, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL55-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cosm-Tuple{Any}" href="#Jchemo.cosm-Tuple{Any}"><code>Jchemo.cosm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cosm(X)</code></pre><p>Cosinus between the columns of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)

cosm(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL104-L116">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cosv-Tuple{Any, Any}" href="#Jchemo.cosv-Tuple{Any, Any}"><code>Jchemo.cosv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cosv(x, y)</code></pre><p>Cosinus between two vectors.</p><ul><li><code>x</code> : vector (n).</li><li><code>y</code> : vector (n).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5
x = rand(n)
y = rand(n)

cosv(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL124-L138">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.covm-Tuple{Any, Jchemo.Weight}" href="#Jchemo.covm-Tuple{Any, Jchemo.Weight}"><code>Jchemo.covm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">covm(X, w)
covm(X, Y, w)</code></pre><p>Compute weighted covariance matrices.</p><ul><li><code>X</code> : Data (n, p).</li><li><code>Y</code> : Data (n, q).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>Uncorrected weighted covariance matrix </p><ul><li>of the columns of <code>X</code>: ==&gt; (p, p) matrix </li><li>or between columns of <code>X</code> and <code>Y</code> : ==&gt; (p, q) matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
Y = rand(n, 3)
w = mweight(collect(1:n))

covm(X, w)
covm(X, Y, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL142-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.cscale-Tuple{Any}" href="#Jchemo.cscale-Tuple{Any}"><code>Jchemo.cscale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cscale(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL452-L454">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dfplsr_cg-Tuple{Any, Any}" href="#Jchemo.dfplsr_cg-Tuple{Any, Any}"><code>Jchemo.dfplsr_cg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dfplsr_cg(X, y; nlv, gs = true, scal::Bool = false)</code></pre><p>Compute the model complexity (df) of PLSR models with the CGLS algorithm.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate Y-data.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>gs</code> : If <code>true</code>, a Gram-Schmidt reorthogonalization of the normal equation    residual vectors is done.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The number of degrees of freedom (df) of the model is returned for 0, 1, ..., nlv LVs.</p><p><strong>References</strong></p><p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697</p><p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9</p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># The example below reproduces the numerical illustration
# given by Kramer &amp; Sugiyama 2011 on the Ozone data (Fig. 1, fcenter).
# Function &quot;pls.model&quot; used for df calculations
# in the R package &quot;plsdof&quot; v0.2-9 (Kramer &amp; Braun 2019)
# automatically scales the X matrix before PLS.
# The example scales X for consistency with plsdof.

using JchemoData, JLD2, DataFrames, CairoMakie 
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ozone.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
dropmissing!(X) 
zX = rmcol(Matrix(X), 4) ;
y = X[:, 4] 

# For consistency with plsdof
xstds = colstd(zX)
zXs = fscale(zX, xstds)
# End

nlv = 12 
df = dfplsr_cg(zXs, y; nlv = nlv, gs = true) 
df_kramer = [1.000000, 3.712373, 6.456417, 11.633565, 12.156760, 11.715101, 12.349716,
    12.192682, 13.000000, 13.000000, 13.000000, 13.000000, 13.000000]
f, ax = plotgrid(0:nlv, df_kramer; step = 2,
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;df&quot;)
scatter!(ax, 0:nlv, df.df; color = &quot;red&quot;)
ablines!(ax, 1, 1; color = :grey, linestyle = :dot)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dfplsr_cg.jl#LL1-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.difmean-Tuple{Any, Any}" href="#Jchemo.difmean-Tuple{Any, Any}"><code>Jchemo.difmean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">difmean(X1, X2; normx = false)</code></pre><p>Compute a 1-D detrimental matrix (for calibration transfer) by difference of      two matrix-column means.</p><ul><li><code>X1</code> : Matrix of spectra (n1, p).</li><li><code>X2</code> : Matrix of spectra (n2, p).</li><li><code>normx</code> : Boolean. If <code>true</code>, the column means vectors    of <code>X1</code> and <code>X2</code> are normed before computing their difference.   Default is <code>false</code>.</li></ul><p>The function returns a matrix <code>D</code> (1, p) computed by the difference  between two mean spectra, i.e. the column means of <code>X1</code> and <code>X2</code>. </p><p><code>D</code> is assumed to contain the detrimental information that can  be removed from <code>X1</code> and <code>X2</code> by orthogonalization (calibration transfer).  For instance, <code>D</code> can be the input of function <code>eposvd</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;) 
@load db dat
pnames(dat)
X1cal = dat.X1cal
X2cal = dat.X2cal
X1val = dat.X1val
X2val = dat.X2val

D = difmean(X1cal, X2cal).D 
res = eposvd(D; nlv = 1)
## Corrected matrices
X1 = X1val * res.M    
X2 = X2val * res.M    

i = 1
f = Figure(resolution = (500, 300))
ax = Axis(f[1, 1])
lines!(X1[i, :]; label = &quot;x1_correct&quot;)
lines!(ax, X2[i, :]; label = &quot;x2_correct&quot;)
axislegend(position = :rb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/difmean.jl#LL1-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplsr-Tuple{Any, Any}" href="#Jchemo.dkplsr-Tuple{Any, Any}"><code>Jchemo.dkplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dkplsr(X, Y, weights = ones(nro(X)); nlv, 
    kern = :krbf, scal::Bool = false, kwargs...)
dkplsr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv, 
    kern = :krbf, scal = scal, kwargs...)</code></pre><p>Direct kernel partial least squares regression (DKPLSR) (Bennett &amp; Embrechts 2003).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider. </li><li>&#39;kern&#39; : Type of kernel used to compute the Gram matrices.   Possible values are :krbf of :kpol (see respective functions <code>krbf</code> and <code>kpol</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li><li><code>kwargs</code> : Named arguments to pass in the kernel function.</li></ul><p>The method builds kernel Gram matrices and then runs a usual PLSR algorithm on them.  This is faster (but not equivalent) to the &quot;true&quot; Nipals KPLSR algorithm described  in Rosipal &amp; Trejo (2001).</p><p><strong>References</strong></p><p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial  least squares regression, in: Advances in Learning Theory: Methods, Models and Applications,  NATO Science Series III: Computer &amp; Systems Sciences. IOS Press Amsterdam, pp. 227-250.</p><p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in  Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20 ; gamma = 1e-1
fm = dkplsr(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;
fm.fm.T

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.B
Jchemo.coef(fm; nlv = 7).B

transf(fm, Xtest)
transf(fm, Xtest; nlv = 7)

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)

res = Jchemo.predict(fm, Xtest; nlv = 1:2)
res.pred[1]
res.pred[2]

fm = dkplsr(Xtrain, ytrain; nlv = nlv, kern = :kpol, degree = 2, gamma = 1e-1, coef0 = 10) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

# Example of fitting the function sinc(x)
# described in Rosipal &amp; Trejo 2001 p. 105-106 

x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
fm = dkplsr(x, y; nlv = 2) ;
pred = Jchemo.predict(fm, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;ted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsr.jl#LL1-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dkplsrda" href="#Jchemo.dkplsrda"><code>Jchemo.dkplsrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dkplsrda(X, y, weights = ones(nro(X)); nlv, kern = :krbf, 
    scal::Bool = false, kwargs...)</code></pre><p>Discrimination based on direct kernel partial least squares regression (DKPLSR-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate class membership.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li>Other arguments to pass in the kernel: See <code>?kplsr</code>.</li></ul><p>This is the same approach as for <code>plsrda</code> except that the PLS2 step  is replaced by a non linear direct kernel PLS2 (DKPLS).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

gamma = .001 
nlv = 15
fm = dkplsrda(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;
pnames(fm)
typeof(fm.fm) # = KPLS2 model

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

transf(fm, Xtest; nlv = 2)

transf(fm.fm, Xtest)
Jchemo.coef(fm.fm)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsrda.jl#LL1-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dmkern-Tuple{Any}" href="#Jchemo.dmkern-Tuple{Any}"><code>Jchemo.dmkern</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dmkern(X; h = nothing, a = 1)</code></pre><p>Gaussian kernel density estimation (KDE).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>h</code> : Define the bandwith, see examples</li><li><code>a</code> : Constant for the Scott&#39;s rule (default bandwith), see thereafter.</li></ul><p>Estimation of the probability density of <code>X</code> (column space) by non parametric Gaussian kernels. </p><p>Data <code>X</code> can be univariate (p = 1) or multivariate (p &gt; 1). In the last case, function <code>dmkern</code> computes a multiplicative kernel such as in Scott &amp; Sain 2005 Eq.19, and the internal bandwidth matrix <code>H</code> is diagonal (see the code). **Note:  <code>H</code> in the code is often noted &quot;H^(1/2)&quot; in the litterature (e.g. Wikipedia).</p><p>The default bandwith is computed by:</p><ul><li><code>h</code> = <code>a</code> * n^(-1 / (p + 4)) * colstd(<code>X</code>)</li></ul><p>(<code>a</code> = 1 in Scott &amp; Sain 2005).</p><p><strong>References</strong></p><p>Scott, D.W., Sain, S.R., 2005. 9 - Multidimensional Density Estimation,  in: Rao, C.R., Wegman, E.J., Solka, J.L. (Eds.), Handbook of Statistics,  Data Mining and Data Visualization. Elsevier, pp. 229–261.  https://doi.org/10.1016/S0169-7161(04)24009-3</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie

using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

nlv = 2
fmda = fda(X, y; nlv = nlv) ;
pnames(fmda)
T = fmda.T
head(T)
n, p = size(T)

####  Probability density in the FDA score space (2D)

fm = Jchemo.dmkern(T) ;
pnames(fm)
fm.H
u = [1; 4; 150]
Jchemo.predict(fm, T[u, :]).pred

h = .3
fm = Jchemo.dmkern(T; h = h) ;
fm.H
Jchemo.predict(fm, T[u, :]).pred

h = [.3; .1]
fm = dmkern(T; h = h) ;
fm.H
Jchemo.predict(fm, T[u, :]).pred

npoints = 2^7
lims = [(minimum(T[:, j]), maximum(T[:, j])) for j = 1:nlv]
x1 = LinRange(lims[1][1], lims[1][2], npoints)
x2 = LinRange(lims[2][1], lims[2][2], npoints)
z = mpar(x1 = x1, x2 = x2)
grid = reduce(hcat, z)
m = nro(grid)
fm = dmkern(T) ;
#fm = dmkern(T; a = .5) ;
#fm = dmkern(T; h = .3) ;
res = Jchemo.predict(fm, grid) ;
pred_grid = vec(res.pred)
f = Figure(resolution = (600, 400))
ax = Axis(f[1, 1]; title = &quot;Density for FDA scores (Iris)&quot;,
    xlabel = &quot;Comp1&quot;, ylabel = &quot;Comp2&quot;)
co = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10)
Colorbar(f[1, 2], co; label = &quot;Density&quot;)
scatter!(ax, T[:, 1], T[:, 2],
    color = :red, markersize = 5)
#xlims!(ax, -15, 15) ;ylims!(ax, -15, 15)
f

## Univariate 
x = T[:, 1]
fm = dmkern(x) ;
#fm = dmkern(x; a = .5) ;
#fm = dmkern(x; h = .3) ;
pred = Jchemo.predict(fm, x).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
scatter!(ax, x, vec(pred);
    color = :red)
f

x = T[:, 1]
npoints = 2^8
lims = [minimum(x), maximum(x)]
#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]
grid = LinRange(lims[1], lims[2], npoints)
fm = dmkern(x) ;
#fm = dmkern(x; a = .5) ;
#fm = dmkern(x; h = .3) ;
pred_grid = Jchemo.predict(fm, grid).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
lines!(ax, grid, vec(pred_grid); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dmkern.jl#LL1-L116">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dmnorm" href="#Jchemo.dmnorm"><code>Jchemo.dmnorm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dmnorm(X = nothing; mu = nothing, S = nothing,
    simpl::Bool = false)
dmnorm!(X = nothing; mu = nothing, S = nothing,
    simpl::Bool = false)</code></pre><p>Normal probability density estimation.</p><ul><li><code>X</code> : X-data (n, p) used to estimate the mean and    the covariance matrix. If <code>nothing</code>, <code>mu</code> and <code>S</code>    must be provided.</li><li><code>mu</code> : Mean vector of the normal distribution.    If <code>nothing</code>, <code>mu</code> is computed by the column-means of <code>X</code>.</li><li><code>S</code> : Covariance matrix of the normal distribution.   If <code>nothing</code>, <code>S</code> is computed by cov(<code>X</code>; corrected = true).</li><li><code>simpl</code> : Boolean. If <code>true</code>, the constant term and the determinant    in the density formula are set to 1. Default to <code>false</code>.</li></ul><p>Data <code>X</code> can be univariate (p = 1) or multivariate (p &gt; 1). See examples.</p><p>When <code>simple = true</code>, the determinant of the covariance matrix (object <code>detS</code>)  and the constant (2 * pi)^(-p / 2) (object <code>cst</code>) in the density formula are  set to 1. The function returns a pseudo density that resumes to exp(-d / 2),  where d is the squared Mahalanobis distance to the fcenter.</p><p>This can for instance be useful when the number of columns (p) of  <code>X</code> becomes too large and when consequently:</p><ul><li><code>detS</code> tends to 0 or, conversely, to infinity</li><li><code>cst</code> tends to 0</li></ul><p>which makes impossible to compute the true density. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie

using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

nlv = 2
fmda = fda(X, y; nlv = nlv) ;
pnames(fmda)
T = fmda.T
head(T)
n, p = size(T)

####  Probability density in the FDA score space (2D)

## Class Setosa 
s = y .== &quot;setosa&quot;
zT = T[s, :]

fm = dmnorm(zT) ;
pnames(fm)
fm.Uinv 
fm.detS
pred = Jchemo.predict(fm, zT).pred
head(pred) 

mu = colmean(zT)
S = cov(zT)
dmnorm(; mu = mu, S = S).Uinv
dmnorm(; mu = mu, S = S).detS

npoints = 2^7
lims = [(minimum(zT[:, j]), maximum(zT[:, j])) for j = 1:nlv]
x1 = LinRange(lims[1][1], lims[1][2], npoints)
x2 = LinRange(lims[2][1], lims[2][2], npoints)
z = mpar(x1 = x1, x2 = x2)
grid = reduce(hcat, z)
m = nro(grid)
fm = dmnorm(zT) ;
res = Jchemo.predict(fm, grid) ;
pred_grid = vec(res.pred)
f = Figure(resolution = (600, 400))
ax = Axis(f[1, 1]; title = &quot;Density for FDA scores (Iris - Setosa)&quot;,
    xlabel = &quot;Comp1&quot;, ylabel = &quot;Comp2&quot;)
co = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10)
Colorbar(f[1, 2], co; label = &quot;Density&quot;)
scatter!(ax, T[:, 1], T[:, 2],
    color = :red, markersize = 5)
scatter!(ax, zT[:, 1], zT[:, 2],
    color = :blue, markersize = 5)
#xlims!(ax, -15, 15) ;ylims!(ax, -15, 15)
f

## Univariate 
x = zT[:, 1]
fm = dmnorm(x) ;
pred = Jchemo.predict(fm, x).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
scatter!(ax, x, vec(pred);
    color = :red)
f

x = zT[:, 1]
npoints = 2^8
lims = [minimum(x), maximum(x)]
#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]
grid = LinRange(lims[1], lims[2], npoints)
fm = dmnorm(x) ;
pred_grid = Jchemo.predict(fm, grid).pred 
f = Figure()
ax = Axis(f[1, 1])
hist!(ax, x; bins = 30, normalization = :pdf)  # area = 1
lines!(ax, grid, vec(pred_grid); color = :red)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dmnorm.jl#LL1-L116">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dmnormlog" href="#Jchemo.dmnormlog"><code>Jchemo.dmnormlog</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dmnormlog(X = nothing; mu = nothing, S = nothing,
    simpl::Bool = false)
dmnormlog!(X = nothing; mu = nothing, S = nothing,
    simpl::Bool = false)</code></pre><p>Logarithm of the normal probability density estimation.</p><ul><li><code>X</code> : X-data (n, p) used to estimate the mean and    the covariance matrix. If <code>nothing</code>, <code>mu</code> and <code>S</code>    must be provided.</li><li><code>mu</code> : Mean vector of the normal distribution.    If <code>nothing</code>, <code>mu</code> is computed by the column-means of <code>X</code>.</li><li><code>S</code> : Covariance matrix of the normal distribution.   If <code>nothing</code>, <code>S</code> is computed by cov(<code>X</code>; corrected = true).</li><li><code>simpl</code> : Boolean. If <code>true</code>, the constant term and the determinant    in the density formula are set to 1. Default to <code>false</code>.    See <code>dmnorm</code> for details.</li></ul><p>See the help of function <code>dmnorm</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, CairoMakie

using JchemoData
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;iris.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)
tab(y) 

s = y .== &quot;setosa&quot;
zX = X[s, :]

fm = dmnormlog(zX) ;
pnames(fm)
fm.Uinv 
fm.logdetS
pred = Jchemo.predict(fm, zX).pred
head(pred) 

fm0 = dmnorm(zX) ;
pred0 = Jchemo.predict(fm0, zX).pred
head(log.(pred0))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dmnormlog.jl#LL1-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dummy" href="#Jchemo.dummy"><code>Jchemo.dummy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dummy(y)</code></pre><p>Build a table of dummy variables from a categorical variable.</p><ul><li><code>y</code> : A categorical variable.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">y = [&quot;d&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;b&quot;, &quot;c&quot;]
#y =  rand(1:3, 7)
res = dummy(y)
pnames(res)
res.Y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL182-L195">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.dupl-Tuple{Any}" href="#Jchemo.dupl-Tuple{Any}"><code>Jchemo.dupl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">dupl(X; digits = 3)</code></pre><p>Find duplicated rows in a dataset.</p><ul><li><code>X</code> : A dataset.</li><li><code>digits</code> : Nb. digits used to round <code>X</code> before checking.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Z = vcat(X, X[1:3, :], X[1:1, :])
dupl(X)
dupl(Z)

M = hcat(X, fill(missing, 5))
Z = vcat(M, M[1:3, :])
dupl(M)
dupl(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL219-L237">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ensure_df-Tuple{DataFrames.DataFrame}" href="#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}"><code>Jchemo.ensure_df</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ensure_df(X)</code></pre><p>Reshape <code>X</code> to a dataframe if necessary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL262-L265">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ensure_mat-Tuple{AbstractMatrix}" href="#Jchemo.ensure_mat-Tuple{AbstractMatrix}"><code>Jchemo.ensure_mat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ensure_mat(X)</code></pre><p>Reshape <code>X</code> to a matrix if necessary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL270-L273">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.eposvd-Tuple{Any}" href="#Jchemo.eposvd-Tuple{Any}"><code>Jchemo.eposvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eposvd(D; nlv = 1)</code></pre><p>Compute an orthogonalization matrix for calibration transfer of spectral data.</p><ul><li><code>D</code> : Data (m, p) containing the &quot;detrimental&quot; information on which spectra   (rows of a matrix X) have to be orthogonalized.</li><li><code>nlv</code> : Nb. of first loadings vectors of <code>D</code> considered for the    orthogonalization.</li></ul><p>The objective is to remove some detrimental information (e.g. humidity  patterns in signals, multiple spectrometers, etc.) from a dataset X (n, p).   The detrimental information is defined by the main row-directions  contained in a matrix <code>D</code> (m, p). </p><p>Function <code>eposvd</code> returns two objects:</p><ul><li><code>P</code> (p, <code>nlv</code>) : The matrix of the <code>nlv</code> first loading vectors of D,    computed from the SVD decomposition (non centered PCA) of <code>D</code>. </li><li><code>M</code> (p, p) : The orthogonalization matrix, i.e. that can be used    to orthogonolize X to <code>P</code>.</li></ul><p>The correction of any matrix X from the detrimental information <code>D</code>  is given by:</p><ul><li>X_corrected = X * <code>M</code>.</li></ul><p>Matrix <code>D</code> can be built from many different choices. For instance, two common  methods are:</p><ul><li>EPO (Roger et al. 2003, 2018): <code>D</code> is built from differences between spectra   collected under different conditions. </li><li>TOP (Andrew &amp; Fearn 2004): Each row of <code>D</code> is the mean spectrum computed for    a given instrument.</li></ul><p>A particular situation is the following. Assume that <code>D</code> is built from  some differences between matrices X1 and X2, and that a bilinear model  (e.g. PLSR) is fitted on X1_corrected = X1 * <code>M</code>. To predict new data  X2new with the fitted model, there is no need to correct X2new.</p><p><strong>References</strong></p><p>Andrew, A., Fearn, T., 2004. Transfer by orthogonal projection: making near-infrared  calibrations robust to between-instrument variation. Chemometrics and Intelligent  Laboratory Systems 72, 51–56. https://doi.org/10.1016/j.chemolab.2004.02.004</p><p>Roger, J.-M., Chauchard, F., Bellon-Maurel, V., 2003. EPO-PLS external parameter  orthogonalisation of PLS application to temperature-independent measurement  of sugar content of intact fruits.  Chemometrics and Intelligent Laboratory Systems 66, 191-204.  https://doi.org/10.1016/S0169-7439(03)00051-0</p><p>Roger, J.-M., Boulet, J.-C., 2018. A review of orthogonal projections for calibration.  Journal of Chemometrics 32, e3045. https://doi.org/10.1002/cem.3045</p><p>Zeaiter, M., Roger, J.M., Bellon-Maurel, V., 2006. Dynamic orthogonal projection.  A new method to maintain the on-line robustness of multivariate calibrations.  Application to NIR-based monitoring of wine fermentations. Chemometrics and Intelligent  Laboratory Systems, 80, 227–235. https://doi.org/10.1016/j.chemolab.2005.06.011</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/caltransfer.jld2&quot;) 
@load db dat
pnames(dat)
X1cal = dat.X1cal
X2cal = dat.X2cal
X1val = dat.X1val
X2val = dat.X2val

D = X1cal - X2cal
nlv = 2
res = eposvd(D; nlv = nlv)
res.M      # orthogonalization matrix
res.P      # detrimental directions (columns of matrix P = loadings of D)

## Corrected matrices
zX1 = X1val * res.M    
zX2 = X2val * res.M    

i = 1
f = Figure(resolution = (500, 300))
ax = Axis(f[1, 1])
lines!(zX1[i, :]; label = &quot;x1_correct&quot;)
lines!(ax, zX2[i, :]; label = &quot;x2_correct&quot;)
axislegend(position = :cb, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/eposvd.jl#LL2-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.err-Tuple{Any, Any}" href="#Jchemo.err-Tuple{Any, Any}"><code>Jchemo.err</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">err(pred, y)</code></pre><p>Compute the classification error rate (ERR).</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

fm = plsrda(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
err(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL62-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.euclsq-Tuple{Any, Any}" href="#Jchemo.euclsq-Tuple{Any, Any}"><code>Jchemo.euclsq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">euclsq(X, Y)</code></pre><p>Squared Euclidean distances  between the rows of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data.</li><li><code>Y</code> : Data.</li></ul><p>For <code>X</code>(n, p) and <code>Y</code> (m, p), the function returns an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)

euclsq(X, Y)

euclsq(X[1:1, :], Y[1:1, :])

euclsq(X[:, 1], 4)
euclsq(1, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/distances.jl#LL1-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fcenter-Tuple{Any, Any}" href="#Jchemo.fcenter-Tuple{Any, Any}"><code>Jchemo.fcenter</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fcenter(X, v)
fcenter!(X::AbstractMatrix, v)</code></pre><p>Center each column of <code>X</code>.</p><ul><li><code>X</code> : Data.</li><li><code>v</code> : Centering factors.</li></ul><p><strong>examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
xmeans = colmean(X)
fcenter(X, xmeans)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_cscale.jl#LL1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fcscale-Tuple{Any, Any, Any}" href="#Jchemo.fcscale-Tuple{Any, Any, Any}"><code>Jchemo.fcscale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fcscale(X, u, v)
fcscale!(X, u, v)</code></pre><p>Center and fscale each column of <code>X</code>.</p><ul><li><code>X</code> : Data.</li><li><code>u</code> : Centering factors.</li><li><code>v</code> : Scaling factors.</li></ul><p><strong>examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
xmeans = colmean(X)
xstds = colstd(X)
fcscale(X, xmeans, xstds)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_cscale.jl#LL37-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fda-Tuple{Any, Any}" href="#Jchemo.fda-Tuple{Any, Any}"><code>Jchemo.fda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fda(X, y; nlv, lb = 0, scal::Bool = false)
fda!(X::Matrix, y; nlv, lb = 0, scal::Bool = false)</code></pre><p>Factorial discriminant analysis (FDA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : y-data (n) (class membership).</li><li><code>nlv</code> : Nb. discriminant components.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>FDA by eigen factorization of Inverse(W) * B, where W is the &#39;Within&#39;-covariance  matrix (pooled over the classes), and B the &#39;Between&#39;-covariance matrix.</p><p>The function maximizes the compromise p&#39;Bp / p&#39;Wp, i.e. max p&#39;Bp with  constraint p&#39;Wp = 1. Vectors p (columns of P) are the linear discrimant  coefficients often referred to as &quot;LD&quot;.</p><p>A ridge regularization can be used:</p><ul><li>If <code>lb</code> &gt; 0, W is replaced by W + <code>lb</code> * I,    where I is the Idendity matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, StatsBase, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

tab(ytrain)
tab(ytest)

fm = fda(Xtrain, ytrain; nlv = 2) ;
#fm = fdasvd(Xtrain, ytrain; nlv = 2) ;
pnames(fm)
lev = fm.lev
nlev = length(lev)

fm.T
# Projections of the class centers to the score space
ct = fm.Tcenters 

group = copy(ytrain)
f, ax = plotxy(fm.T[:, 1], fm.T[:, 2], group;
    ellipse = true, title = &quot;FDA&quot;)
scatter!(ax, ct[:, 1], ct[:, 2];  
    markersize = 10, color = :red)
hlines!(ax, 0; color = :grey)
vlines!(ax, 0; color = :grey)
f

# Projection of Xtest to the score space
transf(fm, Xtest)

# X-loadings matrix
# Columns of P = coefficients of the linear discriminant function
# = &quot;LD&quot; of function lda of R package MASS
fm.P
fm.P&#39; * fm.P    # not orthogonal

fm.eig
fm.sstot
# Explained variance by PCA of the class centers 
# in transformed fscale
Base.summary(fm)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/fda.jl#LL1-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fdasvd-Tuple{Any, Any}" href="#Jchemo.fdasvd-Tuple{Any, Any}"><code>Jchemo.fdasvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fdasvd(X, y; nlv, lb = 0, scal::Bool = false)
fdasvd!(X, y; nlv, lb = 0, scal::Bool = false)</code></pre><p>Factorial discriminant analysis (FDA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate class membership.</li><li><code>nlv</code> : Nb. discriminant components.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>FDA by a weighted SVD factorization of the matrix of the class  centers (after spherical transformaton).  The function gives the same results as function <code>fda</code>.</p><p>A ridge regularization can be used:</p><ul><li>If <code>lb</code> &gt; 0, the within-class (pooled) covariance matrix W    is replaced by W + <code>lb</code> * I, where I is the Idendity matrix.</li></ul><p>See <code>?fda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/fdasvd.jl#LL1-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fdif-Tuple{Any}" href="#Jchemo.fdif-Tuple{Any}"><code>Jchemo.fdif</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fdif(X; npoint = 2)
fdif!(M::Matrix, X::Matrix; npoint = 2)</code></pre><p>Compute finite differences for each row of a matrix X. </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>M</code> : Pre-allocated output matrix (n, p - npoint + 1).</li><li><code>npoint</code> : Nb. points involved in the window for the finite differences.   The range of the window (= nb. intervals of two successive colums) is npoint - 1.</li></ul><p>The finite differences can be used for computing discrete derivates. The method reduces the column-dimension: (n, p) –&gt; (n, p - npoint + 1). </p><p>The in-place function stores the output in <code>M</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
wlstr = names(dat.X)
wl = parse.(Float64, wlstr)

Xp = fdif(X; npoint = 10)
plotsp(Xp[1:30, :]).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL57-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.findmax_cla-Tuple{Any}" href="#Jchemo.findmax_cla-Tuple{Any}"><code>Jchemo.findmax_cla</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">findmax_cla(x, weights = nothing)</code></pre><p>Find the most occurent level in <code>x</code>.</p><ul><li><code>x</code> : A categorical variable.</li></ul><p>If ex-aequos, the function returns the first.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(1:3, 10)
tab(x)
findmax_cla(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL283-L296">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.frob-Tuple{Any}" href="#Jchemo.frob-Tuple{Any}"><code>Jchemo.frob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">frob(X)
frob(X, w)</code></pre><p>Frobenius norm of a matrix.</p><ul><li><code>X</code> : A matrix (n, p).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>The Frobenius norm of <code>X</code> is:</p><ul><li>sqrt(tr(X&#39; * X)).</li></ul><p>The weighted norm is:</p><ul><li>sqrt(tr(X&#39; * D * X)), where D is the diagonal matrix of vector <code>w</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL309-L324">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fscale-Tuple{Any, Any}" href="#Jchemo.fscale-Tuple{Any, Any}"><code>Jchemo.fscale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fscale(X, v)
fscale!(X::AbstractMatrix, v)</code></pre><p>Scale each column of <code>X</code>.</p><ul><li><code>X</code> : Data.</li><li><code>v</code> : Scaling factors.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
fscale(X, colstd(X))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_cscale.jl#LL20-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.fweight-Tuple{Any}" href="#Jchemo.fweight-Tuple{Any}"><code>Jchemo.fweight</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fweight(d; typw = :bisquare, alpha = 0)</code></pre><p>Computation of weights from distances.</p><ul><li><code>d</code> : Vector of distances.</li><li><code>typw</code> : Define the weight function.</li><li><code>alpha</code> : Pareter of the weight function, see below.</li></ul><p>The returned weight vector is: </p><ul><li>w = f(<code>d</code> / q) where f is the weight function and q the 1-<code>alpha</code> </li></ul><p>quantile of <code>d</code> (Cleveland &amp; Grosse 1991).</p><p>Possible values for <code>typw</code> are: </p><ul><li>:bisquare: w = (1 - x^2)^2 </li><li>:cauchy: w = 1 / (1 + x^2) </li><li>:epan: w = 1 - x^2 </li><li>:fair: w =  1 / (1 + x)^2 </li><li>:invexp: w = exp(-x) </li><li>:invexp2: w = exp(-x / 2) </li><li>:gauss: w = exp(-x^2)</li><li>:trian: w = 1 - x  </li><li>:tricube: w = (1 - x^3)^3  </li></ul><p><strong>References</strong></p><p>Cleveland, W.S., Grosse, E., 1991. Computational methods for local regression.  Stat Comput 1, 47–62. https://doi.org/10.1007/BF01890836</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CairoMakie, Distributions

cols = cgrad(:tab10, collect(1:9)) ;
d = sort(sqrt.(rand(Chi(1), 1000)))
alpha = 0
typw = :bisquare
w = fweight(d; typw = typw, alpha = alpha)
f = Figure(resolution = (600, 500))
ax = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
lines!(ax, d, w, label = typw, color = cols[1])
typw = :cauchy
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[2])
typw = :epan
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[3])
typw = :fair
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[4])
typw = :gauss
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[5])
typw = :trian
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[6])
typw = :invexp
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[7])
typw = :invexp2
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[8])
typw = :tricube
w = fweight(d; typw = typw, alpha = alpha)
lines!(ax, d, w, label = typw, color = cols[9])
axislegend(&quot;Function&quot;)
f[1, 1] = ax
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/fweight.jl#LL1-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.getknn-Tuple{Any, Any}" href="#Jchemo.getknn-Tuple{Any, Any}"><code>Jchemo.getknn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">getknn(Xtrain, X; k = 1, metric = :eucl)</code></pre><p>Return the k nearest neighbors in Xtrain of each row of <code>X</code>.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>X</code> : Query X-dta.</li><li><code>metric</code> : Type of distance used for the query.    Possible values are :eucl or :mah.</li></ul><p>The distances (not squared) are also returned.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(5, 3)
X = rand(2, 3)
x = X[1:1, :]

k = 3
res = getknn(Xtrain, X; k = k)
res.ind  # indexes
res.d    # distances

res = getknn(Xtrain, x; k = k)
res.ind

res = getknn(Xtrain, X; k = k, metric = :mah)
res.ind</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/getknn.jl#LL1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcv-Tuple{Any, Any}" href="#Jchemo.gridcv-Tuple{Any, Any}"><code>Jchemo.gridcv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcv(X, Y; segm, score, fun, pars, verbose = false)</code></pre><p>Cross-validation (CV) over a grid of parameters.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>segm</code> : Segments of the CV (output of functions    <a href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>segmts</code></a>, <a href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>segmkf</code></a> etc.).</li><li><code>score</code> : Function (e.g. <code>msep</code>) computing a prediction score.</li><li><code>fun</code> : Function computing the prediction model.</li><li><code>pars</code> : tuple of named vectors (arguments of <code>fun</code>)    defining the grid of parameters (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>Compute a prediction score (= error rate) for a given model over a grid of parameters.</p><p>The score is computed over the training sets <code>X</code> and <code>Y</code> for each combination  of the grid defined in <code>pars</code>. </p><p>The vectors in <code>pars</code> must have same length.</p><p>The function returns two outputs: <code>res</code> (mean results) and <code>res_p</code> (results per replication).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

# Building Train (years &lt;= 2012) and Test  (year = 2012)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)

# KNNR models

K = 5 ; rep = 1
segm = segmkf(ntrain, K; rep = rep)

nlvdis = 15 ; metric = [:mah ]
h = [1 ; 2.5] ; k = [5 ; 10 ; 20 ; 50] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) 
length(pars[1]) 
res = gridcv(Xtrain, ytrain; segm = segm, 
    score = rmsep, fun = knnr, pars = pars, verbose = true).res ;
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]

fm = knnr(Xtrain, ytrain;
    nlvdis = res.nlvdis[u], metric = res.metric[u],
    h = res.h[u], k = res.k[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

################# PLSR models

K = 5 ; rep = 1
segm = segmkf(ntrain, K; rep = rep)

nlv = 0:20
res = gridcvlv(Xtrain, ytrain; segm = segm, 
    score = rmsep, fun = plskern, nlv = nlv).res
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
plotgrid(res.nlv, res.y1;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

fm = plskern(Xtrain, ytrain; nlv = res.nlv[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

# LWPLSR models

K = 5 ; rep = 1
segm = segmkf(ntrain, K; rep = rep)

nlvdis = 15 ; metric = [:mah ]
h = [1 ; 2.5 ; 5] ; k = [50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
nlv = 0:20
res = gridcvlv(Xtrain, ytrain; segm = segm, 
    score = rmsep, fun = lwplsr, pars = pars, nlv = nlv, verbose = true).res
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSECV&quot;).f

fm = lwplsr(Xtrain, ytrain;
    nlvdis = res.nlvdis[u], metric = res.metric[u],
    h = res.h[u], k = res.k[u], nlv = res.nlv[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

################# RR models

K = 5 ; rep = 1
segm = segmkf(ntrain, K; rep = rep)

lb = (10.).^collect(-5:1:-1)
res = gridcvlb(Xtrain, ytrain; segm = segm, 
    score = rmsep, fun = rr, lb = lb).res
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
plotgrid(log.(res.lb), res.y1;
    xlabel = &quot;Lambda&quot;, ylabel = &quot;RMSECV&quot;).f

fm = rr(Xtrain, ytrain; lb = res.lb[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

################# KRR models

K = 5 ; rep = 1
segm = segmkf(ntrain, K; rep = rep)

gamma = (10.).^collect(-4:1:4)
pars = mpar(gamma = gamma)
length(pars[1]) 
lb = (10.).^collect(-5:1:-1)
res = gridcvlb(Xtrain, ytrain; segm = segm, 
    score = rmsep, fun = krr, pars = pars, lb = lb).res
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
group = string.(&quot;gamma=&quot;, res.gamma)
plotgrid(log.(res.lb), res.y1, group;
    xlabel = &quot;Lambda&quot;, ylabel = &quot;RMSECV&quot;).f

fm = krr(Xtrain, ytrain; gamma = res.gamma[u], lb = res.lb[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/gridcv.jl#LL1-L143">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcvlb-Tuple{Any, Any}" href="#Jchemo.gridcvlb-Tuple{Any, Any}"><code>Jchemo.gridcvlb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcvlb(X, Y; segm, score, fun, lb, pars, verbose = false)</code></pre><ul><li>See <code>gridcv</code>.</li><li><code>lb</code> : Value, or collection of values, of the ridge regularization parameter &quot;lambda&quot;.</li></ul><p>Same as <a href="#Jchemo.gridcv-Tuple{Any, Any}"><code>gridcv</code></a> but specific to (and much faster for) models  using ridge regularization (e.g. RR).</p><p>Argument <code>pars</code> must not contain <code>lb</code>.</p><p>See <code>?gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/gridcvlb.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridcvlv-Tuple{Any, Any}" href="#Jchemo.gridcvlv-Tuple{Any, Any}"><code>Jchemo.gridcvlv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridcvlv(X, Y; segm, score, fun, nlv, pars, verbose = false)</code></pre><ul><li>See <code>gridcv</code>.</li><li><code>nlv</code> : Nb., or collection of nb., of latent variables (LVs).</li></ul><p>Same as <a href="#Jchemo.gridcv-Tuple{Any, Any}"><code>gridcv</code></a> but specific to (and much faster for) models  using latent variables (e.g. PLSR).</p><p>Argument <code>pars</code> must not contain <code>nlv</code>.</p><p>See <code>?gridcv</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/gridcvlv.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscore-NTuple{4, Any}" href="#Jchemo.gridscore-NTuple{4, Any}"><code>Jchemo.gridscore</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscore(Xtrain, Ytrain, X, Y; score, fun, pars, verbose = FALSE)</code></pre><p>Model validation over a grid of parameters.</p><ul><li><code>Xtrain</code> : Training X-data (n, p).</li><li><code>Ytrain</code> : Training Y-data (n, q).</li><li><code>X</code> : Validation X-data (m, p).</li><li><code>Y</code> : Validation Y-data (m, q).</li><li><code>score</code> : Function (e.g. <code>msep</code>) computing the prediction score.</li><li><code>fun</code> : Function computing the prediction model.</li><li><code>pars</code> : tuple of named vectors (= arguments of fun) of same length   involved in the calculation of the score (e.g. output of function <code>mpar</code>).</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>Compute a prediction score (= error rate) for a given model over a grid of parameters.</p><p>The score is computed over the validation sets <code>X</code> and <code>Y</code> for each combination  of the grid defined in <code>pars</code>. </p><p>The vectors in <code>pars</code> must have same length.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

# Building Train (years &lt;= 2012) and Test  (year == 2012)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)

# Building Cal and Val within Train

nval = 80
s = sample(1:ntrain, nval; replace = false)
Xcal = rmrow(Xtrain, s)
ycal = rmrow(ytrain, s)
Xval = Xtrain[s, :]
yval = ytrain[s]

# KNNR models

nlvdis = 15 ; metric = [:mah ]
h = [1 ; 2.5] ; k = [5 ; 10 ; 20 ; 50] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) 
length(pars[1]) 
res = gridscore(Xcal, ycal, Xval, yval;
    score = rmsep, fun = knnr, pars = pars, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]

fm = knnr(Xtrain, ytrain;
    nlvdis = res.nlvdis[u], metric = res.metric[u],
    h = res.h[u], k = res.k[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

################# PLSR models

nlv = 0:20
res = gridscorelv(Xcal, ycal, Xval, yval;
    score = rmsep, fun = plskern, nlv = nlv)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
plotgrid(res.nlv, res.y1;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

fm = plskern(Xtrain, ytrain; nlv = res.nlv[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

# LWPLSR models

nlvdis = 15 ; metric = [:mah ]
h = [1 ; 2.5 ; 5] ; k = [50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
length(pars[1]) 
nlv = 0:20
res = gridscorelv(Xcal, ycal, Xval, yval;
    score = rmsep, fun = lwplsr, pars = pars, nlv = nlv, verbose = true)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSECV&quot;).f

fm = lwplsr(Xtrain, ytrain;
    nlvdis = res.nlvdis[u], metric = res.metric[u],
    h = res.h[u], k = res.k[u], nlv = res.nlv[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

################# RR models

lb = (10.).^collect(-5:1:-1)
res = gridscorelb(Xcal, ycal, Xval, yval;
    score = rmsep, fun = rr, lb = lb)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
plotgrid(log.(res.lb), res.y1;
    xlabel = &quot;Lambda&quot;, ylabel = &quot;RMSECV&quot;).f

fm = rr(Xtrain, ytrain; lb = res.lb[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)

################# KRR models

gamma = (10.).^collect(-4:1:4)
pars = mpar(gamma = gamma)
length(pars[1]) 
lb = (10.).^collect(-5:1:-1)
res = gridscorelb(Xcal, ycal, Xval, yval;
    score = rmsep, fun = krr, pars = pars, lb = lb)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
group = string.(&quot;gamma=&quot;, res.gamma)
plotgrid(log.(res.lb), res.y1, group;
    xlabel = &quot;Lambda&quot;, ylabel = &quot;RMSECV&quot;).f

fm = krr(Xtrain, ytrain; gamma = res.gamma[u], lb = res.lb[u]) ;
pred = Jchemo.predict(fm, Xtest).pred 
rmsep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/gridscore.jl#LL1-L135">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscorelb-NTuple{4, Any}" href="#Jchemo.gridscorelb-NTuple{4, Any}"><code>Jchemo.gridscorelb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscorelb(Xtrain, Ytrain, X, Y; score, fun, lb, pars, verbose = FALSE)</code></pre><ul><li>See <code>gridscore</code>.</li><li><code>lb</code> : Value, or collection of values, of the ridge regularization parameter &quot;lambda&quot;.</li></ul><p>Same as <a href="#Jchemo.gridscore-NTuple{4, Any}"><code>gridscore</code></a> but specific to (and much faster for) models  using ridge regularization (e.g. RR).</p><p>Argument <code>pars</code> must not contain <code>lb</code>.</p><p>See <code>?gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/gridscorelb.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.gridscorelv-NTuple{4, Any}" href="#Jchemo.gridscorelv-NTuple{4, Any}"><code>Jchemo.gridscorelv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gridscorelv(Xtrain, Ytrain, X, Y; score, fun, pars, nlv, verbose = FALSE)</code></pre><ul><li>See <code>gridscore</code>.</li><li><code>nlv</code> : Nb., or collection of nb., of latent variables (LVs).</li></ul><p>Same as <a href="#Jchemo.gridscore-NTuple{4, Any}"><code>gridscore</code></a> but specific to (and much faster for) models  using latent variables (e.g. PLSR).</p><p>Argument <code>pars</code> must not contain <code>nlv</code>.</p><p>See <code>?gridscore</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/gridscorelv.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.head-Tuple{Any}" href="#Jchemo.head-Tuple{Any}"><code>Jchemo.head</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">head(X)</code></pre><p>Display the first rows of a dataset.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(100, 5)
head(X)
@head X</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL328-L338">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.interpl-Tuple{Any}" href="#Jchemo.interpl-Tuple{Any}"><code>Jchemo.interpl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">interpl(X, wl; wlfin, fun = cubic_spline)</code></pre><p>Sampling signals by interpolation.</p><ul><li><code>X</code> : Matrix (n, p) of signals (rows).</li><li><code>wl</code> : Values representing the column &quot;names&quot; of <code>X</code>.    Must be a numeric vector of length p, or an AbstractRange.</li><li><code>wlfin</code> : Final values where to interpolate within the range of <code>wl</code>.   Must be a numeric vector, or an AbstractRange.</li><li><code>fun</code> : Function defining the interpolation method.</li></ul><p>The function uses package DataInterpolations.jl.</p><p>Possible values of <code>fun</code> (methods) are:</p><ul><li><code>linear_int</code>: A linear interpolation (LinearInterpolation).</li><li><code>quadratic_int</code>: A quadratic interpolation (QuadraticInterpolation).</li><li><code>quadratic_spline</code>: A quadratic spline interpolation(QuadraticSpline).</li><li><code>cubic_spline</code>: A cubic spline interpolation (CubicSpline)</li></ul><p><strong>References</strong></p><p>Package Interpolations.jl https://github.com/PumasAI/DataInterpolations.jl https://htmlpreview.github.io/?https://github.com/PumasAI/DataInterpolations.jl/blob/v2.0.0/example/DataInterpolations.html</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
wlstr = names(X)
wl = parse.(Float64, wlstr) 

plotsp(X[1:10,:], wl).f

wlfin = collect(range(500, 2400, length = 10))
#wlfin = range(500, 2400, length = 10)
Xp = interpl(X[1:10, :], wl; wlfin = wlfin) 
plotsp(Xp, wlfin).f

Xp = interpl_mon(X[1:10, :], wl; wlfin = wlfin) ;
plotsp(Xp, wlfin).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL110-L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.iqr-Tuple{Any}" href="#Jchemo.iqr-Tuple{Any}"><code>Jchemo.iqr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">iqr(x)</code></pre><p>Compute the interquartile interval (IQR).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(100)
iqr(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL357-L366">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.isel" href="#Jchemo.isel"><code>Jchemo.isel</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isel(X, Y, wl = 1:nco(X); rep = 1, 
    nint = 5, psamp = 1/3, score = rmsep, 
    fun, kwargs...)</code></pre><p>Interval variable selection.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>wl</code> : Optional numeric labels (p, 1) of the X-columns.  </li><li><code>rep</code> : Number of replications. </li><li><code>nint</code> : Nb. intervals. </li><li><code>psamp</code> : Proportion of data used as test set to compute the <code>score</code>   (default: n/3 of the data).</li><li><code>score</code> : Function computing the prediction score (= error rate; e.g. msep).</li><li><code>fun</code> : Function defining the prediction model.</li><li><code>kwarg</code> : Optional other arguments to pass to funtion defined in <code>fun</code>.</li></ul><p>The principle is as follows:</p><ul><li>Data (X, Y) are splitted randomly to a training and a test set.</li><li>Range 1:p in <code>X</code> is segmented to <code>nint</code> intervals of equal (when possible) size. </li><li>The model is fitted on the training set and the score (error rate) on the test set,    firtsly accounting for all the p variables (reference) and secondly    for each of the <code>nint</code> intervals. </li><li>This process is replicated <code>rep</code> times. Average results are provided in the outputs,   as well the results per replication. </li></ul><p><strong>References</strong></p><ul><li>Nørgaard, L., Saudland, A., Wagner, J., Nielsen, J.P., Munck, L., </li></ul><p>Engelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419. https://doi.org/10.1366/0003702001949500</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, DataFrames, JLD2
using CairoMakie

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/tecator.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
Y = dat.Y 
wlstr = names(X)
wl = parse.(Float64, wlstr) 
typ = Y.typ
y = Y.fat

f = 15 ; pol = 3 ; d = 2 
Xp = savgol(snv(X); f = f, pol = pol, d = d) 

s = typ .== &quot;train&quot;
Xtrain = Xp[s, :]
ytrain = y[s]

nint = 10
nlv = 5
res = isel(Xtrain, ytrain, wl; rep = 20, 
    nint = nint, fun = plskern, nlv = nlv) ;
res.res_rep
res.res0_rep
zres = res.res
zres0 = res.res0

f = Figure(resolution = (900, 400))
ax = Axis(f[1, 1],
    xlabel = &quot;Wawelength (nm)&quot;, ylabel = &quot;RMSEP&quot;,
    xticks = zres.lo)
scatter!(ax, zres.mid, zres.y1; color = (:red, .5))
vlines!(ax, zres.lo; color = :grey,
    linestyle = :dash, linewidth = 1)
hlines!(ax, zres0.y1, linestyle = :dash)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/isel.jl#LL1-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kdeda-Tuple{Any, Any}" href="#Jchemo.kdeda-Tuple{Any, Any}"><code>Jchemo.kdeda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kdeda(X, y; prior = :unif, h = nothing, a = 1)</code></pre><p>Discriminant analysis using non-parametric kernel Gaussian      density estimation (KDE-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>h</code> : See <code>?dmkern</code>.</li><li><code>h</code> : See <code>?dmkern</code>.</li></ul><p>The principle is the same as functions <code>lda</code> and <code>qda</code> except  that densities are estimated from <code>dmkern</code> instead of  <code>dmnorm</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

tab(ytrain)
tab(ytest)

prior = :unif
#prior = :prop
fm = kdeda(Xtrain, ytrain; prior = prior) ;
#fm = kdeda(Xtrain, ytrain; prior = prior, a = .5) ;
#fm = kdeda(Xtrain, ytrain; prior = prior, h = .1) ;
pnames(fm)
fm.fm[1].H

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.dens
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kdeda.jl#LL1-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.knnda-Tuple{Any, Any}" href="#Jchemo.knnda-Tuple{Any, Any}"><code>Jchemo.knnda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">knnda(X, y; nlvdis = 0, metric = :eucl, h = Inf, k = 1, tol = 1e-4)</code></pre><p>k-Nearest-Neighbours weighted discrimination (kNN-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors.    Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) computations.</li></ul><p>For each new observation to predict:</p><ul><li>i) a number of <code>k</code> nearest neighbors (= &quot;weighting 1&quot;) is selected</li><li>ii) a weigthed (= &quot;weighting 2&quot;) vote is then computed in this neighborhood    to select the most frequent class. </li></ul><p>Weightings 1 and 2 are computed from the dissimilarities between the observation  to predict and the training observations. Depending on argument <code>nlvdis</code>,  the computation is done from the raw X-data or after a dimension reduction.  In the last case, global PLS2 scores (LVs) are  computed from {<code>X</code>, Y-dummy} (where Y-dummy is the dummy table build from <code>y</code>),  and the dissimilarities are computed over these scores. </p><p>In general, for high dimensional X-data, using the Mahalanobis distance requires  preliminary dimensionality reduction of the data.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 10
fm = knnda(Xtrain, ytrain;
    nlvdis = nlvdis, metric = metric,
    h = h, k = k) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

res.listnn
res.listd
res.listw</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/knnda.jl#LL1-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.knnr-Tuple{Any, Any}" href="#Jchemo.knnr-Tuple{Any, Any}"><code>Jchemo.knnr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">knnr(X, Y; nlvdis = 0, metric = :eucl, h = Inf, k = 1, 
    tol = 1e-4, scal::Bool = false)</code></pre><p>k-Nearest-Neighbours regression (KNNR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) computations.</li></ul><p>The function uses functions <code>getknn</code> and <code>locw</code>;  see the code for details. Many other variants of kNNR pipelines can be built.</p><p>The general principle of the method is as follows.</p><p>For each new observation to predict, the prediction is the weighted mean over the selected neighborhood (in <code>X</code>). Within the selected neighborhood, the weights are defined from the dissimilarities between the new observation  and the neighborhood, and are computed from function &#39;wdist&#39;.</p><p>In general, for high dimensional X-data, using the Mahalanobis distance requires  preliminary dimensionality reduction of the data.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 20 ; metric = :mah 
h = 2 ; k = 100 ; nlv = 15
fm = knnr(Xtrain, ytrain; nlvdis = nlvdis,
    metric = metric, h = h, k = k) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
f, ax = scatter(vec(pred), ytest)
ablines!(ax, 0, 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/knnr.jl#LL1-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kpca-Tuple{Any}" href="#Jchemo.kpca-Tuple{Any}"><code>Jchemo.kpca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kpca(X, weights = ones(nro(X)); nlv, 
    kern = :krbf, scal::Bool = false, kwargs...)</code></pre><p>Kernel PCA  (Scholkopf et al. 1997, Scholkopf &amp; Smola 2002, Tipping 2001).</p><ul><li><code>X</code> : X-data.</li><li><code>weights</code> : vector (n,).</li><li><code>nlv</code> : Nb. principal components (PCs), or collection of nb. PCs, to consider. </li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are :krbf of :kpol (see respective    functions <code>krbf</code> and <code>kpol</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li><li><code>kwargs</code> : Named arguments to pass in the kernel function.    See <code>?krbf</code>, <code>?kpol</code>.</li></ul><p>The method is implemented by SVD factorization of the weighted Gram matrix  D^(1/2) * Phi(<code>X</code>) * Phi(<code>X</code>)&#39; * D^(1/2), where D is a diagonal matrix of weights for  the observations (rows of X).</p><p>The kernel Gram matrices are internally centered. </p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A., MÃ¼ller, K.-R., 1997. Kernel principal component analysis,  in: Gerstner, W., Germond, A., Hasler, M., Nicoud, J.-D. (Eds.), Artificial Neural Networks,  ICANN 97, Lecture Notes in Computer Science. Springer, Berlin, Heidelberg,  pp. 583-588. https://doi.org/10.1007/BFb0020217</p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization,  optimization, and beyond, Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p>Tipping, M.E., 2001. Sparse kernel principal component analysis. Advances in neural information  processing systems, MIT Press. http://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
Xtest = rmrow(X, s)

nlv = 3 ; gamma = 1e-4
fm = kpca(Xtrain; nlv = nlv, gamma = gamma) ;
pnames(fm)
fm.T
fm.T&#39; * fm.T
fm.P&#39; * fm.P

transf(fm, Xtest)

res = Base.summary(fm) ;
pnames(res)
res.explvarx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kpca.jl#LL1-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplsr-Tuple{Any, Any}" href="#Jchemo.kplsr-Tuple{Any, Any}"><code>Jchemo.kplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kplsr(X, Y, weights = ones(nro(X)); 
    nlv, kern = :krbf, tol = 1.5e-8, maxit = 100, 
    scal::Bool = false, kwargs...)
kplsr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); 
    nlv, kern = :krbf, tol = 1.5e-8, maxit = 100, 
    scal::Bool = false, kwargs...)</code></pre><p>Kernel partial least squares regression (KPLSR) implemented with a Nipals  algorithm (Rosipal &amp; Trejo, 2001).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider. </li><li>&#39;kern&#39; : Type of kernel used to compute the Gram matrices.   Possible values are :krbf or :kpol (see respective functions <code>krbf</code> and <code>kpol</code>).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. iterations.</li><li><code>kwargs</code> : Named arguments to pass in the kernel function.</li></ul><p>This algorithm becomes slow for n &gt; 1000.</p><p>The kernel Gram matrices are internally centered. </p><p><strong>References</strong></p><p>Rosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in  Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20 ; gamma = 1e-1
fm = kplsr(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;
fm.T

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.beta
Jchemo.coef(fm; nlv = 7).beta

transf(fm, Xtest)
transf(fm, Xtest; nlv = 7)

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

res = Jchemo.predict(fm, Xtest; nlv = 1:2)
res.pred[1]
res.pred[2]

fm = kplsr(Xtrain, ytrain; nlv = nlv, kern = :kpol, degree = 2, 
    gamma = 1e-1, coef0 = 10) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)

# Example of fitting the function sinc(x)
# described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
fm = kplsr(x, y; nlv = 2) ;
pred = Jchemo.predict(fm, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;ted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kplsr.jl#LL1-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kplsrda" href="#Jchemo.kplsrda"><code>Jchemo.kplsrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">kplsrda(X, y, weights = ones(nro(X)); nlv, kern = :krbf, 
    scal::Bool = false, kwargs...)</code></pre><p>Discrimination based on kernel partial least squares regression (KPLSR-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate class membership.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li>Other arguments to pass in the kernel: See <code>?kplsr</code>.</li></ul><p>This is the same approach as for <code>plsrda</code> except that the PLS2 step  is replaced by a non linear kernel PLS2 (KPLS).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

gamma = .001 
nlv = 15
fm = kplsrda(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;
pnames(fm)
typeof(fm.fm) # = KPLS2 model

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

Jchemo.coef(fm.fm)
transf(fm.fm, Xtest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kplsrda.jl#LL1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.kpol-Tuple{Any, Any}" href="#Jchemo.kpol-Tuple{Any, Any}"><code>Jchemo.kpol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">kpol(X, Y; degree = 1, gamma = 1, coef0 = 0)</code></pre><p>Compute a polynomial kernel Gram matrix. </p><ul><li><code>X</code> : Data.</li><li><code>Y</code> : Data.</li><li><code>degree</code> : Degree of the polynom.</li><li><code>gamma</code> : Scale of the polynom.</li><li><code>coef0</code> : Offset of the polynom.</li></ul><p>Given matrices <code>X</code> and <code>Y</code>of sizes (n, p) and (m, p), respectively, the function returns the (n, m) Gram matrix K(X, Y) = Phi(X) * Phi(Y)&#39;.</p><p>The polynomial kernel between two vectors x and y is computed by  (gamma * (x&#39; * y) + coef0)^degree.</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines,  regularization, optimization, and beyond, Adaptive computation and machine learning.  MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)
kpol(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kernels.jl#LL35-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.krbf-Tuple{Any, Any}" href="#Jchemo.krbf-Tuple{Any, Any}"><code>Jchemo.krbf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">krbf(X, Y; gamma = 1)</code></pre><p>Compute a Radial-Basis-Function (RBF) kernel Gram matrix. </p><ul><li><code>X</code> : Data.</li><li><code>Y</code> : Data.</li><li><code>gamma</code> : Scale parameter.</li></ul><p>Given matrices <code>X</code> and <code>Y</code>of sizes (n, p) and (m, p), respectively, the function returns the (n, m) Gram matrix K(X, Y) = Phi(X) * Phi(Y)&#39;.</p><p>The RBF kernel between two vectors x and y is computed by  exp(-gamma * ||x - y||^2).</p><p><strong>References</strong></p><p>Scholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines,  regularization, optimization, and beyond, Adaptive computation and machine learning.  MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)
krbf(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kernels.jl#LL1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.krr-Tuple{Any, Any}" href="#Jchemo.krr-Tuple{Any, Any}"><code>Jchemo.krr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">krr(X, Y, weights = ones(nro(X)); 
    lb = .01, kern = :krbf, scal::Bool = false, kwargs...)
krr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); 
    lb, kern = :krbf, scal::Bool = false, kwargs...)</code></pre><p>Kernel ridge regression (KRR) implemented by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li>&#39;kern&#39; : Type of kernel used to compute the Gram matrices.   Possible values are :krbf of :kpol (see respective functions <code>krbf</code> and <code>kpol</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li><code>kwargs</code> : Named arguments to pass in the kernel function.</li></ul><p>KRR is also referred to as least squared SVM regression (LS-SVMR). The method is close to the particular case of SVM regression  where there is novmarges excluding the observations (epsilon coefficient  set to zero). The difference is that a L2-norm optimization is done,  instead of L1 in SVM.</p><p>The kernel Gram matrices are internally centered. </p><p><strong>References</strong></p><p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and Applications,  NATO Science Series III: Computer &amp; Systems Sciences. IOS Press Amsterdam, pp. 227-250.</p><p>Cawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel Ridge Regression.  Neural Processing Letters 16, 293-302. https://doi.org/10.1023/A:1021798002258</p><p>Krell, M.M., 2018. Generalizing, Decoding, and Optimizing Support Vector Machine Classification.  arXiv:1801.04929.</p><p>Saunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression Learning Algorithm in Dual Variables,  in: In Proceedings of the 15th International Conference on Machine Learning. Morgan Kaufmann, pp. 515â521.</p><p>Suykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse approximation using  least squares support vector machines. 2000 IEEE International Symposium on Circuits and Systems.  Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439</p><p>Welling, M., n.d. Kernel ridge regression. Department of Computer Science,  University of Toronto, Toronto, Canada. https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

lb = 1e-3 ; gamma = 1e-1
fm = krr(Xtrain, ytrain; lb = lb, gamma = gamma) ;

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.A 
zcoef.df
Jchemo.coef(fm; lb = 1e-6).df

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

res = Jchemo.predict(fm, Xtest; lb = [.01 ; .001])
res.pred[1]
res.pred[2]

fm = krr(Xtrain, ytrain; lb = lb, kern = :kpol, 
    degree = 2, gamma = 1e-1, coef0 = 10) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

# Example of fitting the function sinc(x)
# described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
fm = krr(x, y; lb = 1e-1, gamma = 1 / 3) ;
pred = Jchemo.predict(fm, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;ted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/krr.jl#LL1-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.krrda" href="#Jchemo.krrda"><code>Jchemo.krrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">krrda(X, y, weights = ones(nro(X)); lb, 
    scal = scal, kern = :krbf, kwargs...)</code></pre><p>Discrimination based on kernel ridge regression (KRR-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate class membership.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li>Other arguments to pass in the kernel: See <code>?kplsr</code>.</li></ul><p>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1) variable.  Then, a RR is implemented on the <code>y</code> and each column of Ydummy, returning predictions of the dummy variables (= object <code>posterior</code> returned by  function <code>predict</code>).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

gamma = .01
lb = .001
fm = krrda(Xtrain, ytrain; lb = lb, gamma = gamma) ;    
pnames(fm)
pnames(fm.fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

Jchemo.predict(fm, Xtest; lb = [.1; .01]).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/krrda.jl#LL1-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lda" href="#Jchemo.lda"><code>Jchemo.lda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lda(X, y, weights = ones(nro(X)); 
    prior = :unif)</code></pre><p>Linear discriminant analysis  (LDA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform), :prop (proportional).</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

tab(ytrain)
tab(ytest)

prior = :unif
#prior = :prop
fm = lda(Xtrain, ytrain; prior = prior) ;
pnames(fm)
println(typeof(fm))

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.dens
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lda.jl#LL1-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lg-Tuple{Any, Any}" href="#Jchemo.lg-Tuple{Any, Any}"><code>Jchemo.lg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lg(X, Y; centr = true)
lg(Xbl; centr = true)</code></pre><p>Compute the Lg coefficient between matrices.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>Xbl</code> : A list (vector) of matrices.</li><li><code>centr</code> : Logical indicating if the matrices are internally    centered or not.</li></ul><p>Lg(X, Y) = Sum<em>j(=1..p) Sum</em>k(= 1..q) cov(xj, yk)^2</p><p>RV(X, Y) = Lg(X, Y) / sqrt(Lg(X, X), Lg(Y, Y))</p><p><strong>References</strong></p><p>Escofier, B. &amp; Pagès, J. 1984. L’analyse factorielle multiple.  Cahiers du Bureau universitaire de recherche opérationnelle.  Série Recherche, tome 42, p. 3-68</p><p>Escofier, B. &amp; Pagès, J. (2008). Analyses Factorielles Simples  et Multiples : Objectifs, Méthodes et Interprétation. Dunod,  4e édition.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
Y = rand(5, 3)
lg(X, Y)

X = rand(5, 15) 
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl)
lg(Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/angles.jl#LL1-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.list-Tuple{Integer, Any}" href="#Jchemo.list-Tuple{Integer, Any}"><code>Jchemo.list</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">list(n::Integer, type)</code></pre><p>Create a Vector{type}(undef, n).</p><p><code>isassigned(object, i)</code> can be used to check if cell i is empty.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">list(5, Float64)
list(5, Array{Float64})
list(5, Matrix{Float64})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL382-L394">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.list-Tuple{Integer}" href="#Jchemo.list-Tuple{Integer}"><code>Jchemo.list</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">list(n::Integer)</code></pre><p>Create a Vector{Any}(nothing, n).</p><p><code>isnothing(object, i)</code> can be used to check if cell i is empty.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">list(5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL369-L379">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.locw-Tuple{Any, Any, Any}" href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>Jchemo.locw</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">locw(Xtrain, Ytrain, X ; 
    listnn, listw = nothing, fun, verbose = false, kwargs...)</code></pre><p>Compute predictions for a given kNN model.</p><ul><li><code>Xtrain</code> : Training X-data.</li><li><code>Ytrain</code> : Training Y-data.</li><li><code>X</code> : X-data (m observations) to predict.</li><li><code>listnn</code> : List of m vectors of indexes.</li><li><code>listw</code> : List of m vectors of weights.</li><li><code>fun</code> : Function computing the model on the m neighborhoods.</li><li><code>verbose</code> : If true, fitting information are printed.</li><li><code>kwargs</code> : Keywords arguments to pass in function fun. </li></ul><p>Each component i of <code>listnn</code> and <code>listw</code> contains the indexes and weights, respectively, of the nearest neighbors of x_i in Xtrain. The sizes of the neighborhood  for i = 1,...,m can be different.</p><p>All the arguments in kwargs must have length = 1 (not collections).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/locw.jl#LL1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.locwlv-Tuple{Any, Any, Any}" href="#Jchemo.locwlv-Tuple{Any, Any, Any}"><code>Jchemo.locwlv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">locwlv(Xtrain, Ytrain, X; 
    listnn, listw = nothing, fun, nlv, verbose = true, kwargs...)</code></pre><p>Compute predictions for a given kNN model.</p><ul><li><code>nlv</code> : Nb. or collection of nb. of latent variables (LVs).</li></ul><p>Same as <a href="#Jchemo.locw-Tuple{Any, Any, Any}"><code>locw</code></a> but specific (and much faster) for LV-based (e.g. PLSR) models.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/locwlv.jl#LL1-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwmlr-Tuple{Any, Any}" href="#Jchemo.lwmlr-Tuple{Any, Any}"><code>Jchemo.lwmlr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwmlr(X, Y; metric = :eucl, h, k, 
    tol = 1e-4, verbose = false)</code></pre><p>k-Nearest-Neighbours locally weighted multiple linear regression (kNN-LWMLR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same principle as function <code>lwplsr</code> except that MLR models are fitted (on the neighborhoods) instead of PLSR models.  The neighborhoods  are computed on <code>X</code> (there is no preliminary dimension reduction).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 20
zfm = pcasvd(Xtrain; nlv = nlv) ;
Ttrain = zfm.T 
Ttest = transf(zfm, Xtest)

fm = lwmlr(Ttrain, ytrain; metric = :mah,
    h = 2, k = 100) ;
pred = Jchemo.predict(fm, Ttest).pred
println(rmsep(pred, ytest))
plotxy(pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed (Test)&quot;).f  

####### Example of fitting the function sinc(x)
####### described in Rosipal &amp; Trejo 2001 J of Machine Learning Res. p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
fm = lwmlr(x, y; metric = :eucl, h = 1, k = 20) ;
pred = Jchemo.predict(fm, x).pred 
f = Figure(resolution = (700, 300))
ax = Axis(f[1, 1])
scatter!(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;ted model&quot;)
f[1, 2] = Legend(f, ax, framevisible = false)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlr.jl#LL1-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwmlr_s-Tuple{Any, Any}" href="#Jchemo.lwmlr_s-Tuple{Any, Any}"><code>Jchemo.lwmlr_s</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwmlr_s(X, Y; reduc = :pls, 
    nlv, gamma = 1, psamp = 1, samp = :sys,
    metric = :eucl, h, k, 
    tol = 1e-4, scal::Bool = false, verbose = false)</code></pre><p>kNN-LWMLR after preliminary (linear or non-linear) dimension      reduction (kNN-LWMLR-S).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q). </li><li><code>reduc</code> : Type of dimension reduction. Possible values are:   :pca (PCA), :pls (PLS; default), :dkpls (direct Gaussian kernel PLS, see <code>?dkpls</code>).</li><li><code>nlv</code> : Nb. latent variables (LVs) for preliminary dimension reduction. </li><li><code>gamma</code> : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function <code>krbf</code>.</li><li><code>psamp</code> : Proportion of observations sampled in {<code>X</code>, <code>Y</code>} to compute the    loadings used to compute the scores of the preliminary dimension reduction.</li><li><code>samp</code> : Type of sampling applied for <code>psamp</code>. Possible values are:    :sys (systematic grid sampling over <code>rowsum(Y)</code>; default)    or :rand (random sampling).</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>The principle is as follows. A preliminary dimension reduction (parameter <code>nlv</code>)  of the X-data (n, p) returns a score matrix T (n, <code>nlv</code>). Then, a kNN-LWMLR  is done on {T, <code>Y</code>}.</p><p>The dimension reduction can be linear (PCA, PLS) or non linear (DKPLS), defined  in argument <code>reduc</code>.</p><p>When n is too large, the reduction dimension can become too costly, in particular for a kernel PLS (that requires to compute a matrix (n, n)). Argument <code>psamp</code> allows to sample a proportion of the observations that will be used to compute (approximate) scores T for the all X-data. </p><p>The case <code>reduc = :pca</code> corresponds to the &quot;LWR&quot; algorithm proposed  by Naes et al. (1990).</p><p><strong>References</strong></p><p>Naes, T., Isaksson, T., Kowalski, B., 1990. Locally weighted regression and scatter correction for near-infrared reflectance data.  Analytical Chemistry 664–673.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

fm = lwmlr_s(Xtrain, ytrain; reduc = :pca, 
    nlv = 20, metric = :eucl, h = 2, 
    k = 100) ;
pred = Jchemo.predict(fm, Xtest).pred
println(rmsep(pred, ytest))
plotxy(pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed (Test)&quot;).f  

fm = lwmlr_s(Xtrain, ytrain; reduc = :dkpls, 
    nlv = 20, gamma = .01, metric = :eucl, 
    h = 2, k = 100) ;
pred = Jchemo.predict(fm, Xtest).pred
rmsep(pred, ytest)

fm = lwmlr_s(Xtrain, ytrain; reduc = :dkpls, 
    nlv = 20, gamma = .01, psamp = .5, samp = :rand,
    metric = :eucl, h = 2, k = 100) ;
pred = Jchemo.predict(fm, Xtest).pred
rmsep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlr_s.jl#LL1-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwmlrda-Tuple{Any, Any}" href="#Jchemo.lwmlrda-Tuple{Any, Any}"><code>Jchemo.lwmlrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwmlrda(X, y; metric = :eucl, h, k, 
    tol = 1e-4, verbose = false)</code></pre><p>k-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same principle as function <code>lwmlr</code> except that local MLR-DA models are fitted instead of local MLR models.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlv = 20
zfm = pcasvd(Xtrain; nlv = nlv) ;
Ttrain = zfm.T 
Ttest = transf(zfm, Xtest)

metric = :mah
h = 2 ; k = 100
fm = lwmlrda(Ttrain, ytrain;
    metric = metric, h = h, k = k) ;
res = Jchemo.predict(fm, Ttest) ;
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

res.listnn
res.listd
res.listw</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlrda.jl#LL1-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwmlrda_s-Tuple{Any, Any}" href="#Jchemo.lwmlrda_s-Tuple{Any, Any}"><code>Jchemo.lwmlrda_s</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwmlrda_s(X, y; reduc = :pls, 
    nlv, gamma = 1, psamp = 1, samp = :cla, 
    metric = :eucl, h, k, 
    tol = 1e-4, scal::Bool = false, verbose = false)</code></pre><p>kNN-LWMLR-DA after preliminary (linear or non-linear) dimension      reduction (kNN-LWMLR-DA-S).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership.</li><li><code>reduc</code> : Type of dimension reduction. Possible values are:   :pca (PCA), :pls (PLS; default), :dkpls (direct Gaussian kernel PLS).</li><li><code>nlv</code> : Nb. latent variables (LVs) for preliminary dimension reduction. </li><li><code>gamma</code> : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function <code>krbf</code>.</li><li><code>psamp</code> : Proportion of observations sampled in <code>X, y</code>to compute the    loadings used to compute the scores.</li><li><code>samp</code> : Type of sampling applied for <code>psamp</code>. Possible values are    :cla (stratified random sampling over the classes in <code>y</code>; default)    or :rand (random sampling). </li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same principle as function <code>lwmlr_s</code> except that, locally, MLR-DA models are fitted instead of MLR models.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

fm = lwmlrda_s(Xtrain, ytrain; reduc = :pca, 
    nlv = 20, metric = :eucl, h = 2, k = 100) ;
pred = Jchemo.predict(fm, Xtest).pred
err(pred, ytest)
confusion(pred, ytest).cnt

fm = lwmlrda_s(Xtrain, ytrain; reduc = :dkpls, 
    nlv = 20, gamma = .01,
    metric = :eucl, h = 2, k = 100) ;
pred = Jchemo.predict(fm, Xtest).pred
err(pred, ytest)

fm = lwmlrda_s(Xtrain, ytrain; reduc = :dkpls, 
    nlv = 20, gamma = .01, psamp = .5, samp = :cla,
    metric = :eucl, h = 2, k = 100) ;
pred = Jchemo.predict(fm, Xtest).pred
err(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlrda_s.jl#LL1-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplslda-Tuple{Any, Any}" href="#Jchemo.lwplslda-Tuple{Any, Any}"><code>Jchemo.lwplslda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplslda(X, y; nlvdis, metric, h, k, nlv, prior = :unif, 
    tol = 1e-4, scal::Bool = false, verbose = false)</code></pre><p>kNN-LWPLS-LDA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors.    Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>prior</code> : Type of prior probabilities for class membership   (<code>unif</code>: uniform; <code>prop</code>: proportional).</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same methodology as for <code>lwplsr</code> except that  PLSR is replaced by PLS-LDA.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 100
nlv = 15
fm = lwplslda(Xtrain, ytrain;
    nlvdis = nlvdis, metric = metric,
    h = h, k = k, nlv = nlv) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

res.listnn
res.listd
res.listw</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplslda.jl#LL1-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsqda-Tuple{Any, Any}" href="#Jchemo.lwplsqda-Tuple{Any, Any}"><code>Jchemo.lwplsqda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsqda(X, y; nlvdis, metric, h, k, nlv, 
    alpha = 0, prior = :unif, tol = 1e-4, scal::Bool = false, 
    verbose = false)</code></pre><p>kNN-LWPLS-QDA (with continuum).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors.    Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>; default) and LDA (<code>alpha = 1</code>).</li><li><code>prior</code> : Type of prior probabilities for class membership   (<code>unif</code>: uniform; <code>prop</code>: proportional).</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same methodology as for <code>lwplsr</code> except that  PLSR is replaced by PLS-QDA.</p><p>The present version of the function suffers from frequent stops due to non positive definite matrices when doing local QDA.  The present recommandation is to select a sufficiant large number of neighbors. This will be fixed in the future.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 1000
nlv = 15
fm = lwplsqda(Xtrain, ytrain;
    nlvdis = nlvdis, metric = metric,
    h = h, k = k, nlv = nlv) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

res.listnn
res.listd
res.listw</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsqda.jl#LL1-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsr-Tuple{Any, Any}" href="#Jchemo.lwplsr-Tuple{Any, Any}"><code>Jchemo.lwplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsr(X, Y; nlvdis, metric, h, k, nlv, tol = 1e-4, verbose = false)</code></pre><p>k-Nearest-Neighbours locally weighted partial least squares regression (kNN-LWPLSR).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before computing the dissimilarities.    If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>Function <code>lwplsr</code> fits kNN-LWPLSR models (Lesnoff et al., 2020).  The function uses functions <code>getknn</code>, <code>locw</code> and a PLSR function;  see the code for details. Many other variants of kNN-LWPLSR pipelines can be built.</p><p>The general principles of the method are as follows.</p><p>LWPLSR is a particular case of weighted PLSR (WPLSR) (e.g. Schaal et al. 2002).  In WPLSR, a priori weights, different from the usual 1/n (standard PLSR),  are given to the n training observations. These weights are used for calculating  (i) the scores and loadings of the WPLS and (ii) the regression model that fits  (by weighted least squares) the Y-response(s) to the WPLS scores.  The specificity of LWPLSR (compared to WPLSR) is that the weights are computed  from dissimilarities (e.g. distances) between the new observation to predict  and the training observations (&quot;L&quot; in LWPLSR comes from &quot;localized&quot;).  Note that in LWPLSR the weights and therefore the fitted WPLSR model  change for each new observation to predict.</p><p>In the original LWPLSR, all the n training observations are used for each  observation to predict (e.g. Sicard &amp; Sabatier 2006, Kim et al 2011).  This can be very time consuming, in particular for large n.  A faster and often more efficient strategy is to preliminary select,  in the training set, a number of <code>k</code> nearest neighbors to the observation to predict  (= &quot;weighting 1&quot;) and then to apply LWPLSR only to this pre-selected  neighborhood (= &quot;weighting 2&quot;). This strategy corresponds to a kNN-LWPLSR  and is the one implemented in function <code>lwplsr</code>.</p><p>In <code>lwplsr</code>, the dissimilarities used for weightings 1 and 2 are  computed from the raw X-data or after a dimension reduction, depending on argument <code>nlvdis</code>. In the last case, global PLS2 scores (LVs) are  computed from {<code>X</code>, <code>Y</code>} and the dissimilarities are computed over these scores. </p><p>In general, for high dimensional X-data, using the Mahalanobis distance requires  preliminary dimensionality reduction of the data.</p><p><strong>References</strong></p><p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active  pharmaceutical ingredients content using locally weighted partial least squares  and statistical wavelength selection. Int. J. Pharm., 421, 269-274.</p><p>Lesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS  strategies for regression and discrimination on agronomic NIR data.  Journal of Chemometrics, e3209. https://doi.org/10.1002/cem.3209</p><p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric  statistics for the real time robot learning. Applied Intell., 17, 49-60.</p><p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression  and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 20 ; metric = :mah 
h = 1 ; k = 100 ; nlv = 15
fm = lwplsr(Xtrain, ytrain; nlvdis = nlvdis,
    metric = metric, h = h, k = k, nlv = nlv) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed (Test)&quot;).f  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsr.jl#LL1-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsr_s-Tuple{Any, Any}" href="#Jchemo.lwplsr_s-Tuple{Any, Any}"><code>Jchemo.lwplsr_s</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsr_s(X, Y; reduc = :pls, 
    nlv0, gamma = 1, psamp = 1, samp = :sys, 
    metric = :eucl, h, k, nlv, 
    tol = 1e-4, scal::Bool = false, verbose = false)</code></pre><p>kNN-LWPLSR after preliminary (linear or non-linear) dimension      reduction (kNN-LWPLSR-S).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>reduc</code> : Type of dimension reduction. Possible values are:   :pca (PCA), :pls (PLS; default), :dkpls (direct Gaussian kernel PLS, see <code>?dkpls</code>).</li><li><code>nlv0</code> : Nb. latent variables (LVs) for preliminary dimension reduction. </li><li><code>gamma</code> : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function <code>krbf</code>.</li><li><code>psamp</code> : Proportion of observations sampled in {<code>X</code>, <code>Y</code>} to compute the    loadings used to compute the scores of the preliminary dimension reduction.</li><li><code>samp</code> : Type of sampling applied for <code>psamp</code>. Possible values are:    :sys (systematic grid sampling over <code>rowsum(Y)</code>) or :rand (random sampling).</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the models fitted on preliminary    scores.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>The principle is as follows. A preliminary dimension reduction (parameter <code>nlv0</code>)  of the X-data (n, p) returns a score matrix T (n, <code>nlv</code>). Then, a kNN-LWPLSR  is done on {T, <code>Y</code>}. This is a fast approximation of kNN-LWPLSR using the same  principle as in Shen et al 2019.</p><p>The dimension reduction can be linear (PCA, PLS) or non linear (DKPLS), defined  in argument <code>reduc</code>.</p><p>When n is too large, the reduction dimension can become too costly, in particular for a kernel PLS (that requires to compute a matrix (n, n)). Argument <code>psamp</code> allows to sample a proportion of the observations that will be used to compute (approximate) scores T for the all X-data. </p><p>Setting <code>nlv = nlv0</code> returns the same predicions as function <code>lwmlr_s</code>.</p><p><strong>References</strong></p><p>Lesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS  strategies for regression and discrimination on agronomic NIR data.  Journal of Chemometrics, e3209. https://doi.org/10.1002/cem.3209</p><p>Shen, G., Lesnoff, M., Baeten, V., Dardenne, P., Davrieux, F., Ceballos, H., Belalcazar, J.,  Dufour, D., Yang, Z., Han, L., Pierna, J.A.F., 2019. Local partial least squares based on global PLS scores.  Journal of Chemometrics 0, e3117. https://doi.org/10.1002/cem.3117</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv0 = 20 ; metric = :mah 
h = 2 ; k = 100 ; nlv = 10
fm = lwplsr_s(Xtrain, ytrain; nlv0 = nlv0,
    metric = metric, h = h, k = k, nlv = nlv) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed (Test)&quot;).f  

fm = lwplsr_s(Xtrain, ytrain; reduc = :dkpls, 
    nlv0 = nlv0, gamma = .1, 
    metric = metric, h = h, k = k, 
    nlv = nlv) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed (Test)&quot;).f  

fm = lwplsr_s(Xtrain, ytrain; reduc = :dkpls, 
    nlv0 = nlv0, gamma = .1, psamp = .7, samp = :rand, 
    metric = metric, h = h, k = k,
    nlv = nlv) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsr_s.jl#LL1-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsravg-Tuple{Any, Any}" href="#Jchemo.lwplsravg-Tuple{Any, Any}"><code>Jchemo.lwplsravg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsravg(X, Y; nlvdis, metric, h, k, nlv, 
    typf = :unif, typw = :bisquare, alpha = 0, K = 5, rep = 10,
    tol = 1e-4, scal::Bool = false, verbose = false)</code></pre><p>Averaging kNN-LWPLSR models with different numbers of      latent variables (LVs).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before calculating the dissimilarities.        If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors.    Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : A character string such as &quot;5:20&quot; defining the range of the numbers of LVs    to consider (&quot;5:20&quot;: the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as &quot;10&quot; is also allowed (&quot;10&quot;: correponds to    the single model with 10 LVs).   </li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li><li>Other arguments: see ?plsravg.</li></ul><p>Ensemblist method where the predictions of each local model are computed  are computed by averaging or stacking the predictions of a set of models  built with different numbers of latent variables (LVs).</p><p>For instance, if argument <code>nlv</code> is set to <code>nlv = &quot;5:10&quot;</code>, the prediction for  a new observation is the average (eventually weighted) or stacking of the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.</p><p>See ?plsravg.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlvdis = 20 ; metric = :mah 
h = 1 ; k = 100 ; nlv = &quot;5:15&quot;
fm = lwplsravg(Xtrain, ytrain; nlvdis = nlvdis,
    metric = metric, h = h, k = k, nlv = nlv) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
f, ax = scatter(vec(res.pred), ytest)
ablines!(ax, 0, 1)
f

fm = lwplsravg(Xtrain, ytrain; nlvdis = nlvdis,
    metric = metric, h = h, k = k, nlv = nlv,
    typf = :cv) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsravg.jl#LL1-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsrda-Tuple{Any, Any}" href="#Jchemo.lwplsrda-Tuple{Any, Any}"><code>Jchemo.lwplsrda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsrda(X, y; nlvdis, metric, h, k, nlv, tol = 1e-4,
    scal::Bool = false, verbose = false)</code></pre><p>kNN-LWPLSR-DA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>nlvdis</code> : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction.</li><li><code>metric</code> : Type of dissimilarity used to select the neighbors.    Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same methodology as for <code>lwplsr</code> except that  PLSR is replaced by PLSR-DA.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlvdis = 25 ; metric = :mah
h = 2 ; k = 100
nlv = 15
fm = lwplsrda(Xtrain, ytrain;
    nlvdis = nlvdis, metric = metric,
    h = h, k = k, nlv = nlv) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

res.listnn
res.listd
res.listw</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsrda.jl#LL1-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.lwplsrda_s-Tuple{Any, Any}" href="#Jchemo.lwplsrda_s-Tuple{Any, Any}"><code>Jchemo.lwplsrda_s</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">lwplsrda_s(X, y; reduc = :pls, 
    nlv0, gamma = 1, psamp = 1, samp = :cla, 
    metric = :eucl, h, k, nlv, 
    tol = 1e-4, scal::Bool = false, verbose = false)</code></pre><p>kNN-LWPLSR-DA after preliminary (linear or non-linear) dimension      reduction (kNN-LWPLSR-DA-S).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : Univariate class membership.</li><li><code>reduc</code> : Type of dimension reduction. Possible values are:   :pca (PCA), :pls (PLS; default), :dkpls (direct Gaussian kernel PLS).</li><li><code>nlv0</code> : Nb. latent variables (LVs) for preliminary dimension reduction. </li><li><code>gamma</code> : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function <code>krbf</code>.</li><li><code>psamp</code> : Proportion of observations sampled in <code>X, Y</code>to compute the    loadings used to compute the scores.</li><li><code>samp</code> : Type of sampling applied for <code>psamp</code>. Possible values are    :cla (stratified random sampling over the classes in <code>y</code>; default)    or :rand (random sampling). </li><li><code>metric</code> : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are :eucl (default; Euclidean distance)    and :mah (Mahalanobis distance).</li><li><code>h</code> : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function <code>wdist</code>.</li><li><code>k</code> : The number of nearest neighbors to select for each observation to predict.</li><li><code>nlv</code> : Nb. latent variables (LVs) for the models fitted on preliminary    scores.</li><li><code>tol</code> : For stabilization when very close neighbors.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.</li><li><code>verbose</code> : If true, fitting information are printed.</li></ul><p>This is the same principle as function <code>lwplsr_s</code> except that, locally, PLSR-DA models are fitted instead of PLSR models.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

fm = lwplsrda_s(Xtrain, ytrain; reduc = :pca, 
    nlv0 = 20, metric = :eucl, h = 2, 
    k = 100, nlv = 10) ;
pred = Jchemo.predict(fm, Xtest).pred
err(pred, ytest)
confusion(pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsrda_s.jl#LL1-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mad-Tuple{Any}" href="#Jchemo.mad-Tuple{Any}"><code>Jchemo.mad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mad(x)</code></pre><p>Compute the median absolute deviation (MAD), adjusted by a factor (1.4826) for asymptotically normal consistency. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(100)
mad(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL397-L408">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mahsq-Tuple{Any, Any}" href="#Jchemo.mahsq-Tuple{Any, Any}"><code>Jchemo.mahsq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mahsq(X, Y)
mahsq(X, Y, Sinv)</code></pre><p>Squared Mahalanobis distances  between the rows of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data.</li><li><code>Y</code> : Data.</li><li><code>Sinv</code> : Inverse of a covariance matrix S.   If not given, this is the uncorrected covariance matrix of <code>X</code>.</li></ul><p>When <code>X</code> and <code>Y</code>are (n, p) and (m, p), repectively, it returns an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)

mahsq(X, Y)

S = cov(X, corrected = false)
Sinv = inv(S)
mahsq(X, Y, Sinv)
mahsq(X[1:1, :], Y[1:1, :], Sinv)

mahsq(X[:, 1], 4)
mahsq(1, 4, 2.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/distances.jl#LL30-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mahsqchol-Tuple{Any, Any}" href="#Jchemo.mahsqchol-Tuple{Any, Any}"><code>Jchemo.mahsqchol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mahsqchol(X, Y)
mahsqchol(X, Y, Uinv)</code></pre><p>Compute the squared Mahalanobis distances (with a Cholesky factorization) between the observations (rows) of <code>X</code> and <code>Y</code>.</p><ul><li><code>X</code> : Data.</li><li><code>Y</code> : Data.</li><li><code>Uinv</code> : Inverse of the upper matrix of a Cholesky factorization    of a covariance matrix S.   If not given, the factorization is done on S, the uncorrected covariance matrix of <code>X</code>.</li></ul><p>When <code>X</code> and <code>Y</code> are (n, p) and (m, p), repectively, it returns an object (n, m) with:</p><ul><li>i, j = distance between row i of <code>X</code> and row j of <code>Y</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)
Y = rand(2, 3)

mahsqchol(X, Y)

U = cholesky(Hermitian(S)).U 
Uinv = inv(U)
mahsqchol(X, Y, Uinv)
mahsq(X[1:1, :], Y[1:1, :], Sinv)

mahsqchol(X[:, 1], 4)
mahsqchol(1, 4, sqrt(2.1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/distances.jl#LL74-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.matB" href="#Jchemo.matB"><code>Jchemo.matB</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matB(X, y, weights = ones(nro(X)))</code></pre><p>Between-class covariance matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : A vector (n) defining the class membership.</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li></ul><p>Compute the between-class covariance matrix (B) of <code>X</code>. This is the (non-corrected) covariance matrix of  the weighted class centers.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 20 ; p = 3
X = rand(n, p)
X
y = rand(1:3, n)
res = matB(X, y)
res.B
res.theta
res.ni
res.lev

res = matW(X, y)
pnames(res)
res.W 
res.Wi

matW(X, y).W + matB(X, y).B 
cov(X; corrected = false)

w = ones(n)
matW(X, y, w).W + matB(X, y, w).B
cov(X; corrected = false)

w = rand(n)
matW(X, y, w).theta 
matB(X, y, w).theta 

matW(X, y, w).W + matB(X, y, w).B
covm(X, w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/matW.jl#LL1-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.matW" href="#Jchemo.matW"><code>Jchemo.matW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matW(X, y, weights = ones(nro(X)))</code></pre><p>Within-class covariance matrices.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>y</code> : A vector (n) defing the class membership.</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li></ul><p>Compute the (non-corrected) within-class and pooled covariance  matrices (Wi and W) of <code>X</code>, and the pooled covariance matrix W. </p><p>If class i contains only one observation,  Wi is computed by <code>covm(</code>X<code>,</code>weights<code>)</code>.</p><p>For examples, see <code>?matB</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/matW.jl#LL64-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mavg-Tuple{Any}" href="#Jchemo.mavg-Tuple{Any}"><code>Jchemo.mavg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mavg(X; npoint)
mavg!(X::Matrix; npoint)</code></pre><p>Moving averages smoothing of each row of X-data.</p><ul><li><code>X</code> : X-data.</li><li><code>npoint</code> : Nb. points involved in the window </li></ul><p>The smoothing is computed by convolution (with padding),  with function imfilter of package ImageFiltering.jl. The centered  kernel is ones(<code>npoint</code>) / <code>npoint</code>. Each returned point is located on  the fcenter of the kernel.</p><p><strong>References</strong></p><p>Package ImageFiltering.jl https://github.com/JuliaImages/ImageFiltering.jl</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
wlstr = names(dat.X)
wl = parse.(Float64, wlstr)

Xp = mavg(X; npoint = 10) 
plotsp(Xp[1:30, :], wl).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL186-L217">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mblock-Tuple{Any, Any}" href="#Jchemo.mblock-Tuple{Any, Any}"><code>Jchemo.mblock</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mblock(X, listbl)</code></pre><p>Make blocks from a matrix.</p><ul><li><code>X</code> : X-data.</li><li><code>listbl</code> : A vector whose each component defines the colum numbers   defining a block in <code>X</code>. The length of <code>listbl</code> is the number   of blocks.</li></ul><p>The function returns a list (vector) of blocks.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 5 ; p = 10 
X = rand(n, p) 
listbl = [3:4, 1, [6; 8:10]]

Xbl = mblock(X, listbl)
Xbl[1]
Xbl[2]
Xbl[3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mblock.jl#LL1-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbpca-Tuple{Any}" href="#Jchemo.mbpca-Tuple{Any}"><code>Jchemo.mbpca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbpca(Xbl, weights = ones(nro(Xbl[1])); nlv,
    bscal = :none, tol = sqrt(eps(1.)), maxit = 200,
    scal::Bool = false)
mbpca!(Xbl, weights = ones(nro(Xbl[1])); nlv,
    bscal = :none, tol = sqrt(eps(1.)), maxit = 200,
    scal::Bool = false)</code></pre><p>Consensus principal components analysis (CPCA = MBPCA).</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>, <code>:mfa</code>).    See functions <code>blockscal</code>.</li><li><code>tol</code> : Tolerance value for convergence.</li><li><code>maxit</code> : Maximum number of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>The global scores are equal to the scores of the PCA of  the horizontal concatenation X = [X1 X2 ... Xk].</p><p>The function returns several objects, in particular:</p><ul><li><code>T</code> : The non normed global scores.</li><li><code>U</code> : The normed global scores.</li><li><code>W</code> : The global loadings.</li><li><code>Tbl</code> : The block scores (grouped by blocks, in the original fscale).</li><li><code>Tb</code> : The block scores (grouped by LV, in the metric fscale).</li><li><code>Wbl</code> : The block loadings.</li><li><code>lb</code> : The specific weights &quot;lambda&quot;.</li><li><code>mu</code> : The sum of the specific weights (= eigen value of the global PCA).</li></ul><p>Function <code>summary</code> returns: </p><ul><li><code>explvarx</code> : Proportion of the total inertia of X (sum of the squared norms of the    blocks) explained by each global score.</li><li><code>contr_block</code> : Contribution of each block to the global scores </li><li><code>explX</code> : Proportion of the inertia of the blocks explained by each global score.</li><li><code>corx2t</code> : Correlation between the global scores and the original variables.  </li><li><code>cortb2t</code> : Correlation between the global scores and the block scores.</li><li><code>rv</code> : RV coefficient. </li><li><code>lg</code> : Lg coefficient. </li></ul><p><strong>References</strong></p><p>Mangamana, E.T., Cariou, V., Vigneau, E., Glèlè Kakaï, R.L., Qannari, E.M., 2019.  Unsupervised multiblock data analysis: A unified approach and extensions. Chemometrics and  Intelligent Laboratory Systems 194, 103856. https://doi.org/10.1016/j.chemolab.2019.103856</p><p>Westerhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis of multiblock and hierarchical  PCA and PLS models. Journal of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5&lt;301::AID-CEM515&gt;3.0.CO;2-S</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ham.jld2&quot;) 
@load db dat
pnames(dat) 

X = dat.X
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X, listbl)
# &quot;New&quot; = first two rows of Xbl 
Xbl_new = mblock(X[1:2, :], listbl)

bscal = :frob
fm = mbpca(Xbl; nlv = 4, bscal = bscal) ;
fm.U
fm.T
transf(fm, Xbl).T
transf(fm, Xbl_new).T 

res = summary(fm, Xbl) ;
fm.lb
rowsum(fm.lb)
fm.mu
res.explvarx
res.explX # = fm.lb if bscal = :frob
rowsum(Matrix(res.explX))
res.contr_block
res.corx2t 
res.cortb2t
res.rv</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbpca.jl#LL1-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplsr-Tuple{Any, Any}" href="#Jchemo.mbplsr-Tuple{Any, Any}"><code>Jchemo.mbplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplsr(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, 
    bscal = :none, scal::Bool = false)
mbplsr!(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, 
    bscal = :none, scal::Bool = false)</code></pre><p>Multiblock PLSR (MBPLSR) - Fast version.</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>bscal</code> : Type of <code>Xbl</code> block scaling (<code>:none</code>, <code>:frob</code>).   See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and    of <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>PLSR (X, <code>Y</code>) where X is the horizontal concatenation of the blocks in <code>Xbl</code>. The function gives the same results as function <code>mbplswest</code>,  but is much faster.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ham.jld2&quot;) 
@load db dat
pnames(dat) 

X = dat.X
Y = dat.Y
y = dat.Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X, listbl)
# &quot;New&quot; = first two rows of Xbl 
Xbl_new = mblock(X[1:2, :], listbl)

bscal = :none
nlv = 5
fm = mbplsr(Xbl, y; nlv = nlv, bscal = bscal) ;
pnames(fm)
fm.T
transf(fm, Xbl_new)
[y Jchemo.predict(fm, Xbl).pred]
Jchemo.predict(fm, Xbl_new).pred

summary(fm, Xbl) </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbplsr.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mbplswest-Tuple{Any, Any}" href="#Jchemo.mbplswest-Tuple{Any, Any}"><code>Jchemo.mbplswest</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mbplswest(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, 
    bscal = :none, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)
mbplswest!(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, 
    bscal = :none, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)</code></pre><p>Multiblock PLSR - Nipals algorithm (Westerhuis et al. 1998).</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>bscal</code> : Type of <code>Xbl</code> block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>tol</code> : Tolerance value for convergence.</li><li><code>maxit</code> : Maximum number of iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and    of <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>MBPLSR is equivalent to the the PLSR (X, <code>Y</code>) where X is the horizontal  concatenation of the blocks in <code>Xbl</code>. The function gives the same results as function <code>mbplsr</code>.</p><p><strong>References</strong></p><p>Westerhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis of multiblock and hierarchical  PCA and PLS models. Journal of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5&lt;301::AID-CEM515&gt;3.0.CO;2-S</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ham.jld2&quot;) 
@load db dat
pnames(dat) 

X = dat.X
Y = dat.Y
y = dat.Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X, listbl)
# &quot;New&quot; = first two rows of Xbl 
Xbl_new = mblock(X[1:2, :], listbl)

bscal = :none
nlv = 5
fm = mbplswest(Xbl, y; nlv = nlv, bscal = bscal) ;
pnames(fm)
fm.T
transf(fm, Xbl_new)
[y Jchemo.predict(fm, Xbl).pred]
Jchemo.predict(fm, Xbl_new).pred

summary(fm, Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbplswest.jl#LL1-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.miss-Tuple{Any}" href="#Jchemo.miss-Tuple{Any}"><code>Jchemo.miss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">miss(X)</code></pre><p>Find rows with missing data in a dataset.</p><ul><li><code>X</code> : A dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 4)
zX = hcat(rand(2, 3), fill(missing, 2))
Z = vcat(X, zX)
miss(X)
miss(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL410-L423">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlev-Tuple{Any}" href="#Jchemo.mlev-Tuple{Any}"><code>Jchemo.mlev</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlev(x)</code></pre><p>Return the sorted levels of a dataset. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand([&quot;a&quot;;&quot;b&quot;;&quot;c&quot;], 20)
lev = mlev(x)
nlev = length(lev)

X = reshape(x, 5, 4)
mlev(X)

df = DataFrame(g1 = rand(1:2, n), 
    g2 = rand([&quot;a&quot;; &quot;c&quot;], n))
mlev(df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL431-L449">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlr-Tuple{Any, Any}" href="#Jchemo.mlr-Tuple{Any, Any}"><code>Jchemo.mlr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlr(X, Y, weights = ones(nro(X)); noint::Bool = false)
mlr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); noint::Bool = false)</code></pre><p>Compute a mutiple linear regression model (MLR) by using the QR algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>noint</code> : Define if the model is computed with an intercept or not.</li></ul><p>Safe but can be little slower than other methods.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = Matrix(dat.X[:, 2:4]) 
y = dat.X[:, 1]
n = nro(X)
ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

fm = mlr(Xtrain, ytrain) ;
#fm = mlrchol(Xtrain, ytrain) ;
#fm = mlrpinv(Xtrain, ytrain) ;
#fm = mlrpinvn(Xtrain, ytrain) ;
pnames(fm)
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

zcoef = Jchemo.coef(fm) 
zcoef.int 
zcoef.B 

fm = mlr(Xtrain, ytrain; noint = true) ;
zcoef = Jchemo.coef(fm) 
zcoef.int 
zcoef.B

fm = mlr(Xtrain[:, 1], ytrain) ;
#fm = mlrvec(Xtrain[:, 1], ytrain) ;
zcoef = Jchemo.coef(fm) 
zcoef.int 
zcoef.B</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL1-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrchol-Tuple{Any, Any}" href="#Jchemo.mlrchol-Tuple{Any, Any}"><code>Jchemo.mlrchol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrchol(X, Y, weights = ones(nro(X)))
mlrchol!(X::Matrix, Y::Matrix, weights = ones(nro(X)))</code></pre><p>Compute a mutiple linear regression model (MLR)  using the Normal equations and a Choleski factorization.</p><ul><li><code>X</code> : X-data, with nb. columns &gt;= 2 (required by function cholesky).</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li></ul><p>Compute a model with intercept.</p><p>Faster but can be less accurate (squared element X&#39;X).</p><p>See <code>?mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL86-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrda" href="#Jchemo.mlrda"><code>Jchemo.mlrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mlrda(X, y, weights = ones(nro(X)))</code></pre><p>Discrimination based on multple linear regression (MLR-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : Univariate class membership.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li></ul><p>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1) variable.  Then, a multiple linear regression (MLR) is run between the <code>X</code> and and each column  of Ydummy, returning predictions of the dummy variables (= object <code>posterior</code>  returned by fuction <code>predict</code>).   These predictions can be  considered as unbounded  estimates (i.e. eventuall outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

tab(ytrain)
tab(ytest)

fm = mlrda(Xtrain, ytrain) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlrda.jl#LL1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrpinv-Tuple{Any, Any}" href="#Jchemo.mlrpinv-Tuple{Any, Any}"><code>Jchemo.mlrpinv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrpinv(X, Y, weights = ones(nro(X)); noint::Bool = false)
mlrpinv!(X::Matrix, Y::Matrix, weights = ones(nro(X)); noint::Bool = false)</code></pre><p>Compute a mutiple linear regression model (MLR)  by using a pseudo-inverse. </p><ul><li><code>X</code> : X-data.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>noint</code> : Define if the model is computed with an intercept or not.</li></ul><p>Safe but can be slower.  </p><p>See <code>?mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL124-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrpinvn-Tuple{Any, Any}" href="#Jchemo.mlrpinvn-Tuple{Any, Any}"><code>Jchemo.mlrpinvn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrpinvn(X, Y, weights = ones(nro(X)))
mlrpinvn!(X::Matrix, Y::Matrix, weights = ones(nro(X)))</code></pre><p>Compute a mutiple linear regression model (MLR)  by using the Normal equations and a pseudo-inverse.</p><ul><li><code>X</code> : X-data.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li></ul><p>Safe and fast for p not too large.</p><p>Compute a model with intercept.</p><p>See <code>?mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL170-L184">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mlrvec-Tuple{Any, Any}" href="#Jchemo.mlrvec-Tuple{Any, Any}"><code>Jchemo.mlrvec</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mlrvec(x, Y, weights = ones(length(x));
    noint::Bool = false)
mlrvec!(x::Matrix, Y::Matrix, weights = ones(length(x));
    noint::Bool = false)</code></pre><p>Compute a simple linear regression model (univariate x).</p><ul><li><code>x</code> : Univariate X-data.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li></ul><p>Compute a model with intercept.</p><p>See <code>?mlr</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL209-L222">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mpar" href="#Jchemo.mpar"><code>Jchemo.mpar</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mpar(; kwargs...)</code></pre><p>Return a tuple with all the combinations of the parameter values defined in kwargs.</p><ul><li><code>kwargs</code> : vector(s) of the parameter(s) values.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">nlvdis = 25 ; metric = [:mah] 
h = [1 ; 2 ; Inf] ; k = [500 ; 1000] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) 
length(pars[1])
reduce(hcat, pars)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mpar.jl#LL1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mse-Tuple{Any, Any}" href="#Jchemo.mse-Tuple{Any, Any}"><code>Jchemo.mse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mse(pred, Y; digits = 3)</code></pre><p>Summary of model performance for regression.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
mse(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
mse(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL86-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.msep-Tuple{Any, Any}" href="#Jchemo.msep-Tuple{Any, Any}"><code>Jchemo.msep</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">msep(pred, Y)</code></pre><p>Compute the mean of the squared prediction errors (MSEP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
msep(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
msep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL131-L154">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.mweight-Tuple{Vector}" href="#Jchemo.mweight-Tuple{Vector}"><code>Jchemo.mweight</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mweight(w)
mweight!(w::AbstractVector)</code></pre><p>Return vector <code>w / sum(w)</code> (that sums to 1).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(10)
w = mweight(x)
sum(w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL451-L463">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nco-Tuple{Any}" href="#Jchemo.nco-Tuple{Any}"><code>Jchemo.nco</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nco(X)</code></pre><p>Return the nb. columns of <code>X</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL475-L479">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nipals-Tuple{Any}" href="#Jchemo.nipals-Tuple{Any}"><code>Jchemo.nipals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nipals(X; tol = sqrt(eps(1.)), maxit = 200)
nipals(X, UUt, VVt; tol = sqrt(eps(1.)), maxit = 200)</code></pre><p>Nipals to compute the first score and loading vectors of a matrix.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>UUt</code> : Matrix (n, n) for Gram-Schmidt orthogonalization.</li><li><code>VVt</code> : Matrix (p, p) for Gram-Schmidt orthogonalization.</li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. iterations.</li></ul><p>The function finds {u, v, sv} = argmin(||X - u * sv * v&#39;||), with the constraints  ||u|| = ||v|| = 1, using the alternating least squares algorithm to  compute SVD (Gabriel &amp; Zalir 1979).</p><p>X ~ u * sv * v&#39;, where:</p><ul><li>u : left singular vector (u * sv = scores)</li><li>v : right singular vector (loadings)</li><li>sv : singular value.</li></ul><p>When NIPALS is used sequentially on deflated matrices, vectors u  and v can loose orthogonality due to accumulation of rounding errors.  Orthogonality can be rebuilt from the Gram-Schmidt method  (arguments <code>UUt</code> and <code>VVt</code>). </p><p><strong>References</strong></p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with  any choice of weights, Technometrics 21 (1979) 489–498</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3)

res = nipals(X)
res.niter
res.sv
svd(X).S[1] 
res.v
svd(X).V[:, 1] 
res.u
svd(X).U[:, 1] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/nipals.jl#LL1-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nipalsmiss-Tuple{Any}" href="#Jchemo.nipalsmiss-Tuple{Any}"><code>Jchemo.nipalsmiss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nipalsmiss(X; tol = sqrt(eps(1.)), maxit = 200)
nipalsmiss(X, UUt, VVt; tol = sqrt(eps(1.)), maxit = 200)</code></pre><p>Nipals to compute the first score and loading vectors of a matrix     with missing data.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>UUt</code> : Matrix (n, n) for Gram-Schmidt orthogonalization.</li><li><code>VVt</code> : Matrix (p, p) for Gram-Schmidt orthogonalization.</li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. iterations.</li></ul><p>The function finds {u, v, sv} = argmin(||X - u * sv * v&#39;||), with the constraints  ||u|| = ||v|| = 1, using the alternating least squares algorithm to  compute SVD (Gabriel &amp; Zalir 1979).</p><p>X ~ u * sv * v&#39;, where:</p><ul><li>u : left singular vector (u * sv = scores)</li><li>v : right singular vector (loadings)</li><li>sv : singular value.</li></ul><p>When NIPALS is used sequentially on deflated matrices, vectors u  and v can loose orthogonality due to accumulation of rounding errors.  Orthogonality can be rebuilt from the Gram-Schmidt method  (arguments <code>UUt</code> and <code>VVt</code>). </p><p><strong>References</strong></p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with  any choice of weights, Technometrics 21 (1979) 489–498</p><p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS  with Gram-Schmidt Orthogonalization. https://cran.r-project.org/</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1. 2 missing 4 ; 4 missing 6 7 ; missing 5 6 13 ; 
    missing 18 7 6 ; 12 missing 28 7] 

res = nipalsmiss(X)
res.niter
res.sv
res.v
res.u</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/nipalsmiss.jl#LL1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.normw-Tuple{Any, Jchemo.Weight}" href="#Jchemo.normw-Tuple{Any, Jchemo.Weight}"><code>Jchemo.normw</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">normw(x, w)</code></pre><p>Return the squared weighted norm of a vector.</p><ul><li><code>x</code> : A vector (n).</li><li><code>w</code> : Weights (n) of the observations.   Consider to preliminary normalise <code>w</code> to    sum to 1 (e.g. function <code>mweight</code>).</li></ul><p>The squared weighted norm of vector <code>x</code> is:</p><ul><li>x&#39; * D * x, where D is the diagonal matrix of vector <code>w</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL481-L492">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.nro-Tuple{Any}" href="#Jchemo.nro-Tuple{Any}"><code>Jchemo.nro</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">nro(X)</code></pre><p>Return the nb. rows of <code>X</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL496-L500">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occknndis-Tuple{Any}" href="#Jchemo.occknndis-Tuple{Any}"><code>Jchemo.occknndis</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occknndis(X; nlv, nsamp, k, 
    typc = :mad, cri = 3, alpha = .025,
    scal::Bool = false, kwargs...)</code></pre><p>One-class classification using global k-nearest neighbors distances.</p><ul><li><code>X</code> : X-data (training).</li><li><code>nlv</code> : Nb. PCA components for the distances computations.</li><li><code>nsamp</code> : Nb. of observations (rows) sampled in the training <code>X</code>   used to compute the H0 empirical distribution of outlierness.</li><li><code>k</code> : Nb. of neighbors used to compute the outlierness.</li><li><code>typc</code> : Type of cutoff (:mad or :q). See Thereafter.</li><li><code>cri</code> : When <code>typc = :mad</code>, a constant. See thereafter.</li><li><code>alpha</code> : When <code>typc = :q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>kde</code> of KernelDensity.jl   (see function <code>kde1</code>).</li></ul><p>Let us note q a given observation, and o[q] a neighbor of q  within the training data <code>X</code>. The <code>k</code> nearest neighbors of q define the neighborhood NNk(q) = {o.1[q], ...., o.k[q]}  (if q belongs to the training <code>X</code>, q is removed from NNk(q)). </p><p>The global outlierness of any observation q relatively to <code>X</code>, say dk(q), is computed as the median distance to NNk(q):</p><ul><li>dk(q) = median{dist(q, o.j[q]), j = 1,...,k}.</li></ul><p>Outlierness dk(q) is then compared to the outlierness distribution estimated for the training data <code>X</code>, say distribution H0.    If dk(q) is extreme compared to H0, observation q may come from a  different distribution than the training data <code>X</code>.</p><p>H0 is estimated by Monte Carlo, as follows:</p><ul><li>A number of <code>nsamp</code> observations (rows) are sampled without replacement    within the training data <code>X</code>.</li><li>For each of these <code>nsamp</code> training observations, say q.j {j = 1, ..., nsamp},   outlierness dk(q.j) is computed. This returns a vector of <code>nsamp</code>    outlierness values {dk(q.j), j = 1,...,nsamp}. </li><li>This vector defines the empirical outlierness distribution of    observations assumed to come from the same distribution as    the training data <code>X</code> (&quot;hypothesis H0&quot;). </li></ul><p>Then, function <code>predict</code> computes outlierness dk(q) for each  new observation q.       </p><p>In the function, distances are computed as Mahalanobis distances in a  PCA score space (internally computed), cf. argument <code>nlv</code>.</p><p>See <code>?occsd</code> for details on outputs.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X    
Y = dat.Y
f = 21 ; pol = 3 ; d = 2 ;
Xp = savgol(snv(X); f = f, pol = pol, d = d) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

g1 = &quot;EHH&quot; ; g2 = &quot;PEE&quot;
#g1 = &quot;EHH&quot; ; g2 = &quot;EHH&quot;
s1 = Ytrain.typ .== g1
s2 = Ytest.typ .== g2
zXtrain = Xtrain[s1, :]    
zXtest = Xtest[s2, :] 
ntrain = nro(zXtrain)
ntest = nro(zXtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

fm = pcasvd(zXtrain, nlv = 5) ; 
Ttrain = fm.T
Ttest = transf(fm, zXtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat([&quot;0-Train&quot;], ntrain), repeat([&quot;1-Test&quot;], ntest))
i = 1
plotxy(T[:, i], T[:, i + 1]), group;
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f

#### End data

nlv = 30
nsamp = 300
k = round(.7 * ntrain)
fm = occknndis(zXtrain; nlv = nlv, 
    nsamp = nsamp, k = k) ;
fm.d
hist(fm.d.dstand; bins = 50)

res = Jchemo.predict(fm, zXtest) ;
res.d
res.pred
tab(res.pred)

d1 = fm.d.dstand
d2 = res.d.dstand
d = vcat(d1, d2)
group = [repeat([&quot;0-Train&quot;], length(d1)); repeat([&quot;1-Test&quot;], length(d2))]
f, ax = plotxy(1:length(d), d, group; 
    resolution = (600, 400), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occknndis.jl#LL1-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occlknndis-Tuple{Any}" href="#Jchemo.occlknndis-Tuple{Any}"><code>Jchemo.occlknndis</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occlknndis(X; nlv, nsamp, k, 
    typc = :mad, cri = 3, alpha = .025,
    scal::Bool = false, kwargs...)</code></pre><p>One-class classification using local k-nearest neighbors distances.</p><ul><li><code>X</code> : X-data (training).</li><li><code>nlv</code> : Nb. PCA components for the distances computations.</li><li><code>nsamp</code> : Nb. of observations (rows) sampled in the training <code>X</code>   used to compute the H0 empirical distribution of outlierness.</li><li><code>k</code> : Nb. of neighbors used to compute the outlierness.</li><li><code>typc</code> : Type of cutoff (:mad or :q). See Thereafter.</li><li><code>cri</code> : When <code>typc = :mad</code>, a constant. See thereafter.</li><li><code>alpha</code> : When <code>typc = :q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>kde</code> of KernelDensity.jl   (see function <code>kde1</code>).</li></ul><p>Let us note q a given observation, and o[q] a neighbor of q  within the training data <code>X</code>. The <code>k</code> nearest neighbors of q define the neighborhood NNk(q) = {o.1[q], ...., o.k[q]}  (if q belongs to the training <code>X</code>, q is removed from NNk(q)). </p><p>The median distance of any observation q to its neighborhood NNk(q) is dk(q) = median{dist(q, o.j[q]), j = 1,...,k}, referred to as global outlerness in function <code>occknndis</code>.</p><p>The local outlierness of any observation q relatively to <code>X</code>,  say ldk(q), is computed as follows:</p><ul><li>NNk(q) is selected and dk(q) is computed.</li><li>For each neighbor o.j[q] in NNk(q), global outlierness dk(o.j[q]) is    computed. This returns the vector {dk(o.j[q]), j = 1,...,k}.</li><li>The local outlierness of q is then computed as    dk(q) / median{dk(o.j[q]), j = 1,...,k}.</li></ul><p>Outlierness ldk(q) is then compared to the outlierness distribution estimated for the training data <code>X</code>, say distribution H0.    If ldk(q) is extreme compared to H0, observation q may come from a  different distribution than the training data <code>X</code>.</p><p>H0 is estimated by Monte Carlo, as follows:</p><ul><li>A number of <code>nsamp</code> observations (rows) are sampled without replacement    within the training data <code>X</code>.</li><li>For each of these <code>nsamp</code> training observations, say q.j {j = 1,...,nsamp},   outlierness ldk(q.j) is computed. This returns a vector of <code>nsamp</code>    outlierness values {ldk(q.1), j = 1,...,nsamp}. </li><li>This vector defines the empirical outlierness distribution of    observations assumed to come from the same distribution as    the training data <code>X</code> (hypothesis H0). </li></ul><p>Then, function <code>predict</code> computes outlierness ldk(q) for each  new observation q. </p><p>In the function, distances are computed as Mahalanobis distances in a  PCA score space (internally computed), cf. argument <code>nlv</code>.</p><p>See <code>?occsd</code> for details on outputs.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X    
Y = dat.Y
f = 21 ; pol = 3 ; d = 2 ;
Xp = savgol(snv(X); f = f, pol = pol, d = d) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

g1 = &quot;EHH&quot; ; g2 = &quot;PEE&quot;
#g1 = &quot;EHH&quot; ; g2 = &quot;EHH&quot;
s1 = Ytrain.typ .== g1
s2 = Ytest.typ .== g2
zXtrain = Xtrain[s1, :]    
zXtest = Xtest[s2, :] 
ntrain = nro(zXtrain)
ntest = nro(zXtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

fm = pcasvd(zXtrain, nlv = 5) ; 
Ttrain = fm.T
Ttest = transf(fm, zXtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat([&quot;0-Train&quot;], ntrain), repeat([&quot;1-Test&quot;], ntest))
i = 1
plotxy(T[:, i], T[:, i + 1]), group;
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f

#### End data

nlv = 30
nsamp = 50 ; k = 5
fm = Jchemo.occlknndis(zXtrain; nlv = nlv, 
    nsamp = nsamp, k = k, typc = :mad) ;
fm.d
hist(fm.d.dstand; bins = 50)

res = Jchemo.predict(fm, zXtest) ;
res.d
res.pred
tab(res.pred)

d1 = fm.d.dstand
d2 = res.d.dstand
d = vcat(d1, d2)
group = [repeat([&quot;0-Train&quot;], length(d1)); repeat([&quot;1-Test&quot;], length(d2))]
f, ax = plotxy(1:length(d), d, group; 
    resolution = (600, 400), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occlknndis.jl#LL1-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}" href="#Jchemo.occod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}"><code>Jchemo.occod</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occod(object::Union{Pca, Plsr}, X; nlv = nothing, 
    typc = :mad, cri = 3, alpha = .025, kwargs...)</code></pre><p>One-class classification using PCA/PLS orthognal distance (OD).</p><ul><li><code>object</code> : The model (e.g. PCA) that was fitted on the training data,   assumed to represent the training class.</li><li><code>X</code> : X-data (training) that were used to fit the model.</li><li><code>nlv</code> : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components of the fitted model.</li><li><code>typc</code> : Type of cutoff (:mad or :q). See Thereafter.</li><li><code>cri</code> : When <code>typc = :mad</code>, a constant. See thereafter.</li><li><code>alpha</code> : When <code>typc = :q</code>, a risk-I level. See thereafter.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>kde</code> of    KernelDensity.jl (see function <code>kde1</code>).</li></ul><p>In this method, the outlierness <code>d</code> of an observation is the orthogonal distance (OD =  &quot;X-residuals&quot;) of this observation, ie. the Euclidean distance between the observation and its projection on the  score plan defined by the fitted (e.g. PCA) model (e.g. Hubert et al. 2005,  Van Branden &amp; Hubert 2005 p. 66, Varmuza &amp; Filzmoser 2009 p. 79).</p><p>See <code>?occsd</code> for details on outputs, and examples.</p><p><strong>References</strong></p><p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach  to robust principal components analysis. Technometrics, 47, 64-79.</p><p>K. Vanden Branden, M. Hubert (2005). Robuts classification in high dimension based  on the SIMCA method. Chem. Lab. Int. Syst, 79, 10-21.</p><p>K. Varmuza, P. Filzmoser (2009). Introduction to multivariate statistical analysis  in chemometrics. CRC Press, Boca Raton.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occod.jl#LL1-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occsd-Tuple{Union{Jchemo.Kpca, Jchemo.Pca, Jchemo.Plsr}}" href="#Jchemo.occsd-Tuple{Union{Jchemo.Kpca, Jchemo.Pca, Jchemo.Plsr}}"><code>Jchemo.occsd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occsd(object::Union{Pca, Kpca, Plsr}; nlv = nothing,
    typc = :mad, cri = 3, alpha = .025, kwargs...)</code></pre><p>One-class classification using PCA/PLS score distance (SD).</p><ul><li><code>object</code> : The model (e.g. PCA) that was fitted on the training data,   assumed to represent the training class.</li><li><code>nlv</code> : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components of the fitted model.</li><li><code>typc</code> : Type of cutoff (:mad or :q). See Thereafter.</li><li><code>cri</code> : When <code>typc = :mad</code>, a constant. See thereafter.</li><li><code>alpha</code> : When <code>typc = :q</code>, a risk-I level. See thereafter.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>kde</code> of    KernelDensity.jl (see function <code>kde1</code>).</li></ul><p>In this method, the outlierness <code>d</code> of an observation is defined by its  score distance (SD), ie. the Mahalanobis distance between the projection of  the observation on the score plan defined by the fitted (e.g. PCA) model and  the fcenter of the score plan.</p><p>If a new observation has <code>d</code> higher than a given <code>cutoff</code>, the observation  is assumed to not belong to the training class.  The <code>cutoff</code> is computed with non-parametric heuristics.  Noting [d] the vector of outliernesses computed on the training class:</p><ul><li>If <code>typc = :mad</code>, then <code>cutoff</code> = median([d]) + <code>cri</code> * mad([d]). </li><li>If <code>typc = :q, then</code>cutoff<code>is estimated from the empirical cumulative   density function computed on [d], for a given risk-I (</code>alpha`). </li></ul><p>Alternative approximate cutoffs have been proposed in the literature  (e.g.: Nomikos &amp; MacGregor 1995, Hubert et al. 2005, Pomerantsev 2008). Typically and whatever the approximation method, it is recommended to tune  the cutoff, depending on detection objectives. </p><p><strong>Outputs</strong></p><ul><li><code>pval</code>: Estimate of p-value (see functions <code>kde1</code> and <code>pval</code>) computed    from the KDE of distribution [d], provided for each data observation. </li><li><code>dstand</code>: standardized distance defined as <code>d</code> / <code>cutoff</code>.    A value <code>dstand</code> &gt; 1 may be considered as extreme compared to the distribution   of the training data.  Output <code>gh</code> is the Winisi &quot;GH&quot; (usually, GH &gt; 3 is    considered as &quot;extreme&quot;).</li><li><code>pred</code> (fonction <code>predict</code>): class prediction<ul><li><code>dstand</code> &lt;= 1 ==&gt; <code>0</code>: the observation is expected to belong    the training class, </li><li><code>dstand</code> &gt; 1  ==&gt; <code>1</code>: extreme value, possibly outside of the training class. </li></ul></li></ul><p><strong>References</strong></p><p>M. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust  principal components analysis. Technometrics, 47, 64-79.</p><p>Nomikos, P., MacGregor, J.F., 1995. Multivariate SPC Charts for Monitoring Batch Processes.  null 37, 41-59. https://doi.org/10.1080/00401706.1995.10485888</p><p>Pomerantsev, A.L., 2008. Acceptance areas for multivariate classification derived by  projection methods. Journal of Chemometrics 22, 601-609. https://doi.org/10.1002/cem.1147</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2018.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X    
Y = dat.Y
f = 21 ; pol = 3 ; d = 2 ;
Xp = savgol(snv(X); f = f, pol = pol, d = d) 
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]

g1 = &quot;EHH&quot; ; g2 = &quot;PEE&quot;
#g1 = &quot;EHH&quot; ; g2 = &quot;EHH&quot;
s1 = Ytrain.typ .== g1
s2 = Ytest.typ .== g2
zXtrain = Xtrain[s1, :]    
zXtest = Xtest[s2, :] 
ntrain = nro(zXtrain)
ntest = nro(zXtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

fm = pcasvd(zXtrain, nlv = 5) ; 
Ttrain = fm.T
Ttest = transf(fm, zXtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat([&quot;0-Train&quot;], ntrain), repeat([&quot;1-Test&quot;], ntest))
i = 1
plotxy(T[:, i], T[:, i + 1]), group;
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1)).f

#### End data

nlv = 10
fm0 = pcasvd(zXtrain; nlv = nlv) ;

fm = occsd(fm0) ;
#fm = occsd(fm0; typc = :q, alpha = .025) ;
#fm = occod(fm0, zXtrain) ;
#fm = occsdod(fm0, zXtrain) ;
#fm = occstah(zXtrain)
fm.d
hist(fm.d.dstand; bins = 50)

res = Jchemo.predict(fm, zXtest) ;
res.d
res.pred
tab(res.pred)

d1 = fm.d.dstand
d2 = res.d.dstand
d = vcat(d1, d2)
f, ax = plotxy(1:length(d), d, group; 
    resolution = (600, 400), xlabel = &quot;Obs. index&quot;, 
    ylabel = &quot;Standardized distance&quot;)
hlines!(ax, 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occsd.jl#LL1-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occsdod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}" href="#Jchemo.occsdod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}"><code>Jchemo.occsdod</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occsdod(object::Union{Pca, Plsr}, X; 
    nlv_sd = nothing, nlv_od = nothing, 
    typc = :mad, cri = 3, alpha = .025, kwargs...)</code></pre><p>One-class classification using a compromise between PCA/PLS score (SD) and      orthogonal (OD) distances.</p><ul><li><code>object</code> : The model (e.g. PCA) that was fitted on the training data,   assumed to represent the training class.</li><li><code>X</code> : X-data (training) that were used to fit the model.</li><li><code>nlv_sd</code> : Nb. components (PCs or LVs) to consider for SD. If nothing,    it is the maximum nb. of components of the fitted model.</li><li><code>nlv_od</code> : Nb. components (PCs or LVs) to consider for OD. If nothing,    it is the maximum nb. of components of the fitted model.</li><li><code>typc</code> : Type of cutoff (:mad or :q). See Thereafter.</li><li><code>cri</code> : When <code>typc = :mad</code>, a constant. See thereafter.</li><li><code>alpha</code> : When <code>typc = :q</code>, a risk-I level. See thereafter.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>kde</code> of    KernelDensity.jl (see function <code>kde1</code>).</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is a compromise between the score distance (SD) and the orthogonal distance (OD). The compromise is computed from the  standardized distances by: </p><ul><li><code>dstand</code> = sqrt(<code>dstand_sd</code> * <code>dstand_od</code>).</li></ul><p>See <code>?occsd</code> for details on outputs, and examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occsdod.jl#LL1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.occstah-Tuple{Any}" href="#Jchemo.occstah-Tuple{Any}"><code>Jchemo.occstah</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">occstah(X; a = 2000, typc = :mad, cri = 3, 
    alpha = .025, scal = true, kwargs...)</code></pre><p>One-class classification using the Stahel-Donoho outlierness.</p><ul><li><code>X</code> : X-data (training).</li><li><code>a</code> : Nb. dimensions simulated for the projection-pursuit method.</li><li><code>typc</code> : Type of cutoff (:mad or :q). See Thereafter.</li><li><code>cri</code> : When <code>typc = :mad</code>, a constant. See thereafter.</li><li><code>alpha</code> : When <code>typc = :q</code>, a risk-I level. See thereafter.</li><li><code>scal</code> : Boolean. If <code>true</code>, matrix <code>X</code> is centred (by median)    and scaled (by MAD) before computing the outlierness.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>kde</code> of    KernelDensity.jl (see function <code>kde1</code>).</li></ul><p>In this method, the outlierness <code>d</code> of a given observation is the Stahel-Donoho outlierness (see <code>?stah</code>).</p><p>See <code>?occsd</code> for details on outputs, and examples. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occstah.jl#LL1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.out-Tuple{Any, Any}" href="#Jchemo.out-Tuple{Any, Any}"><code>Jchemo.out</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">out(x)</code></pre><p>Return if elements of a vector are strictly outside of a given range.</p><ul><li><code>x</code> : Univariate data.</li><li><code>y</code> : Univariate data on which is computed the range (min, max).</li></ul><p>Return a BitVector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [-200.; -100; -1; 0; 1; 200]
out(x, [-1; .2; 1])
out(x, (-1, 1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL502-L517">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcaeigen-Tuple{Any}" href="#Jchemo.pcaeigen-Tuple{Any}"><code>Jchemo.pcaeigen</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcaeigen(X, weights = ones(nro(X)); nlv, scal::Bool = false)
pcaeigen!(X::Matrix, weights = ones(nro(X)); nlv, scal::Bool = false)</code></pre><p>PCA by Eigen factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of <code>weights</code> and X the centered matrix in metric D.  The function minimizes ||X - T * P&#39;||^2  in metric D, by  computing an Eigen factorization of X&#39; * D * X. </p><p>See <code>?pcasvd</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcaeigen.jl#LL1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcaeigenk-Tuple{Any}" href="#Jchemo.pcaeigenk-Tuple{Any}"><code>Jchemo.pcaeigenk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcaeigenk(X, weights = ones(nro(X)); nlv, scal::Bool = false)
pcaeigenk!(X::Matrix, weights = ones(nro(X)); nlv, scal::Bool = false)</code></pre><p>PCA by Eigen factorization of the kernel form (XX&#39;).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>This is the &quot;kernel cross-product&quot; version of the PCA algorithm (e.g. Wu et al. 1997).  For wide matrices (n &lt;&lt; p, where p is the nb. columns) and n not too large,  this algorithm can be much faster than the others.</p><p>Let us note D the (n, n) diagonal matrix of <code>weights</code> and X the centered matrix in metric D.  The function minimizes ||X - T * P&#39;||^2  in metric D, by  computing an Eigen factorization of D^(1/2) * X * X&#39; D^(1/2).</p><p>See <code>?pcasvd</code> for examples.</p><p><strong>References</strong></p><p>Wu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA algorithms for wide data.  Part I: Theory and algorithms. Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcaeigenk.jl#LL1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcanipals-Tuple{Any}" href="#Jchemo.pcanipals-Tuple{Any}"><code>Jchemo.pcanipals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcanipals(X, weights = ones(nro(X)); nlv, 
    gs::Bool = true, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)
pcanipals!(X::Matrix, weights = ones(nro(X)); nlv, 
    gs::Bool = true, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)</code></pre><p>PCA by NIPALS algorithm.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt orthogonalization    of the scores and loadings is done. </li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of <code>weights</code> and X the centered  matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D  by NIPALS. </p><p>See <code>?pcasvd</code> for examples.</p><p><strong>References</strong></p><p>Andrecut, M., 2009. Parallel GPU Implementation of Iterative PCA Algorithms.  Journal of Computational Biology 16, 1593-1599. https://doi.org/10.1089/cmb.2008.0221</p><p>K.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with  any choice of weights, Technometrics 21 (1979) 489–498</p><p>Gabriel, R. K., 2002. Le biplot - Outil d&#39;exploration de données multidimensionnelles.  Journal de la Société Française de la Statistique, 143, 5-55.</p><p>Lingen, F.J., 2000. Efficient Gram-Schmidt orthonormalisation on parallel computers.  Communications in Numerical Methods in Engineering 16, 57-66.  https://doi.org/10.1002/(SICI)1099-0887(200001)16:1&lt;57::AID-CNM320&gt;3.0.CO;2-I</p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris, France.</p><p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS  with Gram-Schmidt Orthogonalization. https://cran.r-project.org/</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcanipals.jl#LL1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcanipalsmiss-Tuple{Any}" href="#Jchemo.pcanipalsmiss-Tuple{Any}"><code>Jchemo.pcanipalsmiss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcanipalsmiss(X, weights = ones(nro(X)); nlv, 
    gs::Bool = true, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)
pcanipalsmiss!(X::Matrix, weights = ones(nro(X)); nlv, 
    gs::Bool = true, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)</code></pre><p>PCA by NIPALS algorithm allowing missing data.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>gs</code> : Boolean. If <code>true</code> (default), a Gram-Schmidt orthogonalization    of the scores and loadings is done. </li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of <code>weights</code> and X the centered  matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D  by NIPALS. </p><p><strong>References</strong></p><p>Wright, K., 2018. Package nipals: Principal Components Analysis using NIPALS  with Gram-Schmidt Orthogonalization. https://cran.r-project.org/</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using LinearAlgebra

X = [1. 2 missing 4 ; 4 missing 6 7 ; missing 5 6 13 ; 
    missing 18 7 6 ; 12 missing 28 7] 

tol = 1e-15
nlv = 3 
weights = ones(n) 
#weights = collect(1:n) 
scal = false
#scal = true
gs = false
#gs = true
fm = pcanipalsmiss(X, weights; nlv = nlv, 
    tol = tol, gs = gs, scal = scal, maxit = 500) ;
pnames(fm)
fm.niter
fm.sv
fm.P
fm.T
## Check if orthogonality
fm.P&#39; * fm.P
fm.T&#39; * Diagonal(mweight(weights)) * fm.T

## Impute missing data in x
fm = pcanipalsmiss(X, weights; nlv = 2, 
    gs = true, scal = scal) ;
Xfit = xfit(fm)
s = ismissing.(X)
Xres = copy(X)
Xres[s] .= Xfit[s]
Xres</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcanipalsmiss.jl#LL1-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcasph-Tuple{Any}" href="#Jchemo.pcasph-Tuple{Any}"><code>Jchemo.pcasph</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcasph(X, weights = ones(nro(X)); nlv, typc = &quot;medspa&quot;, 
    delta = .001, scal::Bool = false)
pcasph!(X, weights = ones(nro(X)); nlv, typc = &quot;medspa&quot;, 
    delta = .001, scal::Bool = false)</code></pre><p>Spherical PCA.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>typc</code> : Type of centering.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Spherical PCA (Locantore et al. 1990, Maronna 2005, Daszykowski et al. 2007).  The spatial median used for centering matrix qn{X} is calculated by function <code>Jchemo.colmedspa</code>.</p><p><strong>References</strong></p><p>Daszykowski, M., Kaczmarek, K., Vander Heyden, Y., Walczak, B., 2007.  Robust statistics in data analysis - A review. Chemometrics and Intelligent  Laboratory Systems 85, 203-219. https://doi.org/10.1016/j.chemolab.2006.06.016</p><p>Locantore N., Marron J.S., Simpson D.G., Tripoli N., Zhang J.T., Cohen K.L. Robust principal component analysis for functional data, Test 8 (1999) 1–7</p><p>Maronna, R., 2005. Principal components and orthogonal regression based on  robust scales, Technometrics, 47:3, 264-273, DOI: 10.1198/004017005000000166</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2, JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/octane.jld2&quot;) 
@load db dat
pnames(dat)
  
X = dat.X 
wlstr = names(X)
wl = parse.(Float64, wlstr)
n = nro(X)

nlv = 6
fm = pcasph(X; nlv = nlv) ; 
#fm = pcasvd(X; nlv = nlv) ; 
pnames(fm)
T = fm.T

i = 1
plotxy(T[:, i], T[:, i + 1]); zeros = true,
    xlabel = &quot;PC1&quot;, ylabel = &quot;PC2&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcasph.jl#LL1-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcasvd-Tuple{Any}" href="#Jchemo.pcasvd-Tuple{Any}"><code>Jchemo.pcasvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcasvd(X, weights = ones(nro(X)); nlv, scal::Bool = false)
pcasvd!(X::Matrix, weights = ones(nro(X)); nlv, scal::Bool = false)</code></pre><p>PCA by SVD factorization.</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Let us note D the (n, n) diagonal matrix of <code>weights</code> and X the centered matrix in metric D. The function minimizes ||X - T * P&#39;||^2  in metric D, by  computing a SVD factorization of sqrt(D) * X:</p><ul><li>sqrt(D) * X ~ U * S * V&#39;</li></ul><p>Outputs are:</p><ul><li>T = D^(-1/2) * U * S</li><li>P = V</li><li>The diagonal of S   </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
Xtest = rmrow(X, s)

nlv = 3
fm = pcasvd(Xtrain; nlv = nlv) ;
#fm = pcaeigen(Xtrain; nlv = nlv) ;
#fm = pcaeigenk(Xtrain; nlv = nlv) ;
#fm = pcanipals(Xtrain; nlv = nlv) ;
pnames(fm)
fm.T
fm.T&#39; * fm.T
fm.P&#39; * fm.P

transf(fm, Xtest)

res = Base.summary(fm, Xtrain) ;
pnames(res)
res.explvarx
res.contr_var
res.coord_var
res.cor_circle</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcasvd.jl#LL1-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pcr-Tuple{Any, Any}" href="#Jchemo.pcr-Tuple{Any, Any}"><code>Jchemo.pcr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pcr(X, Y, weights = ones(nro(X)); nlv,
    scal::Bool = false)
pcr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    scal::Bool = false)</code></pre><p>Principal component regression (PCR) with a SVD factorization.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>   is scaled by its uncorrected standard deviation.</li></ul><p><code>X</code> and <code>Y</code> are internally centered. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
fm = pcr(Xtrain, ytrain; nlv = nlv) ;
pnames(fm)
fm.T

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.B
Jchemo.coef(fm; nlv = 7).B

fmpca = fm.fmpca ;
transf(fmpca, Xtest)
transf(fmpca, Xtest; nlv = 7)

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f   

res = Jchemo.predict(fm, Xtest; nlv = 1:2)
res.pred[1]
res.pred[2]

# See ?pcasvd
res = Base.summary(fmpca, Xtrain)
res.explvarx</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcr.jl#LL1-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plist-Tuple{Any}" href="#Jchemo.plist-Tuple{Any}"><code>Jchemo.plist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plist(x)</code></pre><p>Print each element of a list.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL519-L523">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotconf-Tuple{Any}" href="#Jchemo.plotconf-Tuple{Any}"><code>Jchemo.plotconf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plotconf(object; cnt = true, ptext = true, 
    fontsize = 15, coldiag = :red, resolution = (500, 400))</code></pre><p>Plot a confusion matrix.</p><ul><li><code>object</code> : Output of function <code>confusion</code>.</li><li><code>cnt</code> : Boolean. If <code>true</code> (default), plot the occurrences,    else plot the row %s.</li><li><code>ptext</code> : Boolean. If <code>true</code> (default), display the value in each cell.</li><li><code>fontsize</code> : Font size when <code>ptext = true</code>.</li><li><code>coldiag</code> : Font color when <code>ptext = true</code>.</li><li><code>resolution</code> : Resolution (horizontal, vertical) of the figure.</li></ul><p>See examples in help page of function <code>confusion</code>. ```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/confusion.jl#LL57-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotgrid-Tuple{AbstractVector, Any}" href="#Jchemo.plotgrid-Tuple{AbstractVector, Any}"><code>Jchemo.plotgrid</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plotgrid(indx::AbstractVector, r; resolution = (500, 350), 
    step = 5, color = nothing, kwargs...)
plotgrid(indx::AbstractVector, r, group; resolution = (500, 350), 
    step = 5, color = nothing, leg = true, kwargs...)</code></pre><p>Plot error or performance rates of model predictions.</p><ul><li><code>indx</code> : A numeric variable representing the grid of model parameters,    e.g. the nb. LVs if PLSR models.</li><li><code>r</code> : The error/performance rates for the values of <code>x</code>. </li><li><code>group</code> : Categorical variable defining groups.    A separate line is plotted for each level of <code>group</code>.</li><li><code>resolution</code> : Resolution (horizontal, vertical) of the figure.</li><li><code>step</code> : Step used for defining the xticks.</li><li><code>color</code> : Set color. If <code>group</code> if used, must be a vector of same length   as the number of levels in <code>group</code>.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>kwargs</code> : Optional arguments to pass in <code>Axis</code> of CairoMakie.</li></ul><p>The user has to specify a backend (e.g. CairoMakie).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 0:20
res = gridscorelv(Xtrain, ytrain, Xtest, ytest;
    score = rmsep, fun = plskern, nlv = nlv)
plotgrid(res.nlv, res.y1;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

nlvdis = 15 ; metric = [:mah ]
h = [1 ; 2.5 ; 5] ; k = [50 ; 100] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
nlv = 0:20
res = gridscorelv(Xtrain, ytrain, Xtest, ytest;
    score = rmsep, fun = lwplsr, pars = pars, nlv = nlv)
group = string.(&quot;h=&quot;, res.h, &quot; k=&quot;, res.k)
plotgrid(res.nlv, res.y1, group;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSECV&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plotgrid.jl#LL1-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotsp" href="#Jchemo.plotsp"><code>Jchemo.plotsp</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">plotsp(X, wl = 1:nco(X); size = (500, 350),
    color = nothing, nsamp, kwargs...)</code></pre><p>Plotting spectra.</p><ul><li><code>X</code> : X-data.</li><li><code>wl</code> : Column names of <code>X</code>. Must be numeric.</li><li><code>color</code> : Set a unique color (and eventually transparency) to the spectra.</li><li>&#39;size&#39; : Resolution (horizontal, vertical) of the figure.</li><li><code>nsamp</code> : Nb. spectra to plot. If <code>nothing</code> (default), all spectra are plotted.</li><li><code>kwargs</code> : Optional arguments to pass in <code>Axis</code> of CairoMakie.</li></ul><p>The function plots the rows of <code>X</code>.</p><p>The user has to specify a backend (e.g. CairoMakie).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">    using JchemoData, JLD2, CairoMakie
    path_jdat = dirname(dirname(pathof(JchemoData)))
    db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
    @load db dat
    pnames(dat)
    
    X = dat.X
    wlstr = names(X)
    wl = parse.(Float64, wlstr) 
    plotsp(X).f
    plotsp(X; color = (:red, .2)).f
    plotsp(X, wl; xlabel = &quot;Wavelength (nm)&quot;,
        ylabel = &quot;Absorbance&quot;).f

    f, ax = plotsp(X)
    vlines!(ax, 200)
    f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plotsp.jl#LL1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plotxy-Tuple{Any, Any}" href="#Jchemo.plotxy-Tuple{Any, Any}"><code>Jchemo.plotxy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plotxy(x, y; resolution = (600, 400), 
    color = nothing, ellipse::Bool = false, prob = .95, 
    circle::Bool = false, bisect::Bool = false, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;,
    kwargs...)
plotxy(x, y, group; resolution = (600, 400), 
    color = nothing, ellipse::Bool = false, prob = .95, 
    circle::Bool = false, bisect::Bool = false, zeros::Bool = false,
    xlabel = &quot;&quot;, ylabel = &quot;&quot;, title = &quot;&quot;, leg::Bool = true,
    kwargs...)</code></pre><p>Scatter plot of (x, y) data</p><ul><li><code>x</code> : A x-vector (n).</li><li><code>y</code> : A y-vector (n). </li><li><code>group</code> : Categorical variable defining groups (n). </li><li><code>resolution</code> : Resolution (horizontal, vertical) of the figure.</li><li><code>color</code> : Set color(s). If <code>group</code> if used, <code>color</code> must be a vector of    same length as the number of levels in <code>group</code>.</li><li><code>ellipse</code> : Boolean. Draw an ellipse of confidence, assuming a Ch-square distribution   with df = 2. If <code>group</code> is used, one ellipse is drawn per group.</li><li><code>prob</code> : Probability for the ellipse of confidence (default = .95).</li><li><code>bisect</code> : Boolean. Draw a bisector.</li><li><code>zeros</code> : Boolean. Draw horizontal and vertical axes passing through origin (0, 0).</li><li><code>xlabel</code> : Label for the x-axis.</li><li><code>ylabel</code> : Label for the y-axis.</li><li><code>title</code> : Title of the graphic.</li><li><code>leg</code> : Boolean. If <code>group</code> is used, display a legend or not.</li><li><code>kwargs</code> : Optional arguments to pass in function <code>scatter</code> of Makie.</li></ul><p>Before using <code>plotxy</code>, a backend (e.g. CairoMakie) has to be specified.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
lev = mlev(year)
nlev = length(lev)

fm = pcasvd(X, nlv = 3) ; 
T = fm.T

plotxy(T[:, 1], T[:, 2]; color = (:red, .5)).f

plotxy(T[:, 1], T[:, 2], year; ellipse = true).f

i = 1
colm = cgrad(:Dark2_5, nlev; categorical = true)
plotxy(T[:, i], T[:, i + 1], year; 
    color = colm,
    xlabel = string(&quot;PC&quot;, i), ylabel = string(&quot;PC&quot;, i + 1),
    zeros = true, ellipse = true).f

plotxy(T[:, 1], T[:, 2]), year).lev

plotxy(1:5, 1:5).f

y = reshape(rand(5), 5, 1)
plotxy(1:5, y).f

## Several layers can be added
## (same syntax as in Makie)
A = rand(50, 2)
f, ax = plotxy(A[:, 1], A[:, 2]; xlabel = &quot;x1&quot;, ylabel = &quot;x2&quot;)
ylims!(ax, -1, 2)
hlines!(ax, 0.5; color = :red, linestyle = :dot)
f
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plotxy.jl#LL1-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plscan-Tuple{Any, Any}" href="#Jchemo.plscan-Tuple{Any, Any}"><code>Jchemo.plscan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plscan(X, Y, weights = ones(nro(X)); nlv,
    bscal = :none, scal::Bool = false)
plscan!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    bscal = :none, scal::Bool = false)</code></pre><p>Canonical partial least squares regression (Canonical PLS)</p><ul><li><code>X</code> : First block (matrix) of data.</li><li><code>Y</code> : Second block (matrix) of data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>Canonical PLS with the Nipals algorithm (Wold 1984, Tenenhaus 1998 chap.11), referred to as PLS-W2A (i.e. Wold PLS mode A) in Wegelin 2000. The two blocks <code>X</code> and <code>X</code> play a symmetric role.   After each step of scores computation, X and Y are deflated by the x- and  y-scores, respectively. </p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p>Wegelin, J.A., 2000. A Survey of Partial Least Squares (PLS) Methods, with Emphasis  on the Two-Block Case (No. 371). University of Washington, Seattle, Washington, USA.</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear  Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y

fm = plscan(X, Y; nlv = 3)
pnames(fm)

fm.Tx
transf(fm, X, Y).Tx
fscale(fm.Tx, colnorm(fm.Tx))

res = summary(fm, X, Y)
pnames(res)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plscan.jl#LL1-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plskdeda" href="#Jchemo.plskdeda"><code>Jchemo.plskdeda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">plskdeda(X, y, weights = ones(nro(X)); nlv, 
    prior = :unif, h = nothing, a = 1, scal::Bool = false)</code></pre><p>KDE-LDA on PLS latent variables (PLS-KDE-LDA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations.    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>h</code> : See <code>?dmkern</code>.</li><li><code>a</code> : See <code>?dmkern</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The principle is the same as functions <code>plslda</code> and <code>plsqda</code> except  that densities are estimated from <code>dmkern</code> instead of  <code>dmnorm</code>.</p><p>See examples in <code>?plslda</code> for detailed outputs.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;)
@load db dat
pnames(dat)

X = dat.X
Y = dat.Y
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlv = 20
fm = plskdeda(Xtrain, ytrain; nlv = nlv) ;
#fm = plskdeda(Xtrain, ytrain; nlv = nlv, a = .5) ;
res = Jchemo.predict(fm, Xtest) ;
pred = res.pred
err(pred, ytest)
confusion(pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plskdeda.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plskern-Tuple{Any, Any}" href="#Jchemo.plskern-Tuple{Any, Any}"><code>Jchemo.plskern</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plskern(X, Y, weights = ones(nro(X)); nlv,
    scal::Bool = false)
plskern!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    scal::Bool = false)</code></pre><p>Partial least squares regression (PLSR) with the  &quot;improved kernel algorithm #1&quot; (Dayal &amp; McGegor, 1997).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>About the row-weighting in PLS algorithms (<code>weights</code>), see in particular Schaal et al. 2002,  Siccard &amp; Sabatier 2006, Kim et al. 2011, and Lesnoff et al. 2020. </p><p><strong>References</strong></p><p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.</p><p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active  pharmaceutical ingredients content using locally weighted partial  least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.</p><p>Lesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally weighted  PLS strategies for regression and discrimination on agronomic NIR Data.  Journal of Chemometrics. e3209.  https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209</p><p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques  from nonparametric statistics for the real time robot learning.  Applied Intell., 17, 49-60.</p><p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression  and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
fm = plskern(Xtrain, ytrain; nlv = nlv) ;
#fm = plsnipals(Xtrain, ytrain; nlv = nlv) ;
#fm = plsrosa(Xtrain, ytrain; nlv = nlv) ;
#fm = plssimp(Xtrain, ytrain; nlv = nlv) ;
pnames(fm)
fm.T

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.B
Jchemo.coef(fm; nlv = 7).B

transf(fm, Xtest)
transf(fm, Xtest; nlv = 7)

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

res = Jchemo.predict(fm, Xtest; nlv = 1:2)
res.pred[1]
res.pred[2]

res = summary(fm, Xtrain) ;
pnames(res)
z = res.explvarx
lines(z.nlv, z.cumpvar,
    axis = (xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;Prop. Explained X-Variance&quot;))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plskern.jl#LL1-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plslda" href="#Jchemo.plslda"><code>Jchemo.plslda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">plslda(X, y, weights = ones(nro(X)); nlv, 
    prior = :unif, scal::Bool = false)</code></pre><p>PLS-LDA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations.    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>LDA on PLS latent variables.</p><p>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in <code>y</code>. Each column of Ydummy is a dummy variable (0/1).  Then, a PLS2 is implemented on <code>X</code> and Ydummy,  returning <code>nlv</code> latent variables (LVs). Finally, a LDA is run on these LVs and <code>y</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

## nlv must be &gt;=1 
## (conversely to plsrda for which nlv &gt;= 0)
nlv = 20      
fm = plslda(Xtrain, ytrain; nlv = nlv) ;    
#fm = plsqda(Xtrain, ytrain; nlv = nlv) ;
pnames(fm)
pnames(fm.fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

transf(fm, Xtest)
transf(fm, Xtest; nlv = 2)

fmpls = fm.fm.fmpls ;
transf(fmpls, Xtest)
summary(fmpls, Xtrain)
Jchemo.coef(fmpls).B
Jchemo.coef(fmpls, nlv = 1).B
Jchemo.coef(fmpls, nlv = 2).B

fmda = fm.fm.fmda ;
T = transf(fmpls, Xtest)
Jchemo.predict(fmda[nlv], T).pred

Jchemo.predict(fm, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plslda.jl#LL1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsnipals-Tuple{Any, Any}" href="#Jchemo.plsnipals-Tuple{Any, Any}"><code>Jchemo.plsnipals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsnipals(X, Y, weights = ones(nro(X)); nlv,
    scal::Bool = false)
plsnipals!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    scal::Bool = false)</code></pre><p>Partial Least Squares Regression (PLSR) with the Nipals algorithm </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>In this function, for PLS2 (multivariate Y), the Nipals iterations are replaced by a  direct computation of the PLS weights (w) by SVD decomposition of matrix X&#39;Y  (Hoskuldsson 1988 p.213).</p><p>See <code>?plsnipals</code> for examples.</p><p><strong>References</strong></p><p>Hoskuldsson, A., 1988. PLS regression methods. Journal of Chemometrics 2, 211-228. https://doi.org/10.1002/cem.1180020306</p><p>Tenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique. Editions Technip,  Paris, France.</p><p>Wold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression: a basic tool  for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsnipals.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsqda" href="#Jchemo.plsqda"><code>Jchemo.plsqda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">plsqda(X, y, weights = ones(nro(X)); nlv, 
    alpha = 0, prior = :unif, scal::Bool = false)</code></pre><p>PLS-QDA (with continuum).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations.    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>; default) and LDA (<code>alpha = 1</code>).</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>QDA on PLS latent variables. </p><p>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in <code>y</code>. Each column of Ydummy is a dummy variable (0/1).  Then, a PLS2 is implemented on <code>X</code> and Ydummy,  returning <code>nlv</code> latent variables (LVs). Finally, a QDA is run on these LVs and <code>y</code>.</p><p>See <code>?plslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsqda.jl#LL1-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsravg-Tuple{Any, Any}" href="#Jchemo.plsravg-Tuple{Any, Any}"><code>Jchemo.plsravg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsravg(X, Y, weights = ones(nro(X)); nlv, 
    typavg = :unif, typw = :bisquare,
    alpha = 0, K = 5, rep = 10, scal::Bool = false)
plsravg!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv, 
    typavg = :unif, typw = :bisquare, 
    alpha = 0, K = 5, rep = 10, scal::Bool = false)</code></pre><p>Averaging and stacking PLSR models with different numbers of      latent variables (LVs).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q). Must be univariate (q = 1) if <code>typw</code> != :unif.</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>nlv</code> : A character string such as &quot;5:20&quot; defining the range of the numbers of LVs    to consider (&quot;5:20&quot;: the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as &quot;10&quot; is also allowed (&quot;10&quot;: correponds to   the single model with 10 LVs).</li><li><code>typavg</code> : Type of averaging. </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>For <code>typavg</code> in {:aic, :bic, :cv}</p><ul><li><code>typw</code> : Type of weight function. </li><li><code>alpha</code> : Pareter of the weight function.</li></ul><p>For <code>typavg</code> = :stack</p><ul><li><code>K</code> : Nb. of folds segmenting the data in the (K-fold) CV.</li><li><code>rep</code> : Nb. of repetitions of the K-fold CV. </li></ul><p>Ensemblist method where the predictions are computed by averaging  or stacking the predictions of a set of models built with different numbers of  LVs.</p><p>For instance, if argument <code>nlv</code> is set to <code>nlv = &quot;5:10&quot;</code>, the prediction for  a new observation is the average (eventually weighted) or stacking of the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.</p><p>Possible values of <code>typavg</code> are: </p><ul><li>:unif : Simple average.</li><li>:aic : Weighted average based on AIC computed for each model.</li><li>:bic : Weighted average based on BIC computed for each model.</li><li>:cv : Weighted average based on RMSEP_CV computed for each model.</li><li>:shenk : Weighted average using &quot;Shenk et al.&quot; weights computed for each model.</li><li>:stack : Linear stacking. A K-fold CV (eventually repeated) is done and </li></ul><p>the CV predictions are regressed (multiple linear model without intercept) on the observed response data.</p><p>For arguments <code>typw</code> and <code>alpha</code> (weight function): see <code>?fweight</code>.</p><p><strong>References</strong></p><p>Lesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369</p><p>Lesnoff, M., Andueza, D., Barotin, C., Barre, P., Bonnal, L., Fernández Pierna, J.A., Picard,  F., Vermeulen, P., Roger, J.-M., 2022. Averaging and Stacking Partial Least Squares Regression Models  to Predict the Chemical Compositions and the Nutritive Values of Forages from Spectral Near  Infrared Data. Applied Sciences 12, 7850. https://doi.org/10.3390/app12157850</p><p>Shenk, J., Westerhaus, M., Berzaghi, P., 1997. Investigation of a LOCAL calibration  procedure for near infrared instruments.  Journal of Near Infrared Spectroscopy 5, 223. https://doi.org/10.1255/jnirs.115</p><p>Shenk et al. 1998 United States Patent (19). Patent Number: 5,798.526.</p><p>Zhang, M.H., Xu, Q.S., Massart, D.L., 2004. Averaged and weighted average partial  least squares. Analytica Chimica Acta 504, 279–289. https://doi.org/10.1016/j.aca.2003.10.056</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)
  
X = dat.X 
Y = dat.Y
summ(Y)
y = Y.ndf
#y = Y.dm

s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(y, s)
Xtest = X[s, :]
ytest = y[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)

nlv = &quot;0:50&quot;
fm = plsravg(Xtrain, ytrain; nlv = nlv) ;
res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f   

fm = plsravg(Xtrain, ytrain; nlv = nlv,
    typavg = :cv) ;
res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)

predlv = reduce(hcat, res.predlv)
plotsp(predlv, 0:(nco(predlv) - 1); nsamp = 30).f
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsravg.jl#LL1-L110">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsrda" href="#Jchemo.plsrda"><code>Jchemo.plsrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">plsrda(X, y, weights = ones(nro(X)); nlv,
    scal::Bool = false)</code></pre><p>Discrimination based on partial least squares regression (PLSR-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>This is the usual &quot;PLSDA&quot;.  The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1) variable.  Then, a PLS2 is implemented on <code>X</code> and Ydummy, returning <code>nlv</code> LVs. Finally, a multiple linear regression (MLR) is run between the LVs and each  column of Ydummy, returning predictions of the dummy variables  (= object <code>posterior</code> returned by function <code>predict</code>).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlv = 15
fm = plsrda(Xtrain, ytrain; nlv = nlv) ;
pnames(fm)
typeof(fm.fm) # = PLS2 model

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

transf(fm, Xtest)

transf(fm.fm, Xtest)
Jchemo.coef(fm.fm)
summary(fm.fm, Xtrain)

Jchemo.predict(fm, Xtest; nlv = 1:2).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsrda.jl#LL1-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plsrosa-Tuple{Any, Any}" href="#Jchemo.plsrosa-Tuple{Any, Any}"><code>Jchemo.plsrosa</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plsrosa(X, Y, weights = ones(nro(X)); nlv,
    scal::Bool = false)
plsrosa!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    scal::Bool = false)</code></pre><p>Partial Least Squares Regression (PLSR) with the ROSA algorithm (Liland et al. 2016).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>Note:</strong> The function has the following differences with the original  algorithm of Liland et al. (2016):</p><ul><li>Scores T (LVs) are not normed.</li><li>Multivariate Y is allowed.</li></ul><p>See <code>?plsrosa</code> for examples.</p><p><strong>References</strong></p><p>Liland, K.H., Næs, T., Indahl, U.G., 2016. ROSA—a fast extension of partial least  squares regression for multiblock data analysis. Journal of Chemometrics 30,  651–662. https://doi.org/10.1002/cem.2824</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsrosa.jl#LL1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plssimp-Tuple{Any, Any}" href="#Jchemo.plssimp-Tuple{Any, Any}"><code>Jchemo.plssimp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plssimp(X, Y, weights = ones(nro(X)); nlv,
    scal::Bool = false)
plssimp!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    scal::Bool = false)</code></pre><p>Partial Least Squares Regression (PLSR) with the SIMPLS algorithm (de Jong 1993).</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p><strong>Note:</strong> In this function, scores T (LVs) are not normed, conversely to the original  algorithm of de Jong (2013)</p><p><strong>References</strong></p><p>de Jong, S., 1993. SIMPLS: An alternative approach to partial least squares  regression. Chemometrics and Intelligent Laboratory Systems 18, 251–263.  https://doi.org/10.1016/0169-7439(93)85002-X</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plssimp.jl#LL1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plstuck-Tuple{Any, Any}" href="#Jchemo.plstuck-Tuple{Any, Any}"><code>Jchemo.plstuck</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plstuck(X, Y, weights = ones(nro(X)); nlv,
    bscal = :none, scal::Bool = false)
plstuck!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,
    bscal = :none, scal::Bool = false)</code></pre><p>Tucker&#39;s inter-battery method of factor analysis</p><ul><li><code>X</code> : First block (matrix) of data.</li><li><code>Y</code> : Second block (matrix) of data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>Inter-battery method of factor analysis (Tucker 1958, Tenenhaus 1998 chap.3),  The two blocks <code>X</code> and <code>X</code> play a symmetric role.  This method is referred to  as PLS-SVD in Wegelin 2000. The basis of the method is to factorize the covariance  matrix X&#39;Y by SVD. </p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.</p><p>Tishler, A., Lipovetsky, S., 2000. Modelling and forecasting with robust  canonical analysis: method and application. Computers &amp; Operations Research 27,  217–232. https://doi.org/10.1016/S0305-0548(99)00014-3</p><p>Tucker, L.R., 1958. An inter-battery method of factor analysis. Psychometrika 23, 111–136. https://doi.org/10.1007/BF02289009</p><p>Wegelin, J.A., 2000. A Survey of Partial Least Squares (PLS) Methods, with Emphasis  on the Two-Block Case (No. 371). University of Washington, Seattle, Washington, USA.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y

fm = plstuck(X, Y; nlv = 3)
pnames(fm)

fm.Tx
transf(fm, X, Y).Tx
fscale(fm.Tx, colnorm(fm.Tx))

res = summary(fm, X, Y)
pnames(res)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plstuck.jl#LL1-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.plswold-Tuple{Any, Any}" href="#Jchemo.plswold-Tuple{Any, Any}"><code>Jchemo.plswold</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">plswold(X, Y, weights = ones(nro(X)); nlv,
    tol = sqrt(eps(1.)), maxit = 200, scal::Bool = false)
plswold!(X, Y, weights = ones(nro(X)); nlv,
    tol = sqrt(eps(1.)), maxit = 200, scal::Bool = false)</code></pre><p>Partial Least Squares Regression (PLSR) with the Wold algorithm </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider.</li><li><code>tol</code> : Tolerance for the Nipals algorithm.</li><li><code>maxit</code> : Maximum number of iterations for the Nipals algorithm.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Wold Nipals PLSR algorithm: Tenenhaus 1998 p.204.</p><p>See <code>?plskern</code> for examples.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique. Editions Technip,  Paris, France.</p><p>Wold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in  Linear Regression. The Partial Least Squares (PLS). Approach to  Generalized Inverses. SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plswold.jl#LL1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pmod-Tuple{Any}" href="#Jchemo.pmod-Tuple{Any}"><code>Jchemo.pmod</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pmod(foo)</code></pre><p>Shortcut for function <code>parentmodule</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL533-L537">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pnames-Tuple{Any}" href="#Jchemo.pnames-Tuple{Any}"><code>Jchemo.pnames</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pnames(x)</code></pre><p>Return the names of the elements of <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL539-L543">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.CalDs, Any}" href="#Jchemo.predict-Tuple{Jchemo.CalDs, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::CalDs, X; kwargs...)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>kwargs</code> : Optional arguments.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/calds.jl#LL67-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.CalPds, Any}" href="#Jchemo.predict-Tuple{Jchemo.CalPds, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::CalPds, X; kwargs...)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>kwargs</code> : Optional arguments.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/calpds.jl#LL83-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Cglsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Cglsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. iterations, or collection of nb. iterations, to consider. </li></ul><p>If nothing, it is the maximum nb. iterations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/cglsr.jl#LL176-L183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dkplsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsr.jl#LL146-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dkplsrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dkplsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dkplsrda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul><p>If nothing, it is the maximum nb. LVs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsrda.jl#LL75-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dmkern, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dmkern, x)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>x</code> : Data (vector) for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dmkern.jl#LL138-L143">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}" href="#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Dmnorm, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Data (vector) for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dmnorm.jl#LL148-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Knnda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Knnda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Knnda1, X)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/knnda.jl#LL88-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Knnr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Knnr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Knnr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/knnr.jl#LL76-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Kplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Kplsr, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul><p>If nothing, it is the maximum nb. LVs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kplsr.jl#LL214-L221">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Krr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Krr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Krr, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters, &quot;lambda&quot; to consider.    If nothing, it is the parameter stored in the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/krr.jl#LL165-L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lda.jl#LL80-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwmlr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlr.jl#LL77-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.LwmlrS, Any}" href="#Jchemo.predict-Tuple{Jchemo.LwmlrS, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::LwmlrS, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlr_s.jl#LL123-L128">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwmlrda, X)</code></pre><p>Compute y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlrda.jl#LL65-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.LwmlrdaS, Any}" href="#Jchemo.predict-Tuple{Jchemo.LwmlrdaS, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::LwmlrdaS, X)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwmlrda_s.jl#LL108-L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplslda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplslda.jl#LL83-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplsqda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsqda.jl#LL92-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplsr, X; nlv = nothing)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsr.jl#LL115-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}" href="#Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::LwplsrAvg, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsravg.jl#LL89-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.LwplsrS, Any}" href="#Jchemo.predict-Tuple{Jchemo.LwplsrS, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::LwplsrS, X; nlv = nothing)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsr_s.jl#LL139-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Lwplsrda, X; nlv = nothing)</code></pre><p>Compute the y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsrda.jl#LL81-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.LwplsrdaS, Any}" href="#Jchemo.predict-Tuple{Jchemo.LwplsrdaS, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::LwplsrdaS, X; nlv = nothing)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/lwplsrda_s.jl#LL103-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Mlr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Mlr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Mlr, X)</code></pre><p>Compute the Y-predictions from the fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlr.jl#LL263-L268">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Mlrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Mlrda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mlrda.jl#LL60-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occknndis, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occknndis, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occknndis, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occknndis.jl#LL141-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occlknndis, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occlknndis, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occlknndis, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occlknndis.jl#LL157-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occod, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occod, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occod, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occod.jl#LL51-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occsd, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occsd, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occsd, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occsd.jl#LL138-L143">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occsdod, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occsdod, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occsdod.jl#LL48-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Occstah, Any}" href="#Jchemo.predict-Tuple{Jchemo.Occstah, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Occstah, X)</code></pre><p>Compute predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/occstah.jl#LL42-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Plslda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Plslda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plslda.jl#LL98-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Plsravg, Any}" href="#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Plsravg, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsravg.jl#LL128-L133">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Plsrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Plsrda, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsrda.jl#LL84-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Qda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Qda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Qda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/qda.jl#LL95-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Qda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rda.jl#LL141-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Rosaplsr, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rosaplsr.jl#LL230-L236">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Rr, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters,    &quot;lambda&quot; to consider. If nothing, it is the parameter stored in the    fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rr.jl#LL122-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Rrda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Rrda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Rrda, X; lb = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>lb</code> : Regularization parameter, or collection of regularization parameters,    &quot;lambda&quot; to consider. If nothing, it is the parameter stored in the    fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rrda.jl#LL64-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Soplsr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Soplsr, Xbl)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/soplsr.jl#LL120-L125">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Svmda, Any}" href="#Jchemo.predict-Tuple{Jchemo.Svmda, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Svmda, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/svmda.jl#LL113-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.Svmr, Any}" href="#Jchemo.predict-Tuple{Jchemo.Svmr, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Svmr, X)</code></pre><p>Compute y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/svmr.jl#LL125-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}" href="#Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::TreedaDt, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/treeda_dt.jl#LL104-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Jchemo.TreerDt, Any}" href="#Jchemo.predict-Tuple{Jchemo.TreerDt, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::TreerDt, X)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/treer_dt.jl#LL100-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}" href="#Jchemo.predict-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Union{MbplsWest, Mbplsr}, Xbl; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbplswest.jl#LL219-L225">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}, Any}" href="#Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(object::Union{Plsr, Pcr}, X; nlv = nothing)</code></pre><p>Compute Y-predictions from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which predictions are computed.</li><li><code>nlv</code> : Nb. LVs, or collection of nb. LVs, to consider. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plskern.jl#LL212-L218">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.psize-Tuple{Any}" href="#Jchemo.psize-Tuple{Any}"><code>Jchemo.psize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">psize(x)</code></pre><p>Print the type and size of <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL545-L549">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.pval-Tuple{Distributions.Distribution, Any}" href="#Jchemo.pval-Tuple{Distributions.Distribution, Any}"><code>Jchemo.pval</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pval(d::Distribution, q)
pval(x::Array, q)
pval(e_cdf::ECDF, q)</code></pre><p>Compute p-value(s) for a distribution, an ECDF or vector.</p><ul><li><code>d</code> : A distribution computed from <code>Distribution.jl</code>.</li><li><code>x</code> : Univariate data.</li><li><code>e_cdf</code> : An ECDF computed from <code>StatsBase.jl</code>.</li><li><code>q</code> : Value(s) for which to compute the p-value(s).</li></ul><p>Compute or estimate the p-value of quantile <code>q</code>, ie. P(Q &gt; <code>q</code>) where Q is the random variable.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Distributions, StatsBase

d = Distributions.Normal(0, 1)
q = 1.96
#q = [1.64; 1.96]
Distributions.cdf(d, q)    # cumulative density function (CDF)
Distributions.ccdf(d, q)   # complementary CDF (CCDF)
pval(d, q)                 # Distributions.ccdf

x = rand(5)
e_cdf = StatsBase.ecdf(x)
e_cdf(x)                # empirical CDF computed at each point of x (ECDF)
p_val = 1 .- e_cdf(x)   # complementary ECDF at each point of x
q = .3
#q = [.3; .5; 10]
pval(e_cdf, q)          # 1 .- e_cdf(q)
pval(x, q)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL554-L587">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.qda" href="#Jchemo.qda"><code>Jchemo.qda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">qda(X, y, weights = ones(nro(X)); 
    prior = :unif, alpha = 0)</code></pre><p>Quadratic discriminant analysis (QDA, with continuum towards LDA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform), :prop (proportional).</li><li><code>alpha</code> : Scalar (∈ [0, 1]) defining the continuum   between QDA (<code>alpha = 0</code>; default) and LDA (<code>alpha = 1</code>).</li></ul><p>A value <code>alpha</code> &gt; 0 shrinks the QDA separate covariances by class  (Wi) toward a common LDA covariance (W). This corresponds to the first regularization (Eqs.16) proposed by Friedman 1989.</p><p><strong>References</strong></p><p>Friedman JH. Regularized Discriminant Analysis. Journal of the American  Statistical Association. 1989; 84(405):165-175.  doi:10.1080/01621459.1989.10478752.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4] 
y = dat.X[:, 5]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

tab(ytrain)
tab(ytest)

prior = :unif
#prior = :prop
fm = qda(Xtrain, ytrain; prior = prior) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.dens
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/qda.jl#LL1-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.r2-Tuple{Any, Any}" href="#Jchemo.r2-Tuple{Any, Any}"><code>Jchemo.r2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">r2(pred, Y)</code></pre><p>Compute the R2 coefficient.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>The rate R2 is calculated by R2 = 1 - MSEP(current model) / MSEP(null model),  where the &quot;null model&quot; is the overall mean.  For predictions over CV or test sets, and/or for non linear models,  it can be different from the square of the correlation coefficient (<code>cor2</code>)  between the true data and the predictions. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
r2(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
r2(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL160-L189">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rasvd-Tuple{Any, Any}" href="#Jchemo.rasvd-Tuple{Any, Any}"><code>Jchemo.rasvd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rasvd(X, Y, weights = ones(nro(X)); nlv,
    bscal = :none, tau = 1e-8, scal::Bool = false)
rasvd!(X, Y, weights = ones(nro(X)); nlv,
    bscal = :none, tau = 1e-8, scal::Bool = false)</code></pre><p>Redundancy analysis (RA) - PCA on instrumental variables (PCAIV)</p><ul><li><code>X</code> : First block of data (explicative variables).</li><li><code>Y</code> : Second block of data (dependent variables).</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>bscal</code> : Type of block scaling (<code>:none</code>, <code>:frob</code>).    See functions <code>blockscal</code>.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>See e.g. Bougeard et al. 2011a,b and Legendre &amp; Legendre 2012.  Let Y<em>hat be the fitted values of the regression of <code>Y</code> on <code>X</code>.  The scores <code>Ty</code> are the PCA scores of Y</em>hat. The scores <code>Tx</code> are  the fitted values of the regression of <code>Ty</code> on <code>X</code>.</p><p>A continuum regularization is available.   After block centering and scaling, the covariances matrices are  computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li></ul><p>where D is the observation (row) metric.  Value <code>tau</code> = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. <code>tau</code> = 1e-8)  to get similar results as with pseudo-inverses.    </p><p><strong>References</strong></p><p>Bougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011. Multiblock redundancy  analysis from a user&#39;s perspective. Application in veterinary epidemiology.  Electronic Journal of Applied Statistical Analysis 4, 203-214.  https://doi.org/10.1285/i20705948v4n2p203</p><p>Bougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy analysis:  interpretation tools and application in epidemiology. Journal of Chemometrics 25,  467-475. https://doi.org/10.1002/cem.1392</p><p>Legendre, P., Legendre, L., 2012. Numerical Ecology. Elsevier,  Amsterdam, The Netherlands.</p><p>Tenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and Sparse Generalized Canonical  Correlation Analysis for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/linnerud.jld2&quot;) 
@load db dat
pnames(dat)
X = dat.X 
Y = dat.Y

tau = 1e-8
fm = rasvd(X, Y; nlv = 3, tau = tau)
pnames(fm)

fm.Tx
transf(fm, X, Y).Tx
fscale(fm.Tx, colnorm(fm.Tx))

res = summary(fm, X, Y)
pnames(res)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rasvd.jl#LL1-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rd-Tuple{Any, Any}" href="#Jchemo.rd-Tuple{Any, Any}"><code>Jchemo.rd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rd(X, Y; typ = :cor)
rd(X, Y, weights; typ = :cor)</code></pre><p>Compute redundancy coefficients between two matrices.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>weights</code> : Weights (n) of the observations. Internally    normalized to sum to 1.</li><li><code>typ</code> : If :cor (default), correlation is used, else uncorrected    covariance is used. </li></ul><p>Returns the redundancy coefficient between <code>X</code> and each column of <code>Y</code>, i.e.: </p><p>(1 / p) * [Sum(j=1, .., p) cor(xj, y1)^2 ; ... ; Sum(j=1, .., p) cor(xj, yq)^2] </p><p>See Tenenhaus 1998 section 2.2.1 p.10-11.</p><p><strong>References</strong></p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
Y = rand(5, 3)
rd(X, Y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/angles.jl#LL62-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rda" href="#Jchemo.rda"><code>Jchemo.rda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rda(X, y, weights = ones(nro(X)); 
    prior = :unif, alpha = 1, lb = 1e-10, 
    simpl::Bool = false, scal::Bool = false)</code></pre><p>Regularized discriminant analysis (RDA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>alpha</code> : Shrinkage parameter of the separate covariances of    QDA toward a common covariance as in LDA. Must ∈ <a href="`alpha` is referred to as lambda in Friedman 1989">0, 1</a>.</li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot; (&gt;= 0).</li><li><code>simpl</code> : Boolean (default to <code>false</code>). See <code>dmnorm</code>. </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Regularized compromise between LDA and QDA, see Friedman 1989. </p><p>Noting W the (corrected) pooled within-class covariance matrix and  Wi the (corrected) within-class covariance matrix of class i, the  regularization is done by with the two successive steps:</p><ul><li>Continuum between QDA and LDA: Wi(1) = (1 - <code>alpha</code>) * Wi + <code>alpha</code> * W       </li><li>Ridge regularization: Wi(2) = Wi(1) + <code>lb</code> * I</li></ul><p>Then the QDA algorithm is run on matrices Wi(2).</p><p>Function <code>rda</code> is slightly different from the regularization expression  used by Friedman 1989 (Eq.18). It shrinks the covariance matrices Wi(2)  to the diagonal of the Idendity matrix (ridge regularization) (e.g. Guo et al. 2007).  </p><p>Particular cases:</p><ul><li><code>alpha</code> = 1 &amp; <code>lb</code> = 0 : LDA</li><li><code>alpha</code> = 0 &amp; <code>lb</code> = 0 : QDA</li><li><code>alpha</code> = 1 &amp; <code>lb</code> &gt; 0 : Penalized LDA (Hstie et al 1995) with diagonal   regularization matrix</li></ul><p><strong>References</strong></p><p>Friedman JH. Regularized Discriminant Analysis. Journal of the American  Statistical Association. 1989; 84(405):165-175.  doi:10.1080/01621459.1989.10478752.</p><p>Guo Y, Hastie T, Tibshirani R. Regularized linear discriminant  analysis and its application in microarrays. Biostatistics.  2007; 8(1):86-100. doi:10.1093/biostatistics/kxj035.</p><p>Hastie, T., Buja, A., Tibshirani, R., 1995. Penalized Discriminant Analysis.  The Annals of Statistics 23, 73–102.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, StatsBase

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)
  
X = dat.X 
Y = dat.Y
y = Y.typ
wlstr = names(X)
wl = parse.(Float64, wlstr)
ntot = nro(X)

plotsp(X, wl).f

summ(Y)
tab(y)
tab(Y.test)

s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(y, s)
Xtest = X[s, :]
ytest = y[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)

alpha = .5
lb = 1e-8
fm = rda(Xtrain, ytrain; alpha = alpha, 
    lb = lb, simpl = true) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest)
pnames(res)
res.dens
res.posterior
res.pred
err(res.pred, ytest)
confusion(res.pred, ytest).cnt</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rda.jl#LL1-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.recodcat2int-Tuple{Any}" href="#Jchemo.recodcat2int-Tuple{Any}"><code>Jchemo.recodcat2int</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recodcat2int(x; start = 1)</code></pre><p>Recode a categorical variable to a integer variable</p><ul><li><code>x</code> : Variable to recode.</li><li><code>start</code> : Integer value that will be set to the first category.</li></ul><p>The numeric codes returned by the function are <code>Int</code> and  correspond to the sorted categories of <code>x</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [&quot;b&quot;, &quot;a&quot;, &quot;b&quot;]   
[x recodcat2int(x)]
recodcat2int(x; start = 0)
recodcat2int([25, 1, 25])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL594-L610">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.recodnum2cla-Tuple{Any, Any}" href="#Jchemo.recodnum2cla-Tuple{Any, Any}"><code>Jchemo.recodnum2cla</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recodnum2cla(x, q)</code></pre><p>Recode a continuous variable to classes.</p><ul><li><code>x</code> : Variable to recode.</li><li><code>q</code> : Values separating the classes. </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Statistics
x = [collect(1:10); 8.1 ; 3.1] 
q = [3; 8]
zx = recodnum2cla(x, q)  
[x zx]
probs = [.33; .66]
q = quantile(x, probs) 
zx = recodnum2cla(x, q)  
[x zx]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL619-L637">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.recovkwargs-Tuple{Any, Any}" href="#Jchemo.recovkwargs-Tuple{Any, Any}"><code>Jchemo.recovkwargs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recovkwargs(ParamStruct, kwargs)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL651-L653">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.replacebylev-Tuple{Any, Any}" href="#Jchemo.replacebylev-Tuple{Any, Any}"><code>Jchemo.replacebylev</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replacebylev(x, lev)</code></pre><p>Replace the elements of a vector by levels of corresponding order.</p><ul><li><code>x</code> : Vector (n) of values to replace.</li><li><code>lev</code> : Vector (nlev) containing the levels.</li></ul><p><em>Warning</em>: <code>x</code> and <code>lev</code> must contain the same number (nlev) of levels.</p><p>The ith sorted level in <code>x</code> is replaced by the ith sorted level of <code>lev</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [10; 4; 3; 3; 4; 4]
lev = [&quot;B&quot;; &quot;C&quot;; &quot;AA&quot;]
sort(lev)
[x replacebylev(x, lev)]
zx = string.(x)
[zx replacebylev(zx, lev)]

lev = [3; 0; -1]
[x replacebylev(x, lev)]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL663-L685">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}" href="#Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}"><code>Jchemo.replacebylev2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replacebylev2(x::Union{Int, Array{Int}}, lev::Array)</code></pre><p>Replace the elements of an index-vector by levels.</p><ul><li><code>x</code> : Vector (n) of values to replace.</li><li><code>lev</code> : Vector (nlev) containing the levels.</li></ul><p><em>Warning</em>: Let us note nlev the number of levels in <code>lev</code>.  Vector <code>x</code> must contain integer values between 1 and nlev. </p><p>Each element <code>x</code><a href="i = 1, ..., n">i</a> is replaced by sort(<code>lev</code>)[<code>x</code>[i]].</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [2; 1; 2; 2]
lev = [&quot;B&quot;; &quot;C&quot;; &quot;AA&quot;]
sort(lev)
[x replacebylev2(x, lev)]
replacebylev2([2], lev)
replacebylev2(2, lev)

x = [2; 1; 2]
lev = [3; 0; -1]
replacebylev2(x, lev)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL700-L724">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.replacedict-Tuple{Any, Any}" href="#Jchemo.replacedict-Tuple{Any, Any}"><code>Jchemo.replacedict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replacedict(x, dict)</code></pre><p>Replace the elements of a vector by levels defined in a dictionary.</p><ul><li><code>x</code> : Vector (n) of values to replace.</li><li><code>dict</code> : A dictionary of the correpondances betwwen the old and new values.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">dict = Dict(&quot;a&quot; =&gt; 1000, &quot;b&quot; =&gt; 1, &quot;c&quot; =&gt; 2)

x = [&quot;c&quot;; &quot;c&quot;; &quot;a&quot;; &quot;a&quot;; &quot;a&quot;]
replacedict(x, dict)

x = [&quot;c&quot;; &quot;c&quot;; &quot;a&quot;; &quot;a&quot;; &quot;a&quot;; &quot;e&quot;]
replacedict(x, dict)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL736-L752">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.residcla-Tuple{Any, Any}" href="#Jchemo.residcla-Tuple{Any, Any}"><code>Jchemo.residcla</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">residcla(pred, y)</code></pre><p>Compute classification prediction error (0 = no error, 1 = error).</p><ul><li><code>pred</code> : Predictions.</li><li><code>y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
ytrain = rand([&quot;a&quot; ; &quot;b&quot;], 10)
Xtest = rand(4, 5) 
ytest = rand([&quot;a&quot; ; &quot;b&quot;], 4)

fm = plsrda(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
residcla(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL197-L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.residreg-Tuple{Any, Any}" href="#Jchemo.residreg-Tuple{Any, Any}"><code>Jchemo.residreg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">residreg(pred, Y)</code></pre><p>Compute regression prediction errors.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
residreg(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
residreg(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL217-L240">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}" href="#Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}"><code>Jchemo.rfda_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rfda_dt(X, yy::Union{Array{Int}, Array{String}}; 
    n_trees = 10,
    partial_sampling = .7,  
    n_subfeatures = -1,
    max_depth = -1, min_samples_leaf = 5, 
    min_samples_split = 2, scal::Bool = false, 
    mth = true, kwargs...)</code></pre><p>Random forest discrimination with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n obs., p variables).</li><li><code>y</code> : Univariate Y-data (n obs.).</li><li><code>n_trees</code> : Nb. trees built for the forest. </li><li><code>partial_sampling</code> : Proportion of sampled observations for each tree.</li><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: -1 ==&gt; sqrt(#variables)).</li><li><code>max_depth</code> : Maximum depth of the decision trees (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li><code>mth</code> : Boolean indicating if a multi-threading is done when new data are    predicted with function <code>predict</code>.</li><li><code>kwargs</code> : Optional named arguments to pass in function <code>build_forest</code>    of <code>DecisionTree.jl</code>.</li></ul><p>The function fits a random forest discrimination model using package  `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140.  https://doi.org/10.1007/BF00058655</p><p>Breiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Genuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI.</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)
  
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
ntot, p = size(X)
  
ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntest = ntot - ntrain
(ntot = ntot, ntrain, ntest)

tab(ytrain)
tab(ytest)

n_subfeatures = 2 
fm = rfda_dt(Xtrain, ytrain; n_trees = 100,
    n_subfeatures = n_subfeatures) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest)
res.pred
err(res.pred, ytest) </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rfda_dt.jl#LL1-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rfr_dt-Tuple{Any, Any}" href="#Jchemo.rfr_dt-Tuple{Any, Any}"><code>Jchemo.rfr_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rfr_dt(X, y; n_trees = 10,
    partial_sampling = .7,  
    n_subfeatures = -1,
    max_depth = -1, min_samples_leaf = 5, 
    min_samples_split = 2, scal::Bool = false, 
    mth = true, kwargs...)</code></pre><p>Random forest regression with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n obs., p variables).</li><li><code>y</code> : Univariate y-data (n obs.).</li><li><code>n_trees</code> : Nb. trees built for the forest. </li><li><code>partial_sampling</code> : Proportion of sampled observations for each tree.</li><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: -1 ==&gt; sqrt(#variables)).</li><li><code>max_depth</code> : Maximum depth of the decision trees (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li><code>mth</code> : Boolean indicating if a multi-threading is done when new data are    predicted with function <code>predict</code>.</li><li><code>kwargs</code> : Optional named arguments to pass in function <code>build_forest</code>    of <code>DecisionTree.jl</code>.</li></ul><p>The function fits a random forest regression model using package  `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140.  https://doi.org/10.1007/BF00058655</p><p>Breiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Genuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI.</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2021.jld2&quot;)
@load db dat
pnames(dat)

Xtrain = dat.Xtrain
Ytrain = dat.Ytrain
ytrain = Ytrain.y
s = dat.Ytest.inst .== 1 
Xtest = dat.Xtest[s, :]
Ytest = dat.Ytest[s, :]
ytest = Ytest.y
wlstr = names(Xtrain) 
wl = parse.(Float64, wlstr) 
ntrain, p = size(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

f = 21 ; pol = 3 ; d = 2 
Xptrain = savgol(snv(Xtrain); f, pol, d) 
Xptest = savgol(snv(Xtest); f, pol, d) 

n_subfeatures = p / 3 
fm = rfr_dt(Xptrain, ytrain; n_trees = 100,
    n_subfeatures = n_subfeatures) ;
pnames(fm)

res = Jchemo.predict(fm, Xptest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rfr_dt.jl#LL1-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}" href="#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}"><code>Jchemo.rmcol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmcol(X, s)</code></pre><p>Remove the columns of a matrix or the components of a vector  having indexes <code>s</code>.</p><ul><li><code>X</code> : Matrix or vector.</li><li><code>s</code> : Vector of the indexes.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 3) 
rmcol(X, [1, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL758-L770">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmgap-Tuple{Any}" href="#Jchemo.rmgap-Tuple{Any}"><code>Jchemo.rmgap</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmgap(X; indexcol, k = 5)
rmgap!(X; indexcol, k = 5)</code></pre><p>Remove vertical gaps in spectra , e.g. for ASD.  </p><ul><li><code>X</code> : X-data.</li><li><code>indexcol</code> : The indexes of the columns where are located the gaps. </li><li><code>k</code> : The number of columns used on the left side        of the gaps for fitting the linear regressions.</li></ul><p>For each observation (row of matrix <code>X</code>), the corrections are done by extrapolation from simple linear regressions  computed on the left side of the defined gaps. </p><p>For instance, If two gaps are observed between indexes 651-652 and  between indexes 1425-1426, respectively, then the syntax should  be <code>indexcol = [651 ; 1425]</code>.</p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/asdgap.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
wlstr = names(dat.X)
wl = parse.(Float64, wlstr)

z = [1000 ; 1800] 
u = findall(in(z).(wl))
f, ax = plotsp(X, wl)
vlines!(ax, z; linestyle = :dash, color = (:grey, .8))
f

# Corrected data

u = findall(in(z).(wl))
zX = rmgap(X; indexcol = u, k = 5)  
f, ax = plotsp(zX, wl)
vlines!(ax, z; linestyle = :dash, color = (:grey, .8))
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rmgap.jl#LL1-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}" href="#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}"><code>Jchemo.rmrow</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmrow(X, s)</code></pre><p>Remove the rows of a matrix or the components of a vector  having indexes <code>s</code>.</p><ul><li><code>X</code> : Matrix or vector.</li><li><code>s</code> : Vector of the indexes.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
rmrow(X, [1, 4])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL781-L793">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmsep-Tuple{Any, Any}" href="#Jchemo.rmsep-Tuple{Any, Any}"><code>Jchemo.rmsep</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmsep(pred, Y)</code></pre><p>Compute the square root of the mean of the squared prediction  errors (RMSEP).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rmsep(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rmsep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL243-L267">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rmsepstand-Tuple{Any, Any}" href="#Jchemo.rmsepstand-Tuple{Any, Any}"><code>Jchemo.rmsepstand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rmsepstand(pred, Y)</code></pre><p>Compute the standardized square root of the mean of the squared  prediction errors (RMSEP_stand).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>RMSEP is standardized to <code>Y</code>: RMSEP_stand = RMSEP ./ <code>Y</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rmsepstand(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rmsepstand(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL270-L296">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rosaplsr-Tuple{Any, Any}" href="#Jchemo.rosaplsr-Tuple{Any, Any}"><code>Jchemo.rosaplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rosaplsr(Xbl, Y, weights = ones(nro(Xbl[1])); nlv)
rosaplsr!(Xbl, Y, weights = ones(nro(Xbl[1])); nlv)</code></pre><p>Multiblock PLSR with the ROSA algorithm (Liland et al. 2016).</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to consider.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and    of <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>The function has the following differences with the original  algorithm of Liland et al. (2016):</p><ul><li>Scores T are not normed to 1.</li><li>Multivariate <code>Y</code> is allowed. In such a case, the squared residuals are summed    over the columns for finding the winning block for each global LV    (therefore Y-columns should have the same fscale).</li></ul><p><strong>References</strong></p><p>Liland, K.H., Næs, T., Indahl, U.G., 2016. ROSA — a fast extension of partial least  squares regression for multiblock data analysis. Journal of Chemometrics 30,  651–662. https://doi.org/10.1002/cem.2824</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ham.jld2&quot;) 
@load db dat
pnames(dat) 

X = dat.X
y = dat.Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X, listbl)
# &quot;New&quot; = first two rows of Xbl 
Xbl_new = mblock(X[1:2, :], listbl)

nlv = 5
fm = rosaplsr(Xbl, y; nlv = nlv) ;
pnames(fm)
fm.T
transf(fm, Xbl_new)
[y Jchemo.predict(fm, Xbl).pred]
Jchemo.predict(fm, Xbl_new).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rosaplsr.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowmean-Tuple{Any}" href="#Jchemo.rowmean-Tuple{Any}"><code>Jchemo.rowmean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowmean(X)</code></pre><p>Compute the row-means of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
rowmean(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_rowwise.jl#LL1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowstd-Tuple{Any}" href="#Jchemo.rowstd-Tuple{Any}"><code>Jchemo.rowstd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowstd(X)</code></pre><p>Compute the row-standard deviations (uncorrected) of a matrix`.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
rowstd(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_rowwise.jl#LL17-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowsum-Tuple{Any}" href="#Jchemo.rowsum-Tuple{Any}"><code>Jchemo.rowsum</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowsum(X)</code></pre><p>Compute the row-sums of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
rowsum(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_rowwise.jl#LL34-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rowvar-Tuple{Any}" href="#Jchemo.rowvar-Tuple{Any}"><code>Jchemo.rowvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rowvar(X)</code></pre><p>Compute the row-variances (uncorrected) of a matrix.</p><ul><li><code>X</code> : Data (n, p).</li></ul><p>Return a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 6
X = rand(n, p)
rowvar(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility_rowwise.jl#LL49-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rp-Tuple{Any}" href="#Jchemo.rp-Tuple{Any}"><code>Jchemo.rp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rp(X, weights = ones(nro(X)); nlv, fun = rpmatli, scal::Bool = false, kwargs ...)
rp!(X::Matrix, weights = ones(nro(X)); nlv, fun = rpmatli, scal::Bool = false, kwargs ...)</code></pre><p>Make a random projection of matrix X.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>weights</code> : Weights (n) of the observations. Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. dimensions on which <code>X</code> is projected.</li><li><code>fun</code> : A function of random projection.</li><li><code>kwargs</code> : Optional arguments of function <code>fun</code>.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
nlv = 3
fm = rp(X; nlv = nlv)
pnames(fm)
size(fm.P) 
fm.P
fm.T # = X * fm.P 
transf(fm, X[1:2, :])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rp.jl#LL1-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpd-Tuple{Any, Any}" href="#Jchemo.rpd-Tuple{Any, Any}"><code>Jchemo.rpd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rpd(pred, Y)</code></pre><p>Compute the ratio &quot;deviation to model performance&quot; (RPD).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p>This is the ratio of the deviation to the model performance to the deviation, defined by RPD = Std(Y) / RMSEP, where Std(Y) is the standard deviation. </p><p>Since Std(Y) = RMSEP(null model) where the null model is the simple average, this also gives RPD = RMSEP(null model) / RMSEP. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rpd(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rpd(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL302-L331">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpdr-Tuple{Any, Any}" href="#Jchemo.rpdr-Tuple{Any, Any}"><code>Jchemo.rpdr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rpdr(pred, Y)</code></pre><p>Compute a robustified RPD.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rpdr(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
rpdr(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL337-L360">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpmatgauss" href="#Jchemo.rpmatgauss"><code>Jchemo.rpmatgauss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rpmatgauss(p, nlv)</code></pre><p>Build a gaussian random projection matrix.</p><ul><li><code>p</code> : Nb. variables (attributes) to project.</li><li><code>nlv</code> : Nb. of simulated projection dimensions.</li></ul><p>The function returns a random projection matrix P of dimension  <code>p</code> x <code>nlv</code>. The projection of a given matrix X of size n x <code>p</code> is given by X * P.</p><p>P is simulated from i.i.d. N(0, 1)/sqrt(<code>nlv</code>).</p><p><strong>References</strong></p><p>Li, P., Hastie, T.J., Church, K.W., 2006. Very sparse random projections,  in: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge  Discovery and Data Mining, KDD ’06. Association for Computing Machinery, New York, NY, USA, pp. 287–296. https://doi.org/10.1145/1150402.1150436</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">p = 10 ; nlv = 3
rpmatgauss(p, nlv)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rpmat.jl#LL1-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rpmatli" href="#Jchemo.rpmatli"><code>Jchemo.rpmatli</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rpmatli(p, nlv; s = sqrt(p))</code></pre><p>Build a sparse random projection matrix (Achlioptas 2001, Li et al. 2006).</p><ul><li><code>p</code> : Nb. variables (attributes) to project.</li><li><code>nlv</code> : Nb. final dimensions, i.e. after projection.</li><li><code>s</code> : Coefficient defining the sparsity of the returned matrix    (higher is <code>s</code>, higher is the sparsity).</li></ul><p>The function returns a random projection matrix P of dimension  <code>p</code> x <code>nlv</code>. The projection of a given matrix X of size n x <code>p</code> is given by X * P.</p><p>P is simulated from i.i.d. &quot;p_ij&quot; = </p><ul><li>1 with prob. 1/(2 * <code>s</code>)</li><li>0 with prob. 1 - 1 / <code>s</code></li><li>-1 with prob. 1/(2 * <code>s</code>)</li></ul><p>Usual values for <code>s</code> are:</p><ul><li>sqrt(<code>p</code>)       (Li et al. 2006)</li><li><code>p</code> / log(<code>p</code>)  (Li et al. 2006)</li><li>1               (Achlioptas 2001)</li><li>3               (Achlioptas 2001) </li></ul><p><strong>References</strong></p><p>Achlioptas, D., 2001. Database-friendly random projections,  in: Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on  Principles of Database Systems, PODS ’01. Association for Computing Machinery,  New York, NY, USA, pp. 274–281. https://doi.org/10.1145/375551.375608</p><p>Li, P., Hastie, T.J., Church, K.W., 2006. Very sparse random projections,  in: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge  Discovery and Data Mining, KDD ’06. Association for Computing Machinery, New York, NY, USA, pp. 287–296. https://doi.org/10.1145/1150402.1150436</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">p = 10 ; nlv = 3
rpmatli(p, nlv)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rpmat.jl#LL29-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rr-Tuple{Any, Any}" href="#Jchemo.rr-Tuple{Any, Any}"><code>Jchemo.rr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rr(X, Y, weights = ones(nro(X)); lb = .01,
    scal::Bool = false)
rr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); lb = .01,
    scal::Bool = false)</code></pre><p>Ridge regression (RR) implemented by SVD factorization.</p><ul><li><code>X</code> : X-data.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p><code>X</code> and <code>Y</code> are internally centered. The model is computed with an intercept. </p><p><strong>References</strong></p><p>Cule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice  of ridge parameter in ridge regression. arXiv:1205.0686.</p><p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic regularization  for expression arrays. Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010</p><p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.</p><p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.  Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

lb = 10^(-2)
fm = rr(Xtrain, ytrain; lb = lb) ;
#fm = rrchol(Xtrain, ytrain; lb = lb) ;
pnames(fm)

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.B
# Only for rr
Jchemo.coef(fm; lb = .1).B

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f    

# Only for rr
res = Jchemo.predict(fm, Xtest; lb = [.1 ; .01])
res.pred[1]
res.pred[2]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rr.jl#LL1-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rrchol-Tuple{Any, Any}" href="#Jchemo.rrchol-Tuple{Any, Any}"><code>Jchemo.rrchol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rrchol(X, Y, weights = ones(nro(X)); lb = .01, 
    scal::Bool = false)
rrchol!(X::Matrix, Y::Matrix, weights = ones(nro(X)); lb = .01,
    scal::Bool = false)</code></pre><p>Ridge regression (RR) using the Normal equations and a Cholesky factorization.</p><ul><li><code>X</code> : X-data.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p><code>X</code> and <code>Y</code> are internally centered. The model is computed with an intercept. </p><p>See <code>?rr</code> for eaxamples.</p><p><strong>References</strong></p><p>Cule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice  of ridge parameter in ridge regression. arXiv:1205.0686.</p><p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic regularization  for expression arrays. Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010</p><p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.</p><p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.  Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rrchol.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rrda" href="#Jchemo.rrda"><code>Jchemo.rrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rrda(X, y, weights = ones(nro(X)); lb)</code></pre><p>Discrimination based on ridge regression (RR-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>lb</code> : Ridge regularization parameter &quot;lambda&quot;.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>The training variable <code>y</code> (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in <code>y</code>. Each column of Ydummy is a dummy (0/1) variable.  Then, a RR is implemented on the <code>y</code> and each column of Ydummy, returning predictions of the dummy variables (= object <code>posterior</code> returned by  function <code>predict</code>).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

lb = .001
fm = rrda(Xtrain, ytrain; lb = lb) ;    
pnames(fm)
pnames(fm.fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)

Jchemo.predict(fm, Xtest; lb = [.1; .01]).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rrda.jl#LL1-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rrr-Tuple{Any, Any}" href="#Jchemo.rrr-Tuple{Any, Any}"><code>Jchemo.rrr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rrr(X, Y, weights = ones(nro(X)); nlv,
    tau = 1e-5, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)
rrr(X, Y, weights = ones(nro(X)); nlv,
    tau = 1e-5, tol = sqrt(eps(1.)), maxit = 200, 
    scal::Bool = false)</code></pre><p>Reduced rank regression (RRR).</p><ul><li><code>X</code> : First block of data.</li><li><code>Y</code> : Second block of data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs = scores T) to compute.</li><li><code>tau</code> : Regularization parameter (∊ [0, 1]).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> and <code>Y</code>    is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p>Reduced rank regression, also referred to as redundancy analysis  (RA) regression. In this function, the RA uses the Nipals algorithm  presented in Mangamana et al 2021, section 2.1.1.</p><p>A continuum regularization is available.  After block centering and scaling, the covariances matrices are  computed as follows: </p><ul><li>Cx = (1 - <code>tau</code>) * X&#39;DX + <code>tau</code> * Ix</li></ul><p>where D is the observation (row) metric.  Value <code>tau</code> = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. <code>tau</code> = 1e-8)  to get similar results as with pseudo-inverses.  </p><p><strong>References</strong></p><p>Bougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011. Multiblock redundancy  analysis from a user’s perspective. Application in veterinary epidemiology.  Electronic Journal of Applied Statistical Analysis 4, 203-214–214.  https://doi.org/10.1285/i20705948v4n2p203</p><p>Bougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy analysis:  interpretation tools and application in epidemiology. Journal of Chemometrics 25,  467–475. https://doi.org/10.1002/cem.1392 </p><p>Tchandao Mangamana, E., Glèlè Kakaï, R., Qannari, E.M., 2021. A general  strategy for setting up supervised methods of multiblock data analysis.  Chemometrics and Intelligent Laboratory Systems 217, 104388.  https://doi.org/10.1016/j.chemolab.2021.104388</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntrain = nro(Xtrain)
# Building Cal and Val within Train
nval = 80
s = sample(1:ntrain, nval; replace = false)
Xcal = rmrow(Xtrain, s)
ycal = rmrow(ytrain, s)
Xval = Xtrain[s, :]
yval = ytrain[s]

pars = mpar(tau = 1e-4)
nlv = 0:20
res = gridscorelv(Xcal, ycal, Xval, yval;
    score = rmsep, fun = rrr, nlv = nlv, pars = pars)
u = findall(res.y1 .== minimum(res.y1))[1]
res[u, :]
plotgrid(res.nlv, res.y1;
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

tau = 1e-4
nlv = 1
fm = rrr(Xtrain, ytrain; nlv = nlv, tau = tau) ;
res = Jchemo.predict(fm, Xtest)
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f
    
## PLSR 
tau = 1
fm = rrr(Xtrain, ytrain; nlv = 3, tau = tau) ;
head(Jchemo.predict(fm, Xtest).pred)
fm = plskern(Xtrain, ytrain; nlv = 3) ;
head(Jchemo.predict(fm, Xtest).pred)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rrr.jl#LL1-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.rv-Tuple{Any, Any}" href="#Jchemo.rv-Tuple{Any, Any}"><code>Jchemo.rv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rv(X, Y)
rv(Xbl)</code></pre><p>Compute the RV coefficient between matrices.</p><ul><li><code>X</code> : Matrix (n, p).</li><li><code>Y</code> : Matrix (n, q).</li><li><code>Xbl</code> : A list (vector) of matrices.</li><li><code>centr</code> : Logical indicating if the matrices are internally    centered or not.</li></ul><p>RV is bounded in [0, 1]. </p><p>A dissimilarty measure between <code>X</code> and <code>Y</code> can be computed by d = sqrt(2 * (1 - RV)).</p><p><strong>References</strong></p><p>Escoufier, Y., 1973. Le Traitement des Variables Vectorielles.  Biometrics 29, 751–760. https://doi.org/10.2307/2529140</p><p>Josse, J., Holmes, S., 2016. Measuring multivariate association and beyond.  Stat Surv 10, 132–167. https://doi.org/10.1214/16-SS116</p><p>Josse, J., Pagès, J., Husson, F., 2008. Testing the significance of  the RV coefficient. Computational Statistics &amp; Data Analysis 53, 82–91.  https://doi.org/10.1016/j.csda.2008.06.012</p><p>Kazi-Aoual, F., Hitier, S., Sabatier, R., Lebreton, J.-D., 1995.  Refined approximations to permutation tests for multivariate inference.  Computational Statistics &amp; Data Analysis 20, 643–656.  https://doi.org/10.1016/0167-9473(94)00064-2</p><p>Mayer, C.-D., Lorent, J., Horgan, G.W., 2011. Exploratory Analysis  of Multiple Omics Datasets Using the Adjusted RV Coefficient. Statistical  Applications in Genetics and Molecular Biology 10. https://doi.org/10.2202/1544-6115.1540</p><p>Smilde, A.K., Kiers, H.A.L., Bijlsma, S., Rubingh, C.M., van Erk, M.J., 2009.  Matrix correlations for high-dimensional data: the modified RV-coefficient.  Bioinformatics 25, 401–405. https://doi.org/10.1093/bioinformatics/btn634</p><p>Robert, P., Escoufier, Y., 1976. A Unifying Tool for Linear Multivariate  Statistical Methods: The RV-Coefficient. Journal of the Royal Statistical Society:  Series C (Applied Statistics) 25, 257–265. https://doi.org/10.2307/2347233</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 10)
Y = rand(5, 3)
rv(X, Y)

X = rand(5, 15) 
listbl = [3:4, 1, [6; 8:10]]
Xbl = mblock(X, listbl)
rv(Xbl)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/angles.jl#LL115-L169">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampcla" href="#Jchemo.sampcla"><code>Jchemo.sampcla</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sampcla(x, k, y = nothing)</code></pre><p>Build training/test sets by stratified sampling.  </p><ul><li><code>x</code> : Class membership (n) of the observations.</li><li><code>k</code> : Nb. observations to sample in each class (= output <code>test</code>).    If <code>k</code> is a single value, the nb. sampled observations is the same    for each class. Alternatively, <code>k</code> can be a vector of length    equal to the nb. classes in <code>x</code>.</li><li><code>y</code> : Quantitative variable (n) used if systematic sampling.</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>If <code>y = nothing</code> (default), the sampling is random, else it is  systematic over the sorted <code>y</code>(see function <code>sampsys</code>).</p><p><strong>References</strong></p><p>Naes, T., 1987. The design of calibration in near infra-red reflectance  analysis by clustering. Journal of Chemometrics 1, 121-134.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = string.(repeat(1:5, 3))
tab(x)
res = sampcla(x, 2)
res.train
x[res.train]
tab(x[res.train])
tab(x[res.test])

x = string.(repeat(1:5, 3))
n = length(x) ; y = rand(n) 
[x y]
res = sampcla(x, 2, y)
res.train
x[res.train]
tab(x[res.train])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/sampcla.jl#LL1-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampdf" href="#Jchemo.sampdf"><code>Jchemo.sampdf</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sampdf(Y::DataFrame, id = 1:nro(Y); k, 
    sampm = :rand)</code></pre><p>Build training/test sets for each column of a dataframe      (typically, response variables to predict) that can contain missing      values</p><ul><li><code>Y</code> : DataFrame (n, p) whose each column can contain missing values.</li><li><code>id</code> : Vector (n) of IDs.</li><li><code>k</code> : Nb. of test observations selected for each <code>Y</code> column.    The selection is done within the non-missing observations    of the considered column. If <code>k</code> is a single value, the same nb.     observations are selected for each column. Alternatively, <code>k</code> can    be a vector of length p. </li><li><code>sampm</code> : Type of sampling for the test set.   Possible values are: :rand (default) = random sampling,    :sys = systematic sampling over each sorted <code>Y</code> column   (see function <code>sampsys</code>).  </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrames

Y = hcat([rand(5); missing; rand(6)],
   [rand(2); missing; missing; rand(7); missing])
Y = DataFrame(Y, :auto)

sampdf(Y; k = 3)

sampdf(Y; k = 3, sampm = :sys)

## Replicated splitting Train/Test
rep = 10
k = 3
ids = [sampdf(Y[:, namy]; k = k) for i = 1:rep]
length(ids)
i = 1    # replication
ids[i]
ids[i].train 
ids[i].test
j = 1    # variable y  
ids[i].train[j]
ids[i].test[j]
ids[i].nam[j]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/sampdf.jl#LL1-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampdp-Tuple{Any, Any}" href="#Jchemo.sampdp-Tuple{Any, Any}"><code>Jchemo.sampdp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampdp(X, k; metric = :eucl)</code></pre><p>Build training/test sets by DUPLEX sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>k</code> : Nb. pairs of observations to sample    (outputs <code>train</code> and <code>test</code>). Must be &lt;= n / 2. </li><li><code>metric</code> : Metric used for the distance computation.   Possible values are: :eucl, :mah.</li></ul><p>Three outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (<code>k</code>), </li><li><code>test</code> (<code>k</code>),</li><li><code>remain</code> (n - 2 * <code>k</code>). </li></ul><p>Outputs <code>train</code> and <code>test</code> are built from the DUPLEX algorithm  (Snee, 1977 p.421). They are expected to cover approximately the same  X-space region and have similar statistical properties. </p><p>In practice, when output <code>remain</code> is not empty (i.e. theer are remaining  observations), one common strategy is to add it to output <code>train</code>.</p><p><strong>References</strong></p><p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.</p><p>Snee, R.D., 1977. Validation of Regression Models: Methods and Examples.  Technometrics 19, 415-428. https://doi.org/10.1080/00401706.1977.10489581</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
n = nro(X)

k = 140
res = sampdp(X, k)
pnames(res)
res.train 
res.test
res.remain

fm = pcasvd(X; nlv = 15)
T = fm.T
res = sampdp(T, k; metric = :mah)

n = 10 ; k = 25 
X = [repeat(1:n, inner = n) repeat(1:n, outer = n)] 
X = Float64.(X) 
X .= X + .1 * randn(nro(X), nco(X))
s = sampks(X, k).train 
f, ax = scatter(X[:, 1], X[:, 2])
scatter!(X[s, 1], X[s, 2], color = &quot;red&quot;) 
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/sampdp.jl#LL1-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampks-Tuple{Any, Any}" href="#Jchemo.sampks-Tuple{Any, Any}"><code>Jchemo.sampks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampks(X, k; metric = :eucl)</code></pre><p>Build training/test sets by Kennard-Stone sampling.  </p><ul><li><code>X</code> : X-data (n, p).</li><li><code>k</code> : Nb. observations to sample (= output <code>test</code>). </li><li><code>metric</code> : Metric used for the distance computation.   Possible values: :eucl, :mah.</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built from the Kennard-Stone (KS)  algorithm (Kennard &amp; Stone, 1969). </p><p><strong>Note:</strong> By construction, the set of observations  selected by KS sampling contains higher variability than  the set of the remaining observations. In the seminal  article (K&amp;S, 1969), the algorithm is used to select observations that will be used to build a calibration set. To the opposite, in the present function, KS is used to select a test set with  higher variability than the training set. </p><p><strong>References</strong></p><p>Kennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc

k = 200
res = sampks(X, k)
pnames(res)
res.train 
res.test

fm = pcasvd(X; nlv = 15) ;
res = sampks(fm.T, k; metric = :mah)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/sampks.jl#LL1-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.samprand-Tuple{Any, Any}" href="#Jchemo.samprand-Tuple{Any, Any}"><code>Jchemo.samprand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">samprand(n, k; replace = false)</code></pre><p>Build training/test sets by random sampling.  </p><ul><li><code>n</code> : Total nb. observations.</li><li><code>k</code> : Nb. observations to sample (= output <code>test</code>).</li><li><code>replace</code> : Boolean. If false (default), the sampling is    without replacement.</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built by random sampling within <code>1:n</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 10
samprand(n, 7)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/samprand.jl#LL1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sampsys-Tuple{Any, Any}" href="#Jchemo.sampsys-Tuple{Any, Any}"><code>Jchemo.sampsys</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sampsys(y, k)</code></pre><p>Build training/test sets by systematic sampling over     a quantitative variable.  </p><ul><li><code>y</code> : Quantitative variable (n) to sample.</li><li><code>k</code> : Nb. observations to sample (= output <code>test</code>).    Must be &gt;= 2.</li></ul><p>Two outputs (= row indexes of the data) are returned: </p><ul><li><code>train</code> (n - <code>k</code>),</li><li><code>test</code> (<code>k</code>). </li></ul><p>Output <code>test</code> is built by systematic sampling over the rank of  the <code>y</code> observations. For instance if <code>k</code> / n ~ .3, one observation  over three observations over the sorted <code>y</code> is selected.  Output <code>test</code> always contains the indexes of the minimum and  maximum of <code>y</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">y = rand(7)
[y sort(y)]
res = sampsys(y, 4)
sort(y[res.train])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/sampsys.jl#LL1-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.savgk-Tuple{Int64, Int64, Int64}" href="#Jchemo.savgk-Tuple{Int64, Int64, Int64}"><code>Jchemo.savgk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">savgk(nhwindow, degree, deriv)</code></pre><p>Compute the kernel of the Savitzky-Golay filter.</p><ul><li><code>nhwindow</code> : Nb. points of the half window (nhwindow &gt;= 1)    –&gt; the size of the kernel is odd (npoint = 2 * nhwindow + 1):    x[-nhwindow], x[-nhwindow+1], ..., x[0], ...., x[nhwindow-1], x[nhwindow].</li><li><code>degree</code> : Polynom order (1 &lt;= degree &lt;= 2 * nhwindow).   The case &quot;degree = 0&quot; (simple moving average) is not allowed by the funtion.</li><li><code>deriv</code> : Derivation order (0 &lt;= deriv &lt;= degree).   If <code>deriv = 0</code>, there is no derivation (only polynomial smoothing).</li></ul><p><strong>References</strong></p><p>Luo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation  filter for even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">res = savgk(21, 3, 2)
pnames(res)
res.S 
res.G 
res.kern</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL244-L269">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.savgol-Tuple{Any}" href="#Jchemo.savgol-Tuple{Any}"><code>Jchemo.savgol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">savgol(X; npoint, degree, deriv)
savgol!(X::Matrix; npoint, degree, deriv)</code></pre><p>Savitzky-Golay smoothing of each row of a matrix <code>X</code>.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>npoint</code> : Size of the filter (nb. points involved in    the kernel). Must be odd and &gt;= 3. The half-window size is    nhwindow = (npoint - 1) / 2.</li><li><code>degree</code> : Polynom order (1 &lt;= degree &lt;= npoint - 1).</li><li><code>deriv</code> : Derivation order (0 &lt;= deriv &lt;= degree).</li></ul><p>The smoothing is computed by convolution (with padding), with function  imfilter of package ImageFiltering.jl. Each returned point is located on the fcenter  of the kernel. The kernel is computed with function <code>savgk</code>.</p><p><strong>References</strong></p><p>Luo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation filter for  even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002</p><p>Savitzky, A., Golay, M.J.E., 2002. Smoothing and Differentiation of Data by Simplified Least  Squares Procedures. [WWW Document]. https://doi.org/10.1021/ac60214a047</p><p>Schafer, R.W., 2011. What Is a Savitzky-Golay Filter? [Lecture Notes].  IEEE Signal Processing Magazine 28, 111–117. https://doi.org/10.1109/MSP.2011.941097</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
wlstr = names(dat.X)
wl = parse.(Float64, wlstr)

npoint = 21 ; degree = 3 ; deriv = 2 ; 
Xp = savgol(X; npoint = npoint, degree = degree, deriv = deriv) 
plotsp(Xp[1:30, :], wl).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL284-L325">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.scale-Tuple{Any}" href="#Jchemo.scale-Tuple{Any}"><code>Jchemo.scale</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">scale(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL429-L431">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.segmkf-Tuple{Int64, Int64}" href="#Jchemo.segmkf-Tuple{Int64, Int64}"><code>Jchemo.segmkf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">segmkf(n::Int, K::Int; rep = 1)
segmkf(group::Vector, K::Int; rep = 1)</code></pre><p>Build segments for K-fold cross-validation.  </p><ul><li><code>n</code> : Total nb. observations in the dataset. The sampling    is implemented with 1:n.</li><li><code>K</code> : Nb. folds (segments) splitting the data. </li><li><code>group</code> : A vector (n) defining blocks.</li><li><code>rep</code> : Nb. replications of the sampling.</li></ul><p>Build the <code>n</code> observations to K segments that can be used for  K-fold cross-validation. The sampling can be replicated (<code>rep</code>).</p><p>If <code>group</code> is used (vector of length n), the function samples entire  blocks of observations instead of observations. Such a block-sampling is required  when data is structured by blocks and when the response to predict is  correlated within blocks. It prevents underestimation of the generalization error.</p><p>The function returns a list (vector) of <code>rep</code> elements.  Each element of the list contains <code>K</code> segments (= <code>K</code> vectors). Each segment contains the indexes (position within 1:<code>n</code>) of the sampled  observations.    </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 10 ; K = 3 ; rep = 4 
segm = segmkf(n, K; rep) 
i = 1 
segm[i] # = replication &quot;i&quot;

# Block-sampling

n = 11 
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;]    # = blocks of the observations
unique(group)   
K = 3 ; rep = 4 
segm = segmkf(n, K, group; rep = rep)
i = 1 
segm[i]
group[segm[i][1]]
group[segm[i][2]]
group[segm[i][3]]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/segmkf.jl#LL1-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.segmts-Tuple{Int64, Int64}" href="#Jchemo.segmts-Tuple{Int64, Int64}"><code>Jchemo.segmts</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">segmts(n::Int, m::Int; rep = 1, seed = nothing)
segmts(group::Vector, m::Int; rep = 1, 
    seed = nothing)</code></pre><p>Build segments for &quot;test-set&quot; validation.</p><ul><li><code>n</code> : Total nb. observations in the dataset. The sampling    is implemented within 1:<code>n</code>.</li><li><code>m</code> : Nb. observations, or groups if <code>group</code> is used, returned    in each segment.</li><li><code>seed</code> : Eventual seed for the <code>Random.MersenneTwister</code>    generator. Must be of length = <code>rep</code>. When <code>nothing</code> (default),    the seed is random at each sampling.</li><li><code>group</code> : A vector (n) defining blocks of observations.</li><li><code>rep</code> : Nb. replications of the sampling.</li></ul><p>Build a test set that can be used to validate a model. The sampling can be replicated (<code>rep</code>).</p><p>If <code>group</code> is used (must be a vector of length n), the function samples entire  groups (= blocks) of observations instead of observations.  Such a block-sampling is required when data is structured by blocks and  when the response to predict is correlated within blocks.  This prevents underestimation of the generalization error.</p><p>The function returns a list (vector) of <code>rep</code> elements.  Each element of the list is a vector of the indexes (positions  within 1:<code>n</code>) of the sampled observations.  </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 10 ; m = 3 ; rep = 4 
segm = segmts(n, m; rep) 
i = 1 
segm[i]

segmts(10, 4; seed = 3)
segmts(10, 4; rep = 2, seed = [1 ; 3])

# Block-sampling

n = 11
group = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;A&quot;]    # = blocks of the observations
tab(group)
m = 2 ; rep = 4 
segm = segmts(n, m, group; rep)
i = 1 
segm[i]
segm[i][1]
group[segm[i][1]]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/segmts.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.selwold-Tuple{Any, Any}" href="#Jchemo.selwold-Tuple{Any, Any}"><code>Jchemo.selwold</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">selwold(indx, r; smooth = true, 
    f = 5, alpha = .05, digits = 3, graph = true, 
    step = 2, xlabel = &quot;Index&quot;, ylabel = &quot;Value&quot;, 
    title = &quot;Score&quot;)</code></pre><p>Wold&#39;s criterion to select dimensionality in LV (e.g. PLSR) models.</p><ul><li><code>indx</code> : A variable representing the model parameter(s), e.g. nb. LVs if PLSR models.</li><li><code>r</code> : A vector of error rates (n), e.g. RMSECV.</li><li><code>smooth</code> : Boolean. If <code>true</code>,  the selection is done on the smoothed R.</li><li>&#39;f&#39; : Window of the savitzky-Golay filter use for the smoothing (function <code>savgol</code>).</li><li><code>alpha</code> : Proportion alpha used as threshold for R.</li><li><code>digits</code> : Number of digits in the outputs.</li><li><code>graph</code> : Boolean. If <code>true</code>, outputs are plotted.</li><li><code>step</code> : Step used for defining the xticks in the graphs.</li><li><code>xlabel</code> : Horizontal label for the plots.</li><li><code>ylabel</code> : Vertical label for the plots.</li><li><code>title</code> : Title of the left plot.</li></ul><p>The slection criterion is the &quot;precision gain ratio&quot; R = 1 - <code>r</code>(a+1) / <code>r</code>(a), where <code>r</code> is an observed error rate quantifying the model performance (e.g. RMSEP, * classification error rate, etc.) and a the model dimensionnality (= nb. LVs).  <code>r</code> can also represent other indicators such as the eigenvalues of a PCA.</p><p>R is the relative gain in perforamnce efficiency after a new LV is added to the model.  The iterations continue until R becomes lower than a threshold value <code>alpha</code>.  By default and only as an indication, the default <code>alpha</code>=.05  is set in the function, but the user should set any other value depending  on his data and parsimony objective.</p><p>In his original article, Wold (1978; see also Bro et al. 2008) used the  ratio of cross-validated over training residual sums of squares, i.e. PRESS over SSR.  Instead, function <code>selwold</code> compares values of consistent nature (the successive values in  the input vector <code>r</code>). For instance, <code>r</code> was set to PRESS values in Li et al. (2002) and  Andries et al. (2011), which is equivalent to the &quot;punish factor&quot; described in Westad &amp; Martens (2000).</p><p>The ratio R can be erratic (particulary when <code>r</code> is the error rate of a discrimination model), making difficult the dimensionnaly selection.  In such a situation, function <code>selwold</code> proposes to calculate a smoothing of R  (argument <code>smooth</code>).</p><p>The function returns two outputs (in addition to eventual plots):</p><ul><li><code>opt</code> : The index corresponding to the minimum value of <code>r</code>.</li><li><code>sel</code> : The index of the selection from the R (or smoothed R) threshold.</li></ul><p><strong>References</strong></p><p>Andries, J.P.M., Vander Heyden, Y., Buydens, L.M.C., 2011. Improved variable reduction in partial least squares modelling based on Predictive-Property-Ranked  Variables and adaptation of partial least squares complexity.  Analytica Chimica Acta 705, 292-305. https://doi.org/10.1016/j.aca.2011.06.037</p><p>Bro, R., Kjeldahl, K., Smilde, A.K., Kiers, H.A.L., 2008. Cross-validation of  component models: A critical look at current methods. Anal Bioanal Chem 390, 1241-1251.  https://doi.org/10.1007/s00216-007-1790-1</p><p>Li, B., Morris, J., Martin, E.B., 2002. Model selection for partial least squares regression.  Chemometrics and Intelligent Laboratory Systems 64, 79-89. https://doi.org/10.1016/S0169-7439(02)00051-5</p><p>Westad, F., Martens, H., 2000. Variable Selection in near Infrared Spectroscopy Based  on Significance Testing in Partial Least Squares Regression. J. Near Infrared Spectrosc., JNIRS 8, 117â124.</p><p>Wold S. Cross-Validatory Estimation of the Number of Components in Factor and Principal  Components Models. Technometrics. 1978;20(4):397-405</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

n = nro(Xtrain)
segm = segmts(n, 50; rep = 10)

nlv = 0:20
res = gridcvlv(Xtrain, ytrain; segm = segm, nlv = nlv, 
    score = rmsep, fun = plskern, verbose = false).res

zres = selwold(res.nlv, res.y1; smooth = true, 
    graph = true) ;
zres.opt     # Nb. LVs correponding to the minimal error rate
zres.sel     # Nb LVs selected with the Wold&#39;s criterion
zres.f       # plots</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/selwold.jl#LL1-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sep-Tuple{Any, Any}" href="#Jchemo.sep-Tuple{Any, Any}"><code>Jchemo.sep</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sep(pred, Y)</code></pre><p>Compute the corrected SEP (&quot;SEP_c&quot;), i.e. the standard deviation of the prediction errors.</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>References</strong></p><p>Bellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B., Roger, J.-M., McBratney, A., 2010. Critical review of chemometric indicators commonly used for assessing the quality of  the prediction of soil attributes by NIR spectroscopy.  TrAC Trends in Analytical Chemistry 29, 1073–1081. https://doi.org/10.1016/j.trac.2010.05.006</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
sep(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
sep(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL370-L399">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.snv-Tuple{Any}" href="#Jchemo.snv-Tuple{Any}"><code>Jchemo.snv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">snv(X; centr = true, scal = true)
snv!(X::Matrix; centr = true, scal = true)</code></pre><p>Standard-normal-variate (SNV) transformation of each row of X-data.</p><ul><li><code>X</code> : X-data.</li><li><code>centr</code> : Logical indicating if the centering in done.</li><li><code>scal</code> : Logical indicating if the scaling in done.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
wlstr = names(dat.X)
wl = parse.(Float64, wlstr)

Xp = snv(X) 
plotsp(Xp[1:30, :], wl).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/preprocessing.jl#LL358-L381">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.soft-Tuple{AbstractFloat, Float64}" href="#Jchemo.soft-Tuple{AbstractFloat, Float64}"><code>Jchemo.soft</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">soft(x::Real, delta)</code></pre><p>Soft thresholding function.</p><ul><li><code>x</code> : Value to transform.</li><li><code>delta</code> : Range for the thresholding.</li></ul><p>The returned value is:</p><ul><li>sign(x) * max(0, abs(x) - delta)</li></ul><p>where delta &gt;= 0.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">delta = .2
soft(3, delta)

x = LinRange(-2, 2, 100)
y = soft.(x, delta)
lines(x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL804-L823">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.softmax-Tuple{AbstractVector}" href="#Jchemo.softmax-Tuple{AbstractVector}"><code>Jchemo.softmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">softmax(x::AbstractVector)
softmax(X::Union{Matrix, DataFrame})</code></pre><p>Softmax function.</p><ul><li><code>x</code> : A vector to transform.</li><li><code>X</code> : A matrix whose rows are to transform.</li></ul><p>Let v be a vector:</p><ul><li>&#39;softmax&#39;(v) = exp.(v) / sum(exp.(v))</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = 1:3
softmax(x)

X = rand(5, 3)
softmax(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL829-L847">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.soplsr-Tuple{Any, Any}" href="#Jchemo.soplsr-Tuple{Any, Any}"><code>Jchemo.soplsr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">soplsr(Xbl, Y, weights = ones(size(Xbl, 1)); nlv,
    scal::Bool = false)</code></pre><p>Multiblock sequentially orthogonalized PLSR (SO-PLSR).</p><ul><li><code>Xbl</code> : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.</li><li><code>Y</code> : Y-data.</li><li><code>weights</code> : Weights of the observations (rows).    Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to consider for each block.    A vector having a length equal to the nb. blocks.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of blocks in <code>Xbl</code> and    of <code>Y</code> is scaled by its uncorrected standard deviation    (before the block scaling).</li></ul><p><strong>References</strong></p><ul><li><p>Biancolillo et al. , 2015. Combining SO-PLS and linear discriminant analysis    for multi-block classification. Chemometrics and Intelligent Laboratory Systems,    141, 58-67.</p></li><li><p>Biancolillo, A. 2016. Method development in the area of multi-block analysis focused on    food analysis. PhD. University of copenhagen.</p></li><li><p>Menichelli et al., 2014. SO-PLS as an exploratory tool for path modelling.    Food Quality and Preference, 36, 122-134.</p><h2>Examples</h2></li></ul><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/ham.jld2&quot;) 
@load db dat
pnames(dat) 

X = dat.X
y = dat.Y.c1
group = dat.group
listbl = [1:11, 12:19, 20:25]
Xbl = mblock(X, listbl)
# &quot;New&quot; = first two rows of Xbl 
Xbl_new = mblock(X[1:2, :], listbl)

nlv = [2; 1; 2]
fm = soplsr(Xbl, y; nlv = nlv) ;
pnames(fm)
fm.T
transf(fm, Xbl_new)
[y Jchemo.predict(fm, Xbl).pred]
Jchemo.predict(fm, Xbl_new).pred</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/soplsr.jl#LL1-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.sourcedir-Tuple{Any}" href="#Jchemo.sourcedir-Tuple{Any}"><code>Jchemo.sourcedir</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sourcedir(path)</code></pre><p>Include all the files contained in a directory.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL863-L866">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.spca-Tuple{Any}" href="#Jchemo.spca-Tuple{Any}"><code>Jchemo.spca</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">spca(X, weights = ones(nro(X)); nlv,
    meth_sp = :soft, delta = 0, nvar = nco(X), 
    tol = sqrt(eps(1.)), maxit = 200, scal::Bool = false)
spca!(X, weights = ones(nro(X)); nlv,
    meth_sp = :soft, delta = 0, nvar = nco(X), 
    tol = sqrt(eps(1.)), maxit = 200, scal::Bool = false)</code></pre><p>Sparse PCA (Shen &amp; Huang 2008).</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. principal components (PCs).</li><li><code>meth_sp</code>: Method used for the thresholding. Possible values   are :soft (default), :mix or :hard. See thereafter.</li><li><code>delta</code> : Range for the thresholding (see function <code>soft</code>)   on the loadings standardized to their maximal absolute value.   Must ∈ [0, 1]. Only used if `meth_sp = :soft.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each    PC. Can be a single integer (same nb. variables   for each PC), or a vector of length <code>nlv</code>.   Only used if <code>meth_sp = :mix</code> or <code>meth_sp = :hard</code>.   </li><li><code>tol</code> : Tolerance value for stopping the iterations.</li><li><code>maxit</code> : Maximum nb. iterations.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Sparse principal component analysis via regularized low rank  matrix approximation (Shen &amp; Huang 2008). A Nipals algorithm is used. </p><p>Function `spca&#39; provides three methods of thresholding to compute  the sparse loadings:</p><ul><li><p><code>meth_sp = :soft</code>: Soft thresholding of standardized loadings.    Noting v the loading vector, at each step, abs(v) is standardized to    its maximal component (= max{abs(v[i]), i = 1..p}). The soft-thresholding    function (see function <code>soft</code>) is applied to this standardized vector,    with the constant <code>delta</code> ∈ [0, 1]. This returns the sparse vector    theta. Vector v is multiplied term-by-term by vector theta, which   finally gives the sparse loadings.</p></li><li><p><code>meth_sp = :mix</code>: Method used in function <code>spca</code> of the R package <code>mixOmics</code>.   For each PC, a number of <code>X</code>-variables showing the largest    values in vector abs(v) are selected. Then a soft-thresholding is    applied to the corresponding selected loadings. Range <code>delta</code> is    automatically (internally) set to the maximal value of the components    of abs(v) corresponding to variables removed from the selection.  </p></li><li><p><code>meth_sp = :hard</code>: For each PC, a number of `X-variables showing    the largest values in vector abs(v) are selected.</p></li></ul><p>The case <code>meth_sp = :mix</code> returns the same results as function  spca of the R package mixOmics.</p><p>Since the resulting sparse loadings vectors (<code>P</code>-columns) are in general  non orthogonal, there is no a unique decomposition of the variance of <code>X</code>  such as in PCA. Function <code>summary</code> returns the following objects:</p><ul><li><code>explvarx</code>: The proportion of variance of <code>X</code> explained by each column    t of <code>T</code>, computed by regressing <code>X</code> on t (such as what is done in PLS).</li><li><code>explvarx_adj</code>: Adjusted explained variance proposed by    Shen &amp; Huang 2008 section 2.3.    </li></ul><p><strong>References</strong></p><p>Kim-Anh Le Cao, Florian Rohart, Ignacio Gonzalez, Sebastien Dejean with key  contributors Benoit Gautier, Francois Bartolo, contributions from Pierre Monget,  Jeff Coquery, FangZou Yao and Benoit Liquet. (2016).  mixOmics: Omics Data Integration Project. R package version 6.1.1.  https://CRAN.R-project.org/package=mixOmics</p><p>https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Shen, H., Huang, J.Z., 2008. Sparse principal component analysis via  regularized low rank matrix approximation. Journal of Multivariate Analysis  99, 1015–1034. https://doi.org/10.1016/j.jmva.2007.06.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie, StatsBase
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)

X = dat.X[:, 1:4]
n = nro(X)

ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
Xtest = rmrow(X, s)

tol = 1e-15
nlv = 3 
scal = false
#scal = true
meth_sp = :soft
#meth_sp = :mix
#meth_sp = :hard
delta = .4 ; nvar = 2 
fm = spca(Xtrain; nlv = nlv, 
    meth_sp = meth_sp, nvar = nvar, delta = delta, 
    tol = tol, scal = scal) ;
fm.niter
fm.sellv 
fm.sel
fm.P
fm.P&#39; * fm.P
head(fm.T)

Ttest = transf(fm, Xtest)

res = Jchemo.summary(fm, Xtrain) ;
res.explvarx
res.explvarx_adj</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/spca.jl#LL1-L116">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splskdeda" href="#Jchemo.splskdeda"><code>Jchemo.splskdeda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">splskdeda(X, y, weights = ones(nro(X)); nlv, 
    meth = :soft, delta = 0, nvar = nco(X), 
    prior = :unif, h = nothing, a = 1, scal::Bool = false)</code></pre><p>Sparse PLS-KDE-DA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code>: Method used for the thresholding. Possible values   are :soft (default), :mix or :hard. See thereafter.</li><li><code>delta</code> : Range for the thresholding (see function <code>soft</code>)   on the loadings standardized to their maximal absolute value.   Must ∈ [0, 1]. Only used if `meth = :soft.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each    LV. Can be a single integer (same nb. variables   for each LV), or a vector of length <code>nlv</code>.   Only used if <code>meth = :mix</code> or <code>meth = :hard</code>. </li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plskdeda</code> (PLS-KDE-DA) except that sparse PLSR (function  <code>splskern</code>) is run on the Y-dummy table instead of a PLSR (function <code>plskern</code>). </p><p>See <code>?splskern</code> and `?plskdeda.</p><p>See <code>?splslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/splskdeda.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splskern-Tuple{Any, Any}" href="#Jchemo.splskern-Tuple{Any, Any}"><code>Jchemo.splskern</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">splskern(X, Y, weights = ones(nro(X)); nlv,
    meth_sp = :soft, delta = 0, nvar = nco(X), 
    scal::Bool = false)
splskern!(X, Y, weights = ones(nro(X)); nlv,
    meth_sp = :soft, delta = 0, nvar = nco(X), 
    scal::Bool = false)</code></pre><p>Sparse PLSR (Shen &amp; Huang 2008).</p><ul><li><code>X</code> : X-data (n, p). </li><li><code>Y</code> : Y-data (n, q).</li><li><code>weights</code> : Weights (n) of the observations.    Internally normalized to sum to 1.</li><li><code>nlv</code> : Nb. latent variables (LVs).</li><li><code>meth_sp</code>: Method used for the thresholding. Possible values   are :soft (default), :mix or :hard. See thereafter.</li><li><code>delta</code> : Range for the thresholding (see function <code>soft</code>)   on the loadings standardized to their maximal absolute value.   Must ∈ [0, 1]. Only used if `meth_sp = :soft.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each    LV. Can be a single integer (same nb. variables   for each LV), or a vector of length <code>nlv</code>.   Only used if <code>meth_sp = :mix</code> or <code>meth_sp = :hard</code>.   </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code> is scaled   by its uncorrected standard deviation.</li></ul><p>Sparse partial least squares regression (Lê Cao et al. 2008), with  the fast &quot;improved kernel algorithm #1&quot; of Dayal &amp; McGregor (1997).  In the present version, the sparseness only concerns <code>X</code> (not <code>Y</code>). </p><p>Function <code>splskern&#39; provides three methods of thresholding to compute  the sparse</code>X<code>-loading weights w, see</code>?spca&#39; for description (same  principles). The case <code>meth_sp = :mix</code> returns the same results as function  spls of the R package mixOmics in regression mode (and with no sparseness  on <code>Y</code>).</p><p><strong>References</strong></p><p>Cao, K.-A.L., Rossouw, D., Robert-Granié, C., Besse, P., 2008. A Sparse PLS  for Variable Selection when Integrating Omics Data. Statistical Applications  in Genetics and Molecular Biology 7. https://doi.org/10.2202/1544-6115.1390</p><p>Kim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien Dejean with key  contributors Benoit Gautier, Francois Bartolo, contributions from Pierre Monget,  Jeff Coquery, FangZou Yao and Benoit Liquet. (2016).  mixOmics: Omics Data Integration Project. R package version 6.1.1.  https://CRAN.R-project.org/package=mixOmics</p><p>https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html</p><p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 15
fm = splskern(Xtrain, ytrain; nlv = nlv,
    meth_sp = :mix, nvar = 5) ;
pnames(fm)
fm.T
fm.W
fm.P
fm.sellv
fm.sel

zcoef = Jchemo.coef(fm)
zcoef.int
zcoef.B
Jchemo.coef(fm; nlv = 7).B

transf(fm, Xtest)
transf(fm, Xtest; nlv = 7)

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, 
    ylabel = &quot;Observed&quot;).f    

res = Jchemo.predict(fm, Xtest; nlv = 1:2)
res.pred[1]
res.pred[2]

res = summary(fm, Xtrain) ;
pnames(res)
z = res.explvarx
lines(z.nlv, z.cumpvar,
    axis = (xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;Prop. Explained X-Variance&quot;))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/splskern.jl#LL1-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splslda" href="#Jchemo.splslda"><code>Jchemo.splslda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">splslda(X, y, weights = ones(nro(X)); nlv, 
    meth = :soft, delta = 0, nvar = nco(X), 
    prior = :unif, scal::Bool = false)</code></pre><p>Sparse PLS-LDA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code>: Method used for the thresholding. Possible values   are :soft (default), :mix or :hard. See thereafter.</li><li><code>delta</code> : Range for the thresholding (see function <code>soft</code>)   on the loadings standardized to their maximal absolute value.   Must ∈ [0, 1]. Only used if `meth = :soft.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each    LV. Can be a single integer (same nb. variables   for each LV), or a vector of length <code>nlv</code>.   Only used if <code>meth = :mix</code> or <code>meth = :hard</code>. </li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plslda</code> (PLS-LDA) except that sparse PLSR (function  <code>splskern</code>) is run on the Y-dummy table instead of a PLSR (function <code>plskern</code>). </p><p>See <code>?splskern</code> and `?plslda.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlv = 15
delta = .8
nvar = 2
scal = false
#scal = true
meth = :soft
#meth = :mix
#meth = :hard
fm = splslda(Xtrain, ytrain; nlv = nlv,
    meth = meth, delta = delta, nvar = nvar,
    scal = scal) ;
pnames(fm)
pnames(fm.fm)
zfm = fm.fm.fmpls ;
zfm.sellv
zfm.sel
res = Jchemo.predict(fm, Xtest)
res.posterior
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

nlv = 1:30 
pars = mpar(meth = [:mix], nvar = [1; 5; 10; 20], 
    scal = [false])
res = gridscorelv(Xtrain, ytrain, Xtest, ytest; 
    score = err, fun = splslda, pars = pars, nlv = nlv)
typ = string.(&quot;nvar=&quot;, res.nvar)
plotgrid(res.nlv, res.y1, typ; step = 2,
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;ERR&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/splslda.jl#LL1-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splsqda" href="#Jchemo.splsqda"><code>Jchemo.splsqda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">splsqda(X, y, weights = ones(nro(X)); nlv, 
    meth = :soft, delta = 0, nvar = nco(X), 
    prior = :unif, scal::Bool = false)</code></pre><p>Sparse PLS-QDA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code>: Method used for the thresholding. Possible values   are :soft (default), :mix or :hard. See thereafter.</li><li><code>delta</code> : Range for the thresholding (see function <code>soft</code>)   on the loadings standardized to their maximal absolute value.   Must ∈ [0, 1]. Only used if `meth = :soft.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each    LV. Can be a single integer (same nb. variables   for each LV), or a vector of length <code>nlv</code>.   Only used if <code>meth = :mix</code> or <code>meth = :hard</code>. </li><li><code>prior</code> : Type of prior probabilities for class membership.   Possible values are: :unif (uniform; default), :prop (proportional).</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsqda</code> (PLS-QDA) except that sparse PLSR (function  <code>splskern</code>) is run on the Y-dummy table instead of a PLSR (function <code>plskern</code>). </p><p>See <code>?splskern</code> and `?plsqda.</p><p>See <code>?splslda</code> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/splsqda.jl#LL1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.splsrda" href="#Jchemo.splsrda"><code>Jchemo.splsrda</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">splsrda(X, y, weights = ones(nro(X)); nlv,
    meth = :soft, delta = 0, nvar = nco(X), 
    scal::Bool = false)</code></pre><p>Sparse PLSR-DA.</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (class membership).</li><li><code>weights</code> : Weights of the observations. Internally normalized to sum to 1. </li><li><code>nlv</code> : Nb. latent variables (LVs) to compute.</li><li><code>meth</code>: Method used for the thresholding. Possible values   are :soft (default), :mix or :hard. See thereafter.</li><li><code>delta</code> : Range for the thresholding (see function <code>soft</code>)   on the loadings standardized to their maximal absolute value.   Must ∈ [0, 1]. Only used if `meth = :soft.</li><li><code>nvar</code> : Nb. variables (<code>X</code>-columns) selected for each    LV. Can be a single integer (same nb. variables   for each LV), or a vector of length <code>nlv</code>.   Only used if <code>meth = :mix</code> or <code>meth = :hard</code>. </li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Same as function <code>plsrda</code> (PLSR-DA) except that sparse PLSR (function  <code>splskern</code>) is run on the Y-dummy table instead of a PLSR (function <code>plskern</code>). </p><p>See <code>?splskern</code> and `?plsrda.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;forages2.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

nlv = 15
delta = .8
nvar = 2
scal = false
#scal = true
meth = :soft
#meth = :mix
#meth = :hard
fm = splsrda(Xtrain, ytrain; nlv = nlv,
    meth = meth, delta = delta, nvar = nvar,
    scal = scal) ;
pnames(fm)
pnames(fm.fm)
zfm = fm.fm ;
zfm.sellv
zfm.sel
res = Jchemo.predict(fm, Xtest)
res.posterior
err(res.pred, ytest)
confusion(res.pred, ytest).cnt

nlv = 0:30 
pars = mpar(meth = [:mix], nvar = [1; 5; 10; 20], 
    scal = [false])
res = gridscorelv(Xtrain, ytrain, Xtest, ytest; 
    score = err, fun = splsrda, pars = pars, nlv = nlv)
typ = string.(&quot;nvar=&quot;, res.nvar)
plotgrid(res.nlv, res.y1, typ; step = 2,
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;ERR&quot;).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/splsrda.jl#LL1-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ssq-Tuple{Any}" href="#Jchemo.ssq-Tuple{Any}"><code>Jchemo.ssq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ssq(X)</code></pre><p>Compute the total inertia of a matrix.</p><ul><li><code>X</code> : Matrix.</li></ul><p>Sum of all the squared components of <code>X</code> (= <code>norm(X)^2</code>; Squared Frobenius norm). </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(5, 2) 
ssq(X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL874-L886">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.ssr-Tuple{Any, Any}" href="#Jchemo.ssr-Tuple{Any, Any}"><code>Jchemo.ssr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ssr(pred, Y)</code></pre><p>Compute the sum of squared prediction errors (SSR).</p><ul><li><code>pred</code> : Predictions.</li><li><code>Y</code> : Observed data.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">Xtrain = rand(10, 5) 
Ytrain = rand(10, 2)
ytrain = Ytrain[:, 1]
Xtest = rand(4, 5) 
Ytest = rand(4, 2)
ytest = Ytest[:, 1]

fm = plskern(Xtrain, Ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
ssr(pred, Ytest)

fm = plskern(Xtrain, ytrain; nlv = 2)
pred = Jchemo.predict(fm, Xtest).pred
ssr(pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/scores.jl#LL402-L425">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.stah-Tuple{Any, Any}" href="#Jchemo.stah-Tuple{Any, Any}"><code>Jchemo.stah</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">stah(X, a; scal = true)</code></pre><p>Stahel-Donoho outlierness.</p><ul><li><code>X</code> : X-data.</li><li><code>a</code> : Nb. dimensions simulated for the projection pursuit method.</li><li><code>scal</code> : Boolean. If <code>true</code>, matrix <code>X</code> is centred (by median)    and scaled (by MAD) before computing the outlierness.</li></ul><p>The outlierness measure is computed from a projection-pursuit approach:</p><ul><li>directions in the column-<code>X</code> space (linear combinations of the columns    of <code>X</code>) are randomly simulated, </li><li>and the observations (rows of <code>X</code>) are projected on these directions.</li></ul><p>See Maronna and Yohai (1995) for details. </p><p><strong>References</strong></p><p>Maronna, R.A., Yohai, V.J., 1995. The Behavior of the Stahel-Donoho Robust Multivariate Estimator.  Journal of the American Statistical Association 90, 330–341. https://doi.org/10.1080/01621459.1995.10476517</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using StatsBase

n = 300 ; p = 700 ; m = 80 ; ntot = n + m
X1 = randn(n, p)
X2 = randn(m, p) .+ sample(1:3, p)&#39;
X = vcat(X1, X2)

a = 100
res = stah(X, a; scal = true) ;
res.d # outlierness

plotxy(1:nro(X), res.d).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/stah.jl#LL1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.summ-Tuple{Any}" href="#Jchemo.summ-Tuple{Any}"><code>Jchemo.summ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">summ(X; digits = 3)
summ(X, group; digits = 3)</code></pre><p>Summarize a dataset (or a variable).</p><ul><li><code>X</code> : A dataset (n, p).</li><li><code>group</code> : A vector (n,) defing the groups.</li><li><code>digits</code> : Nb. digits in the outputs.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = rand(10, 3) 
res = summ(X)
pnames(res)
summ(X[:, 2]).res</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL892-L907">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.svmda-Tuple{Any, Any}" href="#Jchemo.svmda-Tuple{Any, Any}"><code>Jchemo.svmda</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">svmda(X, y; kern = :krbf, 
    gamma = 1. / size(X, 2), degree = 3, coef0 = 0., 
    cost = 1., epsilon = .1,
    scal = false)</code></pre><p>Support vector machine for discrimination &quot;C-SVC&quot; (SVM-DA).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (univariate).</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are :krbf, :kpol, :klin or &quot;ktanh&quot;. </li><li><code>gamma</code> : See below.</li><li><code>degree</code> : See below.</li><li><code>coef0</code> : See below.</li><li><code>cost</code> : Cost of constraints violation C parameter.</li><li><code>epsilon</code> : Epsilon parameter in the loss function.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Kernel types : </p><ul><li>:krbf – radial basis function: exp(-gamma * |x - y|^2)</li><li>:kpol – polynomial: (gamma * x&#39; * y + coef0)^degree</li><li>&quot;klin* – linear: x&#39; * y</li><li>:ktan – sigmoid: tanh(gamma * x&#39; * y + coef0)</li></ul><p>The function uses package LIBSVM.jl (https://github.com/JuliaML/LIBSVM.jl)  that is an interface to library LIBSVM (Chang &amp; Li 2001).</p><p><strong>References</strong></p><p>Package LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl</p><p>Chang, C.-C. &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines.  Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.  Detailed documentation (algorithms, formulae, ...) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</p><p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support vector machines.  ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011.  Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</p><p>Schölkopf, B., Smola, A.J., 2002. Learning with kernels:  support vector machines, regularization, optimization, and beyond. Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;forages.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
Y = dat.Y 
s = Bool.(Y.test)
Xtrain = rmrow(X, s)
ytrain = rmrow(Y.typ, s)
Xtest = X[s, :]
ytest = Y.typ[s]

tab(ytrain)
tab(ytest)

gamma = .01 ; cost = 1000 ; epsilon = 1
fm = svmda(Xtrain, ytrain; 
    gamma = gamma, cost = cost, epsilon = epsilon) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest) ;
pnames(res)
res.pred
err(res.pred, ytest)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/svmda.jl#LL1-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.svmr-Tuple{Any, Any}" href="#Jchemo.svmr-Tuple{Any, Any}"><code>Jchemo.svmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">svmr(X, y; kern = :krbf, 
    gamma = 1. / size(X, 2), degree = 3, coef0 = 0., cost = 1., 
    epsilon = .1, scal = false)</code></pre><p>Support vector machine for regression (Epsilon-SVR).</p><ul><li><code>X</code> : X-data.</li><li><code>y</code> : y-data (univariate).</li><li><code>kern</code> : Type of kernel used to compute the Gram matrices.   Possible values are :krbf, :kpol, :klin or &quot;ktanh&quot;. </li><li><code>gamma</code> : See below.</li><li><code>degree</code> : See below.</li><li><code>coef0</code> : See below.</li><li><code>cost</code> : Cost of constraints violation C parameter.</li><li><code>epsilon</code> : Epsilon parameter in the loss function.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li></ul><p>Kernel types : </p><ul><li>:krbf – radial basis function: exp(-gamma * |x - y|^2)</li><li>:kpol – polynomial: (gamma * x&#39; * y + coef0)^degree</li><li>&quot;klin* – linear: x&#39; * y</li><li>:ktan – sigmoid: tanh(gamma * x&#39; * y + coef0)</li></ul><p>The function uses LIBSVM.jl (https://github.com/JuliaML/LIBSVM.jl)  that is an interface to library LIBSVM (Chang &amp; Li 2001).</p><p><strong>References</strong></p><p>Julia package LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl</p><p>Chang, C.-C. &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines.  Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.  Detailed documentation (algorithms, formulae, ...) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</p><p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support vector machines.  ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011.  Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</p><p>Schölkopf, B., Smola, A.J., 2002. Learning with kernels:  support vector machines, regularization, optimization, and beyond. Adaptive computation and machine learning. MIT Press, Cambridge, Mass.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie
mypath = dirname(dirname(pathof(JchemoData)))
db = joinpath(mypath, &quot;data&quot;, &quot;cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc
year = dat.Y.year
tab(year)
s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

gamma = .1 ; cost = 1000 ; epsilon = 1
fm = svmr(Xtrain, ytrain; kern = :krbf, 
        gamma = gamma, cost = cost, epsilon = epsilon) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest)
res.pred
rmsep(res.pred, ytest)
plotxy(vec(res.pred), ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f   

## Example of fitting the function sinc(x)
## described in Rosipal &amp; Trejo 2001 p. 105-106 
x = collect(-10:.2:10) 
x[x .== 0] .= 1e-5
n = length(x)
zy = sin.(abs.(x)) ./ abs.(x) 
y = zy + .2 * randn(n) 
fm = svmr(x, y; gamma = .1) ;
pred = Jchemo.predict(fm, x).pred 
f, ax = scatter(x, y) 
lines!(ax, x, zy, label = &quot;True model&quot;)
lines!(ax, x, vec(pred), label = &quot;ted model&quot;)
axislegend(&quot;Method&quot;)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/svmr.jl#LL1-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.tab-Tuple{Any}" href="#Jchemo.tab-Tuple{Any}"><code>Jchemo.tab</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tab(x)</code></pre><p>Univariate tabulation.</p><ul><li><code>x</code> : Categorical variable.</li></ul><p>The output cointains sorted levels.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand([&quot;a&quot;;&quot;b&quot;;&quot;c&quot;], 20)
res = tab(x)
res.keys
res.vals</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL933-L947">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.tabdf-Tuple{Any}" href="#Jchemo.tabdf-Tuple{Any}"><code>Jchemo.tabdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tabdf(X; groups = nothing)</code></pre><p>Compute the nb. occurences of groups in categorical variables of      a dataset.</p><ul><li><code>X</code> : Data.</li><li><code>groups</code> : Names of the group variables to consider    in <code>X</code> (by default: all the columns of <code>X</code>).</li></ul><p>The output (dataframe) contains sorted levels.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n = 20
X =  hcat(rand(1:2, n), rand([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], n))
tabdf(X)
tabdf(X[:, 2])

df = DataFrame(X, [:v1, :v2])
tabdf(df)
tabdf(df; groups = [:v1, :v2])
tabdf(df; groups = :v2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL950-L972">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.tabdupl-Tuple{Any}" href="#Jchemo.tabdupl-Tuple{Any}"><code>Jchemo.tabdupl</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tabdupl(x)</code></pre><p>Tabulate duplicated values in a vector.</p><ul><li><code>x</code> : Categorical variable.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;]
tab(x)
res = tabdupl(x)
res.keys
res.vals</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL985-L998">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Cca, Any, Any}" href="#Jchemo.transf-Tuple{Jchemo.Cca, Any, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Cca, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/cca.jl#LL155-L164">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.CcaWold, Any, Any}" href="#Jchemo.transf-Tuple{Jchemo.CcaWold, Any, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::CcaWold, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/ccawold.jl#LL215-L224">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Comdim, Any}" href="#Jchemo.transf-Tuple{Jchemo.Comdim, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Comdim, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of blocks (matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/comdim.jl#LL218-L225">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Dkplsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsr.jl#LL122-L129">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Dkplsrda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Dkplsrda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Dkplsrda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/dkplsrda.jl#LL64-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Kpca, Any}" href="#Jchemo.transf-Tuple{Jchemo.Kpca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Kpca, X; nlv = nothing)</code></pre><p>Compute PCs (scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kpca.jl#LL105-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Kplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Kplsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/kplsr.jl#LL179-L186">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Mbpca, Any}" href="#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Mbpca, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of blocks (matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbpca.jl#LL198-L205">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Pcr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Pcr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Pcr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcr.jl#LL92-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.PlsCan, Any, Any}" href="#Jchemo.transf-Tuple{Jchemo.PlsCan, Any, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::PlsCan, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plscan.jl#LL158-L167">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.PlsTuck, Any, Any}" href="#Jchemo.transf-Tuple{Jchemo.PlsTuck, Any, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::PlsSVd, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plstuck.jl#LL112-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Plslda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Plslda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Plslda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plslda.jl#LL87-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Plsrda, Any}" href="#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Plsrda, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plsrda.jl#LL73-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Rasvd, Any, Any}" href="#Jchemo.transf-Tuple{Jchemo.Rasvd, Any, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Rasvd, X, Y; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which components (LVs) are computed.</li><li><code>Y</code> : Y-data for which components (LVs) are computed.</li><li><code>nlv</code> : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rasvd.jl#LL148-L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Rosaplsr, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of blocks (matrices) of X-data for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rosaplsr.jl#LL193-L200">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Rp, Any}" href="#Jchemo.transf-Tuple{Jchemo.Rp, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Rp, X; nlv = nothing)</code></pre><p>Compute &quot;scores&quot; T from a random projection model and a matrix X.</p><ul><li><code>object</code> : The random projection model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. dimensions to consider. If nothing, it is the maximum nb. dimensions.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/rp.jl#LL60-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Soplsr, Any}" href="#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Soplsr, Xbl)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of blocks (matrices) for which LVs are computed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/soplsr.jl#LL102-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Jchemo.Spca, Any}" href="#Jchemo.transf-Tuple{Jchemo.Spca, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Spca, X; nlv = nothing)
Compute principal components (PCs = scores T) from a fitted model and X-data.</code></pre><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/spca.jl#LL183-L190">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}" href="#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Pca, X; nlv = nothing)</code></pre><p>Compute principal components (PCs = scores T) from a fitted model and X-data.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which PCs are computed.</li><li><code>nlv</code> : Nb. PCs to compute.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/pcasvd.jl#LL97-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}" href="#Jchemo.transf-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Union{MbplsWest, Mbplsr}, Xbl; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model.</p><ul><li><code>object</code> : The fitted model.</li><li><code>Xbl</code> : A list (vector) of blocks (matrices) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/mbplswest.jl#LL199-L206">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}" href="#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}"><code>Jchemo.transf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transf(object::Plsr, X; nlv = nothing)</code></pre><p>Compute latent variables (LVs = scores T) from a fitted model and a matrix X.</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : Matrix (m, p) for which LVs are computed.</li><li><code>nlv</code> : Nb. LVs to consider.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/plskern.jl#LL171-L178">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.treeda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}" href="#Jchemo.treeda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}"><code>Jchemo.treeda_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">treeda_dt(X, yy::Union{Array{Int}, Array{String}}; 
    n_subfeatures = 0,
    max_depth = -1, min_samples_leaf = 5, 
    min_samples_split = 2, scal::Bool = false, 
    kwargs...)</code></pre><p>Discrimination tree (CART) with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n obs., p variables).</li><li><code>y</code> : Univariate y-data (n obs.).</li><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: 0 ==&gt; keep all).</li><li><code>max_depth</code> : Maximum depth of the decision tree (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li><code>kwargs</code> : Optional named arguments to pass in function <code>build_tree</code>    of <code>DecisionTree.jl</code>.</li></ul><p>The function fits a single discrimination tree (CART) using package  `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. Classification And Regression Trees. Chapman &amp; Hall, 1984.</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
using JchemoData
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/iris.jld2&quot;) 
@load db dat
pnames(dat)
summ(dat.X)
  
X = dat.X[:, 1:4] 
y = dat.X[:, 5]
ntot, p = size(X)
  
ntrain = 120
s = sample(1:n, ntrain; replace = false) 
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)
ntest = ntot - ntrain
(ntot = ntot, ntrain, ntest)

tab(ytrain)
tab(ytest)

n_subfeatures = 2 
max_depth = 6
fm = treeda_dt(Xtrain, ytrain;
    n_subfeatures = n_subfeatures, max_depth = max_depth) ;
pnames(fm)

res = Jchemo.predict(fm, Xtest)
res.pred
err(res.pred, ytest) </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/treeda_dt.jl#LL1-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.treer_dt-Tuple{Any, Any}" href="#Jchemo.treer_dt-Tuple{Any, Any}"><code>Jchemo.treer_dt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">treer_dt(X, y; n_subfeatures = 0,
    max_depth = -1, min_samples_leaf = 5, 
    min_samples_split = 2, scal::Bool = false, 
    kwargs...)</code></pre><p>Regression tree (CART) with DecisionTree.jl.</p><ul><li><code>X</code> : X-data (n obs., p variables).</li><li><code>y</code> : Univariate y-data (n obs.).</li><li><code>n_subfeatures</code> : Nb. variables to select at random at each split (default: 0 ==&gt; keep all).</li><li><code>max_depth</code> : Maximum depth of the decision tree (default: -1 ==&gt; no maximum).</li><li><code>min_sample_leaf</code> : Minimum number of samples each leaf needs to have.</li><li><code>min_sample_split</code> : Minimum number of observations in needed for a split.</li><li><code>scal</code> : Boolean. If <code>true</code>, each column of <code>X</code>    is scaled by its uncorrected standard deviation.</li><li><code>kwargs</code> : Optional named arguments to pass in function <code>build_tree</code>    of <code>DecisionTree.jl</code>.</li></ul><p>The function fits a single regression tree (CART) using package  `DecisionTree.jl&#39;.</p><p><strong>References</strong></p><p>Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. Classification And Regression Trees. Chapman &amp; Hall, 1984.</p><p>DecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl</p><p>Gey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, JLD2, CairoMakie

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/challenge2021.jld2&quot;)
@load db dat
pnames(dat)

Xtrain = dat.Xtrain
Ytrain = dat.Ytrain
ytrain = Ytrain.y
s = dat.Ytest.inst .== 1 
Xtest = dat.Xtest[s, :]
Ytest = dat.Ytest[s, :]
ytest = Ytest.y
wlstr = names(Xtrain) 
wl = parse.(Float64, wlstr) 
ntrain, p = size(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)

f = 21 ; pol = 3 ; d = 2 
Xptrain = savgol(snv(Xtrain); f, pol, d) 
Xptest = savgol(snv(Xtest); f, pol, d) 

n_subfeatures = p / 3 
max_depth = 20
fm = treer_dt(Xptrain, ytrain; 
    n_subfeatures = n_subfeatures, 
    max_depth = max_depth) ;
pnames(fm)

res = Jchemo.predict(fm, Xptest)
res.pred
rmsep(res.pred, ytest)
plotxy(res.pred, ytest; color = (:red, .5),
    bisect = true, xlabel = &quot;Prediction&quot;, ylabel = &quot;Observed&quot;).f  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/treer_dt.jl#LL1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vcatdf-Tuple{Any}" href="#Jchemo.vcatdf-Tuple{Any}"><code>Jchemo.vcatdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vcatdf(dat; cols = :intersect)</code></pre><p>Vertical concatenation of a list of dataframes.</p><ul><li><code>dat</code> : List (vector) of dataframes.</li><li><code>cols</code> : Determines the columns of the returned data frame.   See ?DataFrames.vcat.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using DataFrames
dat1 = DataFrame(rand(5, 2), [:v3, :v1]) 
dat2 = DataFrame(100 * rand(2, 2), [:v3, :v1])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)

dat2 = DataFrame(100 * rand(2, 2), [:v1, :v3])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)

dat2 = DataFrame(100 * rand(2, 3), [:v3, :v1, :a])
dat = (dat1, dat2)
Jchemo.vcatdf(dat)
Jchemo.vcatdf(dat; cols = :union)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL1006-L1030">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vcol-Tuple{Any, Any}" href="#Jchemo.vcol-Tuple{Any, Any}"><code>Jchemo.vcol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vcol(X::AbstractMatrix, j)
vcol(X::DataFrame, j)
vcol(x::Vector, j)</code></pre><p>View of the j-th column(s) of a matrix <code>X</code>, or of the j-th element(s) of vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL1044-L1050">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}" href="#Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}"><code>Jchemo.vip</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vip(object::Union{Pcr, Plsr}; nlv = nothing)
vip(object::Union{Pcr, Plsr}, Y; nlv = nothing)</code></pre><p>Variable importance on Projections (VIP).</p><ul><li><code>object</code> : The fitted model (object of structure <code>Plsr</code>).</li><li><code>Y</code> : The Y-data that was used to fit the model.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider.</li></ul><p>For a PLS (or PCR, etc.) model (X, Y) with a number of A latent variables,  and variable xj (column j of X): </p><ul><li>VIP(xj) = Sum(a=1,...,A) R2(Yc, ta) waj^2 / Sum(a=1,...,A) R2(Yc, ta) (1 / p) </li></ul><p>where:</p><ul><li>Yc is the centered Y, </li><li>ta is the ath X-score, </li><li>and R2(Yc, ta) the proportion of Yc-variance explained by ta,    i.e. ||Yc.hat||^2 / ||Yc||^2 (where Yc.hat is the LS estimate of Yc by ta).  </li></ul><p>When <code>Y</code> is used as argument, R2(Yc, ta) is replaced by the redundancy Rd(Yc, ta) (see function <code>rd</code>), such as in Tenenhaus 1998 p.139. </p><p><strong>References</strong></p><p>Chong, I.-G., Jun, C.-H., 2005. Performance of some variable selection methods when  multicollinearity is present. Chemometrics and Intelligent Laboratory Systems 78, 103–112.  https://doi.org/10.1016/j.chemolab.2004.12.011</p><p>Mehmood, T., Sæbø, S., Liland, K.H., 2020. Comparison of variable selection methods  in partial least squares regression. Journal of Chemometrics 34, e3226.  https://doi.org/10.1002/cem.3226</p><p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 27 18 7 6; 12 11 28 7] 
Y = [10. 11 13; 120 131 27; 8 12 4; 1 200 8; 100 10 89] 
y = Y[:, 1] 
ycla = [1; 1; 1; 2; 2]

nlv = 3
fm = plskern(X, Y; nlv = nlv) ;
res = vip(fm)
pnames(res)
mean(res.imp.^2)
vip(fm; nlv = 1).imp

nlv = 2
fm = plsrda(X, ycla; nlv = nlv) ;
fmpls = fm.fm
vip(fmpls).imp
Ydummy = dummy(ycla).Y
vip(fmpls, Ydummy).imp

nlv = 2
fm = plslda(X, ycla; nlv = nlv) ;
fmpls = fm.fm.fmpls
vip(fmpls).imp
Ydummy = dummy(ycla).Y
vip(fmpls, Ydummy).imp</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/vip.jl#LL1-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.viperm-Tuple{Any, Any}" href="#Jchemo.viperm-Tuple{Any, Any}"><code>Jchemo.viperm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">viperm(X, Y; perm = 50,
    psamp = .3, score = rmsep, fun, kwargs...)</code></pre><p>Variable importance by direct permutations.</p><ul><li><code>X</code> : X-data (n, p).</li><li><code>Y</code> : Y-data (n, q).  </li><li><code>perm</code> : Number of replications. </li><li><code>nint</code> : Nb. intervals. </li><li><code>psamp</code> : Proportion of data used as test set to compute the <code>score</code>   (default: 30% of the data).</li><li><code>score</code> : Function computing the prediction score (= error rate; e.g. msep).</li><li><code>fun</code> : Function defining the prediction model.</li><li><code>kwarg</code> : Optional other arguments to pass to funtion defined in <code>fun</code>.</li></ul><p>The principle is as follows:</p><ul><li>Data (X, Y) are splitted randomly to a training and a test set.</li><li>The model is fitted on Xtrain, and the score (error rate) on Xtest.   This gives the reference error rate.</li><li>Rows of a given variable (feature) j in Xtest are randomly permutated   (the rest of Xtest is unchanged). The score is computed on    the permuted Xtest and the new score is computed. The importance   is computed by the difference between this score and the reference score.</li><li>This process is run for each variable separately and replicated <code>perm</code> times.   Average results are provided in the outputs, as well the results per    replication. </li></ul><p>In general, this method returns similar results as the out-of-bag permutation method used in random forests (Breiman, 2001).</p><p><strong>References</strong></p><ul><li>Nørgaard, L., Saudland, A., Wagner, J., Nielsen, J.P., Munck, L., </li></ul><p>Engelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419. https://doi.org/10.1366/0003702001949500</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JchemoData, DataFrames, JLD2
using CairoMakie

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/tecator.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X
Y = dat.Y 
wlstr = names(X)
wl = parse.(Float64, wlstr) 
typ = Y.typ
y = Y.fat

f = 15 ; pol = 3 ; d = 2 
Xp = savgol(snv(X); f = f, pol = pol, d = d) 

s = typ .== &quot;train&quot;
Xtrain = Xp[s, :]
ytrain = y[s]

res = viperm(Xtrain, ytrain; perm = 50, 
    score = rmsep, fun = plskern, nlv = 9)
f = Figure(resolution = (500, 400))
ax = Axis(f[1, 1];
    xlabel = &quot;Wavelength (nm)&quot;, 
    ylabel = &quot;Importance&quot;)
scatter!(ax, wl, vec(res.imp); color = (:red, .5))
u = [910; 950]
vlines!(ax, u; color = :grey, linewidth = 1)
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/viperm.jl#LL1-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.vrow-Tuple{Any, Any}" href="#Jchemo.vrow-Tuple{Any, Any}"><code>Jchemo.vrow</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">vrow(X::AbstractMatrix, i)
vrow(X::DataFrame, i)
vrow(x::Vector, i)</code></pre><p>View of the i-th row(s) of a matrix <code>X</code>, or of the i-th element(s) of vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/utility.jl#LL1055-L1061">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.wdist-Tuple{Any}" href="#Jchemo.wdist-Tuple{Any}"><code>Jchemo.wdist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">wdist(d; h = 2, cri = 4, squared = false)
wdist!(d; h = 2, cri = 4, squared = false)</code></pre><p>Compute weights from distances, using a decreasing exponential function.</p><ul><li><code>d</code> : A vector of distances.</li><li><code>h</code> : A scaling positive scalar defining the shape of the function. </li><li><code>cri</code> : A positive scalar defining outliers in the distances vector.</li><li><code>squared</code>: If true, distances are replaced by the squared distances;   the weight function is then a Gaussian (RBF) kernel function.</li></ul><p>Weights are computed by exp(-d / (h * MAD(d))), or are set to 0 for  distances &gt; Median(d) + cri * MAD(d). This is an adaptation of the weight function presented in Kim et al. 2011.</p><p>The weights decrease with increasing distances. Lower is h, sharper is the decreasing function.  Weights are set to 0 for outliers (extreme distances).</p><p><strong>References</strong></p><p>Kim S, Kano M, Nakagawa H, Hasebe S. Estimation of active pharmaceutical ingredients content  using locally weighted partial least squares and statistical wavelength selection.  Int J Pharm. 2011;421(2):269-274. https://doi.org/10.1016/j.ijpharm.2011.10.007</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CairoMakie, Distributions

x1 = rand(Chisq(10), 100) ;
x2 = rand(Chisq(40), 10) ;
d = [sqrt.(x1) ; sqrt.(x2)]
h = 2 ; cri = 3
w = wdist(d; h = h, cri = cri) ;
f = Figure(resolution = (600, 400))
ax1 = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Nb. observations&quot;)
hist!(ax1, d, bins = 30)
ax2 = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
scatter!(ax2, d, w)
f[1, 1] = ax1 
f[1, 2] = ax2 
f

d = collect(0:.5:15) ;
h = [.5, 1, 1.5, 2.5, 5, 10, Inf] ;
#h = [1, 2, 5, Inf] ;
w = wdist(d; h = h[1]) ;
f = Figure(resolution = (600, 500))
ax = Axis(f, xlabel = &quot;Distance&quot;, ylabel = &quot;Weight&quot;)
lines!(ax, d, w, label = string(&quot;h = &quot;, h[1]))
for i = 2:length(h)
    w = wdist(d; h = h[i])
    lines!(ax, d, w, label = string(&quot;h = &quot;, h[i]))
end
axislegend(&quot;Values of h&quot;)
f[1, 1] = ax
f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/wdist.jl#LL1-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.wshenk-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}, Any}" href="#Jchemo.wshenk-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}, Any}"><code>Jchemo.wshenk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">wshenk(object::Union{Pcr, Plsr}, X; nlv = nothing)</code></pre><p>Compute the Shenk et al. (1997) &quot;LOCAL&quot; PLSR weights</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data on which the weights are computed.</li><li><code>nlv</code> : Nb. latent variables (LVs) to consider. If nothing,    it is the maximum nb. of components.</li></ul><p>For each observation (row) of <code>X</code>, the weights are returned  for the models with 1, ..., nlv LVs. </p><p><strong>References</strong></p><p>Shenk, J., Westerhaus, M., Berzaghi, P., 1997. Investigation of a LOCAL calibration  procedure for near infrared instruments.  Journal of Near Infrared Spectroscopy 5, 223. https://doi.org/10.1255/jnirs.115</p><p>Shenk et al. 1998 United States Patent (19). Patent Number: 5,798.526.</p><p>Zhang, M.H., Xu, Q.S., Massart, D.L., 2004. Averaged and weighted average partial  least squares. Analytica Chimica Acta 504, 279–289. https://doi.org/10.1016/j.aca.2003.10.056</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JLD2
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, &quot;data/cassav.jld2&quot;) 
@load db dat
pnames(dat)

X = dat.X 
y = dat.Y.tbc 
year = dat.Y.year

s = year .&lt;= 2012
Xtrain = X[s, :]
ytrain = y[s]
Xtest = rmrow(X, s)
ytest = rmrow(y, s)

nlv = 30
fm = plskern(Xtrain, ytrain; nlv = nlv) ;
res = Jchemo.wshenk(fm, Xtest) ;
pnames(res) 
plotsp(res.w, 1:nlv).f</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/wshenk.jl#LL1-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.xfit-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}}" href="#Jchemo.xfit-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}}"><code>Jchemo.xfit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">xfit(object::Union{Pca, Pcr, Plsr})
xfit(object::Union{Pca, Pcr, Plsr}, X; nlv = nothing)
xfit!(object::Union{Pca, Pcr, Plsr}, X::Matrix; nlv = nothing)</code></pre><p>Matrix fitting from a PCA, PCR or PLS model</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data to be approximatred from the model.</li><li><code>nlv</code> : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components.</li></ul><p>Compute an approximate of matrix <code>X</code> (X_fit) from a PCA, PCR  or PLS fitted on <code>X</code>.</p><p><code>X</code> and X_fit are in the original fscale, i.e. before centering and eventual scaling.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 3
X = rand(n, p)
y = rand(n)

nlv = 2 ;
fm = pcasvd(X; nlv = nlv) ;
#fm = plskern(X, y; nlv = nlv) ;
xfit(fm)
xfit(fm, X)
xfit(fm, X, nlv = 0)
xfit(fm, X, nlv = 1)

fm = pcasvd(X; nlv = min(n, p)) ;
xfit(fm, X)
xresid(fm, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/xfit.jl#LL1-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Jchemo.xresid-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}, Any}" href="#Jchemo.xresid-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}, Any}"><code>Jchemo.xresid</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">xresid(object::Union{Pca, Pcr, Plsr}, X; nlv = nothing)
xresid!(object::Union{Pca, Pcr, Plsr}, X::Matrix; nlv = nothing)</code></pre><p>Residual matrix after fitting by a PCA, PCR or PLS model</p><ul><li><code>object</code> : The fitted model.</li><li><code>X</code> : X-data for which the residuals have to be computed.</li><li><code>nlv</code> : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components.</li></ul><p>Compute the residual matrix E = X - X_fit.</p><p><code>X</code> and X_fit are in the original fscale, i.e. before centering and eventual scaling.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">n, p = 5, 3
X = rand(n, p)
y = rand(n)

nlv = 2 ;
fm = pcasvd(X; nlv = nlv) ;
#fm = plskern(X, y; nlv = nlv) ;
xresid(fm, X)
xresid(fm, X, nlv = 0)
xresid(fm, X, nlv = 1)

fm = pcasvd(X; nlv = min(n, p)) ;
xfit(fm, X)
xresid(fm, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mlesnoff/Jchemo.jl/blob/d0cdd475c828c7a33d1391c7599a28337216789b/src/xresid.jl#LL1-L31">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../domains/">« Available methods</a><a class="docs-footer-nextpage" href="../news/">News »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 30 November 2023 21:20">Thursday 30 November 2023</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
