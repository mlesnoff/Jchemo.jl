var documenterSearchIndex = {"docs":
[{"location":"api/#Index-of-functions","page":"Index of functions","title":"Index of functions","text":"","category":"section"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"Here is a list of all exported functions from Jchemo.jl. ","category":"page"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"For more details, click on the link and you'll be directed to the function help.","category":"page"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"","category":"page"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"Modules = [Jchemo]\nOrder   = [:function, :type]","category":"page"},{"location":"api/#Base.summary-Tuple{Jchemo.Cca, Any, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Cca, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Ccawold, Any, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Ccawold, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Comdim, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Comdim, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to    fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Fda}","page":"Index of functions","title":"Base.summary","text":"summary(object::Fda)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Kpca}","page":"Index of functions","title":"Base.summary","text":"summary(object::Kpca)\n\nSummarize the fitted model.\n\nobject : The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Mbpca, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Mbpca, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to    fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Mbplsr, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Mbplsr, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to    fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Mbplswest, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Mbplswest, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to    fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Pca, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Pca, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Plscan, Any, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Plscan, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Plstuck, Any, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Plstuck, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Rasvd, Any, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Rasvd, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Spca, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Spca, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Union{Pcr, Spcr}, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Union{Plsr, Splsr}, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to    fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aggstat-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.aggstat","text":"aggstat(X, y; algo = mean)\naggstat(X::DataFrame; vary, vargroup, algo = mean)\n\nCompute column-wise statistics by class in a dataset.\n\nX : Data (n, p).\ny : A categorical variable (n) (class membership).\nalgo : Function to compute (default = mean).\n\nSpecific for dataframes:\n\nvary : Vector of the names of the variables to summarize.\nvargroup : Vector of the names of the categorical variables to consider   for computations by class.\n\nVariables defined in vary and vargroup must be columns of X.\n\nReturn a matrix or, if only argument X::DataFrame is used, a dataframe.\n\nExamples\n\nusing Jchemo, DataFrames, Statistics\n\nn, p = 20, 5\nX = rand(n, p)\ndf = DataFrame(X, :auto)\ny = rand(1:3, n)\nres = aggstat(X, y; algo = sum)\nres.X\naggstat(df, y; algo = sum).X\n\nn, p = 20, 5\nX = rand(n, p)\ndf = DataFrame(X, string.(\"v\", 1:p))\ndf.gr1 = rand(1:2, n)\ndf.gr2 = rand([\"a\", \"b\", \"c\"], n)\ndf\naggstat(df; vary = [:v1, :v2], vargroup = [:gr1, :gr2], algo = var)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aggsum-Tuple{Vector, Union{Vector, BitVector}}","page":"Index of functions","title":"Jchemo.aggsum","text":"aggsum(x::Vector, y::Union{Vector, BitVector})\n\nCompute sub-total sums by class of a categorical variable.\n\nx : A quantitative variable to sum (n) \ny : A categorical variable (n) (class membership).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nx = rand(1000)\ny = vcat(rand([\"a\" ; \"c\"], 900), repeat([\"b\"], 100))\naggsum(x, y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aicplsr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.aicplsr","text":"aicplsr(X, y; alpha = 2, kwargs...)\n\nCompute Akaike's (AIC) and Mallows's (Cp) criteria for univariate PLSR models.\n\nX : X-data (n, p).\ny : Univariate Y-data.\n\nKeyword arguments:\n\nSame arguments as those of function cglsr.\nalpha : Coefficient multiplicating   the model complexity (df) to compute AIC. \n\nThe function uses function dfplsr_cg. \n\nReferences\n\nHansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697\n\nHansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9\n\nLesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369\n\nExamples\n\nusing Jchemo, JchemoData, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 40\nres = aicplsr(X, y; nlv) ;\nres.crit\nres.opt\nres.delta\n\nzaic = res.crit.aic\nf, ax = plotgrid(0:nlv, zaic; xlabel = \"Nb. LVs\", ylabel = \"AIC\")\nscatter!(ax, 0:nlv, zaic)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aov1-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.aov1","text":"aov1(x, Y)\nOne-factor ANOVA test.\n\nx : Univariate categorical (factor) data (n).\nY : Y-data (n, q).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\n@head dat.X\nx = dat.X[:, 5]\nY = dat.X[:, 1:4]\ntab(x) \n\nres = aov1(x, Y) ;\npnames(res)\nres.SSF\nres.SSR \nres.F \nres.pval\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.bias-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.bias","text":"bias(pred, Y)\n\nCompute the prediction bias, i.e. the opposite of the mean prediction error.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern(nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nbias(pred, Ytest)\n\nmodel = plskern(nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nbias(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.blockscal-Tuple{}","page":"Index of functions","title":"Jchemo.blockscal","text":"blockscal(; kwargs...)\nblockscal(Xbl; kwargs...)\nblockscal(Xbl, weights::Weight; kwargs...)\n\nScale multiblock X-data.\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \nweights : Weights (n) of the observations (rows    of the blocks). Must be of type Weight (see e.g.    function mweight).\n\nKeyword arguments:\n\ncentr : Boolean. If true, each column of blocks in Xbl    is centered (before the block scaling).\nscal : Boolean. If true, each column of blocks in Xbl    is scaled by its uncorrected standard deviation    (before the block scaling).\nbscal : Type of block scaling. Possible values are:   :none, :frob, :mfa, :ncol, :sd. See thereafter.\n\nIf implemented, the data transformations follow the order: column centering, column scaling  and finally block scaling. \n\nTypes of block scaling:\n\n:none : No block scaling. \n:frob : Let D be the diagonal matrix of vector weights.w.    Each block X is divided by its Frobenius norm  = sqrt(tr(X' * D * X)).    After this scaling, tr(X' * D * X) = 1.\n:mfa : Each block X is divided by sv, where sv is the dominant singular    value of X (this is the \"MFA\" approach; \"AFM \"in French).\n:ncol : Each block X is divided by the nb. of columns of the block.\n:sd : Each block X is divided by sqrt(sum(weighted variances of the block-columns)).    After this scaling, sum(weighted variances of the block-columns) = 1.\n\nExamples\n\nusing Jchemo\nn = 5 ; m = 3 ; p = 10 \nX = rand(n, p) \nXnew = rand(m, p)\nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl) \nXblnew = mblock(Xnew, listbl) \n@head Xbl[3]\n\ncentr = true ; scal = true\nbscal = :frob\nmodel = blockscal(; centr, scal, bscal)\nfit!(model, Xbl)\n## Data transformation\nzXbl = transf(model, Xbl) ; \n@head zXbl[3]\n\nzXblnew = transf(model, Xblnew) ; \nzXblnew[3]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.calds-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.calds","text":"calds(; algo = plskern, kwargs...)\ncalds(X1, X2; algo = plskern, kwargs...)\n\nDirect standardization (DS) for calibration transfer of spectral data.\n\nX1 : Spectra (n, p) to transfer to the target.\nX2 : Target spectra (n, p).\n\nKeyword arguments:\n\nalgo : Function used as transfer model.  \nkwargs : Optional arguments for algo.\n\nX1 and X2 must represent the same n samples (\"standards\").\n\nThe objective is to transform spectra X1 to new spectra as close  as possible as the target X2. Method DS fits a model  (defined in algo) that predicts X2 from X1.\n\nReferences\n\nY. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\")\n@load db dat\npnames(dat)\n## Objects X1 and X2 are spectra collected \n## on the same samples. \n## X2 represents the target space. \n## We want to transfer X1 in the same space\n## as X2.\n## Data to transfer\nX1cal = dat.X1cal\nX1val = dat.X1val\nn = nro(X1cal)\nm = nro(X1val)\n## Target space\nX2cal = dat.X2cal\nX2val = dat.X2val\n\n## Fitting the model\nfitm = calds(X1cal, X2cal; algo = plskern, nlv = 10) \n#fitm = calds(X1cal, X2cal; algo = mlrpinv)   # less robust \n\n## Transfer of new spectra X1val \n## expected to be close to X2val\npred = predict(fitm, X1val).pred\n\ni = 1\nf = Figure(size = (500, 300))\nax = Axis(f[1, 1])\nlines!(X2val[i, :]; label = \"x2\")\nlines!(ax, X1val[i, :]; label = \"x1\")\nlines!(pred[i, :]; linestyle = :dash, label = \"x1_corrected\")\naxislegend(position = :rb, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.calpds-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.calpds","text":"calpds(; npoint = 5, algo = plskern, kwargs...)\ncalpds(X1, X2; npoint = 5, algo = plskern, kwargs...)\n\nPiecewise direct standardization (PDS) for calibration transfer of spectral data.\n\nX1 : Spectra (n, p) to transfer to the target.\nX2 : Target spectra (n, p).\n\nKeyword arguments:\n\nnpoint : Half-window size (nb. points left or right    to the given wavelength). \nalgo : Function used as transfer model.  \nkwargs : Optional arguments for algo.\n\nX1 and X2 must represent the same n standard samples.\n\nThe objective is to transform spectra X1 to new spectra as close  as possible as the target X2. Method PDS fits models  (defined in algo) that predict X2 from X1.\n\nThe window used in X1 to predict wavelength \"i\" in X2 is:\n\ni - npoint, i - npoint + 1, ..., i, ..., i + npoint - 1, i + npoint\n\nReferences\n\nBouveresse, E., Massart, D.L., 1996. Improvement of the piecewise direct targetisation procedure  for the transfer of NIR spectra for multivariate calibration. Chemometrics and Intelligent Laboratory  Systems 32, 201–213. https://doi.org/10.1016/0169-7439(95)00074-7\n\nY. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.\n\nWülfert, F., Kok, W.Th., Noord, O.E. de, Smilde, A.K., 2000. Correction of Temperature-Induced  Spectral Variation by Continuous Piecewise Direct Standardization. Anal. Chem. 72, 1639–1644. https://doi.org/10.1021/ac9906835\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\")\n@load db dat\npnames(dat)\n## Objects X1 and X2 are spectra collected \n## on the same samples. \n## X2 represents the target space. \n## We want to transfer X1 in the same space\n## as X2.\n## Data to transfer\nX1cal = dat.X1cal\nX1val = dat.X1val\nn = nro(X1cal)\nm = nro(X1val)\n## Target space\nX2cal = dat.X2cal\nX2val = dat.X2val\n\n## Fitting the model\nfitm = calpds(X1cal, X2cal; npoint = 2, algo = plskern, nlv = 2) \n\n## Transfer of new spectra X1val \n## expected to be close to X2val\npred = predict(fitm, X1val).pred\n\ni = 1\nf = Figure(size = (500, 300))\nax = Axis(f[1, 1])\nlines!(X2val[i, :]; label = \"x2\")\nlines!(ax, X1val[i, :]; label = \"x1\")\nlines!(pred[i, :]; linestyle = :dash, label = \"x1_corrected\")\naxislegend(position = :rb, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cca-Tuple{}","page":"Index of functions","title":"Jchemo.cca","text":"cca(; kwargs...)\ncca(X, Y; kwargs...)\ncca(X, Y, weights::Weight; kwargs...)\ncca!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nCanonical correlation Analysis (CCA, RCCA).\n\nX : First block of data.\nY : Second block of data.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. Possible values are:   :none, :frob. See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\nscal : Boolean. If true, each column of blocks X    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThis function implements a CCA algorithm using SVD decompositions and  presented in Weenink 2003 section 2. \n\nA continuum regularization is available (parameter tau).  After block centering and scaling, the function returns  block scores (Tx and Ty) that are proportionnal to the  eigenvectors of Projx * Projy and Projy * Projx, respectively,  defined as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\nCy = (1 - tau) * Y'DY + tau * Iy\nCxy = X'DY \nProjx = sqrt(D) * X * invCx * X' * sqrt(D)\nProjy = sqrt(D) * Y * invCx * Y' * sqrt(D)\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting  the covariance matrices. Often, a better alternative is  to use an epsilon value (e.g. tau = 1e-8) to get similar  results as with pseudo-inverses.  \n\nThe normed scores returned by the function are expected  (using uniform weights) to be the same as those  returned by functions rcc of the R packages CCA (González et al.)  and mixOmics (Lê Cao et al.) whith their parameters lambda1  and lambda2 set to:\n\nlambda1 = lambda2 = tau / (1 - tau) * n / (n - 1) \n\nReferences\n\nGonzález, I., Déjean, S., Martin, P.G.P., Baccini, A., 2008.  CCA: An R Package to Extend Canonical Correlation Analysis.  Journal of Statistical Software 23, 1-14.  https://doi.org/10.18637/jss.v023.i12\n\nHotelling, H. (1936): “Relations between two sets of variates”,  Biometrika 28: pp. 321–377.\n\nLê Cao, K.-A., Rohart, F., Gonzalez, I., Dejean, S., Abadi, A.J.,  Gautier, B., Bartolo, F., Monget, P., Coquery, J., Yao, F.,  Liquet, B., 2022. mixOmics: Omics Data Integration Project.  https://doi.org/10.18129/B9.bioc.mixOmics\n\nWeenink, D. 2003. Canonical Correlation Analysis, Institute of  Phonetic Sciences, Univ. of Amsterdam, Proceedings 25, 81-99.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn, p = size(X)\nq = nco(Y)\n\nnlv = 3\nbscal = :frob ; tau = 1e-8\nmodel = cca(; nlv, bscal, tau)\nfit!(model, X, Y)\npnames(model)\npnames(model.fitm)\n\n@head model.fitm.Tx\n@head transfbl(model, X, Y).Tx\n\n@head model.fitm.Ty\n@head transfbl(model, X, Y).Ty\n\nres = summary(model, X, Y) ;\npnames(res)\nres.cort2t \nres.rdx\nres.rdy\nres.corx2t \nres.cory2t \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ccawold-Tuple{}","page":"Index of functions","title":"Jchemo.ccawold","text":"ccawold(; kwargs...)\nccawold(X, Y; kwargs...)\nccawold(X, Y, weights::Weight; kwargs...)\nccawold!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nCanonical correlation analysis (CCA, RCCA) - Wold      Nipals algorithm.\n\nX : First block of data.\nY : Second block of data.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. Possible values are:   :none, :frob. See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\ntol : Tolerance value for convergence (Nipals).\nmaxit : Maximum number of iterations (Nipals).\nscal : Boolean. If true, each column of blocks X    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThis function implements the Nipals ccawold algorithm  presented by Tenenhaus 1998 p.204 (related to Wold et al. 1984). \n\nIn this implementation, after each step of LVs computation,  X and Y are deflated relatively to their respective scores  (tx and ty). \n\nA continuum regularization is available (parameter tau).  After block centering and scaling, the covariances matrices  are computed as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\nCy = (1 - tau) * Y'DY + tau * Iy\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting  the covariance matrices. Often, a better alternative is  to use an epsilon value (e.g. tau = 1e-8) to get similar  results as with pseudo-inverses.   \n\nThe normed scores returned by the function are expected  (using uniform weights) to be the same as those  returned by function rgcca of the R package RGCCA  (Tenenhaus & Guillemot 2017, Tenenhaus et al. 2017). \n\nReferences\n\nTenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and  Sparse Generalized Canonical Correlation Analysis for  Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html \n\nTenenhaus, M., 1998. La régression PLS: théorie et  pratique. Editions Technip, Paris.\n\nTenenhaus, M., Tenenhaus, A., Groenen, P.J.F., 2017.  Regularized Generalized Canonical Correlation Analysis:  A Framework for Sequential Multiblock Component Methods.  Psychometrika 82, 737–777.  https://doi.org/10.1007/s11336-017-9573-x\n\nWold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984.  The Collinearity Problem in Linear Regression. The Partial  Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5,  735–743. https://doi.org/10.1137/0905052\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn, p = size(X)\nq = nco(Y)\n\nnlv = 2\nbscal = :frob ; tau = 1e-4\nmodel = ccawold(; nlv, bscal, tau, tol = 1e-10)\nfit!(model, X, Y)\npnames(model)\npnames(model.fitm)\n\n@head model.fitm.Tx\n@head transfbl(model, X, Y).Tx\n\n@head model.fitm.Ty\n@head transfbl(model, X, Y).Ty\n\nres = summary(model, X, Y) ;\npnames(res)\nres.explvarx\nres.explvary\nres.cort2t \nres.rdx\nres.rdy\nres.corx2t \nres.cory2t \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.center-Tuple{}","page":"Index of functions","title":"Jchemo.center","text":"center()\ncenter(X)\ncenter(X, weights::Weight)\n\nColumn-wise centering of X-data.\n\nX : X-data (n, p).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = center() \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\ncolmean(Xptrain)\n@head Xptest \n@head Xtest .- colmean(Xtrain)'\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cglsr-Tuple{}","page":"Index of functions","title":"Jchemo.cglsr","text":"cglsr(; kwargs...)\ncglsr(X, y; kwargs...)\ncglsr!(X::Matrix, y::Matrix; kwargs...)\n\nConjugate gradient algorithm for the normal equations (CGLS; Björck 1996).\n\nX : X-data  (n, p).\ny : Univariate Y-data (n).\n\nKeyword arguments:\n\nnlv : Nb. CG iterations.\ngs : Boolean. If true (default), a Gram-Schmidt    orthogonalization of the normal equation    residual vectors is done.\nfilt : Boolean. If true, CG filter factors    are computed (output F). Default = false.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nCGLS algorithm \"7.4.1\" Bjorck 1996, p.289. In the present function, the  part of the code computing the re-orthogonalization (Hansen 1998) and  filter factors (Vogel 1987, Hansen 1998) is a transcription  (with few adaptations) of the Matlab function cgls (Saunders et al.  https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).\n\nReferences\n\nBjörck, A., 1996. Numerical Methods for Least Squares Problems,  Other Titles in Applied Mathematics. Society for Industrial and Applied  Mathematics. https://doi.org/10.1137/1.9781611971484\n\nHansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems,  Mathematical Modeling and Computation. Society for Industrial and Applied  Mathematics. https://doi.org/10.1137/1.9780898719697\n\nHansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9\n\nManne R. Analysis of two partial-least-squares algorithms for  multivariate calibration. Chemometrics Intell. Lab. Syst. 1987, 2: 187–197.\n\nPhatak A, De Hoog F. Exploiting the connection between PLS,  Lanczos methods and conjugate gradients: alternative proofs  of some properties of PLS. J. Chemometrics 2002; 16: 361–367.\n\nVogel, C. R.,  \"Solving ill-conditioned linear systems using  the conjugate gradient method\", Report, Dept. of Mathematical  Sciences, Montana State University, 1987.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 5 ; scal = true\nmodel = cglsr(; nlv, scal)\nfit!(model, Xtrain, ytrain)\npnames(model.fitm) \n@head model.fitm.B\ncoef(model.fitm).B\ncoef(model.fitm).int\n\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Cglsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Cglsr)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Dkplsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Dkplsr; nlv = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Kplsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Kplsr; nlv = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Krr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Krr; lb = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nlb : Ridge regularization parameter    \"lambda\".\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Pcr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Pcr; nlv = nothing)\n\nCompute the b-coefficients of a LV model.\n\nobject : The fitted model.\nnlv : Nb. LVs to consider.\n\nFor a model fitted from X(n, p) and Y(n, q), the returned  object B is a matrix (p, q). If nlv = 0, B is a matrix  of zeros. The returned object int is the intercept.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Rosaplsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Rosaplsr; nlv = nothing)\n\nCompute the X b-coefficients of a model fitted with nlv LVs.\n\nobject : The fitted model.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Rr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Rr; lb = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nlb : Ridge regularization parameter    \"lambda\".\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Mlr)\n\nCompute the coefficients of the fitted model.\n\nobject : The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Union{Plsr, Pcr, Splsr}; nlv = nothing)\n\nCompute the b-coefficients of a LV model.\n\nobject : The fitted model.\nnlv : Nb. LVs to consider.\n\nFor a model fitted from X(n, p) and Y(n, q), the returned  object B is a matrix (p, q). If nlv = 0, B is a matrix  of zeros. The returned object int is the intercept.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colmad-Tuple{Any}","page":"Index of functions","title":"Jchemo.colmad","text":"colmad(X)\n\nCompute column-wise median absolute deviations (MAD) of a matrix.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\n\ncolmad(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colmean-Tuple{Any}","page":"Index of functions","title":"Jchemo.colmean","text":"colmean(X)\ncolmean(X, weights::Weight)\n\nCompute column-wise means of a matrix.\n\nX : Data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nw = mweight(rand(n))\n\ncolmean(X)\ncolmean(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colmed-Tuple{Any}","page":"Index of functions","title":"Jchemo.colmed","text":"colmed(X)\n\nCompute column-wise medians of a matrix.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\n\ncolmed(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colnorm-Tuple{Any}","page":"Index of functions","title":"Jchemo.colnorm","text":"colnorm(X)\ncolnorm(X, weights::Weight)\n\nCompute column-wise norms of a matrix.\n\nX : Data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nThe norm computed for a column x of X is:\n\nsqrt(x' * x)\n\nThe weighted norm is:\n\nsqrt(x' * D * x), where D is the diagonal matrix of weights.w\n\nWarning: colnorm(X, mweight(ones(n))) = colnorm(X) / sqrt(n).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nw = mweight(rand(n))\n\ncolnorm(X)\ncolnorm(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colstd-Tuple{Any}","page":"Index of functions","title":"Jchemo.colstd","text":"colstd(X)\ncolstd(X, weights::Weight)\n\nCompute column-wise standard deviations (uncorrected) of a matrix.\n\nX : Data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nw = mweight(rand(n))\n\ncolstd(X)\ncolstd(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colsum-Tuple{Any}","page":"Index of functions","title":"Jchemo.colsum","text":"colsum(X)\ncolsum(X, weights::Weight)\n\nCompute column-wise sums of a matrix.\n\nX : Data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nw = mweight(rand(n))\n\ncolsum(X)\ncolsum(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colvar-Tuple{Any}","page":"Index of functions","title":"Jchemo.colvar","text":"colvar(X)\ncolvar(X, weights::Weight)\n\nCompute column-wise variances (uncorrected) of a matrix.\n\nX : Data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nw = mweight(rand(n))\n\ncolvar(X)\ncolvar(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.comdim-Tuple{}","page":"Index of functions","title":"Jchemo.comdim","text":"comdim(; kwargs...)\ncomdim(Xbl; kwargs...)\ncomdim(Xbl, weights::Weight; kwargs...)\ncomdim!(Xbl::Matrix, weights::Weight; kwargs...)\n\nCommon components and specific weights analysis (ComDim, aka CCSWA).\n\nXbl : List of blocks (vector of matrices) of X-data.    Typically, output of function mblock.  \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal       for possible values.\ntol : Tolerance value for convergence (Nipals).\nmaxit : Maximum number of iterations (Nipals).\nscal : Boolean. If true, each column of blocks in Xbl    is scaled by its uncorrected standard deviation    (before the block scaling).\n\n\"SVD\" algorithm of Hannafi & Qannari 2008 p.84.\n\nThe function returns several objects, in particular:\n\nT : The non normed global scores.\nU : The normed global scores.\nW : The global loadings.\nTbl : The block scores (grouped by blocks, in the original scale).\nTb : The block scores (grouped by LV, in the metric scale).\nWbl : The block loadings.\nlb : The specific weights (saliences) \"lambda\".\nmu : The sum of the squared saliences.\n\nFunction summary returns: \n\nexplvarx : Proportion of the total inertia of X    (sum of the squared norms of the    blocks) explained by each global score.\nexplvarxx : Proportion of the XX' total inertia    (sum of the squared norms of the products Xk * Xk')    explained by each global score (= indicator \"V\" in Qannari    et al. 2000, Hanafi et al. 2008).\nsal2 : Proportion of the squared saliences   of each block within each global score. \ncontr_block : Contribution of each block    to the global scores (= proportions of the saliences    \"lambda\" within each score).\nexplX : Proportion of the inertia of the blocks \nexplained by each global score.\ncorx2t : Correlation between the global scores    and the original variables.  \ncortb2t : Correlation between the global scores    and the block scores.\nrv : RV coefficient. \nlg : Lg coefficient. \n\nReferences\n\nCariou, V., Qannari, E.M., Rutledge, D.N., Vigneau, E., 2018.  ComDim: From multiblock data analysis to path modeling. Food  Quality and Preference, Sensometrics 2016: Sensometrics-by-the-Sea  67, 27–34. https://doi.org/10.1016/j.foodqual.2017.02.012\n\nCariou, V., Jouan-Rimbaud Bouveresse, D., Qannari, E.M.,  Rutledge, D.N., 2019. Chapter 7 - ComDim Methods for the Analysis  of Multiblock Data in a Data Fusion Perspective, in: Cocchi, M. (Ed.),  Data Handling in Science and Technology,  Data Fusion Methodology and Applications. Elsevier, pp. 179–204.  https://doi.org/10.1016/B978-0-444-63984-4.00007-7\n\nGhaziri, A.E., Cariou, V., Rutledge, D.N., Qannari, E.M., 2016.  Analysis of multiblock datasets using ComDim: Overview and extension  to the analysis of (K + 1) datasets. Journal of Chemometrics 30,  420–429. https://doi.org/10.1002/cem.2810\n\nHanafi, M., 2008. Nouvelles propriétés de l’analyse en composantes  communes et poids spécifiques. Journal de la société française  de statistique 149, 75–97.\n\nQannari, E.M., Wakeling, I., Courcoux, P., MacFie, H.J.H., 2000.  Defining the underlying sensory dimensions. Food Quality and  Preference 11, 151–154.  https://doi.org/10.1016/S0950-3293(99)00069-5\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ham.jld2\") \n@load db dat\npnames(dat) \nX = dat.X\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X[1:6, :], listbl)\nXblnew = mblock(X[7:8, :], listbl)\nn = nro(Xbl[1]) \n\nnlv = 3\nbscal = :frob\nscal = false\n#scal = true\nmodel = comdim(; nlv, bscal, scal)\nfit!(model, Xbl)\npnames(model) \npnames(model.fitm)\n## Global scores \n@head model.fitm.T\n@head transf(model, Xbl)\ntransf(model, Xblnew)\n## Blocks scores\ni = 1\n@head model.fitm.Tbl[i]\n@head transfbl(model, Xbl)[i]\n\nres = summary(model, Xbl) ;\npnames(res) \nres.explvarx\nres.explvarxx\nres.sal2 \nres.contr_block\nres.explX   # = model.fitm.lb if bscal = :frob\nrowsum(Matrix(res.explX))\nres.corx2t \nres.cortb2t\nres.rv\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.conf-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.conf","text":"conf(pred, y; digits = 1)\n\nConfusion matrix.\n\npred : Univariate predictions.\ny : Univariate observed data.\n\nKeyword arguments:\n\ndigits : Nb. digits used to round percentages.\n\nExamples\n\nusing Jchemo, CairoMakie\n\ny = [\"d\"; \"c\"; \"b\"; \"c\"; \"a\"; \"d\"; \"b\"; \"d\"; \n    \"b\"; \"b\"; \"a\"; \"a\"; \"c\"; \"d\"; \"d\"]\npred = [\"a\"; \"d\"; \"b\"; \"d\"; \"b\"; \"d\"; \"b\"; \"d\"; \n    \"b\"; \"b\"; \"a\"; \"a\"; \"d\"; \"d\"; \"d\"]\n#y = rand(1:10, 200); pred = rand(1:10, 200)\n\nres = conf(pred, y) ;\npnames(res)\nres.cnt       # Counts (dataframe built from `A`) \nres.pct       # Row %  (dataframe built from `Apct`))\nres.A         \nres.Apct\nres.diagpct\nres.accpct    # Accuracy (% classification successes)\nres.lev       # Levels\n\nplotconf(res).f\n\nplotconf(res; cnt = false, ptext = false).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.convertdf-Tuple{DataFrames.DataFrame}","page":"Index of functions","title":"Jchemo.convertdf","text":"convertdf(df::DataFrame; miss = nothing, typ)\n\nConvert the columns of a dataframe to given types.\n\ndf : A dataframe.\nmiss : The code used in df to identify the data    to be declared as missing (of type Missing).   See function recod_miss.\ntyp : A vector of the targeted types for the   columns of the new dataframe.  \n\nExamples\n\nusing Jchemo, DataFrames\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cor2-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.cor2","text":"cor2(pred, Y)\n\nCompute the squared linear correlation between data and predictions.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo \n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\ncor2(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\ncor2(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.corm-Tuple{Any}","page":"Index of functions","title":"Jchemo.corm","text":"corm(X) \ncorm(X, Y) \ncorm(X, weights::Weight)\ncorm(X, Y, weights::Weight)\n\nCompute a weighted correlation matrix.\n\nX : Data (n, p).\nY : Data (n, q).\nweights : Weights (n) of the observations. Object of type    Weight (e.g. generated by function mweight).\n\nUncorrected correlation matrix \n\nof X-columns :  ==> (p, p) matrix \nor between X-columns and Y-columns :  ==> (p, q) matrix.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nY = rand(n, 3)\nw = mweight(rand(n))\n\ncorm(X, w)\ncorm(X, Y, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cosm-Tuple{Any}","page":"Index of functions","title":"Jchemo.cosm","text":"cosm(X)\ncosm(X, Y)\n\nCompute a cosinus matrix.\n\nX : Data (n, p).\nY : Data (n, q).\n\nThe function computes the cosinus matrix: \n\nof the columns of X:  ==> (p, p) matrix \nor between columns of X and Y :  ==> (p, q) matrix.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nY = rand(n, 3)\n\ncosm(X)\ncosm(X, Y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cosv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.cosv","text":"cosv(x, y)\n\nCompute cosinus between two vectors.\n\nx : vector (n).\ny : vector (n).\n\nReferences\n\n@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.\n\nExamples\n\nusing Jchemo\n\nn = 5\nx = rand(n)\ny = rand(n)\n\ncosv(x, y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.covm-Tuple{Any}","page":"Index of functions","title":"Jchemo.covm","text":"covm(X)\ncovm(X, weights::Weight)\ncovm(X, Y) \ncovm(X, Y, weights::Weight)\n\nCompute a weighted covariance matrix.\n\nX : Data (n, p).\nY : Data (n, q).\nweights : Weights (n) of the observations. Object of type    Weight (e.g. generated by function mweight).\n\nThe function computes the uncorrected weighted covariance  matrix: \n\nof the columns of X:  ==> (p, p) matrix \nor between columns of X and Y :  ==> (p, q) matrix.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nY = rand(n, 3)\nw = mweight(rand(n))\n\ncovm(X, w)\ncovm(X, Y, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cscale-Tuple{}","page":"Index of functions","title":"Jchemo.cscale","text":"cscale()\ncscale(X)\ncscale(X, weights::Weight)\n\nColumn-wise centering and scaling of X-data.\n\nX : X-data (n, p).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\n\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = cscale() \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\ncolmean(Xptrain)\ncolstd(Xptrain)\n@head Xptest \n@head (Xtest .- colmean(Xtrain)') ./ colstd(Xtrain)'\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.default-Tuple{Function}","page":"Index of functions","title":"Jchemo.default","text":"default(algo::Function)\n\nDisplay the keyword arguments (with their default values) of a function\n\nalgo : The name of the functions.\n\nExamples\n\nusing Jchemo\n\ndefault(svmr)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.detrend_airpls-Tuple{}","page":"Index of functions","title":"Jchemo.detrend_airpls","text":"detrend_airpls(; kwargs...)\ndetrend_airpls(X; kwargs...)\n\nBaseline correction of each row of X-data by adaptive iteratively      reweighted penalized least squares algorithm (AIRPLS).\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nlb : Penalizing (smoothing) parameter \"lambda\".\nmaxit : Maximum number of iterations.\nverbose : If true, nb. iterations are printed.\n\nDe-trend transformation: the function fits a baseline by AIRPLS (see Zhang et al. 2010,  and Baek et al. 2015 section 2) for each observation and returns the residuals  (= signals corrected from the baseline).\n\nReferences\n\nBaek, S.-J., Park, A., Ahn, Y.-J., Choo, J., 2015. Baseline correction using  asymmetrically reweighted penalized least squares smoothing. Analyst 140, 250–257.  https://doi.org/10.1039/C4AN01061B\n\nZhang, Z.-M., Chen, S., Liang, Y.-Z., 2010. Baseline correction using adaptive  iteratively reweighted penalized least squares. Analyst 135, 1138–1146.  https://doi.org/10.1039/B922045C\n\nhttps://github.com/zmzhang/airPLS/tree/master \n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\n## Example on 1 spectrum\ni = 2\nzX = Matrix(X)[i:i, :]\nlb = 1e6\nmodel = detrend_airpls(; lb)\nfit!(model, zX)\nzXc = transf(model, zX)   # = corrected spectrum \nB = zX - zXc              # = estimated baseline\nf, ax = plotsp(zX, wl)\nlines!(wl, vec(B); color = :blue)\nlines!(wl, vec(zXc); color = :black)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.detrend_arpls-Tuple{}","page":"Index of functions","title":"Jchemo.detrend_arpls","text":"detrend_arpls(; kwargs...)\ndetrend_arpls(X; kwargs...)\n\nBaseline correction of each row of X-data by asymmetrically     reweighted penalized least squares smoothing (ARPLS).\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nlb : Penalizing (smoothness) parameter \"lambda\".\ntol : Tolerance value for stopping the iterations.  \nmaxit : Maximum number of iterations.\nverbose : If true, nb. iterations are printed.\n\nDe-trend transformation: the function fits a baseline by ARPLS (see Baek et al. 2015 section 3) for each observation and returns the residuals (= signals corrected from the baseline).\n\nReferences\n\nBaek, S.-J., Park, A., Ahn, Y.-J., Choo, J., 2015. Baseline correction using  asymmetrically reweighted penalized least squares smoothing. Analyst 140, 250–257.  https://doi.org/10.1039/C4AN01061B\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\n## Example on 1 spectrum\ni = 2\nzX = Matrix(X)[i:i, :]\nlb = 1e4\nmodel = detrend_arpls(; lb, p)\nfit!(model, zX)\nzXc = transf(model, zX)   # = corrected spectrum \nB = zX - zXc              # = estimated baseline\nf, ax = plotsp(zX, wl)\nlines!(wl, vec(B); color = :blue)\nlines!(wl, vec(zXc); color = :black)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.detrend_asls-Tuple{}","page":"Index of functions","title":"Jchemo.detrend_asls","text":"detrend_asls(; kwargs...)\ndetrend_asls(X; kwargs...)\n\nBaseline correction of each row of X-data by asymmetric      least squares algorithm (ASLS).\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nlb : Penalizing (smoothness) parameter \"lambda\".\np : Asymmetry parameter (0 < p << 1).\ntol : Tolerance value for stopping the iterations.  \nmaxit : Maximum number of iterations.\nverbose : If true, nb. iterations are printed.\n\nDe-trend transformation: the function fits a baseline by ASLS (see Baek et al. 2015 section 2) for each observation and returns the residuals (= signals corrected from the baseline).\n\nGenerally 0.001 ≤ p ≤ 0.1 is a good choice (for a signal with positive peaks)  and 1e2 ≤ lb ≤ 1e9, but exceptions may occur (Eilers & Boelens 2005).\n\nReferences\n\nBaek, S.-J., Park, A., Ahn, Y.-J., Choo, J., 2015. Baseline correction using  asymmetrically reweighted penalized least squares smoothing. Analyst 140, 250–257.  https://doi.org/10.1039/C4AN01061B\n\nEilers, P. H., & Boelens, H. F. (2005). Baseline correction with asymmetric  least squares smoothing. Leiden University Medical Centre Report, 1(1).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\n## Example on 1 spectrum\ni = 2\nzX = Matrix(X)[i:i, :]\nlb = 1e5 ; p = .001\nmodel = detrend_asls(; lb, p)\nfit!(model, zX)\nzXc = transf(model, zX)   # = corrected spectrum \nB = zX - zXc              # = estimated baseline\nf, ax = plotsp(zX, wl)\nlines!(wl, vec(B); color = :blue)\nlines!(wl, vec(zXc); color = :black)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.detrend_lo-Tuple{}","page":"Index of functions","title":"Jchemo.detrend_lo","text":"detrend_lo(; kwargs...)\ndetrend_lo(X; kwargs...)\n\nBaseline correction of each row of X-data by LOESS regression.\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nspan : Window for neighborhood selection (level of smoothing)   for the local fitting, typically in 0, 1.\ndegree : Polynomial degree for the local fitting.\n\nDe-trend transformation: The function fits a baseline by LOESS regression  (function loessr) for each observation and returns the residuals (= signals corrected  from the baseline).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = detrend_lo(span = .8)\nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain, wl).f\nplotsp(Xptest, wl).f\n\n## Example on 1 spectrum\ni = 2\nzX = Matrix(X)[i:i, :]\nmodel = detrend_lo(span = .75)\nfit!(model, zX)\nzXc = transf(model, zX)   # = corrected spectrum \nB = zX - zXc            # = estimated baseline\nf, ax = plotsp(zX, wl)\nlines!(wl, vec(B); color = :blue)\nlines!(wl, vec(zXc); color = :black)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.detrend_pol-Tuple{}","page":"Index of functions","title":"Jchemo.detrend_pol","text":"detrend_pol(; kwargs...)\ndetrend_pol(X; kwargs...)\n\nBaseline correction of each row of X-data by polynomial linear regression.\n\nX : X-data (n, p).\n\nKeyword arguments:\n\ndegree : Polynom degree.\n\nDe-trend transformation: the function fits a baseline by polynomial regression  for each observation and returns the residuals (= signals corrected from the baseline).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = detrend_pol(degree = 2)\nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain, wl).f\nplotsp(Xptest, wl).f\n\n## Example on 1 spectrum\ni = 2\nzX = Matrix(X)[i:i, :]\nmodel = detrend_pol(degree = 1)\nfit!(model, zX)\nzXc = transf(model, zX)   # = corrected spectrum \nB = zX - zXc            # = estimated baseline\nf, ax = plotsp(zX, wl)\nlines!(wl, vec(B); color = :blue)\nlines!(wl, vec(zXc); color = :black)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dfplsr_cg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.dfplsr_cg","text":"dfplsr_cg(X, y; kwargs...)\n\nCompute the model complexity (df) of PLSR models with the CGLS algorithm.\n\nX : X-data (n, p).\ny : Univariate Y-data.\n\nKeyword arguments:\n\nSame as function cglsr.\n\nThe number of degrees of freedom (df) of the PLSR model  is returned for 0, 1, ..., nlv LVs.\n\nReferences\n\nHansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems,  Mathematical Modeling and Computation. Society for Industrial and  Applied Mathematics. https://doi.org/10.1137/1.9780898719697\n\nHansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9\n\nLesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods  for estimating Mallows’s Cp and AIC criteria for PLSR models. Illustration  on agronomic spectroscopic NIR data. Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369\n\nExamples\n\n## The example below reproduces the numerical illustration\n## given by Kramer & Sugiyama 2011 on the Ozone data \n## (Fig. 1, center).\n## Function \"pls.model\" used for df calculations\n## in the R package \"plsdof\" v0.2-9 (Kramer & Braun 2019)\n## automatically scales the X matrix before PLS.\n## The example scales X for consistency with plsdof.\n\nusing Jchemo, JchemoData, JLD2, DataFrames, CairoMakie \nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ozone.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\ndropmissing!(X) \nzX = rmcol(Matrix(X), 4) \ny = X[:, 4] \n## For consistency with plsdof\nxscales = colstd(zX)\nzXs = fscale(zX, xscales)\n## End\n\nnlv = 12 ; gs = true\nres = dfplsr_cg(zXs, y; nlv, gs) ;\nres.df \ndf_kramer = [1.000000, 3.712373, 6.456417, 11.633565, \n    12.156760, 11.715101, 12.349716,\n    12.192682, 13.000000, 13.000000, \n    13.000000, 13.000000, 13.000000]\nf, ax = plotgrid(0:nlv, df_kramer; step = 2, xlabel = \"Nb. LVs\", ylabel = \"df\")\nscatter!(ax, 0:nlv, res.df; color = \"red\")\nablines!(ax, 1, 1; color = :grey, linestyle = :dot)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.difmean-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.difmean","text":"difmean(X1, X2; normx::Bool = false)\n\nCompute a 1-D detrimental matrix by difference of the column-means of two X-datas.\n\nX1 : Spectra (n1, p).\nX2 : Spectra (n2, p).\n\nKeyword arguments:\n\nnormx : Boolean. If true, the column-means vectors    of X1 and X2 are normed before computing their difference.\n\nThe function returns a matrix D (1, p) computed by the difference  between two mean-spectra, i.e. the column-means of X1 and X2. \n\nD is assumed to contain the detrimental information that can  be removed (by orthogonalization) from X1 and X2  for  calibration transfer. For instance, D can be used as input of  function eposvd. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\")\n@load db dat\npnames(dat)\nX1cal = dat.X1cal\nX1val = dat.X1val\nX2cal = dat.X2cal\nX2val = dat.X2val\n\n## The objective is to remove a detrimental \n## information (here, D) from spaces X1 and X2\nD = difmean(X1cal, X2cal).D\nres = eposvd(D; nlv = 1)\n## Corrected Val matrices\nX1val_c = X1val * res.M\nX2val_c = X2val * res.M\n\ni = 1\nf = Figure(size = (800, 300))\nax1 = Axis(f[1, 1])\nax2 = Axis(f[1, 2])\nlines!(ax1, X1val[i, :]; label = \"x1\")\nlines!(ax1, X2val[i, :]; label = \"x2\")\naxislegend(ax1, position = :cb, framevisible = false)\nlines!(ax2, X1val_c[i, :]; label = \"x1_correct\")\nlines!(ax2, X2val_c[i, :]; label = \"x2_correct\")\naxislegend(ax2, position = :cb, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dkplskdeda-Tuple{}","page":"Index of functions","title":"Jchemo.dkplskdeda","text":"dkplskdeda(; kwargs...)\ndkplskdeda(X, y; kwargs...)\ndkplskdeda(X, y, weights::Weight; kwargs...)\n\nDKPLS-KDEDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nKeyword arguments of function dmkern (bandwidth    definition) can also be specified here.\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plskdeda (PLS-KDEDA) except that  a direct kernel PLSR (function dkplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nSee function dkplslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dkplslda-Tuple{}","page":"Index of functions","title":"Jchemo.dkplslda","text":"dkplslda(; kwargs...)\ndkplslda(X, y; kwargs...)\ndkplslda(X, y, weights::Weight; kwargs...)\n\nDKPLS-LDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nSame as function plslda (PLS-LDA) except that  a direct kernel PLSR (function dkplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\ngamma = .1\nmodel = dkplslda(; nlv, gamma) \n#model = dkplslda(; nlv, gamma, prior = :prop) \n#model = dkplsqda(; nlv, gamma, alpha = .5) \n#model = dkplskdeda(; nlv, gamma, a = .5) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nembfitm = fitm.fitm.embfitm ;\n@head embfitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(embfitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dkplsqda-Tuple{}","page":"Index of functions","title":"Jchemo.dkplsqda","text":"dkplsqda(; kwargs...)\ndkplsqda(X, y; kwargs...)\ndkplsqda(X, y, weights::Weight; kwargs...)\n\nDKPLS-QDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plsqda (PLS-QDA) except that  a direct kernel PLSR (function dkplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nSee function dkplslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dkplsr-Tuple{}","page":"Index of functions","title":"Jchemo.dkplsr","text":"dkplsr(; kwargs...)\ndkplsr(X, Y; kwargs...)\ndkplsr(X, Y, weights::Weight; kwargs...)\ndkplsr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nDirect kernel partial least squares regression (DKPLSR) (Bennett & Embrechts 2003).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to consider. \nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nThe method builds kernel Gram matrices and then runs a usual  PLSR algorithm on them. This is faster (but not equivalent) to the  \"true\" KPLSR (Nipals) algorithm (function kplsr) described in  Rosipal & Trejo (2001).\n\nReferences\n\nBennett, K.P., Embrechts, M.J., 2003. An optimization  perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and Applications,  NATO Science Series III: Computer & Systems Sciences.  IOS Press Amsterdam, pp. 227-250.\n\nRosipal, R., Trejo, L.J., 2001. Kernel Partial Least  Squares Regression in Reproducing Kernel Hilbert Space.  Journal of Machine Learning Research 2, 97-123.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 20\nkern = :krbf ; gamma = 1e-1 ; scal = false\n#gamma = 1e-4 ; scal = true\nmodel = dkplsr(; nlv, kern, gamma, scal) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f  \n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nnlv = 2\ngamma = 1 / 3\nmodel = dkplsr(; nlv, gamma) ;\nfit!(model, x, y)\npred = predict(model, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dkplsrda-Tuple{}","page":"Index of functions","title":"Jchemo.dkplsrda","text":"dkplsrda(; kwargs...)\ndkplsrda(X, y; kwargs...)\ndkplsrda(X, y, weights::Weight; kwargs...)\n\nDiscrimination based on direct kernel partial least squares      regression (KPLSR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation.\n\nSame as function plsrda (PLSR-DA) except that  a direct kernel PLSR (function dkplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nkern = :krbf ; gamma = .001 \nscal = true\nmodel = dkplsrda(; nlv, kern, gamma, scal) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\n@head fitm.fitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(fitm.fitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dmkern-Tuple{}","page":"Index of functions","title":"Jchemo.dmkern","text":"dmkern(; kwargs...)\ndmkern(X; kwargs...)\n\nGaussian kernel density estimation (KDE).\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nh : Define the bandwith, see examples.\na : Constant for the Scott's rule (default bandwith),    see thereafter.\n\nEstimation of the probability density of X (column space) by  non parametric Gaussian kernels. \n\nData X can be univariate (p = 1) or multivariate (p > 1).  In the last case, function dmkern computes a multiplicative  kernel such as in Scott & Sain 2005 Eq.19, and the internal bandwidth  matrix H is diagonal (see the code). \n\nNote:  H in the dmkern code is often noted \"H^(1/2)\" in the  litterature (e.g. Wikipedia).\n\nThe default bandwith is computed by:\n\nh = a * n^(-1 / (p + 4)) * colstd(X)\n\n(a = 1 in Scott & Sain 2005).\n\nReferences\n\nScott, D.W., Sain, S.R., 2005. 9 - Multidimensional Density  Estimation, in: Rao, C.R., Wegman, E.J., Solka, J.L. (Eds.),  Handbook of Statistics, Data Mining and Data Visualization.  Elsevier, pp. 229–261.  https://doi.org/10.1016/S0169-7161(04)24009-3\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"iris.jld2\") \n@load db dat\npnames(dat)\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\ntab(y) \n\nnlv = 2\nmodel0 = fda(; nlv)\nfit!(model0, X, y)\n@head T = transf(model0, X)\nn, p = size(T)\n\n#### Probability density in the FDA score space (2D)\n\nmodel = dmkern()\nfit!(model, T) \npnames(model.fitm)\nmodel.fitm.H\nu = [1; 4; 150]\npredict(model, T[u, :]).pred\n\nh = .3\nmodel = dmkern(; h)\nfit!(model, T) \nmodel.fitm.H\npredict(model, T[u, :]).pred\n\nh = [.3; .1]\nmodel = dmkern(; h)\nfit!(model, T) \nmodel.fitm.H\npredict(model, T[u, :]).pred\n\n## Bivariate distribution\nnpoints = 2^7\nnlv = 2\nlims = [(minimum(T[:, j]), maximum(T[:, j])) for j = 1:nlv]\nx1 = LinRange(lims[1][1], lims[1][2], npoints)\nx2 = LinRange(lims[2][1], lims[2][2], npoints)\nz = mpar(x1 = x1, x2 = x2)\ngrid = reduce(hcat, z)\nm = nro(grid)\nmodel = dmkern() \n#model = dmkern(a = .5) \n#model = dmkern(h = .3) \nfit!(model, T) \n\nres = predict(model, grid) ;\npred_grid = vec(res.pred)\nf = Figure(size = (600, 400))\nax = Axis(f[1, 1];  title = \"Density for FDA scores (Iris)\", xlabel = \"Score 1\", \n    ylabel = \"Score 2\")\nco = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10, labels = true)\nscatter!(ax, T[:, 1], T[:, 2], color = :red, markersize = 5)\n#xlims!(ax, -15, 15) ;ylims!(ax, -15, 15)\nf\n\n## Univariate distribution\nx = T[:, 1]\nmodel = dmkern() \n#model = dmkern(a = .5) \n#model = dmkern(h = .3) \nfit!(model, x) \npred = predict(model, x).pred \nf = Figure()\nax = Axis(f[1, 1])\nhist!(ax, x; bins = 30, normalization = :pdf)  # area = 1\nscatter!(ax, x, vec(pred); color = :red)\nf\n\nx = T[:, 1]\nnpoints = 2^8\nlims = [minimum(x), maximum(x)]\n#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]\ngrid = LinRange(lims[1], lims[2], npoints)\nmodel = dmkern() \n#model = dmkern(a = .5) \n#model = dmkern(h = .3) \nfit!(model, x) \npred_grid = predict(model, grid).pred \nf = Figure()\nax = Axis(f[1, 1])\nhist!(ax, x; bins = 30, normalization = :pdf)  # area = 1\nlines!(ax, grid, vec(pred_grid); color = :red)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dmnorm-Tuple{}","page":"Index of functions","title":"Jchemo.dmnorm","text":"dmnorm(; kwargs...)\ndmnorm(X; kwargs...)\ndmnorm!(X::Matrix; kwargs...)\ndmnorm(mu, S; kwargs...)\ndmnorm!(mu::Vector, S::Matrix; kwargs...)\n\nNormal probability density estimation.\n\nX : X-data (n, p) used to estimate the mean mu and    the covariance matrix S. If X is not given,    mu and S must be provided in kwargs.\nmu : Mean vector of the normal distribution. \nS : Covariance matrix of the Normal distribution.\n\nKeyword arguments:\n\nsimpl : Boolean. If true, the constant term and    the determinant in the Normal density formula are set to 1.\n\nData X can be univariate (p = 1) or multivariate (p > 1). See examples.\n\nWhen simple = true, the determinant of the covariance matrix  (object detS) and the constant (2 * pi)^(-p / 2) (object cst)  in the density formula are set to 1. The function returns a pseudo  density that resumes to exp(-d / 2), where d is the squared Mahalanobis  distance to the center mu. This can for instance be useful when the number  of columns (p) of X becomes too large, with the possible consequences that:\n\ndetS tends to 0 or, conversely, to infinity;\ncst tends to 0,\n\nwhich makes impossible to compute the true density. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"iris.jld2\") \n@load db dat\npnames(dat)\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\ntab(y) \n\nnlv = 2\nmodel0 = fda(; nlv)\nfit!(model0, X, y)\n@head T = transf(model0, X)\nn, p = size(T)\n\n#### Probability density in the FDA score space (2D)\n#### Example of class Setosa \ns = y .== \"setosa\"\nzT = T[s, :]\nm = nro(zT)\n\n#### Bivariate distribution\nmodel = dmnorm()\nfit!(model, zT)\nfitm = model.fitm\npnames(fitm)\nfitm.Uinv \nfitm.detS\n@head pred = predict(model, zT).pred\n\n## Direct syntax\nmu = colmean(zT)\nS = covm(zT, mweight(ones(m))) * m / (m - 1) # corrected cov. matrix\nfitm = dmnorm(mu, S) ; \npnames(fitm)\nfitm.Uinv\nfitm.detS\n\nnpoints = 2^7\nlims = [(minimum(zT[:, j]), maximum(zT[:, j])) for j = 1:nlv]\nx1 = LinRange(lims[1][1], lims[1][2], npoints)\nx2 = LinRange(lims[2][1], lims[2][2], npoints)\nz = mpar(x1 = x1, x2 = x2)\ngrid = reduce(hcat, z)\nmodel = dmnorm()\nfit!(model, zT)\nres = predict(model, grid) ;\npred_grid = vec(res.pred)\nf = Figure(size = (600, 400))\nax = Axis(f[1, 1];  title = \"Density for FDA scores (Iris - Setosa)\", \n    xlabel = \"Score 1\", ylabel = \"Score 2\")\nco = contour!(ax, grid[:, 1], grid[:, 2], pred_grid; levels = 10, labels = true)\nscatter!(ax, T[:, 1], T[:, 2], color = :red, markersize = 5)\nscatter!(ax, zT[:, 1], zT[:, 2], color = :blue, markersize = 5)\n#xlims!(ax, -12, 12) ;ylims!(ax, -12, 12)\nf\n\n#### Univariate distribution\nj = 1\nx = zT[:, j]\nmodel = dmnorm()\nfit!(model, x)\npred = predict(model, x).pred \nf = Figure()\nax = Axis(f[1, 1]; xlabel = string(\"FDA-score \", j))\nhist!(ax, x; bins = 30, normalization = :pdf)  # area = 1\nscatter!(ax, x, vec(pred); color = :red)\nf\n\nx = zT[:, j]\nnpoints = 2^8\nlims = [minimum(x), maximum(x)]\n#delta = 5 ; lims = [minimum(x) - delta, maximum(x) + delta]\ngrid = LinRange(lims[1], lims[2], npoints)\nmodel = dmnorm()\nfit!(model, x)\npred_grid = predict(model, grid).pred \nf = Figure()\nax = Axis(f[1, 1]; xlabel = string(\"FDA-score \", j))\nhist!(ax, x; bins = 30, normalization = :pdf)  # area = 1\nlines!(ax, grid, vec(pred_grid); color = :red)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dmnormlog-Tuple{}","page":"Index of functions","title":"Jchemo.dmnormlog","text":"dmnormlog(; kwargs...)\ndmnormlog(X; kwargs...)\ndmnormlog!(X::Matrix; kwargs...)\ndmnormlog(mu, S; kwargs...)\ndmnormlog!(mu::Vector, S::Matrix; kwargs...)\n\nLogarithm of the normal probability density estimation.     * X : X-data (n, p) used to estimate the mean mu and          the covariance matrix S. If X is not given,          mu and S must be provided in kwargs.     * mu : Mean vector of the normal distribution.      * S : Covariance matrix of the Normal distribution. Keyword arguments:     * simpl : Boolean. If true, the constant term and          the determinant in the Normal density formula are set to 1.\n\nSee the help page of function dmnorm.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"iris.jld2\") \n@load db dat\npnames(dat)\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\ntab(y) \n\n## Example of class Setosa \ns = y .== \"setosa\"\nzX = X[s, :]\n\nmodel = dmnormlog()\nfit!(model, zX)\nfitm = model.fitm\npnames(fitm)\nfitm.Uinv \nfitm.logdetS\n@head pred = predict(model, zX).pred\n\n## Consistency with dmnorm\nmodel0 = dmnorm()\nfit!(model0, zX)\n@head pred0 = predict(model0, zX).pred\n@head log.(pred0)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dummy-Tuple{Any}","page":"Index of functions","title":"Jchemo.dummy","text":"dummy(y)\n\nCompute dummy table from a categorical variable.\n\ny : A categorical variable.\n\nThe output Y (dummy table) is a BitMatrix.\n\nExamples\n\nusing Jchemo\n\ny = [\"d\", \"a\", \"b\", \"c\", \"b\", \"c\"]\n#y =  rand(1:3, 7)\nres = dummy(y)\npnames(res)\nres.Y\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dupl-Tuple{Any}","page":"Index of functions","title":"Jchemo.dupl","text":"dupl(X; digits = 3)\n\nFind duplicated rows in a dataset.\n\nX : A dataset.\ndigits : Nb. digits used to round X before checking.\n\nExamples\n\nusing Jchemo\n\nX = rand(5, 3)\nZ = vcat(X, X[1:3, :], X[1:1, :])\ndupl(X)\ndupl(Z)\n\nM = hcat(X, fill(missing, 5))\nZ = vcat(M, M[1:3, :])\ndupl(M)\ndupl(Z)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}","page":"Index of functions","title":"Jchemo.ensure_df","text":"ensure_df(X)\n\nReshape X to a dataframe if necessary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ensure_mat-Tuple{AbstractMatrix}","page":"Index of functions","title":"Jchemo.ensure_mat","text":"ensure_mat(X)\n\nReshape X to a matrix if necessary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.eposvd-Tuple{Any}","page":"Index of functions","title":"Jchemo.eposvd","text":"eposvd(D; nlv = 1)\n\nCompute an orthogonalization matrix for calibration transfer      of spectral data.\n\nD : Data (m, p) containing the detrimental information    on which spectra (rows of a matrix X) have to be orthogonalized.\n\nKeyword arguments:\n\nnlv : Nb. of first loadings vectors of D considered for the    orthogonalization.\n\nThe objective is to remove some detrimental information  (e.g. humidity patterns in signals, multiple spectrometers, etc.)  from a X-dataset (n, p).  The detrimental information is defined  by the main row-directions computed from a matrix D (m, p). \n\nFunction eposvd returns two objects:\n\nV (p, nlv) : The matrix of the nlv first    loading vectors of the SVD decomposition (non centered PCA)    of D. \nM (p, p) : The orthogonalization matrix, used    to orthogonolize a given matrix X to directions    contained in V.\n\nAny matrix X can then be corrected from D by:\n\nX_corrected = X * M.\n\nMatrix D can be built from many methods. For instance,      two common methods are:\n\nEPO (Roger et al. 2003, 2018): D is built from a set of    differences between spectra collected under different    conditions. \nTOP (Andrew & Fearn 2004): Each row of D is the mean spectrum    computed for a given spectrometer instrument.\n\nA particular situation is the following. Assume that D is  built from some differences between matrices X1 and X2, and that  a bilinear model (e.g. PLSR) is fitted on the data {X1corrected, Y} where X1corrected = X1 * M. To predict new data X2new with  the fitted model, there is no need to correct X2new.\n\nReferences\n\nAndrew, A., Fearn, T., 2004. Transfer by orthogonal projection:  making near-infrared calibrations robust to between-instrument  variation. Chemometrics and Intelligent Laboratory Systems 72,  51–56. https://doi.org/10.1016/j.chemolab.2004.02.004\n\nRoger, J.-M., Chauchard, F., Bellon-Maurel, V., 2003. EPO-PLS  external parameter orthogonalisation of PLS application to  temperature-independent measurement of sugar content of intact  fruits. Chemometrics and Intelligent Laboratory Systems 66,  191-204. https://doi.org/10.1016/S0169-7439(03)00051-0\n\nRoger, J.-M., Boulet, J.-C., 2018. A review of orthogonal  projections for calibration. Journal of Chemometrics 32, e3045.  https://doi.org/10.1002/cem.3045\n\nZeaiter, M., Roger, J.M., Bellon-Maurel, V., 2006. Dynamic  orthogonal projection. A new method to maintain the on-line  robustness of multivariate calibrations. Application to NIR-based  monitoring of wine fermentations. Chemometrics and Intelligent  Laboratory Systems, 80, 227–235.  https://doi.org/10.1016/j.chemolab.2005.06.011\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\")\n@load db dat\npnames(dat)\nX1cal = dat.X1cal\nX1val = dat.X1val\nX2cal = dat.X2cal\nX2val = dat.X2val\n\n## The objective is to remove a detrimental \n## information (here, D) from spaces X1 and X2\nD = X1cal - X2cal\nnlv = 2\nres = eposvd(D; nlv)\nres.M # orthogonalization matrix\nres.V # detrimental directions (columns of matrix V = loadings of D)\n\n## Corrected Val matrices\nX1val_c = X1val * res.M\nX2val_c = X2val * res.M\n\ni = 1\nf = Figure(size = (800, 300))\nax1 = Axis(f[1, 1])\nax2 = Axis(f[1, 2])\nlines!(ax1, X1val[i, :]; label = \"x1\")\nlines!(ax1, X2val[i, :]; label = \"x2\")\naxislegend(ax1, position = :cb, framevisible = false)\nlines!(ax2, X1val_c[i, :]; label = \"x1_correct\")\nlines!(ax2, X2val_c[i, :]; label = \"x2_correct\")\naxislegend(ax2, position = :cb, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.errp-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.errp","text":"errp(pred, y)\n\nCompute the classification error rate (ERRP).\n\npred : Predictions.\ny : Observed data (class membership).\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nytrain = rand([\"a\" ; \"b\"], 10)\nXtest = rand(4, 5) \nytest = rand([\"a\" ; \"b\"], 4)\n\nmodel = plsrda(; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nerrp(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.euclsq-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.euclsq","text":"euclsq(X, Y)\n\nSquared Euclidean distances between the rows of X and Y.\n\nX : Data (n, p).\nY : Data (m, p).\n\nFor X(n, p) and Y (m, p), the function returns  an object (n, m) with:\n\ni, j = distance between row i of X and row j of Y.\n\nExamples\n\nX = rand(5, 3)\nY = rand(2, 3)\n\neuclsq(X, Y)\n\neuclsq(X[1:1, :], Y[1:1, :])\n\neuclsq(X[:, 1], 4)\neuclsq(1, 4)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fcenter-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.fcenter","text":"fcenter(X, v)\nfcenter!(X::AbstractMatrix, v)\n\nCenter each column of a matrix.\n\nX : Data (n, p).\nv : Centering vector (p).\n\nexamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nxmeans = colmean(X)\nfcenter(X, xmeans)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fcscale-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.fcscale","text":"fcscale(X, u, v)\nfcscale!(X, u, v)\n\nCenter and fscale each column of a matrix.\n\nX : Data  (n, p).\nu : Centering vector (p).\nv : Scaling vector (p).\n\nexamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nxmeans = colmean(X)\nxscales = colstd(X)\nfcscale(X, xmeans, xscales)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fda-Tuple{}","page":"Index of functions","title":"Jchemo.fda","text":"fda(; kwargs...)\nfda(X, y; kwargs...)\nfda(X, y, weights; kwargs...)\nfda!(X::Matrix, y, weights; kwargs...)\n\nFactorial discriminant analysis (FDA).\n\nX : X-data (n, p).\ny : y-data (n) (class membership).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of discriminant components.\nlb : Ridge regularization parameter \"lambda\".   Can be used when X has collinearities. \nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nFDA by eigen factorization of Inverse(W) * B, where W is  the \"Within\"-covariance matrix (pooled over the classes),  and B the \"Between\"-covariance matrix.\n\nThe function maximizes the consensus:\n\np'Bp / p'Wp \n\ni.e. max p'Bp with constraint p'Wp = 1. Vectors p (columns of V)  are the linear discrimant coefficients often referred to as \"LD\".\n\nIf X is ill-conditionned, a ridge regularization can be used:\n\nIf lb > 0, W is replaced by W + lb * I,    where I is the Idendity matrix.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie \npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest) \nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\ntab(ytrain)\ntab(ytest)\n\nnlv = 2\nmodel = fda(; nlv)\n#model = fdasvd(; nlv)\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nlev = fitm.lev\nnlev = length(lev)\naggsum(fitm.weights.w, ytrain)\n\n@head fitm.T \n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n\n## X-loadings matrix\n## = coefficients of the linear discriminant function\n## = \"LD\" of function lda of the R package MASS\nfitm.V\nfitm.V' * fitm.V\n\n## Explained variance computed by weighted PCA \n## of the class centers in transformed scale\nsummary(model).explvarx\n\n## Projections of the class centers \n## to the score space\nct = fitm.Tcenters \nf, ax = plotxy(fitm.T[:, 1], fitm.T[:, 2], ytrain; ellipse = true, title = \"FDA\",\n    xlabel = \"Score-1\", ylabel = \"Score-2\")\nscatter!(ax, ct[:, 1], ct[:, 2], marker = :star5, markersize = 15, color = :red)  # see available_marker_symbols()\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fdasvd-Tuple{}","page":"Index of functions","title":"Jchemo.fdasvd","text":"fdasvd(; kwargs...)\nfdasvd(X, y, weights; kwargs...)\nfdasvd!(X::Matrix, y, weights; kwargs...)\n\nFactorial discriminant analysis (FDA).\n\nX : X-data (n, p).\ny : y-data (n) (class membership).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of discriminant components.\nlb : Ridge regularization parameter \"lambda\".   Can be used when X has collinearities. \nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nFDA by a weighted SVD factorization of the matrix of the class centers  (after spherical transformaton). The function gives the same results as  function fda.\n\nSee function fda for details and examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fdif-Tuple{}","page":"Index of functions","title":"Jchemo.fdif","text":"fdif(; kwargs...)\nfdif(X; kwargs...)\n\nFinite differences (discrete derivates) for each row of X-data. \n\nX : X-data (n, p).\n\nKeyword arguments:\n\nnpoint : Nb. points involved in the window for the    finite differences. The range of the window    (= nb. intervals of two successive colums) is npoint - 1.\n\nThe method reduces the column-dimension: \n\n(n, p) –> (n, p - npoint + 1). \n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = fdif(npoint = 2) \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.findmax_cla-Tuple{Any}","page":"Index of functions","title":"Jchemo.findmax_cla","text":"findmax_cla(x)\nfindmax_cla(x, weights::Weight)\n\nFind the most occurent level in x.\n\nx : A categorical variable.\nweights : Weights (n) of the observations. Object of type    Weight (e.g. generated by function mweight).\n\nIf ex-aequos, the function returns the first.\n\nExamples\n\nusing Jchemo\n\nx = rand(1:3, 10)\ntab(x)\nfindmax_cla(x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.findmiss-Tuple{Any}","page":"Index of functions","title":"Jchemo.findmiss","text":"findmiss(X)\n\nFind rows with missing data in a dataset.\n\nX : A dataset.\n\nFor dataframes, see also DataFrames.completecases and DataFrames.dropmissing.\n\nExamples\n\nusing Jchemo\n\nX = rand(5, 4)\nzX = hcat(rand(2, 3), fill(missing, 2))\nZ = vcat(X, zX)\nfindmiss(X)\nfindmiss(Z)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.frob-Tuple{Any}","page":"Index of functions","title":"Jchemo.frob","text":"frob(X)\nfrob(X, weights::Weight)\nfrob2(X)\nfrob2(X, weights::Weight)\n\nFrobenius norm of a matrix.\n\nX : A matrix (n, p).\nweights : Weights (n) of the observations. Object of type    Weight (e.g. generated by function mweight).\n\nThe Frobenius norm of X is:\n\nsqrt(tr(X' * X)).\n\nThe Frobenius weighted norm is:\n\nsqrt(tr(X' * D * X)), where D is the diagonal matrix of vector w.\n\nFunctions frob2 are the squared versions of frob.\n\nReferences\n\n@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fscale-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.fscale","text":"fscale(X, v)\nfscale!(X::AbstractMatrix, v)\n\nScale each column of a matrix.\n\nX : Data (n, p).\nv : Scaling vector (p).\n\nExamples\n\nusing Jchemo\n\nX = rand(5, 2) \nfscale(X, colstd(X))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fweight-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.fweight","text":"fweight(X, v)\nfweight!(X::AbstractMatrix, v)\n\nWeight each row of a matrix.\n\nX : Data (n, p).\nv : A weighting vector (n).\n\nExamples\n\nusing Jchemo, LinearAlgebra\n\nX = rand(5, 2) \nw = rand(5) \nfweight(X, w)\ndiagm(w) * X\n\nfweight!(X, w)\nX\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.getknn-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.getknn","text":"getknn(Xtrain, X; metric = :eucl, k = 1)\n\nReturn the k nearest neighbors in Xtrain of each row of the query X.\n\nXtrain : Training X-data.\nX : Query X-data.\n\nKeyword arguments:\n\nmetric : Type of distance used for the query.    Possible values are :eucl (Euclidean),   :mah (Mahalanobis), :sam (spectral angular distance),   :cor (correlation distance).\nk : Number of neighbors to return.\n\nThe distances (not squared) are also returned.\n\nSpectral angular and correlation distances between two vectors x and y:\n\nSpectral angular distance (x, y) = acos(x'y / normv(x)normv(y)) / pi\nCorrelation distance (x, y) = sqrt((1 - cor(x, y)) / 2)\n\nBoth distances are bounded within 0 (y = x) and 1 (y = -x).\n\nExamples\n\nusing Jchemo\nXtrain = rand(5, 3)\nX = rand(2, 3)\nx = X[1:1, :]\n\nk = 3\nres = getknn(Xtrain, X; k)\nres.ind  # indexes\nres.d    # distances\n\nres = getknn(Xtrain, x; k)\nres.ind\n\nres = getknn(Xtrain, X; metric = :mah, k)\nres.ind\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcv-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.gridcv","text":"gridcv(model, X, Y; segm, score, pars = nothing, nlv = nothing, lb = nothing, \n    verbose = false)\n\nCross-validation (CV) of a model over a grid of parameters.\n\nmodel : Model to evaluate.\nX : Training X-data (n, p).\nY : Training Y-data (n, q).\n\nKeyword arguments: \n\nsegm : Segments of observations used for    the CV (output of functions segmts,    segmkf, etc.).\nscore : Function computing the prediction    score (e.g. rmsep).\npars : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function mpar).\nverbose : If true, predicting information are printed.\nnlv : Value, or vector of values, of the nb. of latent   variables (LVs).\nlb : Value, or vector of values, of the ridge    regularization parameter \"lambda\".\n\nThe function is used for grid-search: it computed a prediction score  (= error rate) for model model over the combinations of parameters  defined in pars. \n\nFor models based on LV or ridge regularization, using arguments nlv  and lb allow faster computations than including these parameters in  argument `pars. See the examples.   \n\nThe function returns two outputs: \n\nres : mean results\nres_p : results per replication.\n\nExamples\n\n######## Regression\n\nusing JLD2, CairoMakie, JchemoData\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\nmodel = savgol(npoint = 21, deriv = 2, degree = 2)\nfit!(model, X)\nXp = transf(model, X)\ns = year .<= 2012\nXtrain = Xp[s, :]\nytrain = y[s]\nXtest = rmrow(Xp, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\n## Replicated K-fold CV \nK = 3 ; rep = 10\nsegm = segmkf(ntrain, K; rep)\n## Replicated test-set validation\n#m = Int(round(ntrain / 3)) ; rep = 30\n#segm = segmts(ntrain, m; rep)\n\n####-- Plsr\nmodel = plskern()\nnlv = 0:30\nrescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, nlv) ;\npnames(rescv)\nres = rescv.res \nplotgrid(res.nlv, res.y1; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = plskern(; nlv = res.nlv[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n## Adding pars \npars = mpar(scal = [false; true])\nrescv = gridcv(model, Xtrain, ytrain; segm,  score = rmsep, pars, nlv) ;\nres = rescv.res \ntyp = res.scal\nplotgrid(res.nlv, res.y1, typ; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = plskern(nlv = res.nlv[u], scal = res.scal[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n####-- Rr \nlb = (10).^(-8:.1:3)\nmodel = rr() \nrescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, lb) ;\nres = rescv.res \nloglb = log.(10, res.lb)\nplotgrid(loglb, res.y1; step = 2, xlabel = \"log(lambda)\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = rr(lb = res.lb[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f     \n    \n## Adding pars \npars = mpar(scal = [false; true])\nrescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, lb) ;\nres = rescv.res \nloglb = log.(10, res.lb)\ntyp = string.(res.scal)\nplotgrid(loglb, res.y1, typ; step = 2, xlabel = \"log(lambda)\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = rr(lb = res.lb[u], scal = res.scal[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n####-- Kplsr \nmodel = kplsr()\nnlv = 0:30\ngamma = (10).^(-5:1.:5)\npars = mpar(gamma = gamma)\nrescv = gridcv(model, Xtrain, ytrain; segm,  score = rmsep, pars, nlv) ;\nres = rescv.res \nloggamma = round.(log.(10, res.gamma), digits = 1)\nplotgrid(res.nlv, res.y1, loggamma; step = 2, xlabel = \"Nb. LVs\",  ylabel = \"RMSEP\", \n    leg_title = \"Log(gamma)\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = kplsr(nlv = res.nlv[u], gamma = res.gamma[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n####-- Knnr \nnlvdis = [15, 25] ; metric = [:mah]\nh = [1, 2.5, 5]\nk = [1; 5; 10; 20; 50 ; 100] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nlength(pars[1]) \nmodel = knnr()\nrescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, verbose = true) ;\nres = rescv.res \nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = knnr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], \n    k = res.k[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n####-- Lwplsr \nnlvdis = 15 ; metric = [:mah]\nh = [1, 2, 5] ; k = [200, 350, 500] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nlength(pars[1]) \nnlv = 0:20\nmodel = lwplsr()\nrescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, nlv, verbose = true) ;\nres = rescv.res \ngroup = string.(\"h=\", res.h, \" k=\", res.k)\nplotgrid(res.nlv, res.y1, group; xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], \n    h = res.h[u], k = res.k[u], nlv = res.nlv[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n####-- LwplsrAvg \nnlvdis = 15 ; metric = [:mah]\nh = [1, 2, 5] ; k = [200, 350, 500] \nnlv = [0:20, 5:20] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k, nlv = nlv)\nlength(pars[1]) \nmodel = lwplsravg()\nrescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, verbose = true) ;\nres = rescv.res \nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = lwplsravg(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], \n    k = res.k[u], nlv = res.nlv[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f     \n\n######## Discrimination\n## The principle is the same as for regression\n\nusing JLD2, CairoMakie, JchemoData\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\ntab(Y.typ)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\n## Replicated K-fold CV \nK = 3 ; rep = 10\nsegm = segmkf(ntrain, K; rep)\n## Replicated test-set validation\n#m = Int(round(ntrain / 3)) ; rep = 30\n#segm = segmts(ntrain, m; rep)\n\n####-- Plslda\nmodel = plslda()\nnlv = 1:30\nprior = [:unif; :prop]\npars = mpar(prior = prior)\nrescv = gridcv(model, Xtrain, ytrain; segm, score = errp, pars, nlv)\nres = rescv.res\ntyp = res.prior\nplotgrid(res.nlv, res.y1, typ; step = 2, xlabel = \"Nb. LVs\", ylabel = \"ERR\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = plslda(nlv = res.nlv[u], prior = res.prior[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show errp(pred, ytest)\nconf(pred, ytest).pct\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcv_br-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcv_br","text":"gridcv_br(X, Y; segm, algo, score, pars, verbose = false)\n\nWorking function for gridcv.\n\nSee function gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcv_lb-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcv_lb","text":"gridcv_lb(X, Y; segm, algo, score, pars = nothing, lb, verbose = false)\n\nWorking function for gridcv.\n\nSpecific and faster than gridcv_br for models  using ridge regularization (e.g. RR). Argument pars  must not contain nlv.\n\nSee function gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcv_lv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcv_lv","text":"gridcv_lv((X, Y; segm, algo, score, pars = nothing, nlv, verbose = false)\n\nWorking function for gridcv.\n\nSpecific and faster than gridcv_br for models  using latent variables (e.g. PLSR). Argument pars  must not contain nlv.\n\nSee function gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscore-NTuple{5, Any}","page":"Index of functions","title":"Jchemo.gridscore","text":"gridscore(model, Xtrain, Ytrain, X, Y; score, pars = nothing, nlv = nothing, \n    lb = nothing, verbose = false)\n\nTest-set validation of a model over a grid of parameters.\n\nmodel : Model to evaluate.\nXtrain : Training X-data (n, p).\nYtrain : Training Y-data (n, q).\nX : Validation X-data (m, p).\nY : Validation Y-data (m, q).\n\nKeyword arguments: \n\nscore : Function computing the prediction    score (e.g. rmsep).\npars : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function mpar).\nverbose : If true, predicting information are printed.\nnlv : Value, or vector of values, of the nb. of latent   variables (LVs).\nlb : Value, or vector of values, of the ridge    regularization parameter \"lambda\".\n\nThe function is used for grid-search: it computed a prediction score  (= error rate) for model model over the combinations of parameters  defined in pars. The score is computed over sets {X,Y`}. \n\nFor models based on LV or ridge regularization, using arguments nlv  and lb allow faster computations than including these parameters in  argument `pars. See the examples.   \n\nExamples\n\n######## Regression \n\nusing JLD2, CairoMakie, JchemoData\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\nmodel = savgol(npoint = 21, deriv = 2, degree = 2)\nfit!(model, X)\nXp = transf(model, X)\ns = year .<= 2012\nXtrain = Xp[s, :]\nytrain = y[s]\nXtest = rmrow(Xp, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n## Building Cal and Val \n## within Train\nnval = Int(round(.3 * ntrain))\ns = samprand(ntrain, nval)\nXcal = Xtrain[s.train, :]\nycal = ytrain[s.train]\nXval = Xtrain[s.test, :]\nyval = ytrain[s.test]\n\n####-- Plsr\nmodel = plskern()\nnlv = 0:30\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, nlv)\nplotgrid(res.nlv, res.y1; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = plskern(nlv = res.nlv[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n## Adding pars \npars = mpar(scal = [false; true])\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv)\ntyp = res.scal\nplotgrid(res.nlv, res.y1, typ; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = plskern(nlv = res.nlv[u], scal = res.scal[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####-- Rr \nlb = (10).^(-8:.1:3)\nmodel = rr() \nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, lb)\nloglb = log.(10, res.lb)\nplotgrid(loglb, res.y1; step = 2, xlabel = \"log(lambda)\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = rr(lb = res.lb[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n    \n## Adding pars \npars = mpar(scal = [false; true])\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, lb)\nloglb = log.(10, res.lb)\ntyp = string.(res.scal)\nplotgrid(loglb, res.y1, typ; step = 2, xlabel = \"log(lambda)\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = rr(lb = res.lb[u], scal = res.scal[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####-- Kplsr \nmodel = kplsr()\nnlv = 0:30\ngamma = (10).^(-5:1.:5)\npars = mpar(gamma = gamma)\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv)\nloggamma = round.(log.(10, res.gamma), digits = 1)\nplotgrid(res.nlv, res.y1, loggamma; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\",\n    leg_title = \"Log(gamma)\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = kplsr(nlv = res.nlv[u], gamma = res.gamma[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####-- Knnr \nnlvdis = [15; 25] ; metric = [:mah]\nh = [1, 2.5, 5]\nk = [1, 5, 10, 20, 50, 100] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nlength(pars[1]) \nmodel = knnr()\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = knnr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], \n    k = res.k[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####-- Lwplsr \nnlvdis = 15 ; metric = [:mah]\nh = [1, 2, 5] ; k = [200, 350, 500] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nlength(pars[1]) \nnlv = 0:20\nmodel = lwplsr()\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv, verbose = true)\ngroup = string.(\"h=\", res.h, \" k=\", res.k)\nplotgrid(res.nlv, res.y1, group; xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], \n    k = res.k[u], nlv = res.nlv[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####-- LwplsrAvg \nnlvdis = 15 ; metric = [:mah]\nh = [1, 2, 5] ; k = [200, 350, 500] \nnlv = [0:20, 5:20] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k, nlv = nlv)\nlength(pars[1]) \nmodel = lwplsravg()\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = lwplsravg(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], \n    k = res.k[u], nlv = res.nlv[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n####-- Mbplsr\nlistbl = [1:525, 526:1050]\nXbltrain = mblock(Xtrain, listbl)\nXbltest = mblock(Xtest, listbl) \nXbl_cal = mblock(Xcal, listbl) \nXbl_val = mblock(Xval, listbl) \n\nmodel = mbplsr()\nbscal = [:none, :frob]\npars = mpar(bscal = bscal) \nnlv = 0:30\nres = gridscore(model, Xbl_cal, ycal, Xbl_val, yval; score = rmsep, pars, nlv)\ngroup = res.bscal \nplotgrid(res.nlv, res.y1, group; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = mbplsr(bscal = res.bscal[u], nlv = res.nlv[u])\nfit!(model, Xbltrain, ytrain)\npred = predict(model, Xbltest).pred\n@show rmsep(pred, ytest)\nplotxy(vec(pred), ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n    \n######## Discrimination\n## The principle is the same as for regression\n\nusing JLD2, CairoMakie, JchemoData\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\ntab(Y.typ)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n## Building Cal and Val \n## within Train\nnval = Int(round(.3 * ntrain))\ns = samprand(ntrain, nval)\nXcal = Xtrain[s.train, :]\nycal = ytrain[s.train]\nXval = Xtrain[s.test, :]\nyval = ytrain[s.test]\n\n####-- Plslda\nmodel = plslda()\nnlv = 1:30\nprior = [:unif, :prop]\npars = mpar(prior = prior)\nres = gridscore(model, Xcal, ycal, Xval, yval; score = errp, pars, nlv)\ntyp = res.prior\nplotgrid(res.nlv, res.y1, typ; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel = plslda(nlv = res.nlv[u], prior = res.prior[u])\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\n@show errp(pred, ytest)\nconf(pred, ytest).pct\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscore-Tuple{Jchemo.Pipeline, Any, Any, Any, Any}","page":"Index of functions","title":"Jchemo.gridscore","text":"gridscore(model::Pipeline, Xtrain, Ytrain, X, Y; score, pars = nothing, \n    nlv = nothing, lb = nothing, verbose = false)\n\nTest-set validation of a model pipeline over a grid of parameters.\n\nmodel : A pipeline of models to evaluate.\nXtrain : Training X-data (n, p).\nYtrain : Training Y-data (n, q).\nX : Validation X-data (m, p).\nY : Validation Y-data (m, q).\n\nKeyword arguments: \n\nscore : Function computing the prediction    score (e.g. rmsep).\npars : tuple of named vectors of same length defining    the parameter combinations (e.g. output of function mpar).\nverbose : If true, predicting information are printed.\nnlv : Value, or vector of values, of the nb. of latent   variables (LVs).\nlb : Value, or vector of values, of the ridge    regularization parameter \"lambda\".\n\nIn the present version of the function, only the last model  of the pipeline (= the final predictor) is validated.\n\nFor other details, see function gridscore for simple models. \n\nExamples\n\nusing JLD2, CairoMakie, JchemoData\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n## Building Cal and Val \n## within Train\nnval = Int(round(.3 * ntrain))\ns = samprand(ntrain, nval)\nXcal = Xtrain[s.train, :]\nycal = ytrain[s.train]\nXval = Xtrain[s.test, :]\nyval = ytrain[s.test]\n\n####-- Pipeline Snv :> Savgol :> Plsr\n## Only the last model is validated\n## model1\nmodel1 = snv()\n## model2 \nnpoint = 11 ; deriv = 2 ; degree = 3\nmodel2 = savgol(; npoint, deriv, degree)\n## model3\nnlv = 0:30\nmodel3 = plskern()\n##\nmodel = pip(model1, model2, model3)\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, nlv) ;\nplotgrid(res.nlv, res.y1; step = 2, xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel3 = plskern(nlv = res.nlv[u])\nmodel = pip(model1, model2, model3)\nfit!(model, Xtrain, ytrain)\nres = predict(model, Xtest) ; \n@head res.pred \nrmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",\n      ylabel = \"Observed\").f\n\n####-- Pipeline Pca :> Svmr\n## Only the last model is validated\n## model1\nnlv = 15 ; scal = true\nmodel1 = pcasvd(; nlv, scal)\n## model2\nkern = [:krbf]\ngamma = (10).^(-5:1.:5)\ncost = (10).^(1:3)\nepsilon = [.1, .2, .5]\npars = mpar(kern = kern, gamma = gamma, cost = cost, epsilon = epsilon)\nmodel2 = svmr()\n##\nmodel = pip(model1, model2)\nres = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, verbose = true)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nmodel2 = svmr(kern = res.kern[u], gamma = res.gamma[u], cost = res.cost[u], epsilon = res.epsilon[u])\nmodel = pip(model1, model2) \nfit!(model, Xtrain, ytrain)\nres = predict(model, Xtest) ; \n@head res.pred \nrmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",\n      ylabel = \"Observed\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscore_br-NTuple{4, Any}","page":"Index of functions","title":"Jchemo.gridscore_br","text":"gridscore_br(Xtrain, Ytrain, X, Y; algo, score, pars, \n    verbose = false)\n\nWorking function for gridscore.\n\nSee function gridscore for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscore_lb-NTuple{4, Any}","page":"Index of functions","title":"Jchemo.gridscore_lb","text":"gridscore_lb(Xtrain, Ytrain, X, Y; algo, score, pars = nothing, \n    lb, verbose = false)\n\nWorking function for gridscore.\n\nSpecific and faster than gridscore_br for models  using ridge regularization (e.g. RR). Argument pars  must not contain lb.\n\nSee function gridscore for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscore_lv-NTuple{4, Any}","page":"Index of functions","title":"Jchemo.gridscore_lv","text":"gridscore_lv(Xtrain, Ytrain, X, Y; algo, score, pars = nothing, \n    nlv, verbose = false)\n\nWorking function for gridscore.\n\nSpecific and faster than gridscore_br for models  using latent variables (e.g. PLSR). Argument pars  must not contain nlv.\n\nSee function gridscore for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.head-Tuple{Any}","page":"Index of functions","title":"Jchemo.head","text":"head(X)\n\nDisplay the first rows of a dataset.\n\nExamples\n\nusing Jchemo\n\nX = rand(100, 5)\nhead(X)\n@head X\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.interpl-Tuple{}","page":"Index of functions","title":"Jchemo.interpl","text":"interpl(; kwargs...)\ninterpl(X; kwargs...)\n\nSampling spectra by interpolation.\n\nX : Matrix (n, p) of spectra (rows).\n\nKeyword arguments:\n\nwl : Values representing the column \"names\" of X.    Must be a numeric vector of length p, or an AbstractRange,   with growing values.\nwlfin : Final values (within the range of wl) where to interpolate   each spectrum. Must be a numeric vector, or an AbstractRange,   with growing values.\n\nThe function implements a cubic spline interpolation using  package DataInterpolations.jl.\n\nReferences\n\nPackage DAtaInterpolations.jl https://github.com/PumasAI/DataInterpolations.jl https://htmlpreview.github.io/?https://github.com/PumasAI/DataInterpolations.jl/blob/v2.0.0/example/DataInterpolations.html\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nwlfin = range(500, 2400, length = 10)\n#wlfin = collect(range(500, 2400, length = 10))\nmodel = interpl(; wl, wlfin)\nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.iqrv-Tuple{Any}","page":"Index of functions","title":"Jchemo.iqrv","text":"iqrv(x)\n\nCompute the interquartile interval (IQR) of a vector.\n\nx : A vector (n).\n\nExamples\n\nx = rand(100)\niqrv(x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.isel!","page":"Index of functions","title":"Jchemo.isel!","text":"isel!(model, X, Y, wl = 1:nco(X); rep = 1, nint = 5, psamp = .3, score = rmsep)\n\nInterval variable selection.\n\nmodel : Model to evaluate.\nX : X-data (n, p).\nY : Y-data (n, q).\nwl : Optional numeric labels (p, 1) of the X-columns.\n\nKeyword arguments:  \n\nrep : Number of replications of the splitting   training/test. \nnint : Nb. intervals. \npsamp : Proportion of data used as test set    to compute the score.\nscore : Function computing the prediction score.\n\nThe principle is as follows:\n\nData (X, Y) are splitted randomly to a training and a test set.\nRange 1:p in X is segmented to nint intervals, when possible    of equal size. \nThe model is fitted on the training set and the score (error rate)    on the test set, firtsly accounting for all the p variables    (reference) and secondly for each of the nint intervals. \nThis process is replicated rep times. Average results are provided    in the outputs, as well the results per replication. \n\nReferences\n\nNørgaard, L., Saudland, A., Wagner, J., Nielsen, J.V., Munck, L., \n\nEngelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419.  https://doi.org/10.1366/0003702001949500\n\nExamples\n\nusing Jchemo, JchemoData, DataFrames, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"tecator.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y \nwl_str = names(X)\nwl = parse.(Float64, wl_str) \nntot, p = size(X)\ntyp = Y.typ\nnamy = names(Y)[1:3]\nplotsp(X, wl; xlabel = \"Wavelength (nm)\", ylabel = \"Absorbance\").f\n\ns = typ .== \"train\"\nXtrain = X[s, :]\nYtrain = Y[s, namy]\nXtest = rmrow(X, s)\nYtest = rmrow(Y[:, namy], s)\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\n## Work on the j-th y-variable \nj = 2\nnam = namy[j]\nytrain = Ytrain[:, nam]\nytest = Ytest[:, nam]\n\nmodel = plskern(nlv = 5)\nnint = 10\nres = isel!(model, Xtrain, ytrain, wl; rep = 30, nint) ;\nres.res_rep\nres.res0_rep\nzres = res.res\nzres0 = res.res0\nf = Figure(size = (650, 300))\nax = Axis(f[1, 1], xlabel = \"Wawelength (nm)\", ylabel = \"RMSEP_Val\",\n    xticks = zres.lo)\nscatter!(ax, zres.mid, zres.y1; color = (:red, .5))\nvlines!(ax, zres.lo; color = :grey, linestyle = :dash, linewidth = 1)\nhlines!(ax, zres0.y1, linestyle = :dash)\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.kdeda-Tuple{}","page":"Index of functions","title":"Jchemo.kdeda","text":"kdeda(; kwargs...)\nkdeda(X, y; kwargs...)\n\nDiscriminant analysis using non-parametric kernel Gaussian      density estimation (KDE-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nKeyword arguments of function dmkern (bandwidth    definition) can also be specified here.\n\nThe principle is the same as functions qda except that densities by class  are estimated from function dmkern instead of function dmnorm. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\")\n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest)\nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\nntrain = n - ntest\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nprior = :unif\n#prior = :prop\nmodel = kdeda(; prior)\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\nmodel = kdeda(; prior, a = .5) \n#model = kdeda(; prior, h = .1) \nfit!(model, Xtrain, ytrain)\nmodel.fitm.fitm[1].H\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.knnda-Tuple{}","page":"Index of functions","title":"Jchemo.knnda","text":"knnda(; kwargs...)\nknnda(X, y; kwargs...)\n\nk-Nearest-Neighbours weighted discrimination (KNN-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of the global X    is scaled by its uncorrected standard deviation before    the distance and weight computations.\n\nThis function has the same principle as function  knnr except that a discrimination replaces the regression.  A weighted vote is done over the neighborhood, and the prediction  corresponds to the most frequent class.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nmetric = :eucl\nh = 2 ; k = 10\nmodel = knnda(; metric, h, k) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show errp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n## With dimension reduction\nmodel1 = pcasvd(; nlv = 15)\nmetric = :mah ; h = 1 ; k = 3 \nmodel2 = knnda(; metric, h, k) \nmodel = pip(model1, model2)\nfit!(model, Xtrain, ytrain)\n@head pred = predict(model, Xtest).pred \nerrp(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.knnr-Tuple{}","page":"Index of functions","title":"Jchemo.knnr","text":"knnr(; kwargs...)\nknnr(X, Y; kwargs...)\n\nk-Nearest-Neighbours weighted regression (KNNR).\n\nX : X-data (n, p).\nY : Y-data (n, q).\n\nKeyword arguments:\n\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of the global X    is scaled by its uncorrected standard deviation before    the distance and weight computations.\n\nThe general principle of this function is as follows (many other  variants of kNNR pipelines can be built): a) For each new observation to predict, the prediction is the      weighted mean of y over a selected neighborhood (in X) of      size k.  b) Within the selected neighborhood, the weights  are defined from      the dissimilarities between the new observation and the neighborhood,      and are computed from function 'winvs'.\n\nIn general, for X-data with high dimensions, using the  Mahalanobis distance requires a preliminary dimensionality  reduction (see examples).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nh = 1 ; k = 3 \nmodel = knnr(; h, k) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\ndump(model.fitm.par)\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n## With dimension reduction\nmodel1 = pcasvd(nlv = 15)\nmetric = :eucl ; h = 1 ; k = 3 \nmodel2 = knnr(; metric, h, k) \nmodel = pip(model1, model2)\nfit!(model, Xtrain, ytrain)\nres = predict(model, Xtest) ; \n@head res.pred\n@show rmsep(res.pred, ytest)\n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nmodel = knnr(k = 15, h = 5) \nfit!(model, x, y)\npred = predict(model, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kpca-Tuple{}","page":"Index of functions","title":"Jchemo.kpca","text":"kpca(; kwargs...)\nkpca(X; kwargs...)\nkpca(X, weights::Weight; kwargs...)\n\nKernel PCA  (Scholkopf et al. 1997, Scholkopf & Smola 2002, Tipping 2001).\n\nX : X-data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. principal components (PCs) to consider. \nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nThe method is implemented by SVD factorization of the weighted  Gram matrix: \n\nD^(1/2) * Phi(X) * Phi(X)' * D^(1/2)\n\nwhere X is the cenetred matrix and D is a diagonal matrix  of weights (weights.w) of the observations (rows of X).\n\nReferences\n\nScholkopf, B., Smola, A., MÃ¼ller, K.-R., 1997. Kernel principal  component analysis, in: Gerstner, W., Germond, A., Hasler,  M., Nicoud, J.-D. (Eds.), Artificial Neural Networks,  ICANN 97, Lecture Notes in Computer Science. Springer, Berlin,  Heidelberg, pp. 583-588. https://doi.org/10.1007/BFb0020217\n\nScholkopf, B., Smola, A.J., 2002. Learning with kernels: support  vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.\n\nTipping, M.E., 2001. Sparse kernel principal component analysis.  Advances in neural information processing systems, MIT Press.  http://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf\n\nExamples\n\nusing Jchemo, JchemoData, JLD2 \npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest) \nXtrain = X[s.train, :]\nXtest = X[s.test, :]\n\nnlv = 3\nkern = :krbf ; gamma = 1e-4\nmodel = kpca(; nlv, kern, gamma) ;\nfit!(model, Xtrain)\npnames(model.fitm)\n@head T = model.fitm.T\nT' * T\nmodel.fitm.V' * model.fitm.V\n\n@head Ttest = transf(model, Xtest)\n\nres = summary(model) ;\npnames(res)\nres.explvarx\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kplskdeda-Tuple{}","page":"Index of functions","title":"Jchemo.kplskdeda","text":"kplskdeda(; kwargs...)\nkplskdeda(X, y; kwargs...)\nkplskdeda(X, y, weights::Weight; kwargs...)\n\nKPLS-KDEDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nKeyword arguments of function dmkern (bandwidth    definition) can also be specified here.\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plskdeda (PLS-KDEDA) except that  a kernel PLSR (function kplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nSee function kplslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kplslda-Tuple{}","page":"Index of functions","title":"Jchemo.kplslda","text":"kplslda(; kwargs...)\nkplslda(X, y; kwargs...)\nkplslda(X, y, weights::Weight; kwargs...)\n\nKPLS-LDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plslda (PLS-LDA) except that  a kernel PLSR (function kplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\ngamma = .1\nmodel = kplslda(; nlv, gamma) \n#model = kplslda(; nlv, gamma, prior = :prop) \n#model = kplsqda(; nlv, gamma, alpha = .5) \n#model = kplskdeda(; nlv, gamma, a = .5) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nembfitm = fitm.fitm.embfitm ;\n@head embfitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(embfitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kplsqda-Tuple{}","page":"Index of functions","title":"Jchemo.kplsqda","text":"kplsqda(; kwargs...)\nkplsqda(X, y; kwargs...)\nkplsqda(X, y, weights::Weight; kwargs...)\n\nKPLS-QDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plsqda (PLS-QDA) except that  a kernel PLSR (function kplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nSee function kplslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kplsr-Tuple{}","page":"Index of functions","title":"Jchemo.kplsr","text":"kplsr(; kwargs...)\nkplsr(X, Y; kwargs...)\nkplsr(X, Y, weights::Weight; kwargs...)\nkplsr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nKernel partial least squares regression (KPLSR) implemented with a Nipals      algorithm (Rosipal & Trejo, 2001).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to consider. \nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nThis algorithm becomes slow for n > 1000.  Use function dkplsr instead.\n\nReferences\n\nRosipal, R., Trejo, L.J., 2001. Kernel Partial Least  Squares Regression in Reproducing Kernel Hilbert Space.  Journal of Machine Learning Research 2, 97-123.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 20\nkern = :krbf ; gamma = 1e-1\nmodel = kplsr(; nlv, kern, gamma) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nnlv = 2\nkern = :krbf ; gamma = 1 / 3\nmodel = kplsr(; nlv, kern, gamma) \nfit!(model, x, y)\npred = predict(model, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kplsrda-Tuple{}","page":"Index of functions","title":"Jchemo.kplsrda","text":"kplsrda(; kwargs...)\nkplsrda(X, y; kwargs...)\nkplsrda(X, y, weights::Weight; kwargs...)\n\nDiscrimination based on kernel partial least squares     regression (KPLSR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation.\n\nSame as function plsrda (PLSR-DA) except that  a kernel PLSR (function kplsr), instead of a  PLSR (function plskern), is run on the Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nkern = :krbf ; gamma = .001 \nscal = true\nmodel = kplsrda(; nlv, kern, gamma, scal) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\n@head fitm.fitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(fitm.fitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kpol-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.kpol","text":"kpol(X, Y; kwargs...)\n\nCompute a polynomial kernel Gram matrix. \n\nX : X-data (n, p).\nY : Y-data (m, p).\n\nKeyword arguments:\n\ngamma : Scale of the polynom.\ncoef0 : Offset of the polynom.\ndegree : Degree of the polynom.\n\nGiven matrices X and Y of sizes (n, p) and (m, p),  respectively, the function returns the (n, m) Gram matrix:\n\nK(X, Y) = Phi(X) * Phi(Y)'.\n\nThe polynomial kernel between two vectors x and y is  computed by (gamma * (x' * y) + coef0)^degree.\n\nReferences\n\nScholkopf, B., Smola, A.J., 2002. Learning with kernels: support  vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.\n\nExamples\n\nusing Jchemo\nX = rand(5, 3)\nY = rand(2, 3)\nkpol(X, Y; gamma = .1, coef0 = 10, degree = 3)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.krbf-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.krbf","text":"krbf(X, Y; kwargs...)\n\nCompute a Radial-Basis-Function (RBF) kernel Gram matrix. \n\nX : X-data (n, p).\nY : Y-data (m, p).\n\nKeyword arguments:\n\ngamma : Scale parameter.\n\nGiven matrices X and Y of sizes (n, p) and (m, p),  respectively, the function returns the (n, m) Gram matrix:\n\nK(X, Y) = Phi(X) * Phi(Y)'.\n\nThe RBF kernel between two vectors x and y is computed by  exp(-gamma * ||x - y||^2).\n\nReferences\n\nScholkopf, B., Smola, A.J., 2002. Learning with kernels: support  vector machines, regularization, optimization, and beyond, Adaptive  computation and machine learning. MIT Press, Cambridge, Mass.\n\nExamples\n\nusing Jchemo\nX = rand(5, 3)\nY = rand(2, 3)\nkrbf(X, Y; gamma = .1)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.krr-Tuple{}","page":"Index of functions","title":"Jchemo.krr","text":"krr(; kwargs...)\nkrr(X, Y; kwargs...)\nkrr(X, Y, weights::Weight; kwargs...)\nkrr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nKernel ridge regression (KRR) implemented by SVD factorization.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nlb : Ridge regularization parameter \"lambda\".\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nscal : Boolean. If true, each column of `X    is scaled by its uncorrected standard deviation.\n\nKRR is also referred to as least squared SVM regression  (LS-SVMR). The method is close to the particular case of  SVM regression where there is no marge excluding the  observations (epsilon coefficient set to zero). The difference  is that a L2-norm optimization is done, instead of L1 in SVM.\n\nReferences\n\nBennett, K.V., Embrechts, M.J., 2003. An optimization  perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and  Applications, NATO Science Series III: Computer & Systems  Sciences. IOS Press Amsterdam, pp. 227-250.\n\nCawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel  Ridge Regression. Neural Processing Letters 16, 293-302.  https://doi.org/10.1023/A:1021798002258\n\nKrell, M.M., 2018. Generalizing, Decoding, and Optimizing  Support Vector Machine Classification. arXiv:1801.04929.\n\nSaunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression  Learning Algorithm in Dual Variables, in: In Proceedings of the  15th International Conference on Machine Learning. Morgan  Kaufitmann, pp. 515-521.\n\nSuykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse  approximation using least squares support vector machines. 2000 IEEE  International Symposium on Circuits and Systems. Emerging Technologies  for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439\n\nWelling, M., n.d. Kernel ridge regression. Department of  Computer Science, University of Toronto, Toronto, Canada.  https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nlb = 1e-3\nkern = :krbf ; gamma = 1e-1\nmodel = krr(; lb, kern, gamma) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\ncoef(model)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n   ylabel = \"Observed\").f    \n\ncoef(model; lb = 1e-1)\nres = predict(model, Xtest; lb = [.1 ; .01])\n@head res.pred[1]\n@head res.pred[2]\n\nlb = 1e-3\nkern = :kpol ; degree = 1\nmodel = krr(; lb, kern, degree) \nfit!(model, Xtrain, ytrain)\nres = predict(model, Xtest)\nrmsep(res.pred, ytest)\n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nlb = 1e-1\nkern = :krbf ; gamma = 1 / 3\nmodel = krr(; lb, kern, gamma) \nfit!(model, x, y)\npred = predict(model, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.krrda-Tuple{}","page":"Index of functions","title":"Jchemo.krrda","text":"krrda(; kwargs...)\nkrrda(X, y; kwargs...)\nkrrda(X, y, weights::Weight; kwargs...)\n\nDiscrimination based on kernel ridge regression (KRR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nlb : Ridge regularization parameter \"lambda\".\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol. See respective    functions krbf and kpol for their keyword arguments.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nSame as function rrda (RR-DA) except that a kernel  RR (function krr), instead of a RR (function rr),  is run on the Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nlb = 1e-5\nkern = :krbf ; gamma = .001 \nscal = true\nmodel = krrda(; lb, kern, gamma, scal) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\ncoef(fitm.fitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; lb = [.1, .001]).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lda-Tuple{}","page":"Index of functions","title":"Jchemo.lda","text":"lda(; kwargs...)\nlda(; kwargs...)\nlda(X, y; kwargs...)\nlda(X, y, weights::Weight; kwargs...)\n\nLinear discriminant analysis (LDA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\")\n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest)\nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\nntrain = n - ntest\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nmodel = lda()\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\naggsum(fitm.weights.w, ytrain)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lg","text":"lg(X, Y; centr = true)\nlg(Xbl; centr = true)\n\nCompute the Lg coefficient between matrices.\n\nX : Matrix (n, p).\nY : Matrix (n, q).\nXbl : A list (vector) of matrices.\n\nKeyword arguments:\n\ncentr : Boolean indicating if the matrices will    be internally centered or not.\n\nLg(X, Y) = Sum.(j=1..p) Sum.(k= 1..q) cov(xj, yk)^2\n\nRV(X, Y) = Lg(X, Y) / sqrt(Lg(X, X), Lg(Y, Y))\n\nReferences\n\nEscofier, B. & Pagès, J. 1984. L’analyse factorielle multiple.  Cahiers du Bureau universitaire de recherche opérationnelle.  Série Recherche, tome 42, p. 3-68\n\nEscofier, B. & Pagès, J. (2008). Analyses Factorielles Simples  et Multiples : Objectifs, Méthodes et Interprétation. Dunod,  4e édition.\n\nExamples\n\nusing Jchemo\nX = rand(5, 10)\nY = rand(5, 3)\nlg(X, Y)\n\nX = rand(5, 15) \nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl)\nlg(Xbl)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.list-Tuple{Any, Integer}","page":"Index of functions","title":"Jchemo.list","text":"list(Q, n::Integer)\n\nCreate a Vector {Q}(undef, n).\n\nisassigned(object, i) can be used to check if cell i is empty.\n\nExamples\n\nusing Jchemo\n\nlist(Float64, 5)\nlist(Array{Float64}, 5)\nlist(Matrix{Int}, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.list-Tuple{Integer}","page":"Index of functions","title":"Jchemo.list","text":"list(n::Integer)\n\nCreate a Vector{Any}(nothing, n).\n\nisnothing(object, i) can be used to check if cell i is empty.\n\nExamples\n\nusing Jchemo\n\nlist(5)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.locw-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.locw","text":"locw(Xtrain, Ytrain, X; listnn, listw = nothing, algo, verbose = false, kwargs...)\n\nCompute predictions for a given kNN model.\n\nXtrain : Training X-data.\nYtrain : Training Y-data.\nX : X-data (m observations) to predict.\n\nKeyword arguments:\n\nlistnn : List (vector) of m vectors of indexes.\nlistw : List (vector) of m vectors of weights.\nalgo : Function computing the model on    the m neighborhoods.\nverbose : Boolean. If true, predicting information   are printed.\nkwargs : Keywords arguments to pass in function algo.   Each argument must have length = 1 (not be a collection).\n\nEach component i of listnn and listw contains the indexes  and weights, respectively, of the nearest neighbors of x_i in Xtrain.  The sizes of the neighborhood for i = 1,...,m can be different.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.locwlv-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.locwlv","text":"locwlv(Xtrain, Ytrain, X; listnn, listw = nothing, algo, nlv, verbose = true, kwargs...)\n\nCompute predictions for a given kNN model.\n\nXtrain : Training X-data.\nYtrain : Training Y-data.\nX : X-data (m observations) to predict.\n\nKeyword arguments:\n\nlistnn : List (vector) of m vectors of indexes.\nlistw : List (vector) of m vectors of weights.\nalgo : Function computing the model on    the m neighborhoods.\nnlv : Nb. or collection of nb. of latent variables (LVs).\nverbose : Boolean. If true, predicting information    are printed.\nkwargs : Keywords arguments to pass in function algo.   Each argument must have length = 1 (not be a collection).\n\nSame as locw but specific and much faster  for LV-based models (e.g. PLSR).\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.loessr-Tuple{}","page":"Index of functions","title":"Jchemo.loessr","text":"loessr(; kwargs...)\nloessr(X, y; kwargs...)\n\nCompute a locally weighted regression model (LOESS).\n\nX : X-data (n, p).\ny : Univariate y-data (n).\n\nKeyword arguments:\n\nspan : Window for neighborhood selection (level of smoothing)   for the local fitting, typically in 0, 1.\ndegree : Polynomial degree for the local fitting.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe function fits a LOESS model using package `Loess.jl'. \n\nSmaller values of span result in smaller local context in fitting (less smoothing).\n\nReferences\n\nhttps://github.com/JuliaStats/Loess.jl\n\nCleveland, W. S. (1979). Robust locally weighted regression and smoothing  scatterplots. Journal of the American statistical association, 74(368), 829-836.  DOI: 10.1080/01621459.1979.10481038\n\nCleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: an approach  to regression analysis by local fitting. Journal of the American statistical association,  83(403), 596-610. DOI: 10.1080/01621459.1988.10478639\n\nCleveland, W. S., & Grosse, E. (1991). Computational methods for local regression.  Statistics and computing, 1(1), 47-62. DOI: 10.1007/BF01890836\n\nExamples\n\nusing Jchemo, CairoMakie\n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nmodel = loessr(span = 1 / 3) \nfit!(model, x, y)\npred = predict(model, x).pred \nf = Figure(size = (700, 300))\nax = Axis(f[1, 1], xlabel = \"x\", ylabel = \"y\")\nscatter!(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred); label = \"Loess\")\nf[1, 2] = Legend(f, ax, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwmlr-Tuple{}","page":"Index of functions","title":"Jchemo.lwmlr","text":"lwmlr(; kwargs...)\nlwmlr(X, Y; kwargs...)\n\nk-Nearest-Neighbours locally weighted multiple linear regression (kNN-LWMLR).\n\nX : X-data (n, p).\nY : Y-data (n, q).\n\nKeyword arguments:\n\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of the global X    is scaled by its uncorrected standard deviation before    the distance and weight computations.\nverbose : Boolean. If true, predicting information   are printed.\n\nThis is the same principle as function lwplsr except  that MLR models are fitted on the neighborhoods, instead of  PLSR models.  The neighborhoods are computed directly on X  (there is no preliminary dimension reduction).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 20\nmodel0 = pcasvd(; nlv) ;\nfit!(model0, Xtrain) \n@head Ttrain = model0.fitm.T \n@head Ttest = transf(model0, Xtest)\n\nmetric = :eucl \nh = 2 ; k = 100 \nmodel = lwmlr(; metric, h, k) \nfit!(model, Ttrain, ytrain)\npnames(model)\npnames(model.fitm)\ndump(model.fitm.par)\n\nres = predict(model, Ttest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nmodel = lwmlr(metric = :eucl, h = 1.5, k = 20) ;\nfit!(model, x, y)\npred = predict(model, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwmlrda-Tuple{}","page":"Index of functions","title":"Jchemo.lwmlrda","text":"lwmlrda(; kwargs...)\nlwmlrda(X, y; kwargs...)\n\nk-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of the global X    is scaled by its uncorrected standard deviation before    the distance and weight computations.\nverbose : Boolean. If true, predicting information   are printed.\n\nThis is the same principle as function lwmlr except  that MLR-DA models, instead of MLR models, are fitted  on the neighborhoods.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\")\n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest)\nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\nntrain = n - ntest\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nmetric = :mah\nh = 2 ; k = 10\nmodel = lwmlrda(; metric, h, k) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplslda-Tuple{}","page":"Index of functions","title":"Jchemo.lwplslda","text":"lwplslda(; kwargs...)\nlwplslda(X, y; kwargs...)\n\nkNN-LWPLS-LDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nnlvdis : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nnlv : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional).\nscal : Boolean. If true, (a) each column of the global X    (and of the global Ydummy if there is a preliminary PLS reduction dimension)    is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Ydummy scaling is also done    within each neighborhood (local level) for the weighted PLS.\nverbose : Boolean. If true, predicting information   are printed.\n\nThis is the same principle as function lwplsr except  that a PLS-LDA model, instead of a PLSR model, is fitted  on each neighborhoods.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = :mah\nh = 2 ; k = 200\nnlv = 10\nmodel = lwplslda(; nlvdis, metric, h, k, nlv, prior = :prop) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show errp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsqda-Tuple{}","page":"Index of functions","title":"Jchemo.lwplsqda","text":"lwplsqda(; kwargs...)\nlwplsqda(X, y; kwargs...)\n\nkNN-LWPLS-QDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nnlvdis : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nnlv : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nscal : Boolean. If true, (a) each column of the global X    (and of the global Ydummy if there is a preliminary PLS reduction dimension)    is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Ydummy scaling is also done    within each neighborhood (local level) for the weighted PLS.\nverbose : Boolean. If true, predicting information   are printed.\n\nThis is the same principle as function lwplsr except  that a PLS-QDA model, instead of a PLSR model, is fitted  on each neighborhoods.\n\nWarning: The present version of this function can suffer from    stops due to non positive definite matrices when doing QDA   on neighborhoods. This is due to that some classes within the neighborhood can    have very few observations. It is recommended to select a sufficiantly large number    of neighbors or/and to use a regularized QDA (alpha > 0).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = :mah\nh = 2 ; k = 200\nnlv = 10\nmodel = lwplsqda(; nlvdis, metric, h, k, nlv, prior = :prop, alpha = .5) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show errp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsr-Tuple{}","page":"Index of functions","title":"Jchemo.lwplsr","text":"lwplsr(; kwargs...)\nlwplsr(X, Y; kwargs...)\n\nk-Nearest-Neighbours locally weighted partial least squares regression (kNN-LWPLSR).\n\nX : X-data (n, p).\nY : Y-data (n, q).\n\nKeyword arguments:\n\nnlvdis : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nnlv : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.\nscal : Boolean. If true, (a) each column of the global X    (and of the global Y if there is a preliminary PLS reduction dimension)    is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Y scaling is also done    within each neighborhood (local level) for the weighted PLSR.\nverbose : Boolean. If true, predicting information   are printed.\n\nFunction lwplsr fits kNN-LWPLSR models such as in  Lesnoff et al. 2020. The general principle of the pipeline  is as follows (many other variants of pipelines can  be built):\n\nLWPLSR is a particular case of weighted PLSR (WPLSR)  (e.g. Schaal et al. 2002). In WPLSR, a priori weights,  different from the usual 1/n (standard PLSR), are given  to the n training observations. These weights are used for  calculating (i) the scores and loadings of the WPLS and  (ii) the regression model that fits (by weighted  least squares) the Y-response(s) to the WPLS scores.  The specificity of LWPLSR (compared to WPLSR) is that the  weights are computed from dissimilarities (e.g. distances)  between the new observation to predict and the training  observations (\"L\" in LWPLSR comes from \"localized\"). Note  that in LWPLSR the weights and therefore the fitted WPLSR  model change for each new observation to predict.\n\nIn the original LWPLSR, all the n training observations  are used for each observation to predict (e.g. Sicard & Sabatier 2006,  Kim et al 2011). This can be very time consuming when n is large.  A faster (and often more efficient) strategy is to preliminary select,  in the training set, a number of k nearest neighbors to the  observation to predict (= \"weighting 1\") and then to apply  LWPLSR only to this pre-selected neighborhood (= \"weighting 2\"). T his strategy corresponds to a kNN-LWPLSR and is the one  implemented in function lwplsr.\n\nIn lwplsr, the dissimilarities used for weightings 1 and 2  are computed from the raw X-data, or after a dimension reduction, depending on argument nlvdis. In the last case, global PLS2  scores (LVs) are computed from {X, Y} and the dissimilarities  are computed over these scores. \n\nIn general, for high dimensional X-data, using the  Mahalanobis distance requires preliminary dimensionality  reduction of the data. In function knnr', the  preliminary reduction (argumentnlvdis) is done by PLS on {X,Y`}.\n\nReferences\n\nKim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of  active pharmaceutical ingredients content using locally weighted  partial least squares and statistical wavelength selection.  Int. J. Pharm., 421, 269-274.\n\nLesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally  weighted PLS strategies for regression and discrimination on  agronomic NIR data. Journal of Chemometrics, e3209.  https://doi.org/10.1002/cem.3209\n\nSchaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable  techniques from nonparametric statistics for the real time  robot learning. Applied Intell., 17, 49-60.\n\nSicard, E. Sabatier, R., 2006. Theoretical framework for local  PLS1 regression and application to a rainfall dataset.  Comput. Stat. Data Anal., 51, 1393-1410.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlvdis = 15 ; metric = :mah \nh = 1 ; k = 500 ; nlv = 10\nmodel = lwplsr(; nlvdis, metric, h, k, nlv) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsravg-Tuple{}","page":"Index of functions","title":"Jchemo.lwplsravg","text":"lwplsravg(; kwargs...)\nlwplsravg(X, Y; kwargs...)\n\nAveraging kNN-LWPLSR models with different numbers of latent variables (kNN-LWPLSR-AVG).\n\nX : X-data (n, p).\nY : Y-data (n, q).\n\nKeyword arguments:\n\nnlvdis : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nnlv : A range of nb. of latent variables (LVs)    to compute for the local (i.e. inside each neighborhood)    models.\nscal : Boolean. If true, (a) each column of the global X    (and of the global Y if there is a preliminary PLS reduction dimension)    is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Y scaling is also done    within each neighborhood (local level) for the weighted PLSR.\nverbose : Boolean. If true, predicting information   are printed.\n\nEnsemblist method where the predictions are computed  by averaging the predictions of a set of models built  with different numbers of LVs, such as in Lesnoff 2023. On each neighborhood, a PLSR-averaging (Lesnoff et al. \n\nis done instead of a PLSR.\n\nFor instance, if argument nlv is set to nlv = 5:10,  the prediction for a new observation is the simple average of the predictions returned by the models with 5 LVs, 6 LVs,  ... 10 LVs, respectively.\n\nReferences\n\nLesnoff, M., Andueza, D., Barotin, C., Barre, V., Bonnal, L.,  Fernández Pierna, J.A., Picard, F., Vermeulen, V., Roger,  J.-M., 2022. Averaging and Stacking Partial Least Squares  Regression Models to Predict the Chemical Compositions and  the Nutritive Values of Forages from Spectral Near Infrared  Data. Applied Sciences 12, 7850.  https://doi.org/10.3390/app12157850\n\nM. Lesnoff, Averaging a local PLSR pipeline to predict  chemical compositions and nutritive values of forages  and feed from spectral near infrared data, Chemometrics and  Intelligent Laboratory Systems. 244 (2023) 105031.  https://doi.org/10.1016/j.chemolab.2023.105031.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlvdis = 5 ; metric = :mah \nh = 1 ; k = 200 ; nlv = 4:20\nmodel = lwplsravg(; nlvdis, metric, h, k, nlv) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f  \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsrda-Tuple{}","page":"Index of functions","title":"Jchemo.lwplsrda","text":"lwplsrda(; kwargs...)\nlwplsrda(X, y; kwargs...)\n\nkNN-LWPLSR-DA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nnlvdis : Number of latent variables (LVs) to consider    in the global PLS used for the dimension reduction    before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the    neighbors and to compute the weights. Possible values    are: :eucl (Euclidean distance), :mah (Mahalanobis    distance).\nh : A scalar defining the shape of the weight    function computed by function winvs. Lower is h,    sharper is the function. See function winvs for    details (keyword arguments criw and squared of    winvs can also be specified here).\nk : The number of nearest neighbors to select for    each observation to predict.\ntolw : For stabilization when very close neighbors.\nnlv : Nb. latent variables (LVs) for the local (i.e.    inside each neighborhood) models.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional).\nscal : Boolean. If true, (a) each column of the global X    (and of the global Ydummy if there is a preliminary PLS reduction dimension)    is scaled by its uncorrected standard deviation before to compute    the distances and the weights, and (b) the X and Ydummy scaling is also done    within each neighborhood (local level) for the weighted PLS.\nverbose : Boolean. If true, predicting information   are printed.\n\nThis is the same principle as function lwplsr except  that PLSR-DA models, instead of PLSR models, are fitted  on the neighborhoods.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = :mah\nh = 2 ; k = 200\nnlv = 10\nmodel = lwplsrda(; nlvdis, metric, h, k, nlv) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \nres.listnn\nres.listd\nres.listw\n@head res.pred\n@show errp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.madv-Tuple{Any}","page":"Index of functions","title":"Jchemo.madv","text":"madv(x)\n\nCompute the median absolute deviation (MAD) of a vector. \n\nx : A vector (n).\n\nThis is the MAD adjusted by a factor (1.4826) for asymptotically  normal consistency.\n\nExamples\n\nusing Jchemo\n\nx = rand(100)\nmadv(x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mahsq-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mahsq","text":"mahsq(X, Y)\nmahsq(X, Y, Sinv)\n\nSquared Mahalanobis distances between      the rows of X and Y.\n\nX : Data (n, p).\nY : Data (m, p).\nSinv : Inverse of a covariance matrix S.   If not given, S is computed as the uncorrected    covariance matrix of X.\n\nWhen X and Y are (n, p) and (m, p), repectively, it returns  an object (n, m) with:\n\ni, j = distance between row i of X and row j of Y.\n\nExamples\n\nusing StatsBase \n\nX = rand(5, 3)\nY = rand(2, 3)\n\nmahsq(X, Y)\n\nS = cov(X, corrected = false)\nSinv = inv(S)\nmahsq(X, Y, Sinv)\nmahsq(X[1:1, :], Y[1:1, :], Sinv)\n\nmahsq(X[:, 1], 4)\nmahsq(1, 4, 2.1)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mahsqchol-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mahsqchol","text":"mahsqchol(X, Y)\nmahsqchol(X, Y, Uinv)\n\nCompute the squared Mahalanobis distances (with a Cholesky factorization) between the observations (rows) of X and Y.\n\nX : Data (n, p).\nY : Data (m, p).\nUinv : Inverse of the upper matrix of a Cholesky factorization    of a covariance matrix S.   If not given, the factorization is done on S,    the uncorrected covariance matrix of X.\n\nWhen X and Y are (n, p) and (m, p), repectively, it returns  an object (n, m) with:\n\ni, j = distance between row i of X and row j of Y.\n\nExamples\n\nusing LinearAlgebra, StatsBase\n\nX = rand(5, 3)\nY = rand(2, 3)\n\nmahsqchol(X, Y)\n\nS = cov(X, corrected = false)\nU = cholesky(Hermitian(S)).U \nUinv = inv(U)\nmahsqchol(X, Y, Uinv)\n\nmahsqchol(X[:, 1], 4)\nmahsqchol(1, 4, sqrt(2.1))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.matB","page":"Index of functions","title":"Jchemo.matB","text":"matB(X, y, weights::Weight)\n\nBetween-class covariance matrix.\n\nX : X-data (n, p).\ny : A vector (n) defining the class membership.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nCompute the between-class covariance matrix (output B)  of X. This is the (non-corrected) covariance matrix of  the weighted class centers.\n\nExamples\n\nusing Jchemo, StatsBase\n\nn = 20 ; p = 3\nX = rand(n, p)\ny = rand(1:3, n)\ntab(y) \nweights = mweight(ones(n)) \n\nres = matB(X, y, weights) ;\nres.B\nres.priors\nres.ni\nres.lev\n\nres = matW(X, y, weights) ;\nres.W\nres.Wi\n\nmatW(X, y, weights).W + matB(X, y, weights).B\ncov(X; corrected = false)\n\nv = mweight(collect(1:n))\nmatW(X, y, v).priors \nmatB(X, y, v).priors \nmatW(X, y, v).W + matB(X, y, v).B\ncovm(X, v)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.matW","page":"Index of functions","title":"Jchemo.matW","text":"matW(X, y, weights::Weight)\n\nWithin-class covariance matrices.\n\nX : X-data (n, p).\ny : A vector (n) defing the class membership.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nCompute the (non-corrected) within-class and pooled covariance  matrices  (outputs Wi and W, respectively) of X. \n\nIf class i contains only one observation, Wi is computed by:\n\ncovm(X,weights).\n\nFor examples, see function matB. \n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mavg-Tuple{}","page":"Index of functions","title":"Jchemo.mavg","text":"mavg(; kwargs...)\nmavg(X; kwargs...)\n\nSmoothing by moving averages of each row of X-data.\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nnpoint : Nb. points involved in the window.\n\nThe function returns a matrix (n, p).\n\nThe smoothing is computed by convolution with padding,  using function imfilter of package ImageFiltering.jl.  The centered kernel is ones(npoint) / npoint.  Each returned point is located on the center of the kernel. Assume a signal x of length p (row of X) correponding to  a vector wl of p wavelengths (or other indexes). \n\nIf npoint = 3, the  kernel is kern = [.33, .33, .33], and: \n\nThe output value at index i = 4 is: dot(kern, [x[3], x[4], x[5]]).   The output wavelength is: wl[4]\nThe output value at index i = 1 is: dot(kern, [x[1], x[1], x[2]])    (padding). The corresponding wavelength is: wl[1].\n\nIf npoint = 4, the  kernel is kern = [.25, .25, .25, .25], and: \n\nThe output value at index i = 4 is: dot(kern, [x[3], x[4], x[5], x[6]]).   The corresponding wavelength is: (wl[4] + wl[5]) / 2.\nThe output value at index i = 1 is: dot(kern, x[1], x[1], x[2], x[3])    (padding). The corresponding wavelength is: (wl[1] + wl[2]) / 2.\n\nReferences\n\nPackage ImageFiltering.jl https://github.com/JuliaImages/ImageFiltering.jl\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = mavg(npoint = 10) \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbconcat-Tuple{}","page":"Index of functions","title":"Jchemo.mbconcat","text":"mbconcat()\nmbconcat(Xbl)\n\nConcatenate horizontaly multiblock X-data.\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \n\nExamples\n\nusing Jchemo\nn = 5 ; m = 3 ; p = 10 \nX = rand(n, p) \nXnew = rand(m, p)\nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl) \nXblnew = mblock(Xnew, listbl) \n@head Xbl[3]\n\nmodel = mbconcat() \nfit!(model, Xbl)\ntransf(model, Xbl)\ntransf(model, Xblnew)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mblock-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mblock","text":"mblock(X, listbl)\n\nMake blocks from a matrix.\n\nX : X-data (n, p).\nlistbl : A vector whose each component defines    the colum numbers defining a block in X.   The length of listbl is the number of blocks.\n\nThe function returns a list (vector) of blocks.\n\nExamples\n\nusing Jchemo\nn = 5 ; p = 10 \nX = rand(n, p) \nlistbl = [3:4, 1, [6; 8:10]]\n\nXbl = mblock(X, listbl)\nXbl[1]\nXbl[2]\nXbl[3]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbpca-Tuple{}","page":"Index of functions","title":"Jchemo.mbpca","text":"mbpca(; kwargs...)\nmbpca(Xbl; kwargs...)\nmbpca(Xbl, weights::Weight; kwargs...)\nmbpca!(Xbl::Matrix, weights::Weight; kwargs...)\n\nConsensus principal components analysis (CPCA = MBPCA).\n\nXbl : List of blocks (vector of matrices) of X-data.    Typically, output of function mblock.  \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal       for possible values.\ntol : Tolerance value for Nipals convergence.\nmaxit : Maximum number of iterations (Nipals).\nscal : Boolean. If true, each column of blocks in Xbl    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThe MBPCA global scores are equal to the scores of the PCA  of the horizontal concatenation X = [X1 X2 ... Xk].\n\nThe function returns several objects, in particular:\n\nT : The non-normed global scores.\nU : The normed global scores.\nW : The global loadings.\nTb : The block scores in the metric scale, grouped by LV.\nTbl : The block scores in original scale, grouped by block.\nVbl : The block loadings.\nlb : The specific weights \"lambda\".\nmu : The sum of the specific weights (= eigen value of the global PCA).\n\nFunction summary returns: \n\nexplvarx : Proportion of the total inertia of X (sum of the squared norms of the blocks)    explained by each global score.\nexplX : Proportion of the inertia of each block explained by each global score.\ncontr_block : Contribution of each block to the global scores. \ncorx2t : Correlation between the global scores and the original variables.  \ncortb2t : Correlation between the global scores and the block scores in the metric scale.\nrv : RV coefficient. \nlg : Lg coefficient. \n\nReferences\n\nMangamana, E.T., Cariou, V., Vigneau, E., Glèlè Kakaï, R.L.,  Qannari, E.M., 2019. Unsupervised multiblock data  analysis: A unified approach and extensions. Chemometrics and  Intelligent Laboratory Systems 194, 103856.  https://doi.org/10.1016/j.chemolab.2019.103856\n\nWesterhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis  of multiblock and hierarchical PCA and PLS models. Journal  of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5<301::AID-CEM515>3.0.CO;2-S\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ham.jld2\") \n@load db dat\npnames(dat) \nX = dat.X\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X[1:6, :], listbl)\nXblnew = mblock(X[7:8, :], listbl)\nn = nro(Xbl[1]) \n\nnlv = 3\nbscal = :frob\nscal = false\n#scal = true\nmodel = mbpca(; nlv, bscal, scal)\nfit!(model, Xbl)\npnames(model) \npnames(model.fitm)\n## Global scores \n@head model.fitm.T\n@head transf(model, Xbl)\ntransf(model, Xblnew)\n## Blocks scores\ni = 1\n@head model.fitm.Tbl[i]\n@head transfbl(model, Xbl)[i]\n\nres = summary(model, Xbl) ;\npnames(res) \nres.explvarx\nres.explX   # = model.fitm.lb if bscal = :frob\nrowsum(Matrix(res.explX))\nres.contr_block\nres.corx2t \nres.cortb2t\nres.rv\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbplskdeda-Tuple{}","page":"Index of functions","title":"Jchemo.mbplskdeda","text":"mbplskdeda(; kwargs...)\nmbplskdeda(Xbl, y; kwargs...)\nmbplskdeda(Xbl, y, weights::Weight; kwargs...)\n\nMultiblock PLS-KDEDA.\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal   for possible values.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nKeyword arguments of function dmkern (bandwidth    definition) can also be specified here.\nscal : Boolean. If true, each column of blocks in Xbl    and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling) in the MBPLS computation.\n\nThe principle is the same as function mbplsqda except that the  densities by class are estimated from dmkern instead of dmnorm.\n\nSee function mbplslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbplslda-Tuple{}","page":"Index of functions","title":"Jchemo.mbplslda","text":"mbplslda(; kwargs...)\nmbplslda(Xbl, y; kwargs...)\nmbplslda(Xbl, y, weights::Weight; kwargs...)\n\nMultiblock PLS-LDA.\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal   for possible values.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of blocks in Xbl    and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling) in the MBPLS computation.\n\nThe method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nA multivariate MBPLSR (MBPLSR2) is run on {X, Ydummy}, returning   a score matrix T.\nA LDA is done on {T, y}, returning estimates of posterior probabilities  (∊ [0, 1]) of class membership.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JLD2, CairoMakie, JchemoData\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\ntab(Y.typ)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\nwlst = names(X)\nwl = parse.(Float64, wlst)\n#plotsp(X, wl; nsamp = 20).f\n##\nlistbl = [1:350, 351:700]\nXbltrain = mblock(Xtrain, listbl)\nXbltest = mblock(Xtest, listbl) \n\nnlv = 15\nscal = false\n#scal = true\nbscal = :none\n#bscal = :frob\nmodel = mbplslda(; nlv, bscal, scal)\n#model = mbplsqda(; nlv, bscal, alpha = .5, scal)\n#model = mbplskdeda(; nlv, bscal, scal)\nfit!(model, Xbltrain, ytrain) \npnames(model) \n\n@head transf(model, Xbltrain)\n@head transf(model, Xbltest)\n\nres = predict(model, Xbltest) ; \n@head res.pred \n@show errp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xbltest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbplsqda-Tuple{}","page":"Index of functions","title":"Jchemo.mbplsqda","text":"mbplsqda(; kwargs...)\nmbplsqda(Xbl, y; kwargs...)\nmbplsqda(Xbl, y, weights::Weight; kwargs...)\n\nMultiblock PLS-QDA.\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal   for possible values.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nscal : Boolean. If true, each column of blocks in Xbl    and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling) in the MBPLS computation.\n\nThe method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nA multivariate MBPLSR (MBPLSR2) is run on {X, Ydummy}, returning   a score matrix T.\nA QDA (possibly with continuum) is done on {T, y}, returning estimates  of posterior probabilities (∊ [0, 1]) of class membership.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nSee function mbplslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbplsr-Tuple{}","page":"Index of functions","title":"Jchemo.mbplsr","text":"mbplsr(; kwargs...)\nmbplsr(Xbl, Y; kwargs...)\nmbplsr(Xbl, Y, weights::Weight; kwargs...)\nmbplsr!(Xbl::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nMultiblock PLSR (MBPLSR).\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal   for possible values.\nscal : Boolean. If true, each column of blocks in Xbl    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThis function runs a PLSR on {X, Y} where X is the horizontal  concatenation of the blocks in Xbl. The function gives the  same results as function mbplswest, but is much faster.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ham.jld2\") \n@load db dat\npnames(dat) \nX = dat.X\nY = dat.Y\ny = Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\ns = 1:6\nXbltrain = mblock(X[s, :], listbl)\nXbltest = mblock(rmrow(X, s), listbl)\nytrain = y[s]\nytest = rmrow(y, s) \nntrain = nro(ytrain) \nntest = nro(ytest) \nntot = ntrain + ntest \n(ntot = ntot, ntrain , ntest)\n\nnlv = 3\nbscal = :frob\nscal = false\n#scal = true\nmodel = mbplsr(; nlv, bscal, scal)\nfit!(model, Xbltrain, ytrain)\npnames(model) \npnames(model.fitm)\n@head model.fitm.T\n@head transf(model, Xbltrain)\ntransf(model, Xbltest)\n\nres = predict(model, Xbltest)\nres.pred \nrmsep(res.pred, ytest)\n\nres = summary(model, Xbltrain) ;\npnames(res) \nres.explvarx\nres.corx2t \nres.rdx\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbplsrda-Tuple{}","page":"Index of functions","title":"Jchemo.mbplsrda","text":"mbplsrda(; kwargs...)\nmbplsrda(Xbl, y; kwargs...)\nmbplsrda(Xbl, y, weights::Weight; kwargs...)\n\nDiscrimination based on multiblock partial least squares      regression (MBPLSR-DA).\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal   for possible values.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of blocks in Xbl    and Ydummy is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThe method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nThen, a multivariate MBPLSR (MBPLSR2) is run on {X, Ydummy}, returning   predictions of the dummy variables (= object posterior returned by   fuction predict).  These predictions can be considered as unbounded estimates   (i.e. eventuall outside of [0, 1]) of the class membership probabilities.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JLD2, CairoMakie, JchemoData\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\ntab(Y.typ)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\nwlst = names(X)\nwl = parse.(Float64, wlst)\n#plotsp(X, wl; nsamp = 20).f\n##\nlistbl = [1:350, 351:700]\nXbltrain = mblock(Xtrain, listbl)\nXbltest = mblock(Xtest, listbl) \n\nnlv = 15\nscal = false\n#scal = true\nbscal = :none\n#bscal = :frob\nmodel = mbplsrda(; nlv, bscal, scal)\nfit!(model, Xbltrain, ytrain) \npnames(model) \n\n@head model.fitm.fitm.T \n@head transf(model, Xbltrain)\n@head transf(model, Xbltest)\n\nres = predict(model, Xbltest) ; \n@head res.pred \n@show errp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xbltest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbplswest-Tuple{}","page":"Index of functions","title":"Jchemo.mbplswest","text":"mbplswest(; kwargs...)\nmbplswest(Xbl, Y; kwargs...)\nmbplswest(Xbl, Y, weights::Weight; kwargs...)\nmbplswest!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nMultiblock PLSR (MBPLSR) - Nipals algorithm.\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. See function blockscal   for possible values.\ntol : Tolerance value for convergence (Nipals).\nmaxit : Maximum number of iterations (Nipals).\nscal : Boolean. If true, each column of blocks in Xbl    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThis functions implements the MBPLSR Nipals algorithm such  as in Westerhuis et al. 1998. The function gives the same  results as function mbplsr.\n\nReferences\n\nWesterhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis  of multiblock and hierarchical PCA and PLS models. Journal of  Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5<301::AID-CEM515>3.0.CO;2-S\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ham.jld2\") \n@load db dat\npnames(dat) \nX = dat.X\nY = dat.Y\ny = Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\ns = 1:6\nXbltrain = mblock(X[s, :], listbl)\nXbltest = mblock(rmrow(X, s), listbl)\nytrain = y[s]\nytest = rmrow(y, s) \nntrain = nro(ytrain) \nntest = nro(ytest) \nntot = ntrain + ntest \n(ntot = ntot, ntrain , ntest)\n\nnlv = 3\nbscal = :frob\nscal = false\n#scal = true\nmodel = mbplswest(; nlv, bscal, scal)\nfit!(model, Xbltrain, ytrain)\npnames(model) \npnames(model.fitm)\n@head model.fitm.T\n@head transf(model, Xbltrain)\ntransf(model, Xbltest)\n\nres = predict(model, Xbltest)\nres.pred \nrmsep(res.pred, ytest)\n\nres = summary(model, Xbltrain) ;\npnames(res) \nres.explvarx\nres.corx2t \nres.cortb2t \nres.rdx\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.meanv-Tuple{Any}","page":"Index of functions","title":"Jchemo.meanv","text":"meanv(x)\nmeanv(x, weights::Weight)\n\nCompute the mean of a vector. \n\nx : A vector (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nExamples\n\nusing Jchemo\n\nn = 100\nx = rand(n)\nw = mweight(rand(n)) \n\nmeanv(x)\nmeanv(x, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.merrp-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.merrp","text":"merrp(pred, y)\n\nCompute the mean intra-class classification error rate.\n\npred : Predictions.\ny : Observed data (class membership).\n\nERRP (see function errp) is computed for each class. Function merrp returns the average of these intra-class ERRPs.   \n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nytrain = rand([\"a\" ; \"b\"], 10)\nXtest = rand(4, 5) \nytest = rand([\"a\" ; \"b\"], 4)\n\nmodel = plsrda(; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nmerrp(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlev-Tuple{Any}","page":"Index of functions","title":"Jchemo.mlev","text":"mlev(x)\n\nReturn the sorted levels of a vector or a dataset. \n\nExamples\n\nusing Jchemo\n\nx = rand([\"a\";\"b\";\"c\"], 20)\nlev = mlev(x)\nnlev = length(lev)\n\nX = reshape(x, 5, 4)\nmlev(X)\n\ndf = DataFrame(g1 = rand(1:2, n), g2 = rand([\"a\"; \"c\"], n))\nmlev(df)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlr-Tuple{}","page":"Index of functions","title":"Jchemo.mlr","text":"mlr(; kwargs...)\nmlr(X, Y; kwargs...)\nmlr(X, Y, weights::Weight; kwargs...)\nmlr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nCompute a mutiple linear regression model (MLR) by using the QR algorithm.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnoint : Boolean. Define if the model is computed    with an intercept or not.\n\nSafe but can be little slower than other methods.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie \npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 2:4]\ny = dat.X[:, 1]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest) \nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\n\nmodel = mlr()\n#model = mlrchol()\n#model = mlrpinv()\n#model = mlrpinvn() \nfit!(model, Xtrain, ytrain) \npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.B\nfitm.int \ncoef(model) \nres = predict(model, Xtest)\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\nmodel = mlr(noint = true)\nfit!(model, Xtrain, ytrain) \ncoef(model) \n\nmodel = mlrvec()\nfit!(model, Xtrain[:, 1], ytrain) \ncoef(model) \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlrchol-Tuple{}","page":"Index of functions","title":"Jchemo.mlrchol","text":"mlrchol()\nmlrchol(X, Y)\nmlrchol(X, Y, weights::Weight)\nmlrchol!mlrchol!(X::Matrix, Y::Matrix, weights::Weight)\n\nCompute a mutiple linear regression model (MLR) using the Normal equations      and a Choleski factorization.\n\nX : X-data, with nb. columns >= 2 (required by function cholesky).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nCompute only a model with intercept.\n\nFaster but can be less accurate (based on squared element X'X).\n\nSee function mlr for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlrda-Tuple{}","page":"Index of functions","title":"Jchemo.mlrda","text":"mlrda(; kwargs...)\nmlrda(X, y; kwargs...)\nmlrda(X, y, weights::Weight)\n\nDiscrimination based on multple linear regression (MLR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments:\n\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\n\nThe method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nThen, a multiple linear regression (MLR) is run on {X, Ydummy}, returning   predictions of the dummy variables (= object posterior returned by   fuction predict).  These predictions can be considered as unbounded estimates   (i.e. eventuall outside of [0, 1]) of the class membership probabilities.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\")\n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest)\nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\nntrain = n - ntest\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nmodel = mlrda()\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlrpinv-Tuple{}","page":"Index of functions","title":"Jchemo.mlrpinv","text":"mlrpinv()\nmlrpinv(X, Y; kwargs...)\nmlrpinv(X, Y, weights::Weight; kwargs...)\nmlrpinv!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nCompute a mutiple linear regression model (MLR)  by using      a pseudo-inverse. \n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments:\n\nnoint : Boolean. Define if the model is computed    with an intercept or not.\n\nSafe but can be slower.  \n\nSee function mlr for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlrpinvn-Tuple{}","page":"Index of functions","title":"Jchemo.mlrpinvn","text":"mlrpinvn() \nmlrpinvn(X, Y)\nmlrpinvn(X, Y, weights::Weight)\nmlrpinvn!mlrchol!(X::Matrix, Y::Matrix, \n    weights::Weight)\n\nCompute a mutiple linear regression model (MLR)      by using the Normal equations and a pseudo-inverse.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nSafe and fast for p not too large.\n\nCompute only a model with intercept.\n\nSee function mlr for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlrvec-Tuple{}","page":"Index of functions","title":"Jchemo.mlrvec","text":"mlrvec(; kwargs...)\nmlrvec(X, Y; kwargs...)\nmlrvec(X, Y, weights::Weight; kwargs...)\nmlrvec!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nCompute a simple (univariate x) linear regression model.\n\nx : Univariate X-data (n).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments:\n\nnoint : Boolean. Define if the model is computed    with an intercept or not.\n\nSee function mlr for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mpar","page":"Index of functions","title":"Jchemo.mpar","text":"mpar(; kwargs...)\n\nReturn a tuple with all the combinations of the parameter      values defined in kwargs. Keyword arguments:\n\nkwargs : Vector(s) of the parameter(s) values.\n\nExamples\n\nusing Jchemo\nnlvdis = 25 ; metric = [:mah] \nh = [1 ; 2 ; Inf] ; k = [500 ; 1000] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) \nlength(pars[1])\nreduce(hcat, pars)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mse-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mse","text":"mse(pred, Y; digits = 3)\n\nSummary of model performance for regression.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nmse(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nmse(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.msep-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.msep","text":"msep(pred, Y)\n\nCompute the mean of the squared prediction errors (MSEP).\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo \n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nmsep(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mweight-Tuple{Vector}","page":"Index of functions","title":"Jchemo.mweight","text":"mweight(x::Vector)\n\nReturn an object of type Weight containing vector  w = x / sum(x) (if ad'hoc building, w must sum to 1).\n\nExamples\n\nusing Jchemo\n\nx = rand(10)\nw = mweight(x)\nsum(w.w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mweightcla-Tuple{AbstractVector}","page":"Index of functions","title":"Jchemo.mweightcla","text":"mweightcla(x::AbstractVector; prior::Union{Symbol, Vector} = :unif)\nmweightcla(Q::DataType, x::Vector; prior::Union{Symbol, Vector} = :unif)\n\nCompute observation weights for a categorical variable,      given specified sub-total weights for the classes.\n\nx : A categorical variable (n) (class membership).\nQ : A data type (e.g. Float32).\n\nKeyword arguments:\n\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (the vector must be sorted in the same order as mlev(x)).\n\nReturn an object of type Weight (see function mweight) containing a vector w (n) that sums to 1.\n\nExamples\n\nusing Jchemo\n\nx = vcat(rand([\"a\" ; \"c\"], 900), repeat([\"b\"], 100))\ntab(x)\nweights = mweightcla(x)\n#weights = mweightcla(x; prior = :prop)\n#weights = mweightcla(x; prior = [.1, .7, .2])\nres = aggstat(weights.w, x; algo = sum)\n[res.lev res.X]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nco-Tuple{Any}","page":"Index of functions","title":"Jchemo.nco","text":"nco(X)\n\nReturn the nb. columns of X.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nipals-Tuple{Any}","page":"Index of functions","title":"Jchemo.nipals","text":"nipals(X; kwargs...)\nnipals(X, UUt, VVt; kwargs...)\n\nNipals to compute the first score and loading vectors of a matrix.\n\nX : X-data (n, p).\nUUt : Matrix (n, n) for Gram-Schmidt orthogonalization.\nVVt : Matrix (p, p) for Gram-Schmidt orthogonalization.\n\nKeyword arguments:\n\ntol : Tolerance value for stopping    the iterations.\nmaxit : Maximum nb. of iterations.\n\nThe function finds:\n\n{u, v, sv} = argmin(||X - u * sv * v'||)\n\nwith the constraints:\n\n||u|| = ||v|| = 1\n\nusing the alternating least squares algorithm to compute  SVD (Gabriel & Zalir 1979).\n\nAt the end, X ~ u * sv * v', where:\n\nu : left singular vector (u * sv = scores)\nv : right singular vector (loadings)\nsv : singular value.\n\nWhen NIPALS is used on sequentially deflated matrices,  vectors u and v can loose orthogonality due to accumulation  of rounding errors. Orthogonality can be rebuilt from the  Gram-Schmidt method (arguments UUt and VVt). \n\nReferences\n\nK.R. Gabriel, S. Zamir, Lower rank approximation of matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.\n\nExamples\n\nusing Jchemo, LinearAlgebra\n\nX = rand(5, 3)\n\nres = nipals(X)\nres.niter\nres.sv\nsvd(X).S[1] \nres.v\nsvd(X).V[:, 1] \nres.u\nsvd(X).U[:, 1] \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nipalsmiss-Tuple{Any}","page":"Index of functions","title":"Jchemo.nipalsmiss","text":"nipalsmiss(X; kwargs...)\nnipalsmiss(X, UUt, VVt; kwargs...)\n\nNipals to compute the first score and loading vectors      of a matrix with missing data.\n\nX : X-data (n, p).\nUUt : Matrix (n, n) for Gram-Schmidt orthogonalization.\nVVt : Matrix (p, p) for Gram-Schmidt orthogonalization.\n\nKeyword arguments:\n\ntol : Tolerance value for stopping    the iterations.\nmaxit : Maximum nb. of iterations.\n\nSee function nipals. \n\nReferences\n\nK.R. Gabriel, S. Zamir, Lower rank approximation of  matrices by least squares with any choice of weights,  Technometrics 21 (1979) 489–498.\n\nWright, K., 2018. Package nipals: Principal Components  Analysis using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/\n\nExamples\n\nusing Jchemo \n\nX = [1. 2 missing 4 ; 4 missing 6 7 ; \n    missing 5 6 13 ; missing 18 7 6 ; \n    12 missing 28 7] \n\nres = nipalsmiss(X)\nres.niter\nres.sv\nres.v\nres.u\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.normv-Tuple{Any}","page":"Index of functions","title":"Jchemo.normv","text":"normv(x)\nnormv(x, weights::Weight)\n\nCompute the norm of a vector.\n\nx : A vector (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nThe weighted norm of vector x is computed by:\n\nsqrt(x' * D * x), where D is the diagonal matrix of vector weights.w.\n\nReferences\n\n@gdkrmr, https://discourse.julialang.org/t/julian-way-to-write-this-code/119348/17\n\n@Stevengj,  https://discourse.julialang.org/t/interesting-post-about-simd-dot-product-and-cosine-similarity/123282.\n\nExamples\n\nusing Jchemo\n\nn = 1000\nx = rand(n)\nw = mweight(ones(n))\n\nnormv(x)\nsqrt(n) * normv(x, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nro-Tuple{Any}","page":"Index of functions","title":"Jchemo.nro","text":"nro(X)\n\nReturn the nb. rows of X.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occod-Tuple{}","page":"Index of functions","title":"Jchemo.occod","text":"occod(; kwargs...)\noccod(fitm, X; kwargs...)\n\nOne-class classification using PCA/PLS orthognal distance (OD).\n\nfitm : The preliminary model (e.g. object fitm returned by function    pcasvd) that was fitted on the training data assumed to represent    the training class.\nX : Training X-data (n, p), on which was fitted the model fitm.\n\nKeyword arguments:\n\nmcut : Type of cutoff. Possible values are: :mad, :q.    See Thereafter.\ncri : When mcut = :mad, a constant. See thereafter.\nrisk : When mcut = :q, a risk-I level. See thereafter.\n\nIn this method, the outlierness d of an observation is the orthogonal distance (=  'X-residuals') of this  observation, ie. the Euclidean distance between the observation  and its projection on the  score plan defined by the fitted  (e.g. PCA) model (e.g. Hubert et al. 2005, Van Branden & Hubert  2005 p. 66, Varmuza & Filzmoser 2009 p. 79).\n\nSee function occsd for details on outputs.\n\nReferences\n\nM. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005).  ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.\n\nK. Vanden Branden, M. Hubert (2005). Robuts classification  in high dimension based on the SIMCA method. Chem. Lab. Int.  Syst, 79, 10-21.\n\nK. Varmuza, V. Filzmoser (2009). Introduction to multivariate  statistical analysis in chemometrics. CRC Press, Boca Raton.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X    \nY = dat.Y\nmodel = savgol(npoint = 21, deriv = 2, degree = 3)\nfit!(model, X) \nXp = transf(model, X) \ns = Bool.(Y.test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\n\n## Below, the reference class is \"EEH\"\ncla1 = \"EHH\" ; cla2 = \"PEE\" ; cod = \"out\"   # here cla2 should be detected\n#cla1 = \"EHH\" ; cla2 = \"EHH\" ; cod = \"in\"   # here cla2 should not be detected\ns1 = Ytrain.typ .== cla1\ns2 = Ytest.typ .== cla2\nzXtrain = Xtrain[s1, :]    \nzXtest = Xtest[s2, :] \nntrain = nro(zXtrain)\nntest = nro(zXtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\nytrain = repeat([\"in\"], ntrain)\nytest = repeat([cod], ntest)\n\n## Group description\nmodel = pcasvd(nlv = 10) \nfit!(model, zXtrain) \nTtrain = model.fitm.T\nTtest = transf(model, zXtest)\nT = vcat(Ttrain, Ttest)\ngroup = vcat(repeat([\"1\"], ntrain), repeat([\"2\"], ntest))\ni = 1\nplotxy(T[:, i], T[:, i + 1], group; leg_title = \"Class\", xlabel = string(\"PC\", i), \n    ylabel = string(\"PC\", i + 1)).f\n\n#### Occ\n## Preliminary PCA fitted model\nmodel0 = pcasvd(nlv = 10) \nfit!(model0, zXtrain)\n## Outlierness\nmodel = occod()\n#model = occod(mcut = :mad, cri = 4)\n#model = occod(mcut = :q, risk = .01)\n#model = occsdod()\nfit!(model, model0.fitm, zXtrain) \npnames(model) \npnames(model.fitm) \n@head d = model.fitm.d\nd = d.dstand\nf, ax = plotxy(1:length(d), d; size = (500, 300), \n    xlabel = \"Obs. index\", ylabel = \"Standardized distance\")\nhlines!(ax, 1; linestyle = :dot)\nf\n\nres = predict(model, zXtest) ;\npnames(res)\n@head res.d\n@head res.pred\ntab(res.pred)\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\nd1 = model.fitm.d.dstand\nd2 = res.d.dstand\nd = vcat(d1, d2)\nf, ax = plotxy(1:length(d), d, group; size = (500, 300), leg_title = \"Class\", \n    xlabel = \"Obs. index\", ylabel = \"Standardized distance\")\nhlines!(ax, 1; linestyle = :dot)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occsd-Tuple{}","page":"Index of functions","title":"Jchemo.occsd","text":"occsd(; kwargs...)\noccsd(fitm; kwargs...)\n\nOne-class classification using PCA/PLS score distance (SD).\n\nfitm : The preliminary model (e.g. object fitm returned by function    pcasvd) that was fitted on the training data assumed to represent    the training class.\n\nKeyword arguments:\n\nmcut : Type of cutoff. Possible values are: :mad, :q.    See Thereafter.\ncri : When mcut = :mad, a constant. See thereafter.\nrisk : When mcut = :q, a risk-I level. See thereafter.\n\nIn this method, the outlierness d of an observation is  defined by its score distance (SD), ie. the Mahalanobis distance  between the projection of the observation on the score plan  defined by the fitted (e.g. PCA) model and the center of the  score plan.\n\nIf a new observation has d higher than a given cutoff, the  observation is assumed to not belong to the training (= reference)  class. The cutoff is computed with non-parametric heuristics.  Noting [d] the vector of outliernesses computed on the training class:\n\nIf mcut = :mad, then cutoff = median([d]) + cri * madv([d]). \nIf mcut = :q, then cutoff is estimated from the empirical    cumulative density function computed on [d], for a given    risk-I (risk). \n\nAlternative approximate cutoffs have been proposed in the  literature (e.g.: Nomikos & MacGregor 1995, Hubert et al. 2005, Pomerantsev 2008). Typically, and whatever the approximation method  used to compute the cutoff, it is recommended to tune this cutoff  depending on the detection objectives. \n\nOutputs\n\npval: Estimate of p-value (see functions pval) computed    from the training distribution [d]. \ndstand: standardized distance defined as d / cutoff.    A value dstand > 1 may be considered as extreme compared to    the distribution of the training data.  \ngh is the Winisi \"GH\" (usually, GH > 3 is considered as    extreme).\n\nSpecific for function predict:\n\npred: class prediction\ndstand <= 1 ==> in: the observation is expected to    belong to the training class, \ndstand > 1  ==> out: extreme value, possibly not    belonging to the same class as the training. \n\nReferences\n\nM. Hubert, V. J. Rousseeuw, K. Vanden Branden (2005).  ROBPCA: a new approach to robust principal components  analysis. Technometrics, 47, 64-79.\n\nNomikos, V., MacGregor, J.F., 1995. Multivariate SPC Charts for  Monitoring Batch Processes. null 37, 41-59.  https://doi.org/10.1080/00401706.1995.10485888\n\nPomerantsev, A.L., 2008. Acceptance areas for multivariate  classification derived by projection methods. Journal of Chemometrics  22, 601-609. https://doi.org/10.1002/cem.1147\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X    \nY = dat.Y\nmodel = savgol(npoint = 21, deriv = 2, degree = 3)\nfit!(model, X) \nXp = transf(model, X) \ns = Bool.(Y.test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\n\n## Below, the reference class is \"EEH\"\ncla1 = \"EHH\" ; cla2 = \"PEE\" ; cod = \"out\"   # here cla2 should be detected\n#cla1 = \"EHH\" ; cla2 = \"EHH\" ; cod = \"in\"   # here cla2 should not be detected\ns1 = Ytrain.typ .== cla1\ns2 = Ytest.typ .== cla2\nzXtrain = Xtrain[s1, :]    \nzXtest = Xtest[s2, :] \nntrain = nro(zXtrain)\nntest = nro(zXtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\nytrain = repeat([\"in\"], ntrain)\nytest = repeat([cod], ntest)\n\n## Group description\nmodel = pcasvd(nlv = 10) \nfit!(model, zXtrain) \nTtrain = model.fitm.T\nTtest = transf(model, zXtest)\nT = vcat(Ttrain, Ttest)\ngroup = vcat(repeat([\"1\"], ntrain), repeat([\"2\"], ntest))\ni = 1\nplotxy(T[:, i], T[:, i + 1], group; leg_title = \"Class\", xlabel = string(\"PC\", i),  \n    ylabel = string(\"PC\", i + 1)).f\n\n#### Occ\n## Preliminary PCA fitted model\nmodel0 = pcasvd(nlv = 30) \nfit!(model0, zXtrain)\n## Outlierness\nmodel = occsd()\n#model = occsd(mcut = :mad, cri = 4)\n#model = occsd(mcut = :q, risk = .01)\nfit!(model, model0.fitm) \npnames(model) \npnames(model.fitm) \n@head d = model.fitm.d\nd = d.dstand\nf, ax = plotxy(1:length(d), d; size = (500, 300), xlabel = \"Obs. index\", \n    ylabel = \"Standardized distance\")\nhlines!(ax, 1; linestyle = :dot)\nf\n\nres = predict(model, zXtest) ;\npnames(res)\n@head res.d\n@head res.pred\ntab(res.pred)\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\nd1 = model.fitm.d.dstand\nd2 = res.d.dstand\nd = vcat(d1, d2)\nf, ax = plotxy(1:length(d), d, group; size = (500, 300), leg_title = \"Class\", xlabel = \"Obs. index\", \n    ylabel = \"Standardized distance\")\nhlines!(ax, 1; linestyle = :dot)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occsdod-Tuple{}","page":"Index of functions","title":"Jchemo.occsdod","text":"occsdod(; kwargs...)\noccsdod(object, X; kwargs...)\n\nOne-class classification using a consensus between      PCA/PLS score and orthogonal (SD and OD) distances.\n\nfitm : The preliminary model (e.g. object fitm returned by function    pcasvd) that was fitted on the training data assumed to represent    the training class.\nX : Training X-data (n, p), on which was fitted the model fitm.\n\nKeyword arguments:\n\nmcut : Type of cutoff. Possible values are: :mad, :q.    See Thereafter.\ncri : When mcut = :mad, a constant. See thereafter.\nrisk : When mcut = :q, a risk-I level. See thereafter.\n\nIn this method, the outlierness d of a given observation is a consensus between the score distance (SD) and the orthogonal distance (OD). The consensus is computed from the  standardized distances by: \n\ndstand = sqrt(dstand_sd * dstand_od).\n\nSee functions:\n\noccsd for details of the outputs,\nand occod for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occstah-Tuple{}","page":"Index of functions","title":"Jchemo.occstah","text":"occstah(; kwargs...)\noccstah(X; kwargs...)\n\nOne-class classification using the Stahel-Donoho outlierness.\n\nX : Training X-data (n, p).\n\nKeyword arguments:\n\nnlv : Nb. dimensions on which X is projected. \nmcut : Type of cutoff. Possible values are: :mad, :q.    See Thereafter.\ncri : When mcut = :mad, a constant. See thereafter.\nrisk : When mcut = :q, a risk-I level. See thereafter.\nscal : Boolean. If true, each column of X    is scaled such as in function outstah.\n\nIn this method, the outlierness d of a given observation is the Stahel-Donoho outlierness (see ?outstah).\n\nSee function occsd for details on outputs.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X    \nY = dat.Y\nmodel = savgol(npoint = 21, deriv = 2, degree = 3)\nfit!(model, X) \nXp = transf(model, X) \ns = Bool.(Y.test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\n\n## Below, the reference class is \"EEH\"\ncla1 = \"EHH\" ; cla2 = \"PEE\" ; cod = \"out\"   # here cla2 should be detected\n#cla1 = \"EHH\" ; cla2 = \"EHH\" ; cod = \"in\"   # here cla2 should not be detected\ns1 = Ytrain.typ .== cla1\ns2 = Ytest.typ .== cla2\nzXtrain = Xtrain[s1, :]    \nzXtest = Xtest[s2, :] \nntrain = nro(zXtrain)\nntest = nro(zXtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\nytrain = repeat([\"in\"], ntrain)\nytest = repeat([cod], ntest)\n\n## Group description\nmodel = pcasvd(nlv = 10) \nfit!(model, zXtrain) \nTtrain = model.fitm.T\nTtest = transf(model, zXtest)\nT = vcat(Ttrain, Ttest)\ngroup = vcat(repeat([\"1\"], ntrain), repeat([\"2\"], ntest))\ni = 1\nplotxy(T[:, i], T[:, i + 1], group; leg_title = \"Class\", xlabel = string(\"PC\", i),  \n    ylabel = string(\"PC\", i + 1)).f\n\n#### Occ\n## Preliminary dimension reduction \n## (Not required but often more efficient)\nnlv = 50\nmodel0 = pcasvd(; nlv)\nfit!(model0, zXtrain)\nTtrain = model0.fitm.T\nTtest = transf(model0, zXtest)\n## Outlierness\nmodel = occstah(; nlv, scal = true)\nfit!(model, Ttrain) \npnames(model) \npnames(model.fitm) \n@head d = model.fitm.d\nd = d.dstand\nf, ax = plotxy(1:length(d), d; size = (500, 300), xlabel = \"Obs. index\", \n    ylabel = \"Standardized distance\")\nhlines!(ax, 1; linestyle = :dot)\nf\n\nres = predict(model, Ttest) ;\npnames(res)\n@head res.d\n@head res.pred\ntab(res.pred)\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\nd1 = model.fitm.d.dstand\nd2 = res.d.dstand\nd = vcat(d1, d2)\nf, ax = plotxy(1:length(d), d, group; size = (500, 300), leg_title = \"Class\", \n    xlabel = \"Obs. index\", ylabel = \"Standardized distance\")\nhlines!(ax, 1; linestyle = :dot)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.out-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.out","text":"out(x)\n\nReturn if elements of a vector are strictly outside of a given range.\n\nx : Univariate data.\ny : Univariate data on which is computed the range (min, max).\n\nReturn a BitVector.\n\nExamples\n\nusing Jchemo\n\nx = [-200.; -100; -1; 0; 1; 200]\nout(x, [-1; .2; 1])\nout(x, (-1, 1))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.outeucl-Tuple{Any}","page":"Index of functions","title":"Jchemo.outeucl","text":"outeucl(X, V; kwargs...)\nouteucl!(X::Matrix, V::Matrix; kwargs...)\n\nCompute an outlierness from Euclidean distances to center.\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nscal : Boolean. If true, each column of X is scaled by its MAD    before computing the outlierness.\n\nOutlyingness is calculated by the Euclidean distance between  the observation (rows of X) and a robust estimate of the center of the data  (in the present function, the spatial median). Such outlyingness was for  instance used in the robust PLSR algorithm of Serneels et al. 2005 (PRM). \n\nReferences\n\nSerneels, S., Croux, C., Filzmoser, V., Van Espen, V.J., 2005.  Partial robust M-regression.  Chemometrics and Intelligent Laboratory Systems 79, 55-64.  https://doi.org/10.1016/j.chemolab.2005.04.007\n\nExamples\n\nusing Jchemo, CairoMakie\nn = 300 ; p = 700 ; m = 80\nntot = n + m\nX1 = randn(n, p)\nX2 = randn(m, p) .+ rand(1:3, p)'\nX = vcat(X1, X2)\n\nnlv = 10\nscal = false\n#scal = true\nres = outeucl(X; scal) ;\npnames(res)\nres.d    # outlierness \nplotxy(1:ntot, res.d).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.outstah-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.outstah","text":"outstah(X, V; kwargs...)\noutstah!(X::Matrix, V::Matrix; kwargs...)\n\nCompute the Stahel-Donoho outlierness.\n\nX : X-data (n, p).\nV : A projection matrix (p, nlv) representing the directions    of the projection pursuit.\n\nKeyword arguments:\n\nscal : Boolean. If true, each column of X is scaled by its MAD    before computing the outlierness.\n\nSee Maronna and Yohai 1995 for details on the outlierness  measure. \n\nA projection-pursuit approach is used: given a projection matrix V (p, nlv)  (in general built randomly), the observations (rows of X) are projected on  the nlv directions and the Stahel-Donoho outlierness is computed for each observation  from these projections.\n\nReferences\n\nMaronna, R.A., Yohai, V.J., 1995. The Behavior of the  Stahel-Donoho Robust Multivariate Estimator. Journal of the  American Statistical Association 90, 330–341.  https://doi.org/10.1080/01621459.1995.10476517\n\nExamples\n\nusing Jchemo, CairoMakie\n\nn = 300 ; p = 700 ; m = 80\nntot = n + m\nX1 = randn(n, p)\nX2 = randn(m, p) .+ rand(1:3, p)'\nX = vcat(X1, X2)\n\nnlv = 10\nV = rand(0:1, p, nlv)\nscal = false\n#scal = true\nres = outstah(X, V; scal) ;\npnames(res)\nres.d    # outlierness \nplotxy(1:ntot, res.d).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.parsemiss-Tuple{Any, Vector{Union{Missing, String}}}","page":"Index of functions","title":"Jchemo.parsemiss","text":"parsemiss(Q, x::Vector{Union{String, Missing}})\n\nParsing a string vector allowing missing data.\n\nQ : Type that results from the parsing of type `String'. \nx : A string vector containing missing (of type Missing)    observations.\n\nSee examples.\n\nExamples\n\nusing Jchemo\n\nx = [\"1\"; \"3.2\"; missing]\nx_p = parsemiss(Float64, x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcaeigen-Tuple{}","page":"Index of functions","title":"Jchemo.pcaeigen","text":"pcaeigen(; kwargs...)\npcaeigen(X; kwargs...)\npcaeigen(X, weights::Weight; kwargs...)\npcaeigen!(X::Matrix, weights::Weight; kwargs...)\n\nPCA by Eigen factorization.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nLet us note D the (n, n) diagonal matrix of weights (weights.w) and X the centered matrix in metric D. The function minimizes ||X - T * V'||^2  in metric D, by  computing an Eigen factorization of X' * D * X. \n\nSee function pcasvd for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcaeigenk-Tuple{}","page":"Index of functions","title":"Jchemo.pcaeigenk","text":"pcaeigenk(; kwargs...)\npcaeigenk(X; kwargs...)\npcaeigenk(X, weights::Weight; kwargs...)\npcaeigenk!(X::Matrix, weights::Weight; kwargs...)\n\nPCA by Eigen factorization of the kernel matrix XX'.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThis is the \"kernel cross-product\" version of the PCA  algorithm (e.g. Wu et al. 1997). For wide matrices (n << p,  where p is the nb. columns) and n not too large, this algorithm  can be much faster than the others.\n\nLet us note D the (n, n) diagonal matrix of weights (weights.w) and X the centered matrix in metric D. The function minimizes ||X - T * V'||^2  in metric D, by  computing an Eigen factorization of D^(1/2) * X * X' D^(1/2).\n\nSee function pcasvd for examples.\n\nReferences\n\nWu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA  algorithms for wide data. Part I: Theory and algorithms.  Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcanipals-Tuple{}","page":"Index of functions","title":"Jchemo.pcanipals","text":"pcanipals(; kwargs...)\npcanipals(X; kwargs...)\npcanipals(X, weights::Weight; kwargs...)\npcanipals!(X::Matrix, weights::Weight; kwargs...)\n\nPCA by NIPALS algorithm.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\ngs : Boolean. If true (default), a Gram-Schmidt    orthogonalization of the scores and loadings is done   before each X-deflation. \ntol : Tolerance value for stopping    the iterations.\nmaxit : Maximum nb. of iterations.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nLet us note D the (n, n) diagonal matrix of weights (weights.w) and X the centered matrix in metric D. The function minimizes ||X - T * V'||^2  in metric D  by NIPALS. \n\nSee function pcasvd for examples.\n\nReferences\n\nAndrecut, M., 2009. Parallel GPU Implementation of Iterative  PCA Algorithms. Journal of Computational Biology 16, 1593-1599.  https://doi.org/10.1089/cmb.2008.0221\n\nK.R. Gabriel, S. Zamir, Lower rank approximation of matrices  by least squares with any choice of weights, Technometrics 21 (1979) 489–498.\n\nGabriel, R. K., 2002. Le biplot - Outil d'exploration de données  multidimensionnelles. Journal de la Société Française de la Statistique,  143, 5-55.\n\nLingen, F.J., 2000. Efficient Gram-Schmidt orthonormalisation  on parallel computers. Communications in Numerical Methods in  Engineering 16, 57-66.  https://doi.org/10.1002/(SICI)1099-0887(200001)16:1<57::AID-CNM320>3.0.CO;2-I\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris, France.\n\nWright, K., 2018. Package nipals: Principal Components Analysis  using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcanipalsmiss-Tuple{}","page":"Index of functions","title":"Jchemo.pcanipalsmiss","text":"pcanipalsmiss(; kwargs...)\npcanipals(X; kwargs...)\npcanipals(X, weights::Weight; kwargs...)\npcanipals!(X::Matrix, weights::Weight; kwargs...)\n\nPCA by NIPALS algorithm allowing missing data.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\ngs : Boolean. If true (default), a Gram-Schmidt    orthogonalization of the scores and loadings is done   before each X-deflation. \ntol : Tolerance value for stopping    the iterations.\nmaxit : Maximum nb. of iterations.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nReferences\n\nWright, K., 2018. Package nipals: Principal Components Analysis  using NIPALS with Gram-Schmidt Orthogonalization.  https://cran.r-project.org/\n\nExamples\n\nX = [1 2. missing 4 ; 4 missing 6 7 ; \n    missing 5 6 13 ; missing 18 7 6 ; \n    12 missing 28 7] \n\nnlv = 3 \ntol = 1e-15\nscal = false\n#scal = true\ngs = false\n#gs = true\nmodel = pcanipalsmiss(; nlv, tol, gs, maxit = 500, scal)\nfit!(model, X)\npnames(model) \npnames(model.fitm)\nfitm = model.fitm ;\nfitm.niter\nfitm.sv\nfitm.V\nfitm.T\n## Orthogonality \n## only if gs = true\nfitm.T' * fitm.T\nfitm.V' * fitm.V\n\n## Impute missing data in X\nmodel = pcanipalsmiss(; nlv = 2, gs = true) ;\nfit!(model, X)\nXfit = xfit(model.fitm)\ns = ismissing.(X)\nX_imp = copy(X)\nX_imp[s] .= Xfit[s]\nX_imp\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcaout-Tuple{}","page":"Index of functions","title":"Jchemo.pcaout","text":"pcaout(; kwargs...)\npcaout(X; kwargs...)\npcaout(X, weights::Weight; kwargs...)\npcaout!(X::Matrix, weights::Weight; kwargs...)\n\nRobust PCA using outlierness.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\nprm : Proportion of the data removed (hard rejection of outliers)    for each outlierness measure.\nscal : Boolean. If true, each column of X is scaled by its MAD    when computing the outlierness and by its uncorrected standard    deviation when computing weighted PCA. \n\nRobust PCA combining outlyingness measures and weighted PCA (WPCA). \n\nThe objective is to remove the effect of multivariate X-outliers  that have potentially bad leverages. Observations (X-rows) receive  weights depending on two outlyingness indicators:\n\nThe Stahel-Donoho outlyingness (Maronna and Yohai, 1995) is computed   (function outstah) on X. The proportion prm of the observations with the   highest outlyingness values receive a weight w1 = 0 (the other receive a weight w1 = 1).\nAn outlyingness based on the Euclidean distance to center   (function outstah) is computed. The proportion prm of the observations   with the highest outlyingness values receive a weight w2 = 0   (the other receive a weight w2 = 1).\n\nThe final weights of the observations are computed by weights.w * w1 * w2 that is used in a weighted PCA.\n\nBy default, the function uses prm = .3 (such as in the ROBPCA algorithm  of Hubert et al. 2005, 2009). \n\nReferences\n\nHubert, M., Rousseeuw, V.J., Vanden Branden, K., 2005. ROBPCA: A New Approach  to Robust Principal Component Analysis. Technometrics 47, 64-79.  https://doi.org/10.1198/004017004000000563\n\nHubert, M., Rousseeuw, V., Verdonck, T., 2009. Robust PCA for skewed data and its  outlier map. Computational Statistics & Data Analysis 53, 2264-2274.  https://doi.org/10.1016/j.csda.2008.05.027\n\nMaronna, R.A., Yohai, V.J., 1995. The Behavior of the  Stahel-Donoho Robust Multivariate Estimator. Journal of the  American Statistical Association 90, 330–341.  https://doi.org/10.1080/01621459.1995.10476517\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie \nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"octane.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nwlst = names(X)\nwl = parse.(Float64, wlst)\nn = nro(X)\n\nnlv = 3\nmodel = pcaout(; nlv)  \n#model = pcasvd(; nlv) \nfit!(model, X)\npnames(model)\npnames(model.fitm)\n@head T = model.fitm.T\n## Same as:\ntransf(model, X)\n\ni = 1\nplotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = string(\"PC\", i), \n    ylabel = string(\"PC\", i + 1)).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcapp-Tuple{}","page":"Index of functions","title":"Jchemo.pcapp","text":"pcapp(; kwargs...)\npcapp(X; kwargs...)\npcapp!(X::Matrix; kwargs...)\n\nRobust PCA by projection pursuit.\n\nX : X-data (n, p). \n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\nnsim : Nb. of additional (to X-rows) simulated directions for the    projection pursuit.\nscal : Boolean. If true, each column of X    is scaled by its MAD.\n\nFor nsim = 0, this is the Croux & Ruiz-Gazen (C-R, 2005) PCA algorithm that  uses a projection pursuit (PP) method. Data X are robustly centered by the  spatial median, and the observations are projected to the \"PP\" directions  defined by the observations (rows of X) after they are normed.  The first PCA loading vector is the direction (within the PP directions) that  maximizes a given 'projection index', here the median absolute deviation (MAD).  Then, X is deflated to this loading vector, and the process is re-run to define the next loading vector. And so on. \n\nA possible extension of this algorithm is to randomly simulate additionnal  candidate PP directions to the n row observations. If nsim > 0, the function  simulates nsim additional PP directions to the n initial ones, as proposed  in Hubert et al. (2005): random couples of observations are sampled in X and,  for each couple, the direction passes through the two observations of the      couple (see function simpphub).\n\nReferences\n\nCroux, C., Ruiz-Gazen, A., 2005. High breakdown estimators for  principal components: the projection-pursuit approach revisited.  Journal of Multivariate Analysis 95, 206–226.  https://doi.org/10.1016/j.jmva.2004.08.002\n\nHubert, M., Rousseeuw, V.J., Vanden Branden, K., 2005. ROBPCA:  A New Approach to Robust Principal Component Analysis.  Technometrics 47, 64-79. https://doi.org/10.1198/004017004000000563\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie \nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"octane.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nwlst = names(X)\nwl = parse.(Float64, wlst)\nn = nro(X)\n\nnlv = 3\nmodel = pcapp(; nlv, nsim = 2000)  \n#model = pcasvd(; nlv) \nfit!(model, X)\npnames(model)\npnames(model.fitm)\n@head T = model.fitm.T\n## Same as:\n@head transf(model, X)\n\ni = 1\nplotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = string(\"PC\", i), \n    ylabel = string(\"PC\", i + 1)).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcasph-Tuple{}","page":"Index of functions","title":"Jchemo.pcasph","text":"pcasph(; kwargs...)\npcasph(X; kwargs...)\npcasph(X, weights::Weight; kwargs...)\npcasph!(X::Matrix, weights::Weight; kwargs...)\n\nSpherical PCA.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nSpherical PCA (Locantore et al. 1990, Maronna 2005, Daszykowski et al. 2007).  Matrix X is centered by the spatial median computed by function Jchemo.colmedspa.\n\nReferences\n\nDaszykowski, M., Kaczmarek, K., Vander Heyden, Y., Walczak, B., 2007.  Robust statistics in data analysis - A review. Chemometrics and Intelligent  Laboratory Systems 85, 203-219. https://doi.org/10.1016/j.chemolab.2006.06.016\n\nLocantore N., Marron J.S., Simpson D.G., Tripoli N., Zhang J.T., Cohen K.L. Robust principal component analysis for functional data, Test 8 (1999) 1–7\n\nMaronna, R., 2005. Principal components and orthogonal regression based on  robust scales, Technometrics, 47:3, 264-273, DOI: 10.1198/004017005000000166\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie \nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"octane.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nwlst = names(X)\nwl = parse.(Float64, wlst)\nn = nro(X)\n\nnlv = 3\nmodel = pcasph(; nlv)  \n#model = pcasvd(; nlv) \nfit!(model, X)\npnames(model)\npnames(model.fitm)\n@head T = model.fitm.T\n## Same as:\ntransf(model, X)\n\ni = 1\nplotxy(T[:, i], T[:, i + 1]; zeros = true, xlabel = string(\"PC\", i), \n    ylabel = string(\"PC\", i + 1)).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcasvd-Tuple{}","page":"Index of functions","title":"Jchemo.pcasvd","text":"pcasvd(; kwargs...)\npcasvd(X; kwargs...)\npcasvd(X, weights::Weight; kwargs...)\npcasvd!(X::Matrix, weights::Weight; kwargs...)\n\nPCA by SVD factorization.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of principal components (PCs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nLet us note D the (n, n) diagonal matrix of weights (weights.w) and X the centered matrix in metric D. The function minimizes ||X - T * V'||^2  in metric D, by  computing a SVD factorization of sqrt(D) * X:\n\nsqrt(D) * X ~ U * S * V'\n\nOutputs are:\n\nT = D^(-1/2) * U * S\nV = V\nThe diagonal of S   \n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie \npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest) \n@head Xtrain = X[s.train, :]\n@head Xtest = X[s.test, :]\n\nnlv = 3\nmodel = pcasvd(; nlv)\n#model = pcaeigen(; nlv)\n#model = pcaeigenk(; nlv)\n#model = pcanipals(; nlv)\nfit!(model, Xtrain)\npnames(model)\npnames(model.fitm)\n@head T = model.fitm.T\n## Same as:\n@head transf(model, X)\nT' * T\n@head V = model.fitm.V\nV' * V\n\n@head Ttest = transf(model, Xtest)\n\nres = summary(model, Xtrain) ;\npnames(res)\nres.explvarx\nres.contr_var\nres.coord_var\nres.cor_circle\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcr-Tuple{}","page":"Index of functions","title":"Jchemo.pcr","text":"pcr(; kwargs...)\npcr(X, Y; kwargs...)\npcr(X, Y, weights::Weight; kwargs...)\npcr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nPrincipal component regression (PCR) with a SVD factorization.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nSame as function pcasvd\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nmodel = pcr(; nlv) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\nfitm = model.fitm ;\npnames(fitm)\npnames(fitm.fitm)\n\n@head fitm.fitm.T\n@head transf(model, X)\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\nres = predict(model, Xtest; nlv = 1:2)\n@head res.pred[1]\n@head res.pred[2]\n\nres = summary(model, Xtrain) ;\npnames(res)\nz = res.explvarx\nplotgrid(z.nlv, z.cumpvar; step = 2, xlabel = \"Nb. LVs\", ylabel = \"Prop. Explained X-Variance\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pip-Tuple","page":"Index of functions","title":"Jchemo.pip","text":"pip(args...)\n\nBuild a pipeline of models.\n\nargs... : Succesive models, see examples.\n\nExamples\n\nusing JLD2, CairoMakie, JchemoData\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\n## Pipeline Snv :> Savgol :> Pls :> Svmr\n\nmodel1 = snv()\nmodel2 = savgol(npoint = 11, deriv = 2, degree = 3)\nmodel3 = plskern(nlv = 15)\nmodel4 = svmr(gamma = 1e3, cost = 1000, epsilon = .1)\nmodel = pip(model1, model2, model3, model4)\nfit!(model, Xtrain, ytrain)\nres = predict(model, Xtest) ; \n@head res.pred \nrmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",\n      ylabel = \"Observed\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plist-Tuple{Any}","page":"Index of functions","title":"Jchemo.plist","text":"plist(x)\n\nPrint each element of a list.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plotconf-Tuple{Any}","page":"Index of functions","title":"Jchemo.plotconf","text":"plotconf(object; size = (500, 400), cnt = true, ptext = true, \n    fontsize = 15, coldiag = :red, )\n\nPlot a confusion matrix.\n\nobject : Output of function conf.\n\nKeyword arguments:\n\nsize : Size (horizontal, vertical) of the figure.\ncnt : Boolean. If true, plot the occurrences,    else plot the row %s.\nptext : Boolean. If true, display the value in each cell.\nfontsize : Font size when ptext = true.\ncoldiag : Font color when ptext = true.\n\nSee examples in help page of function conf. ```\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plotgrid-Tuple{AbstractVector, Any}","page":"Index of functions","title":"Jchemo.plotgrid","text":"plotgrid(indx::AbstractVector, r; size = (500, 300), step = 5, \n    color = nothing, kwargs...)\nplotgrid(indx::AbstractVector, r, group; size = (700, 350), \n    step = 5, color = nothing, leg = true, leg_title = \"Group\", kwargs...)\n\nPlot error/performance rates of a model.\n\nindx : A numeric variable representing the grid of    model parameters, e.g. the nb. LVs if PLSR models.\nr : The error/performance rate.\n\nKeyword arguments: \n\ngroup : Categorical variable defining groups.    A separate line is plotted for each level of group.\nsize : Size (horizontal, vertical) of the figure.\nstep : Step used for defining the xticks.\ncolor : Set color. If group if used, must be a vector    of same length as the number of levels in group.\nleg : Boolean. If group is used, display a legend or not.\nleg_title : Title of the legend.\nkwargs : Optional arguments to pass in Axis of CairoMakie.\n\nTo use plotgrid, a backend (e.g. CairoMakie) has to  be specified.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nmodel = plskern() \nnlv = 0:20\nres = gridscore(model, Xtrain, ytrain, \n    Xtest, ytest; score = rmsep, nlv)\nplotgrid(res.nlv, res.y1; xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\n\nmodel = lwplsr() \nnlvdis = 15 ; metric = [:mah]\nh = [1 ; 2.5 ; 5] ; k = [50 ; 100] \npars = mpar(nlvdis = nlvdis, metric = metric, \n    h = h, k = k)\nnlv = 0:20\nres = gridscore(model, Xtrain, ytrain, Xtest, ytest; score = rmsep, \n    pars, nlv)\ngroup = string.(\"h=\", res.h, \" k=\", res.k)\nplotgrid(res.nlv, res.y1, group; xlabel = \"Nb. LVs\", ylabel = \"RMSECV\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plotsp","page":"Index of functions","title":"Jchemo.plotsp","text":"plotsp(X, wl = 1:nco(X); size = (500, 300), color = nothing, nsamp = nothing, \n    kwargs...)\n\nPlotting spectra.\n\nX : X-data (n, p).\nwl : Column names of X. Must be numeric.\n\nKeyword arguments:\n\nsize : Size (horizontal, vertical) of the figure.\ncolor : Set a unique color (and eventually transparency)    to the spectra.\nnsamp : Nb. spectra (X-rows) to plot. If nothing,    all spectra are plotted.\nkwargs : Optional arguments to pass in Axis of CairoMakie.\n\nThe function plots the rows of X.\n\nTo use plotxy, a backend (e.g. CairoMakie) has  to be specified.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nwlst = names(X)\nwl = parse.(Float64, wlst) \n\nplotsp(X).f\nplotsp(X; color = (:red, .2)).f\nplotsp(X, wl; xlabel = \"Wavelength (nm)\", ylabel = \"Absorbance\").f\n\ntck = collect(wl[1]:200:wl[end]) ;\nplotsp(X, wl; xlabel = \"Wavelength (nm)\", ylabel = \"Absorbance\", xticks = tck).f\n\nf, ax = plotsp(X, wl; color = (:red, .2))\nxmeans = colmean(X)\nlines!(ax, wl, xmeans; color = :black, linewidth = 2)\nvlines!(ax, 1200)\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plotxy-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.plotxy","text":"plotxy(x, y; size = (500, 300), color = nothing, ellipse::Bool = false, \n    prob = .95, circle::Bool = false, bisect::Bool = false, zeros::Bool = false,\n    xlabel = \"\", ylabel = \"\", title = \"\", kwargs...)\nplotxy(x, y, group; size = (600, 350), color = nothing, ellipse::Bool = false, \n    prob = .95, circle::Bool = false, bisect::Bool = false, zeros::Bool = false,\n    xlabel = \"\", ylabel = \"\", title = \"\", leg::Bool = true, leg_title = \"Group\", \n    kwargs...)\n\nScatter plot of (x, y) data\n\nx : A x-vector (n).\ny : A y-vector (n). \ngroup : Categorical variable defining groups (n). \n\nKeyword arguments:\n\nsize : Size (horizontal, vertical) of the figure.\ncolor : Set color(s). If group if used, color must be    a vector of same length as the number of levels in group.\nellipse : Boolean. Draw an ellipse of confidence,    assuming a Ch-square distribution with df = 2. If group    is used, one ellipse is drawn per group.\nprob : Probability for the ellipse of confidence.\nbisect : Boolean. Draw a bisector.\nzeros : Boolean. Draw horizontal and vertical axes passing    through origin (0, 0).\nxlabel : Label for the x-axis.\nylabel : Label for the y-axis.\ntitle : Title of the graphic.\nleg : Boolean. If group is used, display a legend    or not.\nleg_title : Title of the legend.\nkwargs : Optional arguments to pass in function scatter    of Makie.\n\nTo use plotxy, a backend (e.g. CairoMakie) has  to be specified.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\nlev = mlev(year)\nnlev = length(lev)\n\nmodel = pcasvd(nlv = 5)  \nfit!(model, X) \n@head T = model.fitm.T\n\nplotxy(T[:, 1], T[:, 2]; color = (:red, .5)).f\n\nplotxy(T[:, 1], T[:, 2], year; ellipse = true, xlabel = \"PC1\", ylabel = \"PC2\").f\n\ni = 2\ncolm = cgrad(:Dark2_5, nlev; categorical = true)\nplotxy(T[:, i], T[:, i + 1], year; color = colm, xlabel = string(\"PC\", i), \n    ylabel = string(\"PC\", i + 1), zeros = true, ellipse = true).f\n\nplotxy(T[:, 1], T[:, 2], year).lev\n\nplotxy(1:5, 1:5).f\n\ny = reshape(rand(5), 5, 1)\nplotxy(1:5, y).f\n\n## Several layers can be added\n## (same syntax as in Makie)\nA = rand(50, 2)\nf, ax = plotxy(A[:, 1], A[:, 2]; xlabel = \"x1\", ylabel = \"x2\")\nylims!(ax, -1, 2)\nhlines!(ax, 0.5; color = :red, linestyle = :dot)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plscan-Tuple{}","page":"Index of functions","title":"Jchemo.plscan","text":"plscan(; kwargs...)\nplscan(X, Y; kwargs...)\nplscan(X, Y, weights::Weight; kwargs...)\nplscan!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nCanonical partial least squares regression (Canonical PLS).\n\nX : First block of data.\nY : Second block of data.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. Possible values are:   :none, :frob. See functions blockscal.\nscal : Boolean. If true, each column of blocks X    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nCanonical PLS with the Nipals algorithm (Wold 1984,  Tenenhaus 1998 chap.11), referred to as PLS-W2A (i.e. Wold  PLS mode A) in Wegelin 2000. The two blocks X and X  play a symmetric role.  After each step of scores computation,  X and Y are deflated by the x- and y-scores, respectively. \n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie  et pratique. Editions Technip, Paris.\n\nWegelin, J.A., 2000. A Survey of Partial Least  Squares (PLS) Methods, with Emphasis on the Two-Block  Case (No. 371). University of Washington, Seattle,  Washington, USA.\n\nWold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984.  The Collinearity Problem in Linear Regression. The Partial  Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5,  735–743. https://doi.org/10.1137/0905052\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn, p = size(X)\nq = nco(Y)\n\nnlv = 2\nbscal = :frob\nmodel = plscan(; nlv, bscal)\nfit!(model, X, Y)\npnames(model)\npnames(model.fitm)\n\nfitm = model.fitm\n@head fitm.Tx\n@head transfbl(model, X, Y).Tx\n\n@head fitm.Ty\n@head transfbl(model, X, Y).Ty\n\nres = summary(model, X, Y) ;\npnames(res)\nres.explvarx\nres.explvary\nres.cort2t \nres.rdx\nres.rdy\nres.corx2t \nres.cory2t \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plskdeda-Tuple{}","page":"Index of functions","title":"Jchemo.plskdeda","text":"plskdeda(; kwargs...)\nplskdeda(X, y; kwargs...)\nplskdeda(X, y, weights::Weight; kwargs...)\n\nKDE-DA on PLS latent variables (PLS-KDEDA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\nKeyword arguments of function dmkern (bandwidth    definition) can also be specified here.\n\nThe principle is the same as function plsqda except that the  densities by class are estimated from dmkern instead of dmnorm.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nmodel = plskdeda(; nlv) \n#model = plskdeda(; nlv, a = .5)\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nembfitm = fitm.fitm.embfitm ;\n@head embfitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(embfitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\nsummary(embfitm, Xtrain)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plskern-Tuple{}","page":"Index of functions","title":"Jchemo.plskern","text":"plskern(; kwargs...)\nplskern(X, Y; kwargs...)\nplskern(X, Y, weights::Weight; kwargs...)\nplskern!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nPartial least squares regression (PLSR) with the \"improved kernel algorithm #1\"      (Dayal & McGegor, 1997).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nAbout the row-weighting in PLS algorithms (weights): See in particular Schaal et al. 2002, Siccard & Sabatier 2006,  Kim et al. 2011, and Lesnoff et al. 2020. \n\nReferences\n\nDayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.\n\nKim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation  of active pharmaceutical ingredients content using locally  weighted partial least squares and statistical wavelength  selection. Int. J. Pharm., 421, 269-274.\n\nLesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally  weighted PLS strategies for regression and discrimination on  agronomic NIR Data. Journal of Chemometrics. e3209.  https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209\n\nSchaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable  techniques from nonparametric statistics for the real time  robot learning. Applied Intell., 17, 49-60.\n\nSicard, E. Sabatier, R., 2006. Theoretical framework for local  PLS1 regression and application to a rainfall dataset. Comput. Stat.  Data Anal., 51, 1393-1410.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nmodel = plskern(; nlv) ;\n#model = plsnipals(; nlv) ;\n#model = plswold(; nlv) ;\n#model = plsrosa(; nlv) ;\n#model = plssimp(; nlv) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\nres = predict(model, Xtest; nlv = 1:2)\n@head res.pred[1]\n@head res.pred[2]\n\nres = summary(model, Xtrain) ;\npnames(res)\nz = res.explvarx\nplotgrid(z.nlv, z.cumpvar; step = 2, xlabel = \"Nb. LVs\", \n    ylabel = \"Prop. Explained X-Variance\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plslda-Tuple{}","page":"Index of functions","title":"Jchemo.plslda","text":"plslda(; kwargs...)\nplslda(X, y; kwargs...)\nplslda(X, y, weights::Weight; kwargs...)\n\nLDA on PLS latent variables (PLS-LDA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nLDA on PLS latent variables. The method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nA multivariate PLSR (PLSR2) is run on {X, Ydummy}, returning   a score matrix T.\nA LDA is done on {T, y}, returning estimates of posterior probabilities  (∊ [0, 1]) of class membership.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nmodel = plslda(; nlv) \n#model = plslda(; nlv, prior = :prop) \n#model = plsqda(; nlv, alpha = .1) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nembfitm = fitm.fitm.embfitm ;\n@head embfitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(embfitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\nsummary(embfitm, Xtrain)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plsnipals-Tuple{}","page":"Index of functions","title":"Jchemo.plsnipals","text":"plsnipals(; kwargs...)\nplsnipals(X, Y; kwargs...)\nplsnipals(X, Y, weights::Weight; kwargs...)\nplsnipals!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nPartial Least Squares Regression (PLSR) with the Nipals algorithm.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nIn this function, for PLS2 (multivariate Y), the Nipals  iterations are replaced by a direct computation of the  PLS weights (w) by SVD decomposition of matrix X'Y  (Hoskuldsson 1988 p.213).\n\nSee function plskern for examples.\n\nReferences\n\nHoskuldsson, A., 1988. PLS regression methods. Journal of  Chemometrics 2, 211-228.https://doi.org/10.1002/cem.1180020306\n\nTenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique.  Editions Technip, Paris, France.\n\nWold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression:  a basic tool for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plsqda-Tuple{}","page":"Index of functions","title":"Jchemo.plsqda","text":"plsqda(; kwargs...)\nplsqda(X, y; kwargs...)\nplsqda(X, y, weights::Weight; kwargs...)\n\nQDA on PLS latent variables (PLS-QDA) with continuum.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nQDA on PLS latent variables. The method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nA multivariate PLSR (PLSR2) is run on {X, Ydummy}, returning   a score matrix T.\nA QDA (possibly with continuum) is done on {T, y}, returning estimates  of posterior probabilities (∊ [0, 1]) of class membership.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nSee function plslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plsravg-Tuple{}","page":"Index of functions","title":"Jchemo.plsravg","text":"plsravg(; kwargs...)\nplsravg(X, Y; kwargs...)\nplsravg(X, Y, weights::Weight; kwargs...)\nplsravg!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nAveraging PLSR models with different numbers of  latent variables (PLSR-AVG).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : A range of nb. of latent variables (LVs)    to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nEnsemblist method where the predictions are computed  by averaging the predictions of a set of models built  with different numbers of LVs.\n\nFor instance, if argument nlv is set to nlv = 5:10,  the prediction for a new observation is the simple average of the predictions returned by the models with 5 LVs, 6 LVs,  ... 10 LVs, respectively.\n\nReferences\n\nLesnoff, M., Andueza, D., Barotin, C., Barre, V., Bonnal, L.,  Fernández Pierna, J.A., Picard, F., Vermeulen, V., Roger,  J.-M., 2022. Averaging and Stacking Partial Least Squares  Regression Models to Predict the Chemical Compositions and  the Nutritive Values of Forages from Spectral Near Infrared  Data. Applied Sciences 12, 7850.  https://doi.org/10.3390/app12157850\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n@head Y\ny = Y.ndf\n#y = Y.dm\nn = nro(X)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(y, s)\nXtest = X[s, :]\nytest = y[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\n\nnlv = 0:30\n#nlv = 5:20\n#nlv = 25\nmodel = plsravg(; nlv) ;\nfit!(model, Xtrain, ytrain)\n\nres = predict(model, Xtest)\n@head res.pred\nres.predlv   # predictions for each nb. of LVs \n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plsrda-Tuple{}","page":"Index of functions","title":"Jchemo.plsrda","text":"plsrda(; kwargs...)\nplsrda(X, y; kwargs...)\nplsrda(X, y, weights::Weight; kwargs...)\n\nDiscrimination based on partial least squares regression (PLSR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation.\n\nThis is the usual \"PLSDA\". The method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nThen, a multivariate PLSR (PLSR2) is run on {X, Ydummy}, returning   predictions of the dummy variables (= object posterior returned by   fuction predict).  These predictions can be considered as unbounded estimates   (i.e. eventuall outside of [0, 1]) of the class membership probabilities.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior): \n\nthe sub-totals by class of the observation weights are set equal to the    prior probabilities.\n\nThe low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nmodel = plsrda(; nlv) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\npnames(fitm.fitm)\naggsum(fitm.fitm.weights.w, ytrain)\n\n@head fitm.fitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(fitm.fitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\nsummary(fitm.fitm, Xtrain)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plsrosa-Tuple{}","page":"Index of functions","title":"Jchemo.plsrosa","text":"plsrosa(; kwargs...)\nplsrosa(X, Y; kwargs...)\nplsrosa(X, Y, weights::Weight; kwargs...)\nplsrosa!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nPartial Least Squares Regression (PLSR) with the  ROSA algorithm (Liland et al. 2016).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nNote: The function has the following differences with  the original algorithm of Liland et al. (2016):\n\nScores T (LVs) are not normed.\nMultivariate Y is allowed.\n\nSee function plskern for examples.\n\nReferences\n\nLiland, K.H., Næs, T., Indahl, U.G., 2016. ROSA—a fast  extension of partial least squares regression for multiblock  data analysis. Journal of Chemometrics 30, 651–662.  https://doi.org/10.1002/cem.2824\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plsrout-Tuple{}","page":"Index of functions","title":"Jchemo.plsrout","text":"plsrout(; kwargs...)\nplsrout(X, Y; kwargs...)\nplsrout(X, Y, weights::Weight; kwargs...)\npcaout!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nRobust PLSR using outlierness.\n\nX : X-data (n, p). \nY : Y-data (n, q). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. of latent variables (LVs).\nprm : Proportion of the data removed (hard rejection of outliers)    for each outlierness measure.\nscal : Boolean. If true, each column of X is scaled by its MAD    when computing the outlierness and by its uncorrected standard    deviation when computing weighted PCA. \n\nRobust PLSR combining outlyingness measures and weighted PLSR (WPLSR). This is the same principle as function pcaout (see the help page) but the final step is a weighted PLSR instead of a weighted PCA.  \n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nmodel = plsrout(; nlv)\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\",  \n    ylabel = \"Observed\").f    \n\nres = predict(model, Xtest; nlv = 1:2)\n@head res.pred[1]\n@head res.pred[2]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plssimp-Tuple{}","page":"Index of functions","title":"Jchemo.plssimp","text":"plssimp(; kwargs...)\nplssimp(X, Y; kwargs...)\nplssimp(X, Y, weights::Weight; kwargs...)\nplssimp!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nPartial Least Squares Regression (PLSR) with the SIMPLS algorithm (de Jong 1993).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nNote: In this function, scores T (LVs) are not normed,  conversely to the original algorithm of de Jong (2013).\n\nSee function plskern for examples.\n\nReferences\n\nde Jong, S., 1993. SIMPLS: An alternative approach to  partial least squares regression. Chemometrics and Intelligent  Laboratory Systems 18, 251–263.  https://doi.org/10.1016/0169-7439(93)85002-X\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plstuck-Tuple{}","page":"Index of functions","title":"Jchemo.plstuck","text":"plstuck(; kwargs...)\nplstuck(X, Y; kwargs...)\nplstuck(X, Y, weights::Weight; kwargs...)\nplstuck!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nTucker's inter-battery method of factor analysis\n\nX : First block of data.\nY : Second block of data.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. Possible values are:   :none, :frob. See functions blockscal.\nscal : Boolean. If true, each column of blocks X    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nInter-battery method of factor analysis (Tucker 1958,  Tenenhaus 1998 chap.3). The two blocks X and X play  a symmetric role.  This method is referred to as PLS-SVD  in Wegelin 2000. The basis of the method is to factorize  the covariance matrix X'Y by SVD. \n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie  et pratique. Editions Technip, Paris.\n\nTishler, A., Lipovetsky, S., 2000. Modelling and forecasting  with robust canonical analysis: method and application.  Computers & Operations Research 27, 217–232.  https://doi.org/10.1016/S0305-0548(99)00014-3\n\nTucker, L.R., 1958. An inter-battery method of factor  analysis. Psychometrika 23, 111–136. https://doi.org/10.1007/BF02289009\n\nWegelin, J.A., 2000. A Survey of Partial Least Squares (PLS)  Methods, with Emphasis on the Two-Block Case (No. 371).  University of Washington, Seattle, Washington, USA.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n\nmodel = plstuck(nlv = 3)\nfit!(model, X, Y) \npnames(model)\npnames(model.fitm)\n\nfitm = model.fitm\n@head fitm.Tx\n@head transfbl(model, X, Y).Tx\n\n@head fitm.Ty\n@head transfbl(model, X, Y).Ty\n\nres = summary(fitm, X, Y)\npnames(res)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plswold-Tuple{}","page":"Index of functions","title":"Jchemo.plswold","text":"plswold(; kwargs...)\nplswold(X, Y; kwargs...)\nplswold(X, Y, weights::Weight; kwargs...)\nplswold!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nPartial Least Squares Regression (PLSR) with the Wold algorithm \n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\ntol : Tolerance for the Nipals algorithm.\nmaxit : Maximum number of iterations for the Nipals algorithm.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nWold Nipals PLSR algorithm: Tenenhaus 1998 p.204.\n\nSee function plskern for examples.\n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique.  Editions Technip, Paris, France.\n\nWold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The  Collinearity Problem in Linear Regression. The Partial Least  Squares (PLS). Approach to Generalized Inverses. SIAM Journal on  Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pmod-Tuple{Any}","page":"Index of functions","title":"Jchemo.pmod","text":"pmod(foo)\n\nShortcut for function parentmodule.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pnames-Tuple{Any}","page":"Index of functions","title":"Jchemo.pnames","text":"pnames(x)\n\nReturn the names of the elements of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Calds, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Calds, X; kwargs...)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Calpds, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Calpds, X; kwargs...)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Cglsr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. iterations, or collection of    nb. iterations, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Dkplsr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Dmkern, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Dmkern, x)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nx : Data (vector) for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Dmnorm, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Dmnorm, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : Data (vector) for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Knnda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Knnda1, X)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Knnr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Knnr, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Kplsr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\nIf nothing, it is the maximum nb. LVs.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Krr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Krr, X; lb = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nlb : Regularization parameter, or collection of    regularization parameters, \"lambda\" to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Loessr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Loessr, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwmlr, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwmlrda, X)\n\nCompute y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplslda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplslda, X; nlv = nothing)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplsqda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplsqda, X; nlv = nothing)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplsr, X; nlv = nothing)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplsravg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplsravg, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplsrda, X; nlv = nothing)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Mbplsprobda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Mbplsprobda, Xbl; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Mbplsrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Mbplsrda, Xbl; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Mlrda, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occod, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occod, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occsd, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occsd, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occsdod, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occstah, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occstah, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Pcr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Pcr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plsprobda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plsprobda, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plsravg, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plsrda, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs,    to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Qda, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Rosaplsr, Xbl; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Rr, X; lb = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nlb : Regularization parameter, or collection of    regularization parameters, \"lambda\" to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Rrda, X; lb = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nlb : Regularization parameter, or collection of regularization parameters,    \"lambda\" to consider. If nothing, it is the parameter stored in the    fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Soplsr, Xbl)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Spcr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Spcr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Svmda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Svmda, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Svmr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Svmr, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Treeda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Treeda, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Treer, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Treer, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Union{Jchemo.Lda, Jchemo.Qda}, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Union{Lda, Qda}, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Union{Mbplsr, Mbplswest}, Xbl; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Union{Jchemo.Mlr, Jchemo.MlrNoArg, Jchemo.Rrchol}, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Mlr, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Union{Plsr, Pcr, Splsr}, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.psize-Tuple{Any}","page":"Index of functions","title":"Jchemo.psize","text":"psize(x)\n\nPrint the type and size of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pval-Tuple{Distributions.Distribution, Any}","page":"Index of functions","title":"Jchemo.pval","text":"pval(d::Distribution, q)\npval(x::Array, q)\npval(e_cdf::ECDF, q)\n\nCompute p-value(s) for a distribution, an ECDF or vector.\n\nd : A distribution computed from Distribution.jl.\nx : Univariate data.\ne_cdf : An ECDF computed from StatsBase.jl.\nq : Value(s) for which to compute the p-value(s).\n\nCompute or estimate the p-value of quantile q, ie. V(Q > q) where Q is the random variable.\n\nExamples\n\nusing Jchemo, Distributions, StatsBase\n\nd = Distributions.Normal(0, 1)\nq = 1.96\n#q = [1.64; 1.96]\nDistributions.cdf(d, q)    # cumulative density function (CDF)\nDistributions.ccdf(d, q)   # complementary CDF (CCDF)\npval(d, q)                 # Distributions.ccdf\n\nx = rand(5)\ne_cdf = StatsBase.ecdf(x)\ne_cdf(x)                # empirical CDF computed at each point of x (ECDF)\np_val = 1 .- e_cdf(x)   # complementary ECDF at each point of x\nq = .3\n#q = [.3; .5; 10]\npval(e_cdf, q)          # 1 .- e_cdf(q)\npval(x, q)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.qda-Tuple{}","page":"Index of functions","title":"Jchemo.qda","text":"qda(; kwargs...)\nqda(X, y; kwargs...)\nqda(X, y, weights::Weight; kwargs...)\n\nQuadratic discriminant analysis (QDA, with continuum towards LDA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nFor the continuum approach, a value alpha > 0 shrinks the class-covariances  by class (Wi) toward a common LDA covariance (\"within\" W). This corresponds to  the \"first regularization (Eqs.16)\" described in Friedman 1989 (where alpha  is referred to as \"lambda\").\n\nReferences\n\nFriedman JH. Regularized Discriminant Analysis. Journal  of the American Statistical Association. 1989;  84(405):165-175. doi:10.1080/01621459.1989.10478752.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\")\n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest)\nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\nntrain = n - ntest\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nmodel = qda()\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\naggsum(fitm.weights.w, ytrain)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n## With regularization\nmodel = qda(alpha = .5)\n#model = qda(alpha = 1) # = LDA\nfit!(model, Xtrain, ytrain)\nmodel.fitm.Wi\nres = predict(model, Xtest) ;\nerrp(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.r2-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.r2","text":"r2(pred, Y)\n\nCompute the R2 coefficient.\n\npred : Predictions.\nY : Observed data.\n\nThe rate R2 is calculated by:\n\nR2 = 1 - MSEP(current model) / MSEP(null model) \n\nwhere the \"null model\" is the overall mean.  For predictions over CV or test sets, and/or for  non linear models, it can be different from the square  of the correlation coefficient (cor2) between the true  data and the predictions. \n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nr2(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nr2(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rasvd-Tuple{}","page":"Index of functions","title":"Jchemo.rasvd","text":"rasvd(; kwargs...)\nrasvd(X, Y; kwargs...)\nrasvd(X, Y, weights::Weight; kwargs...)\nrasvd!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nRedundancy analysis (RA), aka PCA on instrumental variables (PCAIV)\n\nX : First block of data.\nY : Second block of data.\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling. Possible values are:   :none, :frob. See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\nscal : Boolean. If true, each column of blocks X    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nSee e.g. Bougeard et al. 2011a,b and Legendre & Legendre 2012.  Let Yhat be the fitted values of the regression of Y on X.  The scores Ty are the PCA scores of Yhat. The scores Tx are  the fitted values of the regression of Ty on X.\n\nA continuum regularization is available.  After block  centering and scaling, the covariances matrices are computed  as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting  the covariance matrices. Often, a better alternative is  to use an epsilon value (e.g. tau = 1e-8) to get similar  results as with pseudo-inverses.    \n\nReferences\n\nBougeard, S., Qannari, E.M., Lupo, C., Chauvin, C.,  2011-a. Multiblock redundancy analysis from a user's  perspective. Application in veterinary epidemiology.  Electronic Journal of Applied Statistical Analysis  4, 203-214. https://doi.org/10.1285/i20705948v4n2p203\n\nBougeard, S., Qannari, E.M., Rose, N., 2011-b. Multiblock  redundancy analysis: interpretation tools and application  in epidemiology. Journal of Chemometrics 25,  467-475. https://doi.org/10.1002/cem.1392\n\nLegendre, V., Legendre, L., 2012. Numerical Ecology.  Elsevier, Amsterdam, The Netherlands.\n\nTenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized  and Sparse Generalized Canonical Correlation Analysis  for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn, p = size(X)\nq = nco(Y)\n\nnlv = 2\nbscal = :frob ; tau = 1e-4\nmodel = rasvd(; nlv, bscal, tau)\nfit!(model, X, Y)\npnames(model)\npnames(model.fitm)\n\n@head model.fitm.Tx\n@head transfbl(model, X, Y).Tx\n\n@head model.fitm.Ty\n@head transfbl(model, X, Y).Ty\n\nres = summary(model, X, Y) ;\npnames(res)\nres.explvarx\nres.cort2t \nres.rdx\nres.rdy\nres.corx2t \nres.cory2t \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rd-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rd","text":"rd(X, Y; typ = :cor)\nrd(X, Y, weights::Weight; typ = :cor)\n\nCompute redundancy coefficients between two matrices.\n\nX : Matrix (n, p).\nY : Matrix (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\ntyp : Possibles values are: :cor (correlation),    :cov (uncorrected covariance). \n\nReturns the redundancy coefficient between X and each column of Y, i.e.: \n\n(1 / p) * [Sum.(j=1, .., p) cor(xj, y1)^2 ; ... ; Sum.(j=1, .., p) cor(xj, yq)^2] \n\nSee Tenenhaus 1998 section 2.2.1 p.10-11.\n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.\n\nExamples\n\nusing Jchemo\nX = rand(5, 10)\nY = rand(5, 3)\nrd(X, Y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rda-Tuple{}","page":"Index of functions","title":"Jchemo.rda","text":"rda(; kwargs...)\nrda(X, y; kwargs...)\nrda(X, y, weights::Weight; kwargs...)\n\nRegularized discriminant analysis (RDA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nlb : Ridge regularization parameter \"lambda\" (>= 0).\nsimpl : Boolean. See function dmnorm. \nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nLet us note W the (corrected) pooled within-class  covariance matrix and Wi the (corrected) within-class  covariance matrix of class i. The regularization is done  by the two following successive steps (for each class i):\n\nContinuum between QDA and LDA: Wi(1) = (1 - alpha) * Wi + alpha * W       \nRidge regularization: Wi(2) = Wi(1) + lb * I\n\nThen the QDA algorithm is run on matrices {Wi(2)}.\n\nFunction rda is slightly different from the regularization  expression used by Friedman 1989 (Eq.18): the choice is to shrink  the covariance matrices Wi(2) to the diagonal of the Idendity  matrix (ridge regularization; e.g. Guo et al. 2007).  \n\nParticular cases:\n\nalpha = 1 & lb = 0 : LDA\nalpha = 0 & lb = 0 : QDA\nalpha = 1 & lb > 0 : Penalized LDA    (Hastie et al 1995) with diagonal regularization    matrix\n\nSee functions lda and qda for other details (arguments weights and prior).\n\nReferences\n\nFriedman JH. Regularized Discriminant Analysis.  Journal of the American Statistical Association. 1989;  84(405):165-175. doi:10.1080/01621459.1989.10478752.\n\nGuo Y, Hastie T, Tibshirani R. Regularized linear  discriminant analysis and its application in microarrays.  Biostatistics. 2007; 8(1):86-100.  doi:10.1093/biostatistics/kxj035.\n\nHastie, T., Buja, A., Tibshirani, R., 1995. Penalized  Discriminant Analysis. The Annals of Statistics 23, 73–102.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\")\n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\ny = dat.X[:, 5]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest)\nXtrain = X[s.train, :]\nytrain = y[s.train]\nXtest = X[s.test, :]\nytest = y[s.test]\nntrain = n - ntest\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nalpha = .5\nlb = 1e-8\nmodel = rda(; alpha, lb)\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_catbydict-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.recod_catbydict","text":"recod_catbydict(x, dict)\n\nRecode a categorical variable to dictionnary levels.\n\nx : Categorical variable (n) to replace.\ndict : Dictionary giving the correpondances between the old    and new levels.\n\nSee examples.\n\nExamples\n\nusing Jchemo\n\ndict = Dict(\"a\" => 1000, \"b\" => 1, \"c\" => 2)\nx = [\"c\" ; \"c\" ; \"a\" ; \"a\" ; \"a\"]\nrecod_catbydict(x, dict)\n\nx = [\"c\" ; \"c\" ; \"a\" ; \"a\" ; \"a\" ; \"e\"]\nrecod_catbydict(x, dict)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_catbyind-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.recod_catbyind","text":"recod_catbyind(x, lev)\n\nRecode a categorical variable to indexes of levels.\n\nx : Categorical variable (n) to replace.\nlev : Vector containing categorical levels.\n\nSee examples.\n\nWarning: The levels in x must be contained in lev.\n\nExamples\n\nusing Jchemo\n\nlev = [\"EHH\" ; \"FFS\" ; \"ANF\" ; \"CLZ\" ; \"CNG\" ; \"FRG\" ; \"MPW\" ; \"PEE\" ; \"SFG\" ; \"SFG\" ; \"TTS\"]\nslev = mlev(lev)\n[slev 1:length(slev)] \nx = [\"EHH\" ; \"TTS\" ; \"FRG\" ; \"EHH\"]\nrecod_catbyind(x, lev)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_catbyint-Tuple{Any}","page":"Index of functions","title":"Jchemo.recod_catbyint","text":"recod_catbyint(x; start = 1)\n\nRecode a categorical variable to integers.\n\nx : Categorical variable (n) to replace.\nstart : Integer labelling the first categorical level in x.\n\nThe integers returned by the function correspond to the sorted  levels of x, see examples.\n\nExamples\n\nusing Jchemo\n\nx = [\"b\", \"a\", \"b\"]\nmlev(x)   \n[x recod_catbyint(x)]\nrecod_catbyint(x; start = 0)\n\nrecod_catbyint([25, 1, 25])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_catbylev-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.recod_catbylev","text":"recod_catbylev(x, lev)\n\nRecode a categorical variable to levels.\n\nx : Variable (n) to replace.\nlev : Vector containing the categorical levels.\n\nThe ith sorted level in x is replaced by the ith sorted level in lev,  see examples.\n\nWarning: x and lev must contain the same number of levels.\n\nExamples\n\nusing Jchemo\n\nx = [10 ; 4 ; 3 ; 3 ; 4 ; 4]\nlev = [\"B\" ; \"C\" ; \"AA\" ; \"AA\"]\nmlev(x)\nmlev(lev)\n[x recod_catbylev(x, lev)]\nxstr = string.(x)\n[xstr recod_catbylev(xstr, lev)]\n\nlev = [3; 0; 0; -1]\nmlev(x)\nmlev(lev)\n[x recod_catbylev(x, lev)]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_indbylev-Tuple{Union{Int64, Array{Int64}}, Array}","page":"Index of functions","title":"Jchemo.recod_indbylev","text":"recod_indbylev(x::Union{Int, Array{Int}}, lev::Array)\n\nRecode an index variable to levels.\n\nx : Index variable (n) to replace.\nlev : Vector containing the categorical levels.\n\nAssuming slev = 'sort(unique(lev))', each element x[i] (i = 1, ..., n) is  replaced by slev[x[i]], see examples.\n\nWarning: Vector x must contain integers between 1 and nlev, where nlev is the number of levels in lev. \n\nExamples\n\nusing Jchemo\n\nx = [2 ; 1 ; 2 ; 2]\nlev = [\"B\" ; \"C\" ; \"AA\" ; \"AA\"]\nmlev(x)\nmlev(lev)\n[x recod_indbylev(x, lev)]\nrecod_indbylev([2], lev)\nrecod_indbylev(2, lev)\n\nx = [2 ; 1 ; 2]\nlev = [3 ; 0 ; 0 ; -1]\nmlev(x)\nmlev(lev)\nrecod_indbylev(x, lev)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_miss-Tuple{AbstractArray}","page":"Index of functions","title":"Jchemo.recod_miss","text":"recod_miss(X; miss = nothing)\nrecod_miss(df; miss = nothing)\n\nDeclare data as missing in a dataset.\n\nX : A dataset (array).\nmiss : The code used in the dataset to identify the data    to be declared as missing (of type Missing).\n\nSpecific for dataframes:\n\ndf : A dataset (dataframe).\n\nThe case miss = nothing has the only action to allow missing in X or df. \n\nSee examples.\n\nExamples\n\nusing Jchemo, DataFrames\n\nX = hcat(1:5, [0, 0, 7., 10, 1.2])\nX_miss = recod_miss(X; miss = 0)\n\ndf = DataFrame(i = 1:5, x = [0, 0, 7., 10, 1.2])\ndf_miss = recod_miss(df; miss = 0)\n\ndf = DataFrame(i = 1:5, x = [\"0\", \"0\", \"c\", \"d\", \"e\"])\ndf_miss = recod_miss(df; miss = \"0\")\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recod_numbyint-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.recod_numbyint","text":"recod_numbyint(x, q)\n\nRecode a continuous variable to integers.\n\nx : Continuous variable (n) to replace.\nq : Numerical values separating classes in x.   The first class is labelled to 1.  \n\nSee examples.\n\nExamples\n\nusing Jchemo, Statistics\nx = [collect(1:10); 8.1 ; 3.1] \n\nq = [3; 8]\nzx = recod_numbyint(x, q)  \n[x zx]\nprobs = [.33; .66]\nq = quantile(x, probs) \nzx = recod_numbyint(x, q)  \n[x zx]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recovkw-Tuple{DataType, Any}","page":"Index of functions","title":"Jchemo.recovkw","text":"recovkw(ParStruct, kwargs)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.residcla-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.residcla","text":"residcla(pred, y)\n\nCompute the discrimination residual vector (0 = no error, 1 = error).\n\npred : Predictions.\ny : Observed data (class membership).\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nytrain = rand([\"a\" ; \"b\"], 10)\nXtest = rand(4, 5) \nytest = rand([\"a\" ; \"b\"], 4)\n\nmodel = plsrda(; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nresidcla(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.residreg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.residreg","text":"residreg(pred, Y)\n\nCompute the regression residual vector.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nresidreg(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nresidreg(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rfda-Tuple{}","page":"Index of functions","title":"Jchemo.rfda","text":"rfda(; kwargs...)\nrfda(X, y::Union{Array{Int}, Array{String}}; kwargs...)\n\nRandom forest discrimination with DecisionTree.jl.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nn_trees : Nb. trees built for the forest. \npartial_sampling : Proportion of sampled    observations for each tree.\nn_subfeatures : Nb. variables to select at random    at each split (default: -1 ==> sqrt(#variables)).\nmax_depth : Maximum depth of the decision trees    (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples    each leaf needs to have.\nmin_sample_split : Minimum number of observations   in needed for a split.\nmth : Boolean indicating if a multi-threading is    done when new data are predicted with function predict.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe function fits a random forest discrimination² model using  package `DecisionTree.jl'.\n\nFor DA in DecisionTree.jl, 'y' components must be Int or String\n\nReferences\n\nBreiman, L., 1996. Bagging predictors. Mach Learn 24,  123–140. https://doi.org/10.1007/BF00058655\n\nBreiman, L., 2001. Random Forests. Machine Learning  45, 5–32. https://doi.org/10.1023/A:1010933404324\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGenuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis.  Université Paris Sud - Paris XI.\n\nGey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en  régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn, p = size(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nn_trees = 200\nn_subfeatures = p / 3 \nmax_depth = 10\nmodel = rfda(; n_trees, n_subfeatures, max_depth) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rfr-Tuple{}","page":"Index of functions","title":"Jchemo.rfr","text":"rfr(; kwargs...)\nrfr(X, y; kwargs...)\n\nRandom forest regression with DecisionTree.jl.\n\nX : X-data (n, p).\ny : Univariate y-data (n).\n\nKeyword arguments:\n\nn_trees : Nb. trees built for the forest. \npartial_sampling : Proportion of sampled    observations for each tree.\nn_subfeatures : Nb. variables to select at random    at each split (default: -1 ==> sqrt(#variables)).\nmax_depth : Maximum depth of the decision trees    (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples    each leaf needs to have.\nmin_sample_split : Minimum number of observations   in needed for a split.\nmth : Boolean indicating if a multi-threading is    done when new data are predicted with function predict.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe function fits a random forest regression model using  package `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., 1996. Bagging predictors. Mach Learn 24,  123–140. https://doi.org/10.1007/BF00058655\n\nBreiman, L., 2001. Random Forests. Machine Learning  45, 5–32. https://doi.org/10.1023/A:1010933404324\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGenuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis.  Université Paris Sud - Paris XI.\n\nGey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en  régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\np = nco(X)\n\nn_trees = 200\nn_subfeatures = p / 3\nmax_depth = 15\nmodel = rfr(; n_trees, n_subfeatures, max_depth) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}","page":"Index of functions","title":"Jchemo.rmcol","text":"rmcol(X::Union{AbstractMatrix, DataFrame}, s::Union{Vector, BitVector, UnitRange, Number})\nrmcol(X::Vector, s::Union{Vector, BitVector, UnitRange, Number})\n\nRemove the columns of a matrix or the components of a vector  having indexes s.\n\nX : Matrix or vector.\ns : Vector of the indexes.\n\nExamples\n\nusing Jchemo\n\nX = rand(5, 3) \nrmcol(X, [1, 3])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmgap-Tuple{}","page":"Index of functions","title":"Jchemo.rmgap","text":"rmgap(; kwargs...)\nrmgap(X; kwargs...)\n\nRemove vertical gaps in spectra (e.g. for ASD).  \n\nX : X-data (n, p).\n\nKeyword arguments:\n\nindexcol : Indexes (∈ [1, p]) of the X-columns where are    located the gaps to remove. \nnpoint : The number of X-columns used on the left side        of each gap for fitting the linear regressions.\n\nFor each spectra (row-observation of matrix X) and each  defined gap, the correction is done by extrapolation from  a simple linear regression computed on the left side of the gap. \n\nFor instance, If two gaps are observed between column-indexes 651-652 and between column-indexes 1425-1426, respectively, the syntax should  be indexcol = [651 ; 1425].\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/asdgap.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\n\nwl_target = [1000 ; 1800] \nindexcol = findall(in(wl_target).(wl))\n\nf, ax = plotsp(X, wl)\nvlines!(ax, wl_target; linestyle = :dot, color = (:grey, .8))\nf\n\n## Corrected data\nmodel = rmgap(; indexcol, npoint = 5)\nfit!(model, X)\nXc = transf(model, X)\nf, ax = plotsp(Xc, wl)\nvlines!(ax, wl_target; linestyle = :dot, color = (:grey, .8))\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, UnitRange, Vector, BitVector}}","page":"Index of functions","title":"Jchemo.rmrow","text":"rmrow(X::Union{AbstractMatrix, DataFrame}, s::Union{Vector, BitVector, UnitRange, Number})\nrmrow(X::Union{Vector, BitVector}, s::Union{Vector, BitVector, UnitRange, Number})\n\nRemove the rows of a matrix or the components of a vector  having indexes s.\n\nX : Matrix or vector.\ns : Vector of the indexes.\n\nExamples\n\nusing Jchemo\n\nX = rand(5, 2) \nrmrow(X, [1, 4])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmsep-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rmsep","text":"rmsep(pred, Y)\n\nCompute the square root of the mean of the squared      prediction errors (RMSEP).\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo \n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nrmsep(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nrmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmsepstand-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rmsepstand","text":"rmsepstand(pred, Y)\n\nCompute the standardized square root of the mean of the squared prediction errors      (RMSEP_stand).\n\npred : Predictions.\nY : Observed data.\n\nRMSEP is standardized to Y: \n\nRMSEP_stand = RMSEP ./ Y.\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nrmsepstand(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nrmsepstand(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rosaplsr-Tuple{}","page":"Index of functions","title":"Jchemo.rosaplsr","text":"rosaplsr(; kwargs...)\nrosaplsr(Xbl, Y; kwargs...)\nrosaplsr(Xbl, Y, weights::Weight; kwargs...)\nrosaplsr!(Xbl::Vector, Y::Matrix, weights::Weight; kwargs...)\n\nMultiblock ROSA PLSR (Liland et al. 2016).\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nscal : Boolean. If true, each column of blocks in Xbl    and Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThe function has the following differences with the  original algorithm of Liland et al. (2016):\n\nScores T are not normed to 1.\nMultivariate Y is allowed. In such a case,    the squared residuals are summed over the columns    for finding the winning block for each global LV    (therefore Y-columns should have the same fscale).\n\nReferences\n\nLiland, K.H., Næs, T., Indahl, U.G., 2016. ROSA — a fast  extension of partial least squares regression for multiblock  data analysis. Journal of Chemometrics 30, 651–662.  https://doi.org/10.1002/cem.2824\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ham.jld2\") \n@load db dat\npnames(dat) \nX = dat.X\nY = dat.Y\ny = Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\ns = 1:6\nXbltrain = mblock(X[s, :], listbl)\nXbltest = mblock(rmrow(X, s), listbl)\nytrain = y[s]\nytest = rmrow(y, s) \nntrain = nro(ytrain) \nntest = nro(ytest) \nntot = ntrain + ntest \n(ntot = ntot, ntrain , ntest)\n\nnlv = 3\nscal = false\n#scal = true\nmodel = rosaplsr(; nlv, scal)\nfit!(model, Xbltrain, ytrain)\npnames(model) \npnames(model.fitm)\n@head model.fitm.T\n@head transf(model, Xbltrain)\ntransf(model, Xbltest)\n\nres = predict(model, Xbltest)\nres.pred \nrmsep(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rowmean-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowmean","text":"rowmean(X)\n\nCompute row-wise means of a matrix.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nrowmean(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rownorm-Tuple{Any}","page":"Index of functions","title":"Jchemo.rownorm","text":"rownorm(X)\n\nCompute row-wise norms of a matrix.\n\nX : Data (n, p).\n\nThe norm computed for a row x of X is:\n\nsqrt(x' * x)\n\nReturn a vector.\n\nNote: Thanks to @mcabbott  at https://discourse.julialang.org/t/orders-of-magnitude-runtime-difference-in-row-wise-norm/96363.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\n\nrownorm(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rowstd-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowstd","text":"rowstd(X)\n\nCompute row-wise standard deviations (uncorrected) of a matrix`.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nrowstd(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rowsum-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowsum","text":"rowsum(X)\n\nCompute row-wise sums of a matrix.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n \nX = rand(5, 2) \nrowsum(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rowvar-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowvar","text":"rowvar(X)\n\nCompute row-wise variances (uncorrected) of a matrix.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nusing Jchemo\n\nn, p = 5, 6\nX = rand(n, p)\nrowvar(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rp-Tuple{}","page":"Index of functions","title":"Jchemo.rp","text":"rp(; kwargs...)\nrp(X; kwargs...)\nrp(X, weights::Weight; kwargs...)\nrp!(X::Matrix, weights::Weight; kwargs...)\n\nMake a random projection of X-data.\n\nX : X-data (n, p).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. dimensions on which X is projected.\nmeth : Method of random projection. Possible   values are: :gauss, :li. See the respective    functions rpmatgauss and rpmatli for their    keyword arguments.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nExamples\n\nusing Jchemo\nn, p = (5, 10)\nX = rand(n, p)\nnlv = 3\nmeth = :li ; s = sqrt(p) \n#meth = :gauss\nmodel = rp(; nlv, meth, s)\nfit!(model, X)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T \n@head model.fitm.V \ntransf(model, X[1:2, :])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rpd-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rpd","text":"rpd(pred, Y)\n\nCompute the ratio \"deviation to model performance\" (RPD).\n\npred : Predictions.\nY : Observed data.\n\nThis is the ratio of the deviation to the model performance  to the deviation, defined by:\n\nRPD = Std(Y) / RMSEP\n\nwhere Std(Y) is the standard deviation. \n\nSince Std(Y) = RMSEP(null model) where the null model is  the simple average, this also gives:\n\nRPD = RMSEP(null model) / RMSEP \n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nrpd(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nrpd(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rpdr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rpdr","text":"rpdr(pred, Y)\n\nCompute a robustified RPD.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nrpdr(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nrpdr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rpmatgauss","page":"Index of functions","title":"Jchemo.rpmatgauss","text":"rpmatgauss(p::Int, nlv::Int, Q = Float64)\n\nBuild a gaussian random projection matrix.\n\np : Nb. variables (attributes) to project.\nnlv : Nb. of simulated projection    dimensions.\nQ : Type of components of the built    projection matrix.\n\nThe function returns a random projection matrix V of  dimension p x nlv. The projection of a given matrix X  of size n x p is given by X * V.\n\nV is simulated from i.i.d. N(0, 1) / sqrt(nlv).\n\nReferences\n\nLi, V., Hastie, T.J., Church, K.W., 2006. Very sparse random  projections, in: Proceedings of the 12th ACM SIGKDD International  Conference on Knowledge Discovery and Data Mining, KDD ’06.  Association for Computing Machinery, New York, NY, USA, pp. 287–296.  https://doi.org/10.1145/1150402.1150436\n\nExamples\n\nusing Jchemo\np = 10 ; nlv = 3\nrpmatgauss(p, nlv)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rpmatli","page":"Index of functions","title":"Jchemo.rpmatli","text":"rpmatli(p::Int, nlv::Int, Q = Float64; s)\n\nBuild a sparse random projection matrix (Achlioptas 2001, Li et al. 2006).\n\np : Nb. variables (attributes) to project.\nnlv : Nb. of simulated projection    dimensions.\nQ : Type of components of the built    projection matrix.\n\nKeyword arguments:\n\ns : Coefficient defining the sparsity of the    returned matrix (higher is s, higher is the sparsity).\n\nThe function returns a random projection matrix V of  dimension p x nlv. The projection of a given matrix X  of size n x p is given by X * V.\n\nMatrix V is simulated from i.i.d. discrete  sampling within values: \n\n1 with prob. 1/(2 * s)\n0 with prob. 1 - 1 / s\n-1 with prob. 1/(2 * s)\n\nUsual values for s are:\n\nsqrt(p)       (Li et al. 2006)\np / log(p)  (Li et al. 2006)\n1               (Achlioptas 2001)\n3               (Achlioptas 2001) \n\nReferences\n\nAchlioptas, D., 2001. Database-friendly random projections,  in: Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART  Symposium on Principles of Database Systems, PODS ’01.  Association for Computing Machinery, New York, NY, USA, pp. 274–281.  https://doi.org/10.1145/375551.375608\n\nLi, V., Hastie, T.J., Church, K.W., 2006. Very sparse random  projections, in: Proceedings of the 12th ACM SIGKDD International  Conference on Knowledge Discovery and Data Mining, KDD ’06. Association  for Computing Machinery, New York, NY, USA, pp. 287–296.  https://doi.org/10.1145/1150402.1150436\n\nExamples\n\nusing Jchemo\np = 10 ; nlv = 3\nrpmatli(p, nlv)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rr-Tuple{}","page":"Index of functions","title":"Jchemo.rr","text":"rr(; kwargs...)\nrr(X, Y; kwargs...)\nrr(X, Y, weights::Weight; kwargs...)\nrr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nRidge regression (RR) implemented by SVD factorization.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nlb : Ridge regularization parameter \"lambda\".\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nReferences\n\nCule, E., De Iorio, M., 2012. A semi-automatic method  to guide the choice of ridge parameter in ridge regression.  arXiv:1205.0686.\n\nHastie, T., Tibshirani, R., 2004. Efficient quadratic  regularization for expression arrays. Biostatistics 5, 329-340.  https://doi.org/10.1093/biostatistics/kxh010\n\nHastie, T., Tibshirani, R., Friedman, J., 2009. The  elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.\n\nHoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased  Estimation for Nonorthogonal Problems. Technometrics 12, 55-67.  https://doi.org/10.1080/00401706.1970.10488634\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nlb = 1e-3\nmodel = rr(; lb) \n#model = rrchol(; lb) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\ncoef(model)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n## !! Only for function 'rr' (not for 'rrchol')\ncoef(model; lb = 1e-1)\nres = predict(model, Xtest; lb = [.1 ; .01])\n@head res.pred[1]\n@head res.pred[2]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rrchol-Tuple{}","page":"Index of functions","title":"Jchemo.rrchol","text":"rrchol(; kwargs...)\nrrchol(X, Y; kwargs...)\nrrchol(X, Y, weights::Weight; kwargs...)\nrrchol!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nRidge regression (RR) using the Normal equations      and a Cholesky factorization.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nlb : Ridge regularization parameter \"lambda\".\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nSee function rr for examples.\n\nReferences\n\nCule, E., De Iorio, M., 2012. A semi-automatic method  to guide the choice of ridge parameter in ridge regression.  arXiv:1205.0686.\n\nHastie, T., Tibshirani, R., 2004. Efficient quadratic  regularization for expression arrays. Biostatistics 5, 329-340.  https://doi.org/10.1093/biostatistics/kxh010\n\nHastie, T., Tibshirani, R., Friedman, J., 2009. The  elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.\n\nHoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased  Estimation for Nonorthogonal Problems. Technometrics 12, 55-67.  https://doi.org/10.1080/00401706.1970.10488634\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rrda-Tuple{}","page":"Index of functions","title":"Jchemo.rrda","text":"rrda(; kwargs...)\nrrda(X, y; kwargs...)\nrrda(X, y, weights::Weight; kwargs...)\n\nDiscrimination based on ridge regression (RR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nlb : Ridge regularization parameter \"lambda\".\nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe method is as follows:\n\nThe training variable y (univariate class membership) is   transformed to a dummy table (Ydummy) containing nlev columns,   where nlev is the number of classes present in y. Each column of   Ydummy is a dummy (0/1) variable. \nThen, a ridge regression (RR) is run on {X, Ydummy}, returning   predictions of the dummy variables (= object posterior returned by   fuction predict).  These predictions can be considered as unbounded estimates   (i.e. eventuall outside of [0, 1]) of the class membership probabilities.\nFor a given observation, the final prediction is the class   corresponding to the dummy variable for which the probability   estimate is the highest.\n\nIn the high-level version of the present functions, the observation  weights are automatically defined by the given priors (argument prior):  the sub-totals by class of the observation weights are set equal to the prior  probabilities. The low-level version (argument weights) allows to implement  other choices.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nlb = 1e-5\nmodel = rrda(; lb) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\npnames(fitm.fitm)\naggsum(fitm.fitm.weights.w, ytrain)\n\ncoef(fitm.fitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; lb = [.1; .01]).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rrr-Tuple{}","page":"Index of functions","title":"Jchemo.rrr","text":"rrr(; kwargs...)\nrrr(X, Y; kwargs...)\nrrr(X, Y, weights::Weight; kwargs...)\nrr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nReduced rank regression (RRR, aka RA).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\ntau : Regularization parameter (∊ [0, 1]).\ntol : Tolerance for the Nipals algorithm.\nmaxit : Maximum number of iterations for the Nipals algorithm.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nReduced rank regression, also referred to as redundancy  analysis (RA) regression. In this function, the RA uses  the Nipals algorithm presented in Mangamana et al 2021,  section 2.1.1.\n\nA continuum regularization is available. After block centering  and scaling, the covariances matrices are computed as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\n\nwhere D is the observation (row) metric. Value tau = 0  can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value  (e.g. tau = 1e-8) to get similar results as with pseudo-inverses.  \n\nReferences\n\nBougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011.  Multiblock redundancy analysis from a user’s perspective.  Application in veterinary epidemiology. Electronic Journal of  Applied Statistical Analysis 4, 203-214–214.  https://doi.org/10.1285/i20705948v4n2p203\n\nBougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy  analysis: interpretation tools and application in epidemiology.  Journal of Chemometrics 25, 467–475. https://doi.org/10.1002/cem.1392 \n\nTchandao Mangamana, E., Glèlè Kakaï, R., Qannari, E.M., 2021.  A general strategy for setting up supervised methods of multiblock  data analysis. Chemometrics and Intelligent Laboratory Systems  217, 104388.  https://doi.org/10.1016/j.chemolab.2021.104388\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 1\ntau = 1e-4\nmodel = rrr(; nlv, tau) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f   \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rv","text":"rv(X, Y; centr = true)\nrv(Xbl::Vector; centr = true)\n\nCompute the RV coefficient between matrices.\n\nX : Matrix (n, p).\nY : Matrix (n, q).\nXbl : A list (vector) of matrices.\ncentr : Boolean indicating if the matrices    will be internally centered or not.\n\nRV is bounded within [0, 1]. \n\nA dissimilarty measure between X and Y can be computed by d = sqrt(2 * (1 - RV)).\n\nReferences\n\nEscoufier, Y., 1973. Le Traitement des Variables Vectorielles.  Biometrics 29, 751–760. https://doi.org/10.2307/2529140\n\nJosse, J., Holmes, S., 2016. Measuring multivariate association and beyond.  Stat Surv 10, 132–167. https://doi.org/10.1214/16-SS116\n\nJosse, J., Pagès, J., Husson, F., 2008. Testing the significance of  the RV coefficient. Computational Statistics & Data Analysis 53, 82–91.  https://doi.org/10.1016/j.csda.2008.06.012\n\nKazi-Aoual, F., Hitier, S., Sabatier, R., Lebreton, J.-D., 1995.  Refined approximations to permutation tests for multivariate inference.  Computational Statistics & Data Analysis 20, 643–656.  https://doi.org/10.1016/0167-9473(94)00064-2\n\nMayer, C.-D., Lorent, J., Horgan, G.W., 2011. Exploratory Analysis  of Multiple Omics Datasets Using the Adjusted RV Coefficient. Statistical  Applications in Genetics and Molecular Biology 10. https://doi.org/10.2202/1544-6115.1540\n\nSmilde, A.K., Kiers, H.A.L., Bijlsma, S., Rubingh, C.M., van Erk, M.J., 2009.  Matrix correlations for high-dimensional data: the modified RV-coefficient.  Bioinformatics 25, 401–405. https://doi.org/10.1093/bioinformatics/btn634\n\nRobert, P., Escoufier, Y., 1976. A Unifying Tool for Linear Multivariate  Statistical Methods: The RV-Coefficient. Journal of the Royal Statistical Society:  Series C (Applied Statistics) 25, 257–265. https://doi.org/10.2307/2347233\n\nExamples\n\nusing Jchemo\nX = rand(5, 10)\nY = rand(5, 3)\nrv(X, Y)\n\nX = rand(5, 15) \nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl)\nrv(Xbl)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampcla","page":"Index of functions","title":"Jchemo.sampcla","text":"sampcla(x, k::Union{Int, Vector{Int}}, y = nothing)\n\nBuild training vs. test sets by stratified sampling.  \n\nx : Class membership (n) of the observations.\nk : Nb. test observations to sample in each class.    If k is a single value, the nb. of sampled    observations is the same for each class. Alternatively,    k can be a vector of length equal to the nb. of    classes in x.\ny : Quantitative variable (n) used if systematic sampling.\n\nTwo outputs are returned (= row indexes of the data): \n\ntrain (n - k),\ntest (k). \n\nIf y = nothing, the sampling is random, else it is  systematic over the sorted y (see function sampsys).\n\nReferences\n\nNaes, T., 1987. The design of calibration in near infra-red reflectance  analysis by clustering. Journal of Chemometrics 1, 121-134.\n\nExamples\n\nusing Jchemo\nx = string.(repeat(1:3, 5))\nn = length(x)\ntab(x)\nk = 2 \nres = sampcla(x, k)\nres.test\nx[res.test]\ntab(x[res.test])\n\ny = rand(n)\nres = sampcla(x, k, y)\nres.test\nx[res.test]\ntab(x[res.test])\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.sampdf","page":"Index of functions","title":"Jchemo.sampdf","text":"sampdf(Y::DataFrame, k::Union{Int, Vector{Int}}, id = 1:nro(Y); meth = :rand)\n\nBuild training vs. test sets from each column of a dataframe. \n\nY : DataFrame (n, p). Can contain missing values.\nk : Nb. of test observations selected for each Y-column.    The selection is done within the non-missing observations    of the considered column. If k is a single value, the same nb.     of observations are selected for each column. Alternatively,    k can be a vector of length p. \nid : Vector (n) of IDs.\n\nKeyword arguments:\n\nmeth : Type of sampling for the test set. Possible values are:    :rand = random sampling, :sys = systematic sampling over each    sorted Y-column (see function sampsys).  \n\nTypically, dataframe Y contains a set of response variables  to predict.\n\nExamples\n\nusing Jchemo, DataFrames\n\nY = hcat([rand(5); missing; rand(6)],\n   [rand(2); missing; missing; rand(7); missing])\nY = DataFrame(Y, :auto)\nn = nro(Y)\n\nk = 3\nres = sampdf(Y, k) \n#res = sampdf(Y, k, string.(1:n))\npnames(res)\nres.nam\nlength(res.test)\nres.train\nres.test\n\n## Replicated splitting Train/Test\nrep = 10\nk = 3\nids = [sampdf(Y, k) for i = 1:rep]\nlength(ids)\ni = 1    # replication\nids[i]\nids[i].train \nids[i].test\nj = 1    # variable y  \nids[i].train[j]\nids[i].test[j]\nids[i].nam[j]\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.sampdp-Tuple{Any, Int64}","page":"Index of functions","title":"Jchemo.sampdp","text":"sampdp(X, k::Int; metric = :eucl)\n\nBuild training vs. test sets by DUPLEX sampling.  \n\nX : X-data (n, p).\nk : Nb. pairs (training/test) of observations    to sample. Must be <= n / 2. \n\nKeyword arguments:\n\nmetric : Metric used for the distance computation.   Possible values are: :eucl (Euclidean),    :mah (Mahalanobis).\n\nThree outputs (= row indexes of the data) are returned: \n\ntrain (k), \ntest (k),\nremain (n - 2 * k). \n\nOutputs train and test are built from the DUPLEX algorithm  (Snee, 1977 p.421). They are expected to cover approximately the same  X-space region and have similar statistical properties. \n\nIn practice, when output remain is not empty (i.e. when there  are remaining observations), one common strategy is to add  it to output train.\n\nReferences\n\nKennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.\n\nSnee, R.D., 1977. Validation of Regression Models: Methods and Examples.  Technometrics 19, 415-428. https://doi.org/10.1080/00401706.1977.10489581\n\nExamples\n\nusing Jchemo\n\nX = [0.381392  0.00175002 ; 0.1126    0.11263 ; \n    0.613296  0.152485 ; 0.726536  0.762032 ;\n    0.367451  0.297398 ; 0.511332  0.320198 ; \n    0.018514  0.350678] \n\nk = 3\nsampdp(X, k)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampks-Tuple{Any, Int64}","page":"Index of functions","title":"Jchemo.sampks","text":"sampks(X, k::Int; metric = :eucl)\n\nBuild training vs. test sets by Kennard-Stone sampling.  \n\nX : X-data (n, p).\nk : Nb. test observations to sample.\n\nKeyword arguments: \n\nmetric : Metric used for the distance computation.   Possible values are: :eucl (Euclidean),    :mah (Mahalanobis).\n\nTwo outputs (= row indexes of the data) are returned: \n\ntrain (n - k),\ntest (k). \n\nOutput test is built from the Kennard-Stone (KS)  algorithm (Kennard & Stone, 1969). \n\nNote: By construction, the set of observations  selected by KS sampling contains higher variability than  the set of the remaining observations. In the seminal  article (K&S, 1969), the algorithm is used to select observations that will be used to build a calibration set. To the opposite, in the present function, KS is used to select a test set with  higher variability than the training set. \n\nReferences\n\nKennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\n\nk = 80\nres = sampks(X, k)\npnames(res)\nres.train \nres.test\n\nmodel = pcasvd(nlv = 15) \nfit!(model, X) \n@head T = model.fitm.T\nres = sampks(T, k; metric = :mah)\n\n#####################\n\nn = 10\nk = 25 \nX = [repeat(1:n, inner = n) repeat(1:n, outer = n)] \nX = Float64.(X) \nX .= X + .1 * randn(nro(X), nco(X))\ns = sampks(X, k).test\nf, ax = plotxy(X[:, 1], X[:, 2])\nscatter!(ax, X[s, 1], X[s, 2]; color = \"red\") \nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.samprand-Tuple{Int64, Int64}","page":"Index of functions","title":"Jchemo.samprand","text":"samprand(n::Int, k::Int; replace = false)\n\nBuild training vs. test sets by random sampling.  \n\nn : Total nb. of observations.\nk : Nb. test observations to sample.\n\nKeyword arguments:\n\nreplace : Boolean. If false, the sampling is    without replacement.\n\nTwo outputs are returned (= row indexes of the data): \n\ntrain (n - k),\ntest (k). \n\nOutput test is built by random sampling within 1:n. \n\nExamples\n\nusing Jchemo\n\nn = 10\nsamprand(n, 4)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampsys-Tuple{Any, Int64}","page":"Index of functions","title":"Jchemo.sampsys","text":"sampsys(y, k::Int)\n\nBuild training vs. test sets by systematic sampling      over a quantitative variable.  \n\ny : Quantitative variable (n) to sample.\nk : Nb. test observations to sample.    Must be >= 2.\n\nTwo outputs are returned (= row indexes of the data): \n\ntrain (n - k),\ntest (k). \n\nOutput test is built by systematic sampling over the rank of  the y observations. For instance if k / n ~ .3, one observation  over three observations over the sorted y is selected. \n\nOutput test always contains the indexes of the minimum and  maximum of y.\n\nExamples\n\nusing Jchemo \n\ny = rand(7)\n[y sort(y)]\nres = sampsys(y, 3)\nsort(y[res.test])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampwsp-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.sampwsp","text":"sampwsp(X, dmin; recod = false, maxit = nro(X))\n\nBuild training vs. test sets by WSP sampling.  \n\nX : X-data (n, p).\ndmin : Distance \"dmin\" (Santiago et al. 2012).\n\nKeyword arguments: \n\nrecod : Boolean indicating if X is recoded or not    before the sampling (see below).\nmaxit : Maximum number of iterations.\n\nTwo outputs (= row indexes of the data) are returned: \n\ntrain (n - k),\ntest (k). \n\nOutput test is built from the \"Wootton, Sergent, Phan-Tan-Luu\" (WSP)  algorithm, assumed to generate samples uniformely distributed in the X domain  (Santiago et al. 2012).\n\nIf recod = true, each column x of X is recoded within [0, 1] and the center of  the domain is the vector repeat([.5], p). Column x is recoded such as: \n\nvmin = minimum(x)\nvmax = maximum(x)\nvdiff = vmax - vmin\nx .=  0.5 .+ (x .- (vdiff / 2 + vmin)) / vdiff\n\nReferences\n\nBéal A. 2015. Description et sélection de données en grande dimensio. Thèse de doctorat. Laboratoire d’Instrumentation et de sciences analytiques, Ecole doctorale des siences chimiques, Université d'Aix-Marseille.\n\nSantiago, J., Claeys-Bruno, M., Sergent, M., 2012. Construction of space-filling  designs using WSP algorithm for high dimensional spaces.  Chemometrics and Intelligent Laboratory Systems, Selected Papers from  Chimiométrie 2010 113, 26–31. https://doi.org/10.1016/j.chemolab.2011.06.003\n\nExamples\n\nusing Jchemo\n\nn = 600 ; p = 2\nX = rand(n, p)\ndmin = .5\ns = sampwsp(X, dmin)\npnames(res)\n@show length(s.test)\nplotxy(X[s.test, 1], X[s.test, 2]).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.savgk-Tuple{Int64, Int64, Int64}","page":"Index of functions","title":"Jchemo.savgk","text":"savgk(nhwindow::Int, degree::Int, deriv::Int)\n\nCompute the kernel of the Savitzky-Golay filter.\n\nnhwindow : Nb. points (>= 1) of the half window.\ndegree : Degree of the smoothing polynom, where  1 <= degree <= 2 * nhwindow.\nderiv : Derivation order, where 0 <= deriv <= degree.\n\nThe size of the kernel is odd (npoint = 2 * nhwindow + 1): \n\nx[-nhwindow], x[-nhwindow+1], ..., x[0], ...., x[nhwindow-1], x[nhwindow].\n\nIf deriv = 0, there is no derivation (only polynomial smoothing).\n\nThe case degree = 0 (i.e. simple moving average) is not  allowed by the funtion.\n\nReferences\n\nLuo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation  filter for even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002\n\nExamples\n\nusing Jchemo\nres = savgk(21, 3, 2)\npnames(res)\nres.S \nres.G \nres.kern\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.savgol-Tuple{}","page":"Index of functions","title":"Jchemo.savgol","text":"savgol(; kwargs...)\nsavgol(X; kwargs...)\n\nSavitzky-Golay derivation and smoothing of each row of X-data.\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nnpoint : Size of the filter (nb. points involved in    the kernel). Must be odd and >= 3. The half-window size is    nhwindow = (npoint - 1) / 2.\nderiv : Derivation order. Must be: 0 <= deriv <= degree.\ndegree : Degree of the smoothing polynom.   Must be: 1 <= degree <= npoint - 1.\n\nThe smoothing is computed by convolution (with padding), using  function imfilter of package ImageFiltering.jl. Each returned point is  located on the center of the kernel. The kernel is computed with  function savgk.\n\nThe function returns a matrix (n, p).\n\nReferences\n\nLuo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation filter for  even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002\n\nSavitzky, A., Golay, M.J.E., 2002. Smoothing and Differentiation of Data by Simplified Least  Squares Procedures. [WWW Document]. https://doi.org/10.1021/ac60214a047\n\nSchafer, R.W., 2011. What Is a Savitzky-Golay Filter? [Lecture Notes].  IEEE Signal Processing Magazine 28, 111–117. https://doi.org/10.1109/MSP.2011.941097\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nnpoint = 11 ; deriv = 2 ; degree = 2\nmodel = savgol(; npoint, deriv, degree) \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n####### Gaussian signal \n\nu = -15:.1:15\nn = length(u)\nx = exp.(-.5 * u.^2) / sqrt(2 * pi) + .03 * randn(n)\nM = 10  # half window\nN = 3   # degree\nderiv = 0\n#deriv = 1\nmodel = savgol(; npoint = 2M + 1, degree = N, deriv)\nfit!(model, x')\nxp = transf(model, x')\nf, ax = plotsp(x', u; color = :blue)\nlines!(ax, u, vec(xp); color = :red)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.scale-Tuple{}","page":"Index of functions","title":"Jchemo.scale","text":"scale()\nscale(X)\nscale(X, weights::Weight)\n\nColumn-wise scaling of X-data.\n\nX : X-data (n, p).\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = scale() \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\ncolstd(Xptrain)\n@head Xptest \n@head Xtest ./ colstd(Xtrain)'\nplotsp(Xptrain).f\nplotsp(Xptest).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.segmkf-Tuple{Int64, Int64}","page":"Index of functions","title":"Jchemo.segmkf","text":"segmkf(n::Int, K::Int; rep = 1)\nsegmkf(group::Vector, K::Int; rep = 1)\n\nBuild segments of observations for K-fold cross-validation.  \n\nn : Total nb. of observations in the dataset.    The sampling is implemented with 1:n.\ngroup : A vector (n) defining blocks of observations.\nK : Nb. folds (segments) splitting the n observations. \n\nKeyword arguments:\n\nrep : Nb. replications of the sampling.\n\nFor each replication, the function splits the n observations  to K segments  that can be used for K-fold cross-validation. \n\nIf group is used (must be a vector of length n), the function  samples entire groups (= blocks) of observations instead of observations.  Such a block-sampling is required when data is structured by blocks and  when the response to predict is correlated within blocks.  This prevents underestimation of the generalization error.\n\nThe function returns a list (vector) of rep elements.  Each element of the list contains K segments (= K vectors). Each segment contains the indexes (position within 1:n) of  the sampled observations.    \n\nExamples\n\nusing Jchemo \n\nn = 10 ; K = 3\nrep = 4 \nsegm = segmkf(n, K; rep)\ni = 1 \nsegm[i]\nsegm[i][1]\n\nn = 10 \ngroup = [\"A\", \"B\", \"C\", \"D\", \"E\", \"A\", \"B\", \"C\", \"D\", \"E\"]    # blocks of the observations\ntab(group) \nK = 3 ; rep = 4 \nsegm = segmkf(group, K; rep)\ni = 1 \nsegm[i]\nsegm[i][1]\ngroup[segm[i][1]]\ngroup[segm[i][2]]\ngroup[segm[i][3]]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.segmts-Tuple{Int64, Int64}","page":"Index of functions","title":"Jchemo.segmts","text":"segmts(n::Int, m::Int; rep = 1, seed = nothing)\nsegmts(group::Vector, m::Int; rep = 1, seed = nothing)\n\nBuild segments of observations for \"test-set\" validation.\n\nn : Total nb. of observations in the dataset.    The sampling  is implemented within 1:n.\ngroup : A vector (n) defining blocks of observations.\nm : Nb. test observations, or groups if group is used, returned    in each segment.\n\nKeyword arguments: \n\nrep : Nb. replications of the sampling.\nseed : Eventual seed for the Random.MersenneTwister    generator. Must be of length = rep. When nothing,    the seed is random at each replication.\n\nFor each replication, the function builds a test set that can  be used to validate a model. \n\nIf group is used (must be a vector of length n), the function  samples entire groups (= blocks) of observations instead of observations.  Such a block-sampling is required when data is structured by blocks and  when the response to predict is correlated within blocks.  This prevents underestimation of the generalization error.\n\nThe function returns a list (vector) of rep elements.  Each element of the list is a vector of the indexes (positions  within 1:n) of the sampled observations.  \n\nExamples\n\nusing Jchemo \n\nn = 10 ; m = 3\nrep = 4 \nsegm = segmts(n, m; rep) \ni = 1\nsegm[i]\nsegm[i][1]\n\nn = 10 \ngroup = [\"A\", \"B\", \"C\", \"D\", \"E\", \"A\", \"B\", \"C\", \"D\", \"E\"]    # blocks of the observations\ntab(group)  \nm = 2 ; rep = 4 \nsegm = segmts(group, m; rep)\ni = 1 \nsegm[i]\nsegm[i][1]\ngroup[segm[i][1]]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.selwold-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.selwold","text":"selwold(indx, r; smooth = true, npoint = 5, alpha = .05, digits = 3, graph = true, \n    step = 2, xlabel = \"Index\", ylabel = \"Value\", title = \"Score\")\n\nWold's criterion to select dimensionality in LV models (e.g. PLSR).\n\nindx : A variable representing the model parameter(s), e.g. nb. LVs if PLSR models.\nr : A vector of error rates (n), e.g. RMSECV.\n\nKeyword arguments:\n\nsmooth : Boolean. If true,  the selection is done    after a moving-average smoothing of rate R   (see function mavg).\nnpoint : Window of the moving-average used to    smooth rate R.\nalpha : Proportion alpha used as threshold    for rate R.\ndigits : Number of digits in the outputs.\ngraph : Boolean. If true, outputs are plotted.\nstep : Step used for defining the xticks    in the graphs.\nxlabel : Horizontal label for the plots.\nylabel : Vertical label for the plots.\ntitle : Title of the left plot.\n\nThe slection criterion is the \"precision gain ratio\": \n\nR = 1 - r(a+1) / r(a)\n\nwhere r is an observed error rate quantifying the model  performance (e.g. RMSEP, classification error rate, etc.)  and a the model dimensionnality (= nb. LVs). r can also represent  other indicators such as the eigenvalues of a PCA.\n\nR is the relative gain in perforamnce efficiency after a new LV  is added to the model. The iterations continue until R becomes lower  than a threshold value alpha. By default and only as an indication,  the default alpha=.05 is set in the function, but the user should set  any other value depending on his data and parsimony objective.\n\nIn his original article, Wold (1978; see also Bro et al. 2008) used  the ratio of cross-validated over training residual sums of squares,  i.e. PRESS over SSR. Instead, function selwold compares values of  consistent nature (the successive values in the input vector r).  For instance, r was set to PRESS values in Li et al. (2002) and  Andries et al. (2011), which is equivalent to the \"punish factor\"  described in Westad & Martens (2000).\n\nThe ratio R can be erratic (particulary when r is the error rate  of a discrimination model), making difficult the dimensionnaly  selection. In such a situation, function selwold proposes to calculate a smoothing of R (argument smooth).\n\nThe function returns two outputs (in addition to eventual plots):\n\nopt : The index corresponding to the minimum value of r.\nsel : The index of the selection from the R (or smoothed R)    threshold.\n\nReferences\n\nAndries, J.V.M., Vander Heyden, Y., Buydens, L.M.C., 2011. Improved  variable reduction in partial least squares modelling based on  Predictive-Property-Ranked Variables and adaptation of partial least  squares complexity. Analytica Chimica Acta 705, 292-305.  https://doi.org/10.1016/j.aca.2011.06.037\n\nBro, R., Kjeldahl, K., Smilde, A.K., Kiers, H.A.L., 2008. Cross-validation  of component models: A critical look at current methods. Anal Bioanal Chem  390, 1241-1251. https://doi.org/10.1007/s00216-007-1790-1\n\nLi, B., Morris, J., Martin, E.B., 2002. Model selection for partial least  squares regression. Chemometrics and Intelligent Laboratory Systems 64, 79-89.  https://doi.org/10.1016/S0169-7439(02)00051-5\n\nWestad, F., Martens, H., 2000. Variable Selection in near Infrared Spectroscopy  Based on Significance Testing in Partial Least Squares Regression. J. Near Infrared  Spectrosc., JNIRS 8, 117â124.\n\nWold S. Cross-Validatory Estimation of the Number of Components in Factor  and Principal Components Models. Technometrics. 1978;20(4):397-405\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nn = nro(Xtrain)\n\nsegm = segmts(n, 50; rep = 30)\nmodel = plskern()\nnlv = 0:20\nres = gridcv(model, Xtrain, ytrain; segm, score = rmsep, nlv).res\nres[res.y1 .== minimum(res.y1), :]\nplotgrid(res.nlv, res.y1;xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\nzres = selwold(res.nlv, res.y1; smooth = true, graph = true) ;\n@show zres.opt\n@show zres.sel\nzres.f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sep-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.sep","text":"sep(pred, Y)\n\nCompute the corrected SEP (\"SEP_c\"), i.e. the standard deviation of      the prediction errors.\n\npred : Predictions.\nY : Observed data.\n\nReferences\n\nBellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B.,  Roger, J.-M., McBratney, A., 2010. Critical review of  chemometric indicators commonly used for assessing the  quality of the prediction of soil attributes by NIR  spectroscopy. TrAC Trends in Analytical Chemistry 29,  1073–1081.  https://doi.org/10.1016/j.trac.2010.05.006\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nsep(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.snorm-Tuple{}","page":"Index of functions","title":"Jchemo.snorm","text":"snorm()\nsnorm(X)\n\nRow-wise norming of X-data.\n\nX : X-data (n, p).\n\nEach row of X is divide by its norm.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = snorm()\nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain).f\nplotsp(Xptest).f\n@head rownorm(Xptrain)\n@head rownorm(Xptest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.snv-Tuple{}","page":"Index of functions","title":"Jchemo.snv","text":"snv(; kwargs...)\nsnv(X; kwargs...)\n\nStandard-normal-variate (SNV) transformation of each row of X-data.\n\nX : X-data (n, p).\n\nKeyword arguments:\n\ncentr : Boolean indicating if the centering in done.\nscal : Boolean indicating if the scaling in done.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nyear = dat.Y.year\ns = year .<= 2012\nXtrain = X[s, :]\nXtest = rmrow(X, s)\nwlst = names(dat.X)\nwl = parse.(Float64, wlst)\nplotsp(X, wl; nsamp = 20).f\n\nmodel = snv() \n#model = snv(scal = false) \nfit!(model, Xtrain)\nXptrain = transf(model, Xtrain)\nXptest = transf(model, Xtest)\nplotsp(Xptrain).f\nplotsp(Xptest).f\n@head rowmean(Xptrain)\n@head rowstd(Xptrain)\n@head rowmean(Xptest)\n@head rowstd(Xptest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.softmax-Tuple{AbstractVector}","page":"Index of functions","title":"Jchemo.softmax","text":"softmax(x::AbstractVector)\nsoftmax(X::Union{Matrix, DataFrame})\n\nSoftmax function.\n\nx : A vector to transform.\nX : A matrix whose rows are transformed.\n\nLet v be a vector:\n\n'softmax'(v) = exp.(v) / sum(exp.(v))\n\nExamples\n\nusing Jchemo\n\nx = 1:3\nsoftmax(x)\n\nX = rand(5, 3)\nsoftmax(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.soplsr-Tuple{}","page":"Index of functions","title":"Jchemo.soplsr","text":"soplsr(; kwargs...)\nsoplsr(Xbl, Y; kwargs...)\nsoplsr(Xbl, Y, weights::Weight; kwargs...)\nsoplsr!(Xbl::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nMultiblock sequentially orthogonalized PLSR (SO-PLSR).\n\nXbl : List of blocks (vector of matrices) of X-data    Typically, output of function mblock from (n, p) data.  \nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs = scores T) to compute.\nscal : Boolean. If true, each column of blocks in Xbl    and Y is scaled by its uncorrected standard deviation.\n\nReferences\n\nBiancolillo et al. , 2015. Combining SO-PLS and linear  discriminant analysis for multi-block classification.  Chemometrics and Intelligent Laboratory Systems, 141, 58-67.\n\nBiancolillo, A. 2016. Method development in the area of  multi-block analysis focused on food analysis. PhD.  University of copenhagen.\n\nMenichelli et al., 2014. SO-PLS as an exploratory tool for path modelling. Food Quality and Preference, 36, 122-134.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"ham.jld2\") \n@load db dat\npnames(dat) \nX = dat.X\nY = dat.Y\ny = Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\ns = 1:6\nXbltrain = mblock(X[s, :], listbl)\nXbltest = mblock(rmrow(X, s), listbl)\nytrain = y[s]\nytest = rmrow(y, s) \nntrain = nro(ytrain) \nntest = nro(ytest) \nntot = ntrain + ntest \n(ntot = ntot, ntrain , ntest)\n\nnlv = 2\n#nlv = [2, 1, 2]\n#nlv = [2, 0, 1]\nscal = false\n#scal = true\nmodel = soplsr(; nlv, scal)\nfit!(model, Xbltrain, ytrain)\npnames(model) \npnames(model.fitm)\n@head model.fitm.T\n@head transf(model, Xbltrain)\ntransf(model, Xbltest)\n\nres = predict(model, Xbltest)\nres.pred \nrmsep(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sourcedir-Tuple{Any}","page":"Index of functions","title":"Jchemo.sourcedir","text":"sourcedir(path)\n\nInclude all the files contained in a directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.spca-Tuple{}","page":"Index of functions","title":"Jchemo.spca","text":"spca(; kwargs...)\nspca(X; kwargs...)\nspca(X, weights::Weight; kwargs...)\nspca!(X::Matrix, weights::Weight; kwargs...)\n\nSparse PCA (Shen & Huang 2008).\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. principal components (PCs).\nmeth : Method used for the sparse thresholding.    Possible values are: :soft, :hard. See thereafter.\nnvar : Nb. variables (X-columns) selected for each principal   component (PC). Can be a single integer (i.e. same nb.    of variables for each PC), or a vector of length nlv.   \ndefl : Type of X-matrix deflation, see below.\ntol : Tolerance value for stopping the Nipals iterations.\nmaxit : Maximum nb. of Nipals iterations.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nsPCA-rSVD algorithm (regularized low rank matrix approximation) of  Shen & Huang 2008. \n\nThe algorithm computes each loadings vector iteratively, by alternating  least squares regressions (Nipals) including a step of thresholding. Function  spca provides thresholding methods '1' and '2' reported in Shen & Huang 2008  Lemma 2 (:soft and :hard):\n\nThe tuning parameter used by Shen & Huang 2008 is the number of null elements    in the loadings vector, referred to as degree of sparsity. Conversely, the    present function spca uses the number of non-zero elements (nvar),    equal to p - degree of sparsity.\nSee the code of function snipals_shen for details on how is computed    the cutoff 'lambda' used inside the thresholding function (Shen & Huang 2008),    given a value for nvar. Differences from other softwares may occur    when there are tied values in the loadings vector (depending on the choices    of method used to compute quantiles).\n\nMatrix X can be deflated in two ways:\n\ndefl = :v : Matrix X is deflated by regression of the X'-columns on  the loadings vector v. This is the method proposed by Shen & Huang 2008  (see in Theorem A.2 p.1033).\ndefl = :t : Matrix X is deflated by regression of the X-columns on  the score vector t. This is the method used in function spca of the  R package mixOmics (Le Cao et al. 2016).\n\nThe method of computation of the % variance explained in X by each PC (returned  by function summary) depends on the type of deflation chosen (see the code).    \n\nReferences\n\nKim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien  Dejean with key contributors Benoit Gautier, Francois Bartolo,  contributions from Pierre Monget, Jeff Coquery, FangZou Yao  and Benoit Liquet. (2016). mixOmics: Omics Data Integration  Project. R package version 6.1.1.  https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html\n\nShen, H., Huang, J.Z., 2008. Sparse principal component  analysis via regularized low rank matrix approximation.  Journal of Multivariate Analysis 99, 1015–1034.  https://doi.org/10.1016/j.jmva.2007.06.007\n\nExamples\n\nusing Jchemo, JchemoData, JLD2 \npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\n@head dat.X\nX = dat.X[:, 1:4]\nn = nro(X)\nntest = 30\ns = samprand(n, ntest) \nXtrain = X[s.train, :]\nXtest = X[s.test, :]\n\nnlv = 3 \nmeth = :soft\n#meth = :hard\nnvar = 2\nmodel = spca(; nlv, meth, nvar) ;\nfit!(model, Xtrain) \nfitm = model.fitm ;\npnames(fitm)\nfitm.niter\nfitm.sellv \nfitm.sel\nV = fitm.V\nV' * V\n@head T = fitm.T\nT' * T\n@head transf(model, Xtrain)\n\n@head Ttest = transf(fitm, Xtest)\n\nres = summary(model, Xtrain) ;\nres.explvarx\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.spcr-Tuple{}","page":"Index of functions","title":"Jchemo.spcr","text":"spcr(; kwargs...)\nspcr(X, Y; kwargs...)\nspcr(X, Y, weights::Weight; kwargs...)\nspcr!(X::Matrix, Y::Matrix, weights::Weight; kwargs...)\n\nSparse principal component regression (sPCR). \n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nSame as function spca.\n\nRegression on scores computed from a sparse PCA (sPCA-rSVD algorithm of  Shen & Huang 2008 ). See function spca for details.\n\nReferences\n\nShen, H., Huang, J.Z., 2008. Sparse principal component  analysis via regularized low rank matrix approximation.  Journal of Multivariate Analysis 99, 1015–1034.  https://doi.org/10.1016/j.jmva.2007.06.007\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nmeth = :soft\n#meth = :hard\nnvar = 20 \nmodel = spcr(; nlv, meth, nvar, defl = :t) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\nfitm = model.fitm ;\npnames(fitm)\n@head fitm.fitm.T\n@head transf(model, X)\n@head fitm.fitm.V\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\nres = summary(model, Xtrain) ;\npnames(res)\nz = res.explvarx\nplotgrid(z.nlv, z.cumpvar; step = 2, xlabel = \"Nb. LVs\", \n    ylabel = \"Prop. Explained X-Variance\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.splskdeda-Tuple{}","page":"Index of functions","title":"Jchemo.splskdeda","text":"splskdeda(; kwargs...)\nsplskdeda(X, y; kwargs...)\nsplskdeda(X, y, weights::Weight; kwargs...)\n\nSparse PLS-KDE-DA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nmeth : Method used for the sparse thresholding.    Possible values are: :soft, :hard. See thereafter.\nnvar : Nb. variables (X-columns) selected for each LV.    Can be a single integer (i.e. same nb. of variables for each LV),    or a vector of length nlv.       \nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nKeyword arguments of function dmkern (bandwidth    definition) can also be specified here.\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plskdeda (PLS-KDEDA) except that a sparse PLSR  (function splsr), instead of a PLSR (function plskern), is run on the  Y-dummy table. \n\nSee function splslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.splslda-Tuple{}","page":"Index of functions","title":"Jchemo.splslda","text":"splslda(; kwargs...)\nsplslda(X, y; kwargs...)\nsplslda(X, y, weights::Weight; kwargs...)\n\nSparse PLS-LDA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nmeth : Method used for the sparse thresholding.    Possible values are: :soft, :hard. See thereafter.\nnvar : Nb. variables (X-columns) selected for each LV.    Can be a single integer (i.e. same nb. of variables for each LV),    or a vector of length nlv.       \nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plslda (PLSR-LDA) except that a sparse PLSR  (function splsr), instead of a PLSR (function plskern), is run on the  Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nmeth = :soft\nnvar = 10\nmodel = splslda(; nlv, meth, nvar) \n#model = splsqda(; nlv, meth, nvar, alpha = .1) \n#model = splskdeda(; nlv, meth, nvar, a = .9) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nembfitm = fitm.fitm.embfitm ; \n@head embfitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(embfitm)\nsummary(embfitm, Xtrain)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.splsqda-Tuple{}","page":"Index of functions","title":"Jchemo.splsqda","text":"splsqda(; kwargs...)\nsplsqda(X, y; kwargs...)\nsplsqda(X, y, weights::Weight; kwargs...)\n\nSparse PLS-QDA (with continuum).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.   Must be >= 1.\nmeth : Method used for the sparse thresholding.    Possible values are: :soft, :hard. See thereafter.\nnvar : Nb. variables (X-columns) selected for each LV.    Can be a single integer (i.e. same nb. of variables for each LV),    or a vector of length nlv.        \nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nalpha : Scalar (∈ [0, 1]) defining the continuum   between QDA (alpha = 0) and LDA (alpha = 1).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation   in the PLS computation.\n\nSame as function plsqda (PLSR-QDA) except that a sparse PLSR  (function splsr), instead of a PLSR (function plskern), is run on the  Y-dummy table.\n\nSee function splslda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.splsr-Tuple{}","page":"Index of functions","title":"Jchemo.splsr","text":"splsr(; kwargs...)\nsplsr(X, Y; kwargs...)\nsplsr(X, Y, weights::Weight; kwargs...)\nsplsr!(X::Matrix, Y::Union{Matrix, BitMatrix}, weights::Weight; kwargs...)\n\nSparse partial least squares regression (Lê Cao et al. 2008)\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\nmeth : Method used for the sparse thresholding.    Possible values are: :soft, :hard. See thereafter.\nnvar : Nb. variables (X-columns) selected for each LV.    Can be a single integer (i.e. same nb. of variables for each LV),    or a vector of length nlv.   \nscal : Boolean. If true, each column of X and Y is    scaled by its uncorrected standard deviation.    \n\nAdaptation of the sparse partial least squares regression algorihm of  Lê Cao et al. 2008. The fast \"improved kernel algorithm #1\" of  Dayal & McGregor (1997) is used instead Nipals. \n\nIn the present version of splsr, the sparse thresholding only concerns  X. The function provides two thresholding methods to compute the sparse  X-loading weights w (:soft and :hard), see function spca for description. \n\nThe case meth = :soft returns the same results as function spls of  the R package mixOmics (Lê Cao et al.) with the regression mode and without  sparseness on Y.\n\nThe COVSEL regression method described in Roger et al 2011 (see also Höskuldsson 1992) is implemented by setting nvar = 1.\n\nReferences\n\nDayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.\n\nHöskuldsson, A., 1992. The H-principle in modelling with applications  to chemometrics. Chemometrics and Intelligent Laboratory Systems,  Proceedings of the 2nd Scandinavian Symposium on Chemometrics 14,  139–153. https://doi.org/10.1016/0169-7439(92)80099-P\n\nLê Cao, K.-A., Rossouw, D., Robert-Granié, C., Besse, P., 2008.  A Sparse PLS for Variable Selection when Integrating Omics Data.  Statistical Applications in Genetics and Molecular Biology 7.  https://doi.org/10.2202/1544-6115.1390\n\nKim-Anh Lê Cao, Florian Rohart, Ignacio Gonzalez, Sebastien Dejean  with key contributors Benoit Gautier, Francois Bartolo, contributions  from Pierre Monget, Jeff Coquery, FangZou Yao and Benoit Liquet.  (2016). mixOmics: Omics Data Integration Project. R package  version 6.1.1. https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html\n\nPackage mixOmics on Bioconductor: https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html\n\nRoger, J.M., Palagos, B., Bertrand, D., Fernandez-Ahumada, E., 2011.  covsel: Variable selection for highly multivariate and multi-response  calibration: Application to IR spectroscopy.  Chem. Lab. Int. Syst. 106, 216-223.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nmeth = :soft\n#meth = :hard\nnvar = 20\nmodel = splsr(; nlv, meth, nvar) ;\nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n@head model.fitm.T\n@head model.fitm.W\n\ncoef(model)\ncoef(model; nlv = 3)\n\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\nres = summary(model, Xtrain) ;\npnames(res)\nz = res.explvarx\nplotgrid(z.nlv, z.cumpvar; step = 2, xlabel = \"Nb. LVs\", \n    ylabel = \"Prop. Explained X-Variance\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.splsrda-Tuple{}","page":"Index of functions","title":"Jchemo.splsrda","text":"splsrda(; kwargs...)\nsplsrda(X, y; kwargs...)\nsplsrda(X, y, weights::Weight; kwargs...)\n\nSparse PLSR-DA.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight). \n\nKeyword arguments: \n\nnlv : Nb. latent variables (LVs) to compute.\nmeth : Method used for the sparse thresholding.    Possible values are: :soft, :hard. See thereafter.\nnvar : Nb. variables (X-columns) selected for each LV.    Can be a single integer (i.e. same nb. of variables for each LV),    or a vector of length nlv.       \nprior : Type of prior probabilities for class    membership. Possible values are: :unif (uniform),    :prop (proportional), or a vector (of length equal to    the number of classes) giving the prior weight for each class    (in case of vector, it must be sorted in the same order as mlev(y)).\nscal : Boolean. If true, each column of X    and Ydummy is scaled by its uncorrected standard deviation.\n\nSame as function plsrda (PLSR-DA) except that a sparse PLSR  (function splsr), instead of a PLSR (function plskern), is run on the  Y-dummy table. \n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X)\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nmeth = :soft\nnvar = 10\nmodel = splsrda(; nlv, meth, nvar) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\n@head fitm.fitm.T\n@head transf(model, Xtrain)\n@head transf(model, Xtest)\n@head transf(model, Xtest; nlv = 3)\n\ncoef(fitm.fitm)\n\nres = predict(model, Xtest) ;\npnames(res)\n@head res.posterior\n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\npredict(model, Xtest; nlv = 1:2).pred\nsummary(fitm.fitm, Xtrain)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ssq-Tuple{Any}","page":"Index of functions","title":"Jchemo.ssq","text":"ssq(X)\n\nCompute the total inertia of a matrix.\n\nX : Matrix (n, p).\n\nSum of all the squared components of X (= frob2(X); Squared Frobenius norm). \n\nExamples\n\nusing Jchemo\n\nX = rand(5, 2) \nssq(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ssr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.ssr","text":"ssr(pred, Y)\n\nCompute the sum of squared prediction errors (SSR).\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nusing Jchemo\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, Ytrain)\npred = predict(model, Xtest).pred\nssr(pred, Ytest)\n\nmodel = plskern; nlv = 2)\nfit!(model, Xtrain, ytrain)\npred = predict(model, Xtest).pred\nssr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.stdv-Tuple{Any}","page":"Index of functions","title":"Jchemo.stdv","text":"stdv(x)\nstdv(x, weights::Weight)\n\nCompute the uncorrected standard deviation of a vector.\n\nx : A vector (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nExamples\n\nusing Jchemo\n\nn = 1000\nx = rand(n)\nw = mweight(rand(n))\n\nstdv(x)\nstdv(x, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.summ-Tuple{Any}","page":"Index of functions","title":"Jchemo.summ","text":"summ(X; digits = 3)\nsumm(X, y; digits = 3)\n\nSummarize a dataset (or a variable).\n\nX : A dataset (n, p).\ny : A categorical variable (n) (class membership).\ndigits : Nb. digits in the outputs.\n\nExamples\n\nusing Jchemo\n\nn = 50\nX = rand(n, 3) \ny = rand(1:3, n)\nres = summ(X)\npnames(res)\nsumm(X[:, 2]).res\n\nsumm(X, y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sumv-Tuple{Any}","page":"Index of functions","title":"Jchemo.sumv","text":"sumv(x)\nsumv(x, weights::Weight)\n\nCompute the sum of a vector. \n\nx : A vector (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nExamples\n\nusing Jchemo\n\nn = 100\nx = rand(n)\nw = mweight(rand(n)) \n\nsumv(x)\nsumv(x, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.svmda-Tuple{}","page":"Index of functions","title":"Jchemo.svmda","text":"svmda(; kwargs...)\nsvmda(X, y; kwargs...)\n\nSupport vector machine for discrimination \"C-SVC\" (SVM-DA).\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol, :klin,    :ktanh. See below.  \ngamma : kern parameter, see below.\ndegree : kern parameter, see below.\ncoef0 : kern parameter, see below.\ncost : Cost of constraints violation C parameter.\nepsilon : Epsilon parameter in the loss function.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nKernel types: \n\n:krbf – radial basis function: exp(-gamma * ||x - y||^2)\n:kpol – polynomial: (gamma * x' * y + coef0)^degree\n\"klin – linear: x' * y\n:ktan – sigmoid: tanh(gamma * x' * y + coef0)\n\nThe function uses LIBSVM.jl (https://github.com/JuliaML/LIBSVM.jl)  that is an interface to library LIBSVM (Chang & Li 2001).\n\nReferences\n\nJulia package LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl\n\nChang, C.-C. & Lin, C.-J. (2001). LIBSVM: a library for support  vector machines. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation  (algorithms, formulae, ...) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz\n\nChih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support  vector machines. ACM Transactions on Intelligent Systems and  Technology, 2:27:1–27:27, 2011. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm\n\nSchölkopf, B., Smola, A.J., 2002. Learning with kernels:  support vector machines, regularization, optimization, and  beyond. Adaptive computation and machine learning. MIT Press,  Cambridge, Mass.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn = nro(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nkern = :krbf ; gamma = 1e4\ncost = 1000 ; epsilon = .5\nmodel = svmda(; kern, gamma, cost, epsilon) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.svmr-Tuple{}","page":"Index of functions","title":"Jchemo.svmr","text":"svmr(; kwargs...)\nsvmr(X, y; kwargs...)\n\nSupport vector machine for regression (Epsilon-SVR).\n\nX : X-data (n, p).\ny : Univariate y-data (n).\n\nKeyword arguments:\n\nkern : Type of kernel used to compute the Gram matrices.   Possible values are: :krbf, :kpol, :klin,    :ktanh. See below.  \ngamma : kern parameter, see below.\ncoef0 : kern parameter, see below.\ndegree : kern parameter, see below.\ncost : Cost of constraints violation C parameter.\nepsilon : Epsilon parameter in the loss function.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nKernel types: \n\n:krbf – radial basis function: exp(-gamma * ||x - y||^2)\n:kpol – polynomial: (gamma * x' * y + coef0)^degree\n\"klin – linear: x' * y\n:ktan – sigmoid: tanh(gamma * x' * y + coef0)\n\nThe function uses LIBSVM.jl (https://github.com/JuliaML/LIBSVM.jl)  that is an interface to library LIBSVM (Chang & Li 2001).\n\nReferences\n\nJulia package LIBSVM.jl: https://github.com/JuliaML/LIBSVM.jl\n\nChang, C.-C. & Lin, C.-J. (2001). LIBSVM: a library for support  vector machines. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm. Detailed documentation  (algorithms, formulae, ...) can be found in http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz\n\nChih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support  vector machines. ACM Transactions on Intelligent Systems and  Technology, 2:27:1–27:27, 2011. Software available at  http://www.csie.ntu.edu.tw/~cjlin/libsvm\n\nSchölkopf, B., Smola, A.J., 2002. Learning with kernels:  support vector machines, regularization, optimization, and  beyond. Adaptive computation and machine learning. MIT Press,  Cambridge, Mass.\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\np = nco(X)\n\nkern = :krbf ; gamma = .1\ncost = 1000 ; epsilon = 1\nmodel = svmr(; kern, gamma, cost, epsilon) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nkern = :krbf ; gamma = .1\nmodel = svmr(; kern, gamma) \nfit!(model, x, y)\npred = predict(model, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.tab-Tuple{AbstractArray}","page":"Index of functions","title":"Jchemo.tab","text":"tab(X::AbstractArray)\ntab(X::DataFrame; vargroup = nothing)\n\nTabulation of categorical variables.\n\nx : Categorical variable or dataset containing categorical variable(s).\n\nSpecific for a dataset:\n\nvargroup : Vector of the names of the group variables to consider    in X (by default: all the columns of X).\n\nThe output cointains sorted levels.\n\nExamples\n\nusing Jchemo, DataFrames\n\nx = rand([\"a\"; \"b\"; \"c\"], 20)\nres = tab(x)\nres.keys\nres.vals\n\nn = 20\nX = hcat(rand(1:2, n), rand([\"a\", \"b\", \"c\"], n))\ndf = DataFrame(X, [:v1, :v2])\n\ntab(X[:, 2])\ntab(string.(X))\n\ntab(df)\ntab(df; vargroup = [:v1, :v2])\ntab(df; vargroup = :v2)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.tabdupl-Tuple{Any}","page":"Index of functions","title":"Jchemo.tabdupl","text":"tabdupl(x)\n\nTabulate duplicated values in a vector.\n\nx : Categorical variable.\n\nExamples\n\nusing Jchemo\n\nx = [\"a\", \"b\", \"c\", \"a\", \"b\", \"b\"]\ntab(x)\nres = tabdupl(x)\nres.keys\nres.vals\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.thresh_hard-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.thresh_hard","text":"thresh_hard(x::Real, delta)\n\nHard thresholding function.\n\nx : Value to transform.\ndelta : Range for the thresholding.\n\nThe returned value is:\n\nabs(x) > delta ? x : 0\n\nwhere delta >= 0.\n\nExamples\n\nusing Jchemo, CairoMakie \n\ndelta = .7\nthresh_hard(3, delta)\n\nx = LinRange(-2, 2, 500)\ny = thresh_hard.(x, delta)\nlines(x, y; axis = (xlabel = \"x\", ylabel = \"f(x)\"))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.thresh_soft-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.thresh_soft","text":"thresh_soft(x::Real, delta)\n\nSoft thresholding function.\n\nx : Value to transform.\ndelta : Range for the thresholding.\n\nThe returned value is:\n\nsign(x) * max(0, abs(x) - delta)\n\nwhere delta >= 0.\n\nExamples\n\nusing Jchemo, CairoMakie \n\ndelta = .7\nthresh_soft(3, delta)\n\nx = LinRange(-2, 2, 100)\ny = thresh_soft.(x, delta)\nlines(x, y; axis = (xlabel = \"x\", ylabel = \"f(x)\"))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Blockscal, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Blockscal, Xbl)\ntransf!(object::Blockscal, Xbl)\n\nCompute the preprocessed data from a model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Center, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Center, X)\ntransf!(object::Center, X::Matrix)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Comdim, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Comdim, Xbl; nlv = nothing)\ntransfbl(object::Comdim, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from      a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Cscale, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Cscale, X)\ntransf!(object::Cscale, X::Matrix)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.DetrendAirpls, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::DetrendAirpls, X)\ntransf!(object::DetrendAirpls, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.DetrendArpls, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::DetrendArpls, X)\ntransf!(object::DetrendArpls, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.DetrendAsls, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::DetrendAsls, X)\ntransf!(object::DetrendAsls, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.DetrendLo, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::DetrendLo, X)\ntransf!(object::DetrendLo, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.DetrendPol, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::DetrendPol, X)\ntransf!(object::DetrendPol, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Dkplsr, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Dkplsr, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Fdif, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Fdif, X)\ntransf!(object::Fdif, X::Matrix, M::Matrix)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\nM : Pre-allocated output matrix (n, p - npoint + 1).\n\nThe in-place function stores the output in M.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Interpl, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Interpl, X)\ntransf!(object::Interpl, X::Matrix, M::Matrix)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\nM : Pre-allocated output matrix (n, p).\n\nThe in-place function stores the output in M.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Kpca, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Kpca, X; nlv = nothing)\n\nCompute PCs (scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which PCs are computed.\nnlv : Nb. PCs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Kplsr, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Kplsr, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Mavg, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Mavg, X)\ntransf!(object::Mavg, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Mbconcat, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Mbconcat, Xbl)\n\nCompute the preprocessed data from a model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Mbpca, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Mbpca, Xbl; nlv = nothing)\ntransfbl(object::Mbpca, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from      a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Mbplsprobda, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Mbplsprobda, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from      a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Mbplsrda, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Mbplsrda, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Plsprobda, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Plsprobda, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from      a fitted model.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Plsrda, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Plsrda, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from      a fitted model.\n\nobject : The fitted model.\nX : X-data (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Rmgap, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Rmgap, X)\ntransf!(object::Rmgap, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Rosaplsr, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Rosaplsr, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Rp, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Rp, X; nlv = nothing)\n\nCompute scores T from a fitted model.\n\nobject : The fitted model.\nX : Matrix (m, p) for which scores T are computed.\nnlv : Nb. scores to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Savgol, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Savgol, X)\ntransf!(object::Savgol, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Scale, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Scale, X)\ntransf!(object::Scale, X::Matrix)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Snorm, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Snorm, X)\ntransf!(object::Snorm, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Snv, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Snv, X)\ntransf!(object::Snv, X)\n\nCompute the preprocessed data from a model.\n\nobject : Model.\nX : X-data to transform.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Soplsr, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Soplsr, Xbl)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Spca, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Spca, X; nlv = nothing)\nCompute principal components (PCs = scores T) from a \n    fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which PCs are computed.\nnlv : Nb. PCs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Jchemo.Umap, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Umap, X)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Union{Pca, Fda}, X; nlv = nothing)\n\nCompute principal components (PCs = scores T) from a      fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which PCs are computed.\nnlv : Nb. PCs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Union{Jchemo.Mbplsr, Jchemo.Mbplswest}, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Union{Mbplsr, Mbplswest}, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list of blocks (vector of matrices)    of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Union{Jchemo.Pcr, Jchemo.Spcr}, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Union{Pcr, Spcr}, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and a matrix X.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transf-Tuple{Union{Jchemo.Plsr, Jchemo.Splsr}, Any}","page":"Index of functions","title":"Jchemo.transf","text":"transf(object::Union{Plsr, Splsr}, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transfbl-Tuple{Jchemo.Cca, Any, Any}","page":"Index of functions","title":"Jchemo.transfbl","text":"transfbl(object::Cca, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transfbl-Tuple{Jchemo.Ccawold, Any, Any}","page":"Index of functions","title":"Jchemo.transfbl","text":"transfbl(object::Ccawold, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transfbl-Tuple{Jchemo.Plscan, Any, Any}","page":"Index of functions","title":"Jchemo.transfbl","text":"transfbl(object::Plscan, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transfbl-Tuple{Jchemo.Plstuck, Any, Any}","page":"Index of functions","title":"Jchemo.transfbl","text":"transfbl(object::Plstuck, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transfbl-Tuple{Jchemo.Rasvd, Any, Any}","page":"Index of functions","title":"Jchemo.transfbl","text":"transfbl(object::Rasvd, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.treeda-Tuple{}","page":"Index of functions","title":"Jchemo.treeda","text":"treeda(; kwargs...)\ntreeda(X, y; kwargs...)\n\nDiscrimination tree (CART) with DecisionTree.jl.\n\nX : X-data (n, p).\ny : Univariate class membership (n).\n\nKeyword arguments:\n\nn_subfeatures : Nb. variables to select at random    at each split (default: 0 ==> keep all).\nmax_depth : Maximum depth of the    decision tree (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples    each leaf needs to have.\nmin_sample_split : Minimum number of observations    in needed for a split.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe function fits a single discrimination tree (CART) using  package `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., Friedman, J. H., Olshen, R. A., and  Stone, C. J. Classification And Regression Trees.  Chapman & Hall, 1984.\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245\n\nExamples\n\nusing Jchemo, JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\")\n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y\nn, p = size(X) \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = n, ntrain, ntest)\ntab(ytrain)\ntab(ytest)\n\nn_subfeatures = p / 3 \nmax_depth = 10\nmodel = treeda(; n_subfeatures, max_depth) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\nfitm = model.fitm ;\nfitm.lev\nfitm.ni\n\nres = predict(model, Xtest) ; \npnames(res) \n@head res.pred\nerrp(res.pred, ytest)\nconf(res.pred, ytest).cnt\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.treer-Tuple{}","page":"Index of functions","title":"Jchemo.treer","text":"treer(; kwargs...)\ntreer(X, y; kwargs...)\n\nRegression tree (CART) with DecisionTree.jl.\n\nX : X-data (n, p).\ny : Univariate y-data (n).\n\nKeyword arguments:\n\nn_subfeatures : Nb. variables to select at random    at each split (default: 0 ==> keep all).\nmax_depth : Maximum depth of the    decision tree (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples    each leaf needs to have.\nmin_sample_split : Minimum number of observations    in needed for a split.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe function fits a single regression tree (CART) using  package `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., Friedman, J. H., Olshen, R. A., and  Stone, C. J. Classification And Regression Trees.  Chapman & Hall, 1984.\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGey, S., 2002. Bornes de risque, détection de ruptures,  boosting : trois thèmes statistiques autour de CART en régression (These de doctorat). Paris 11.  http://www.theses.fr/2002PA112245\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\np = nco(X)\n\nn_subfeatures = p / 3 \nmax_depth = 15\nmodel = treer(; n_subfeatures, max_depth) \nfit!(model, Xtrain, ytrain)\npnames(model)\npnames(model.fitm)\n\nres = predict(model, Xtest)\n@head res.pred\n@show rmsep(res.pred, ytest)\nplotxy(res.pred, ytest; color = (:red, .5), bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.umap-Tuple{}","page":"Index of functions","title":"Jchemo.umap","text":"umap(; kwargs...)\numap(X; kwargs...)\n\nUMAP: Uniform manifold approximation and projection for      dimension reduction\n\nX : X-data (n, p).\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to compute.\npsamp : Proportion of sampling in X for training.\nn_neighbors : Nb. approximate neighbors used to construct    the initial high-dimensional graph.\nmin_dist : Minimum distance between points in low-dimensional    space.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nThe function fits a UMAP dimension reducion using  package `UMAP.jl'. The used metric is the Euclidean distance. \n\nIf psamp < 1, only a proportion psamp of the observations (rows of X)  are used to build the model (systematic sampling over the first score of  the PCA of X). Can be used to decrease computation times when n is large.\n\nReferences\n\nhttps://github.com/dillondaudert/UMAP.jl\n\nMcInnes, L, Healy, J, Melville, J, UMAP: Uniform Manifold Approximation  and Projection for Dimension Reduction. ArXiV 1802.03426, 2018 https://arxiv.org/abs/1802.03426\n\nhttps://umap-learn.readthedocs.io/en/latest/howumapworks.html\n\nhttps://pair-code.github.io/understanding-umap/ \n\nExamples\n\nusing Jchemo, JchemoData\nusing JLD2, GLMakie, CairoMakie, FreqTables\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\nwlst = names(X)\nwl = parse.(Float64, wlst)\nntot = nro(X)\nsumm(Y)\ntyp = Y.typ\ntest = Y.test\ny = Y.conc\n\nmodel1 = snv() \nmodel2 = savgol(npoint = 21, deriv = 2, degree = 3)\nmodel = pip(model1, model2)\nfit!(model, X)\n@head Xp = transf(model, X)\nplotsp(Xp, wl; xlabel = \"Wavelength (nm)\", ylabel = \"Absorbance\", nsamp = 20).f\n\ns = Bool.(test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nytrain = rmrow(y, s)\ntyptrain = rmrow(typ, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\nytest = y[s]\ntyptest = typ[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = ntot, ntrain, ntest)\n\nfreqtable(string.(typ, \"-\", Y.label))\nfreqtable(typ, test)\n\n#################\n\nnlv = 3\nn_neighbors = 50 ; min_dist = .5 \nmodel = umap(; nlv, n_neighbors, min_dist)  \nfit!(model, Xtrain)\n@head T = model.fitm.T\n@head Ttest = transf(model, Xtest)\n\nnlv = 3\nn_neighbors = 50 ; min_dist = .5 \nmodel = umap(; nlv, n_neighbors, min_dist)  \nfit!(model, Xtrain)\n@head T = model.fitm.T\n@head Ttest = transf(model, Xtest)\nGLMakie.activate!() \n#CairoMakie.activate!()\nlev = mlev(typtrain)\nnlev = length(lev)\ncolsh = :tab10\ncolm = cgrad(colsh, nlev; alpha = .7, categorical = true) \nztyp = recod_catbyint(typtrain)\nf = Figure()\ni = 1\nax = Axis3(f[1, 1], xlabel = string(\"LV\", i), ylabel = string(\"LV\", i + 1), \n        zlabel = string(\"LV\", i + 2), title = \"UMAP\", perspectiveness = 0) \nscatter!(ax, T[:, i], T[:, i + 1], T[:, i + 2]; markersize = 8, \n    color = ztyp, colormap = colm) \nscatter!(ax, Ttest[:, i], Ttest[:, i + 1], Ttest[:, i + 2], color = :black, \n    markersize = 10)  \nelt = [MarkerElement(color = colm[i], marker = '●', markersize = 10) for i in 1:nlev]\n#elt = [PolyElement(polycolor = colm[i]) for i in 1:nlev]\ntitle = \"Group\"\nLegend(f[1, 2], elt, lev, title; nbanks = 1, rowgap = 10, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.varv-Tuple{Any}","page":"Index of functions","title":"Jchemo.varv","text":"varv(x)\nvarv(x, weights::Weight)\n\nCompute the uncorrected variance of a vector.\n\nx : A vector (n).\nweights : Weights (n) of the observations.    Must be of type Weight (see e.g. function mweight).\n\nExamples\n\nusing Jchemo\n\nn = 1000\nx = rand(n)\nw = mweight(rand(n))\n\nvarv(x)\nvarv(x, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vcatdf-Tuple{Any}","page":"Index of functions","title":"Jchemo.vcatdf","text":"vcatdf(dat; cols = :intersect)\n\nVertical concatenation of a list of dataframes.\n\ndat : List (vector) of dataframes.\ncols : Determines the columns of the returned dataframe.   See ?DataFrames.vcat.\n\nExamples\n\nusing Jchemo, DataFrames\n\ndat1 = DataFrame(rand(5, 2), [:v3, :v1]) \ndat2 = DataFrame(100 * rand(2, 2), [:v3, :v1])\ndat = (dat1, dat2)\nJchemo.vcatdf(dat)\n\ndat2 = DataFrame(100 * rand(2, 2), [:v1, :v3])\ndat = (dat1, dat2)\nJchemo.vcatdf(dat)\n\ndat2 = DataFrame(100 * rand(2, 3), [:v3, :v1, :a])\ndat = (dat1, dat2)\nJchemo.vcatdf(dat)\nJchemo.vcatdf(dat; cols = :union)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vcol-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.vcol","text":"vcol(X::AbstractMatrix, j)\nvcol(X::DataFrame, j)\nvcol(x::Vector, j)\n\nView of the j-th column(s) of a matrix X, or of the j-th element(s) of vector x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr, Jchemo.Spcr, Jchemo.Splsr}}","page":"Index of functions","title":"Jchemo.vip","text":"vip(object::Union{Plsr, Pcr, Splsr, Spcr}; nlv = nothing)\nvip(object::Union{Plsr, Pcr, Splsr, Spcr}, Y; nlv = nothing)\n\nVariable importance on Projections (VIP).\n\nobject : The fitted model.\nY : The Y-data that was used to fit the model.\n\nKeyword arguments:\n\nnlv : Nb. latent variables (LVs) to consider.   If nothing, the maximal model is considered.\n\nFor a PLS model  (or PCR, etc.) fitted on (X, Y) with a number  of A latent variables, and for variable xj (column j of X): \n\nVIP(xj) = Sum.a(1,...,A) R2(Yc, ta) waj^2 / Sum.a(1,...,A) R2(Yc, ta) (1 / p) \n\nwhere:\n\nYc is the centered Y, \nta is the a-th X-score, \nR2(Yc, ta) is the proportion of Yc-variance explained    by ta, i.e. ||Yc.hat||^2 / ||Yc||^2 (where Yc.hat is the    LS estimate of Yc by ta).  \n\nWhen Y is used, R2(Yc, ta) is replaced by the redundancy Rd(Yc, ta) (see function rd), such as in Tenenhaus 1998 p.139. \n\nReferences\n\nChong, I.-G., Jun, C.-H., 2005. Performance of some variable selection  methods when multicollinearity is present. Chemometrics and Intelligent  Laboratory Systems 78, 103–112. https://doi.org/10.1016/j.chemolab.2004.12.011\n\nMehmood, T., Sæbø, S., Liland, K.H., 2020. Comparison of variable  selection methods in partial least squares regression. Journal of  Chemometrics 34, e3226. https://doi.org/10.1002/cem.3226\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.\n\nExamples\n\nusing Jchemo\n\nX = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 27 18 7 6 ; 12 11 28 7] \nY = [10. 11 13; 120 131 27; 8 12 4; 1 200 8; 100 10 89] \ny = Y[:, 1] \nycla = [1; 1; 1; 2; 2]\n\nnlv = 3\nmodel = plskern(; nlv)\nfit!(model, X, y)\nres = vip(model.fitm)\npnames(res)\nres.imp\n\nfit!(model, X, Y)\nvip(model.fitm).imp\nvip(model.fitm, Y).imp\n\n## For PLSDA\n\nmodel = plsrda(; nlv) \nfit!(model, X, ycla)\npnames(model.fitm)\nfitm = model.fitm.fitm ;  # fitted PLS model\nvip(fitm).imp\nYdummy = dummy(ycla).Y\nvip(fitm, Ydummy).imp\n\nmodel = plslda(; nlv) \nfit!(model, X, ycla)\npnames(model.fitm.fitm)\nfitm = model.fitm.fitm.embfitm ;  # fitted PLS model\nvip(fitm).imp\nvip(fitm, Ydummy).imp\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.viperm-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.viperm","text":"viperm(model, X, Y; rep = 50, psamp = .3, score = rmsep)\n\nVariable importance by direct permutations.\n\nmodel : Model to evaluate.\nX : X-data (n, p).\nY : Y-data (n, q).  \n\nKeyword arguments:\n\nrep : Number of replications of the splitting   training/test. \npsamp : Proportion of data used as test set    to compute the score.\nscore : Function computing the prediction score.\n\nThe principle is as follows:\n\nData (X, Y) are splitted randomly to a training and a test set.\nThe model is fitted on Xtrain, and the score (error rate)    is computed on Xtest. This gives the reference error rate.\nRows of a given variable (feature) j in Xtest are randomly    permutated (the rest of Xtest is unchanged). The score is computed    on the Xtestpermj (i.e. Xtest after thta the rows of variable j    were permuted). The importance of variable j is computed by the    difference between this score and the reference score.\nThis process is run for each variable j separately and replicated    rep times. Average results are provided in the outputs, as well    as the results per replication. \n\nIn general, this method returns similar results as the out-of-bag permutation  method used in random forests (Breiman, 2001).\n\nReferences\n\nNørgaard, L., Saudland, A., Wagner, J., Nielsen, J.V., Munck, L., \n\nEngelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419.  https://doi.org/10.1366/0003702001949500\n\nExamples\n\nusing Jchemo, JchemoData, JLD2, CairoMakie\nmypath = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(mypath, \"data\", \"tecator.jld2\") \n@load db dat\npnames(dat)\nX = dat.X\nY = dat.Y \nwl_str = names(X)\nwl = parse.(Float64, wl_str) \nntot, p = size(X)\ntyp = Y.typ\nnamy = names(Y)[1:3]\nplotsp(X, wl; xlabel = \"Wavelength (nm)\", ylabel = \"Absorbance\").f\ns = typ .== \"train\"\nXtrain = X[s, :]\nYtrain = Y[s, namy]\nXtest = rmrow(X, s)\nYtest = rmrow(Y[:, namy], s)\nntrain = nro(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\n## Work on the j-th y-variable \nj = 2\nnam = namy[j]\nytrain = Ytrain[:, nam]\nytest = Ytest[:, nam]\n\nmodel = plskern(nlv = 9)\nres = viperm(model, Xtrain, ytrain; rep = 50, score = rmsep) ;\nz = vec(res.imp)\nf = Figure(size = (500, 400))\nax = Axis(f[1, 1]; xlabel = \"Wavelength (nm)\", ylabel = \"Importance\")\nscatter!(ax, wl, vec(z); color = (:red, .5))\nu = [910; 950]\nvlines!(ax, u; color = :grey, linewidth = 1)\nf\n\nmodel = rfr(n_trees = 10, max_depth = 2000, min_samples_leaf = 5)\nres = viperm(model, Xtrain, ytrain; rep = 50)\nz = vec(res.imp)\nf = Figure(size = (500, 400))\nax = Axis(f[1, 1];\n    xlabel = \"Wavelength (nm)\", \n    ylabel = \"Importance\")\nscatter!(ax, wl, vec(z); color = (:red, .5))\nu = [910; 950]\nvlines!(ax, u; color = :grey, linewidth = 1)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vrow-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.vrow","text":"vrow(X::AbstractMatrix, i)\nvrow(X::DataFrame, i)\nvrow(x::Vector, i)\n\nView of the i-th row(s) of a matrix X, or of the i-th element(s) of vector x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.wdis-Tuple{Any}","page":"Index of functions","title":"Jchemo.wdis","text":"wdis(d; typw = :bisquare, alpha = 0)\n\nDifferent functions to compute weights from distances.\n\nd : Vector of distances.\n\nKeyword arguments:\n\ntypw : Define the weight function.\nalpha : Parameter of the weight function,    see below.\n\nThe returned weight vector is: \n\nw = f(d / q) where f is the weight function    and q the 1-alpha quantile of d    (Cleveland & Grosse 1991).\n\nPossible values for typw are: \n\n:bisquare: w = (1 - d^2)^2 \n:cauchy: w = 1 / (1 + d^2) \n:epan: w = 1 - d^2 \n:fair: w =  1 / (1 + d)^2 \n:invexp: w = exp(-d) \n:invexp2: w = exp(-d / 2) \n:gauss: w = exp(-d^2)\n:trian: w = 1 - d  \n:tricube: w = (1 - d^3)^3  \n\nReferences\n\nCleveland, W.S., Grosse, E., 1991. Computational methods for local regression.  Stat Comput 1, 47–62. https://doi.org/10.1007/BF01890836\n\nExamples\n\nusing Jchemo, CairoMakie, Distributions\n\nd = sort(sqrt.(rand(Chi(1), 1000)))\ncolm = cgrad(:tab10, collect(1:9)) ;\nalpha = 0\nf = Figure(size = (600, 500))\nax = Axis(f, xlabel = \"d\", ylabel = \"Weight\")\ntypw = :bisquare\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[1])\ntypw = :cauchy\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[2])\ntypw = :epan\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[3])\ntypw = :fair\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[4])\ntypw = :gauss\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[5])\ntypw = :trian\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[6])\ntypw = :invexp\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[7])\ntypw = :invexp2\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[8])\ntypw = :tricube\nw = wdis(d; typw, alpha)\nlines!(ax, d, w, label = String(typw), color = colm[9])\naxislegend(\"Function\", position = :lb)\nf[1, 1] = ax\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.winvs-Tuple{Any}","page":"Index of functions","title":"Jchemo.winvs","text":"winvs(d; h = 2, criw = 4, squared = false)\nwinvs!(d; h = 2, criw = 4, squared = false)\n\nCompute weights from distances using an inverse scaled exponential function.\n\nd : A vector of distances.\n\nKeyword arguments:\n\nh : A scaling positive scalar defining the shape    of the weight function. \ncriw : A positive scalar defining outliers in the    distances vector d.\nsquared: If true, distances are replaced by the squared    distances; the weight function is then a Gaussian (RBF)    kernel function.\n\nWeights are computed by: \n\nexp(-d / (h * MAD(d)))\n\nor are set to 0 for extreme (potentially outlier) distances such  as d > Median(d) + criw * MAD(d). This is an adaptation of the weight  function presented in Kim et al. 2011.\n\nThe weights decrease when distances increase. Lower is h, sharper  is the decreasing function.\n\nReferences\n\nKim S, Kano M, Nakagawa H, Hasebe S. Estimation of active  pharmaceutical ingredients content using locally weighted partial  least squares and statistical wavelength selection. Int J Pharm. 2011; 421(2):269-274. https://doi.org/10.1016/j.ijpharm.2011.10.007\n\nExamples\n\nusing Jchemo, CairoMakie, Distributions\n\nx1 = rand(Chisq(10), 100) ;\nx2 = rand(Chisq(40), 10) ;\nd = [sqrt.(x1) ; sqrt.(x2)]\nh = 2 ; criw = 3\nw = winvs(d; h, criw) ;\nf = Figure(size = (600, 300))\nax1 = Axis(f, xlabel = \"Distance\", ylabel = \"Nb. observations\")\nhist!(ax1, d, bins = 30)\nax2 = Axis(f, xlabel = \"Distance\", ylabel = \"Weight\")\nscatter!(ax2, d, w)\nf[1, 1] = ax1 \nf[1, 2] = ax2 \nf\n\nd = collect(0:.5:15) ;\nh = [.5, 1, 1.5, 2.5, 5, 10, Inf] \n#h = [1, 2, 5, Inf] \nw = winvs(d; h = h[1]) \nf = Figure(size = (500, 400))\nax = Axis(f, xlabel = \"Distance\", ylabel = \"Weight\")\nlines!(ax, d, w, label = string(\"h = \", h[1]))\nfor i = 2:length(h)\n    w = winvs(d; h = h[i])\n    lines!(ax, d, w, label = string(\"h = \", h[i]))\nend\naxislegend(\"Values of h\"; position = :lb)\nf[1, 1] = ax\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.wtal-Tuple{Any}","page":"Index of functions","title":"Jchemo.wtal","text":"wtal(d; a = 1)\n\nCompute weights from distances using the 'talworth' distribution.\n\nd : Vector of distances.\n\nKeyword arguments:\n\na : Parameter of the weight function,    see below.\n\nThe returned weight vector w has component w[i] = 1 if |d[i]| <= a,  and w[i] = 0 if |d[i]| > a.\n\nExamples\n\nd = rand(10)\nwtal(d; a = .8)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.xfit-Tuple{Any}","page":"Index of functions","title":"Jchemo.xfit","text":"xfit(object)\nxfit(object, X; nlv = nothing)\nxfit!(object, X::Matrix; nlv = nothing)\n\nMatrix fitting from a bilinear model (e.g. PCA).\n\nobject : The fitted model.\nX : New X-data to be approximated from the model.   Must be in the same scale as the X-data used to fit   the model object, i.e. before centering    and eventual scaling.\n\nKeyword arguments:\n\nnlv : Nb. components (PCs or LVs) to consider.    If nothing, it is the maximum nb. of components.\n\nCompute an approximate of matrix X from a bilinear  model (e.g. PCA or PLS) fitted on X. The fitted X is  returned in the original scale of the X-data used to fit  the model object.\n\nExamples\n\nusing Jchemo\n\nX = [1. 2 3 4; 4 1 6 7; 12 5 6 13; \n    27 18 7 6; 12 11 28 7] \nY = [10. 11 13; 120 131 27; 8 12 4; \n    1 200 8; 100 10 89] \nn, p = size(X)\nXnew = X[1:3, :]\nYnew = Y[1:3, :]\ny = Y[:, 1]\nynew = Ynew[:, 1]\nweights = mweight(rand(n))\n\nnlv = 2 \nscal = false\n#scal = true\nmodel = pcasvd(; nlv, scal) ;\nfit!(model, X)\nfitm = model.fitm ;\n@head xfit(fitm)\nxfit(fitm, Xnew)\nxfit(fitm, Xnew; nlv = 0)\nxfit(fitm, Xnew; nlv = 1)\nfitm.xmeans\n\n@head X\n@head xfit(fitm) + xresid(fitm, X)\n@head xfit(fitm, X; nlv = 1) + xresid(fitm, X; nlv = 1)\n\n@head Xnew\n@head xfit(fitm, Xnew) + xresid(fitm, Xnew)\n\nmodel = pcasvd(; nlv = min(n, p), scal) \nfit!(model, X)\nfitm = model.fitm ;\n@head xfit(fitm) \n@head xfit(fitm, X)\n@head xresid(fitm, X)\n\nnlv = 3\nscal = false\n#scal = true\nmodel = plskern(; nlv, scal)\nfit!(model, X, Y, weights) \nfitm = model.fitm ;\n@head xfit(fitm)\nxfit(fitm, Xnew)\nxfit(fitm, Xnew, nlv = 0)\nxfit(fitm, Xnew, nlv = 1)\n\n@head X\n@head xfit(fitm) + xresid(fitm, X)\n@head xfit(fitm, X; nlv = 1) + xresid(fitm, X; nlv = 1)\n\n@head Xnew\n@head xfit(fitm, Xnew) + xresid(fitm, Xnew)\n\nmodel = plskern(; nlv = min(n, p), scal) \nfit!(model, X, Y, weights) \nfitm = model.fitm ;\n@head xfit(fitm) \n@head xfit(fitm, Xnew)\n@head xresid(fitm, Xnew)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.xresid-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.xresid","text":"xresid(object, X; nlv = nothing)\nxresid!(object, X::Matrix; nlv = nothing)\n\nResidual matrix from a bilinear model (e.g. PCA).\n\nobject : The fitted model.\nX : New X-data to be approximated from the model.   Must be in the same scale as the X-data used to fit   the model object, i.e. before centering    and eventual scaling.\n\nKeyword arguments:\n\nnlv : Nb. components (PCs or LVs) to consider.    If nothing, it is the maximum nb. of components.\n\nCompute the residual matrix:\n\nE = X - X_fit\n\nwhere X_fit is the fitted X returned by function  xfit. See xfit for examples.  ```\n\n\n\n\n\n","category":"method"},{"location":"see_jchemodemo/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"see_jchemodemo/","page":"Examples","title":"Examples","text":"See Html examples of data analyses.","category":"page"},{"location":"domains/#Available-methods","page":"Available methods","title":"Available methods","text":"","category":"section"},{"location":"domains/#MULTIVARIATE-EXPLORATORY-DATA-ANALYSES","page":"Available methods","title":"MULTIVARIATE EXPLORATORY DATA ANALYSES","text":"","category":"section"},{"location":"domains/#Principal-component-analysis-(PCA)","page":"Available methods","title":"Principal component analysis (PCA)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Usual","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"pcasvd SVD decomposition\npcaeigen Eigen decomposition\npcaeigenk Eigen decomposition for wide matrices (kernel form)\npcanipals NIPALS algorithm","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Allow missing data","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"pcanipalsmiss: NIPALS algorithm allowing missing data","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Robust ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"pcasph Spherical (with spatial median)\npcapp Projection pursuit.\npcaout Outlierness","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Sparse ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"spca sPCA Shen & Huang 2008","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kpca Kernel (KPCA) Scholkopf et al. 2002","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Utilities (PCA and PLS) ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"xfit X-matrix fitting \nxresid X-residual matrix ","category":"page"},{"location":"domains/#Random-projections","page":"Available methods","title":"Random projections","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"rp Random projection\nrpmatgauss Gaussian random projection matrix \nrpmatli Sparse random projection matrix ","category":"page"},{"location":"domains/#Manifold","page":"Available methods","title":"Manifold","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"UMAP – with UMAP.jl","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"umap: Uniform manifold approximation and projection for    dimension reduction","category":"page"},{"location":"domains/#Factorial-discrimination-analysis-(FDA)","page":"Available methods","title":"Factorial discrimination analysis (FDA)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"fda Eigen decomposition of the consensus \"inter/intra\"\nfdasvd Weighted SVD of the class centers","category":"page"},{"location":"domains/#Multiblock","page":"Available methods","title":"Multiblock","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"2 blocks","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"cca Canonical correlation analysis (CCA and RCCA)\nccawold CCA and RCCA - Wold (1984) Nipals algorithm  \nplscan Canonical partial least squares regression (Symmetric PLS)\nplstuck Tucker's inter-battery method of factor analysis (PLS-SVD)\nrasvd Redundancy analysis (RA), aka PCA on    instrumental variables (PCAIV)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"2 or more blocks ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mbconcat Transformer concatenating multi-block X-data\nmbpca Multiblock PCA (MBPCA), aka Consensus principal component analysis (CPCA)\ncomdim Common components and specific weights analysis (ComDim), aka CCSWA or HPCA","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Utilities","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mblock Make blocks from a matrix\nrd Redundancy coefficients between two matrices\nlg Lg coefficient\nrv RV correlation coefficient","category":"page"},{"location":"domains/#REGRESSION","page":"Available methods","title":"REGRESSION","text":"","category":"section"},{"location":"domains/#Ordinary-least-squares-(OLS)","page":"Available methods","title":"Ordinary least squares (OLS)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Multiple linear regression (MLR)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mlr QR algorithm\nmlrchol Normal equations and Choleski factorization\nmlrpinv Pseudo-inverse\nmlrpinvn Normal equations and pseudo-inverse\nmlrvec Simple (Univariate x) linear regression","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Anova","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"aov1 One-factor ANOVA","category":"page"},{"location":"domains/#Partial-least-squares-(PLSR)","page":"Available methods","title":"Partial least squares (PLSR)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Usual (asymetric regression mode)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plskern Fast \"improved kernel #1\" algorithm of Dayal & McGregor 1997\nplsnipals Nipals\nplswold Nipals Wold 1984\nplsrosa ROSA Liland et al. 2016\nplssimp SIMPLS de Jong 1993","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Variants of regularization using latent variables ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"cglsr Conjugate gradient for the least squares normal equations (CGLS)\npcr Principal components regression (SVD factorization)\nrrr Reduced rank regression (RRR), aka  Redundancy analysis regression ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Robust","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plsrout Outlierness","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Sparse","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"splsr \nsPLSR Lê Cao et al. 2008\nCovsel regression Roger et al. 2011\nspcr  \nsPCR Shen & Huang 2008","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Averaging PLSR models of different dimensionalities","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plsravg PLSR-AVG","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kplsr Non linear kernel (KPLSR)    Rosipal & Trejo 2001\ndkplsr Direct non linear kernel (DKPLSR) Bennett & Embrechts 2003","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Multiblock","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mbplsr Multiblock PLSR (MBPLSR) - Fast version (PLSR on concatenated blocks)\nmbplswest MBPLSR - Nipals algorithm Westerhuis et al. 1998 \nrosaplsr ROSA Liland et al. 2016\nsoplsr Sequentially orthogonalized (SO-PLSR) ","category":"page"},{"location":"domains/#Ridge-(RR,-KRR)","page":"Available methods","title":"Ridge (RR, KRR)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"RR","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"rr SVD factorization\nrrchol Choleski factorization","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"krr Non linear kernel (KRR), aka Least squares SVM (LS-SVMR)","category":"page"},{"location":"domains/#Local-models","page":"Available methods","title":"Local models","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"loessr LOESS regression model – With package Loess.jl","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kNN","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"knnr kNN weighted regression (kNNR)\nlwmlr kNN locally weighted MLR (kNN-LWMLR)\nlwplsr kNN locally weighted PLSR (kNN-LWPLSR)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Averaging","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lwplsravg kNN-LWPLSR-AVG ","category":"page"},{"location":"domains/#Support-vector-machines-–-with-LIBSVM.jl","page":"Available methods","title":"Support vector machines – with LIBSVM.jl","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"svmr Epsilon-SVR (SVM-R)","category":"page"},{"location":"domains/#Trees-–-with-DecisionTree.jl","page":"Available methods","title":"Trees – with DecisionTree.jl","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"treer Single tree\nrfr Random forest","category":"page"},{"location":"domains/#DISCRIMINATION-ANALYSIS-(DA)","page":"Available methods","title":"DISCRIMINATION ANALYSIS (DA)","text":"","category":"section"},{"location":"domains/#Based-on-the-prediction-of-the-Y-dummy-table","page":"Available methods","title":"Based on the prediction of the Y-dummy table","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mlrda MLR-DA\nplsrda PLSR-DA, aka usual PLSDA\nrrda RR-DA","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Sparse","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"splsrda Sparse PLSR-DA","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kplsrda KPLSR-DA\ndkplsrda DKPLSR-DA\nkrrda KRR-DA","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Multiblock ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mbplsrda MBPLSR-DA","category":"page"},{"location":"domains/#Probabilistic-DA","page":"Available methods","title":"Probabilistic DA","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Parametric","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lda Linear discriminant analysis (LDA)\nqda Quadratic discriminant analysis (QDA, with continuum towards LDA)\nrda Regularized discriminant analysis (RDA)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non parametric","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kdeda DA by kernel Gaussian density estimation (KDE-DA)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"On PLS latent variables","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"PLSDA\nplslda PLS-LDA\nplsqda PLS-QDA (with continuum)\nplskdeda  PLS-KDEDA\nSparse\nsplslda: Sparse PLS-LDA\nsplsqda: Sparse PLS-QDA\nsplskdeda: Sparse PLS-KDEDA\nNon linear\nkplslda KPLS-LDA\nkplsqda KPLS-QDA\nkplskdeda KPLS-KDEDA\ndkplslda Direct KPLS-LDA\ndkplsqda Direct KPLS-QDA\ndkplskdeda Direct KPLS-KDEDA\nMultiblock \nmbplslda MBPLS-LDA\nmbplsqda MBPLS-QDA\nmbplskdeda MBPLS-KDEDA","category":"page"},{"location":"domains/#Local-models-2","page":"Available methods","title":"Local models","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"knnda kNN-DA (Vote within neighbors)\nlwmlrda kNN locally weighted MLR-DA (kNN-LWMLR-DA)\nlwplsrda kNN Locally weighted PLSR-DA (kNN-LWPLSR-DA)\nlwplslda kNN Locally weighted PLS-LDA (kNN-LWPLS-LDA)\nlwplsqda kNN Locally weighted PLS-QDA (kNN-LWPLS-QDA, with continuum)","category":"page"},{"location":"domains/#Support-vector-machines-–-with-LIBSVM.jl-2","page":"Available methods","title":"Support vector machines – with LIBSVM.jl","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"svmda C-SVC (SVM-DA)","category":"page"},{"location":"domains/#Trees-–-with-DecisionTree.jl-2","page":"Available methods","title":"Trees – with DecisionTree.jl","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"treeda Single tree\nrfda Random forest","category":"page"},{"location":"domains/#ONE-CLASS-CLASSIFICATION-(OCC)","page":"Available methods","title":"ONE-CLASS CLASSIFICATION (OCC)","text":"","category":"section"},{"location":"domains/#From-a-PCA-or-PLS-score-space","page":"Available methods","title":"From a PCA or PLS score space","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"occsd Score distance (SD)\noccod Orthogonal distance (OD) \noccsdod Compromise between SD and OD (aka Simca approach) ","category":"page"},{"location":"domains/#Other-methods","page":"Available methods","title":"Other methods","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"occstah Stahel-Donoho outlierness","category":"page"},{"location":"domains/#Utilities","page":"Available methods","title":"Utilities","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"outstah Stahel-Donoho outlierness\nouteucl: Outlierness from Euclidean distances to center","category":"page"},{"location":"domains/#DISTRIBUTIONS","page":"Available methods","title":"DISTRIBUTIONS","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"dmnorm Normal probability density estimation\ndmnormlog Logarithm of the normal probability density estimation\ndmkern Gaussian kernel density estimation (KDE)\npval Compute p-value(s) for a distribution, a vector or an ECDF\nout Return if elements of a vector are strictly outside of a given range","category":"page"},{"location":"domains/#VARIABLE-IMPORTANCE","page":"Available methods","title":"VARIABLE IMPORTANCE","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"isel! Interval variable selection (e.g. Interval PLSR).\nvip Variable importance on projections (VIP)\nviperm! Variable importance by direct permutations","category":"page"},{"location":"domains/#TUNING-MODELS","page":"Available methods","title":"TUNING MODELS","text":"","category":"section"},{"location":"domains/#Test-set-validation","page":"Available methods","title":"Test-set validation","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"gridscore Compute an error rate over a grid of parameters","category":"page"},{"location":"domains/#Cross-validation-(CV)","page":"Available methods","title":"Cross-validation (CV)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"gridcv Compute an error rate over a grid of parameters","category":"page"},{"location":"domains/#Utilities-2","page":"Available methods","title":"Utilities","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mpar Expand a grid of parameter values\nsegmkf Build segments for K-fold CV\nsegmts Build segments for test-set validation","category":"page"},{"location":"domains/#Performance-scores","page":"Available methods","title":"Performance scores","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Regression","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"ssr SSR\nmsep MSEP\nrmsep, rmsepstand RMSEP\nsep SEP\nbias Bias\ncor2 Squared correlation coefficient\nr2 R2\nrpd, rpdr Ratio of performance to deviation\nmse Summary for regression\nconf Confusion matrix","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Discrimination","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"errp Classification error rate\nmerrp Mean intra-class classification error rate","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Model dimensionality","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"aicplsr AIC and Cp for PLSR\nselwold Wold's criterion to select dimensionality in LV models (e.g. PLSR)","category":"page"},{"location":"domains/#DATA-PROCESSING","page":"Available methods","title":"DATA PROCESSING","text":"","category":"section"},{"location":"domains/#Checking","page":"Available methods","title":"Checking","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"dupl Find replicated rows in a dataset\ntabdupl Tabulate duplicated values in a vector\nfindmiss Find rows with missing data in a dataset","category":"page"},{"location":"domains/#Pre-processing","page":"Available methods","title":"Pre-processing","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"De-trend transformation (baseline correction)\ndetrend_pol Polynomial linear regression\ndetrend_lo LOESS\ndetrend_asls Asymmetric least squares (ASLS)\ndetrend_airpls Adaptive iteratively reweighted penalized least squares (AIRPLS)\ndetrend_arpls Asymmetrically reweighted penalized least squares smoothing (ARPLS)\nsnv Standard-normal-deviation transformation\nfdif Finite differences\nmavg Smoothing by moving average\nsavgk, savgol Savitsky-Golay filtering\nrmgap Remove vertical gaps in spectra, e.g. for ASD NIR data","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Scaling","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"center Column centering\nscale Column scaling\ncscale Column centering and scaling\nblockscal Scaling of multiblock data","category":"page"},{"location":"domains/#Interpolation","page":"Available methods","title":"Interpolation","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"interpl Sampling spectra by interpolation    – From DataInterpolations.jl","category":"page"},{"location":"domains/#Calibration-transfer","page":"Available methods","title":"Calibration transfer","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"difmean Compute a detrimental matrix (for calibration transfer) by difference of    two matrix-column means.\neposvd Compute an orthogonalization matrix for calibration transfer\ncalds Direct standardization (DS)\ncalpds Piecewise direct standardization (PDS)","category":"page"},{"location":"domains/#Build-training-vs.-test-sets-by-sampling","page":"Available methods","title":"Build training vs. test sets by sampling","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"samprand Random (without replacement)\nsampsys Systematic over a quantitative variable\nsampcla Stratified by class\nsampdf From each column of a dataframe (where missing values are allowed)\nsampks Kennard-Stone \nsampdp Duplex  \nsampwsp WSP","category":"page"},{"location":"domains/#PLOTTING","page":"Available methods","title":"PLOTTING","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plotsp Plot spectra\nplotxy x-y scatter plot\nplotgrid Plot error/performance rates of a model\nplotconf Plot confusion matrix","category":"page"},{"location":"domains/#MODELS-AND-PIPELINES","page":"Available methods","title":"MODELS AND PIPELINES","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"model Build a model\npip Build a pipeline of models","category":"page"},{"location":"domains/#UTILITIES","page":"Available methods","title":"UTILITIES","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"aggstat Compute column-wise statistics by class in a dataset\naggsum Compute sub-total sums by class of a categorical variable\nsumv, meanv, normv, stdv, varv, madv, iqrv Vector operations \ncolmad, colmean, colmed, colnorm,    colstd, colsum, colvar  Column-wise operations\ncolmeanskip, colstdskip, colsumskip,    colvarskip Column-wise operations allowing missing data\nconvertdf Convert the columns of a dataframe to given types\ncovm, corm Weighted covariance and correlation matrices\ncosv, cosm Cosinus between vectors\ndefault Display the keyword arguments (with their default values) of a function\ndummy Build dummy table\neuclsq, mahsq, mahsqchol Distances (Euclidean, Mahalanobis) between rows of matrices\nfblockscal_col, _frob, _mfa, _sd Scale blocks\nfcenter, fscale, fcscale Column-wise centering and scaling of a matrix\nfindmax_cla Find the most occurent level in a categorical variable\nfrob, frob2 Frobenius norm of a matrix\nfweight Weight each row of a matrix\ngetknn Find nearest neighbours between rows of matrices\nhead, @head Display the first rows of a dataset\niqrv Interval inter-quartiles\nkrbf, kpol Build kernel Gram matrices\nlocw Working function for local (kNN) models\nmad Median absolute deviation (not exported)\nmatB, matW Between- and within-class covariance matrices\nmlev Return the sorted levels of a vecor or a dataset \nmweight Normalize a vector to sum to 1\nmweightcla Compute observation weights for a categorical variable,    given specified sub-total weights for the classes\nnco, nro, Nb. rows and columns of an object\nnormv Norm of a vector\nparsemiss Parsing a string vector allowing missing data\nplist Print each element of a list\npnames Return the names of the elements of an object\npsize Return the type and size of a dataset\npval Compute p-value(s) for a distribution, an ECDF or vector\nrecod_catbydict  Recode a categorical variable to dictionnary levels\nrecod_catbyind  Recode a categorical variable to indexes of levels\nrecod_catbyint  Recode a categorical variable to integers\nrecod_catbylev  Recode a categorical variable to levels\nrecod_indbylev  Recode an index variable to levels\nrecod_numbyint  Recode a continuous variable to integers\nrecod_miss Declare data as missing in a dataset\nrmcol Remove the columns of a matrix or the components of a vector having indexes s\nrmrow Remove the rows of a matrix or the components of a vector having indexes s\nrowmean, rownorm, rowstd, rowsum, rowvar: Row-wise operations\nrowmeanskip, rowstdskip, rowsumskip, rowvarskip: Row-wise operations    allowing missing data\nthresh_soft, thresh_hard Thresholding functions\nsoftmax Softmax function\nsourcedir Include all the files contained in a directory\nssq Total inertia of a matrix\nsumm Summarize the columns of a dataset\ntab, tabdupl Tabulations for categorical variables\nvcatdf Vertical concatenation of a list of dataframes\nwdis Different functions to compute weights from distances\nwtal Compute weights from distances using the 'talworth' distribution\nwinvs Compute weights from distances using an inverse scaled exponential function\nOther utility functions in file utility.jl","category":"page"},{"location":"news/#News","page":"News","title":"News","text":"","category":"section"},{"location":"news/#*Version-0.8.5*","page":"News","title":"Version 0.8.5","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.8.4*","page":"News","title":"Version 0.8.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\ninterpl: update to be consistent with DataInterpolation.jl.\nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.8.3*","page":"News","title":"Version 0.8.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nspca and spcr were improved.\nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.8.2*","page":"News","title":"Version 0.8.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.8.1*","page":"News","title":"Version 0.8.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nA bug was inserted in spca incidentally between v0.7.0 and v0.8.0. \nCorrected now.  ","category":"page"},{"location":"news/#*Version-0.8.0*","page":"News","title":"Version 0.8.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Breaking\nObject 'P' when representing matrix of laodings in Pca, Pls etc.:\nrenamed to 'V'  in the code of functions and their outputs.\nModifications\nspca  and spcr were improved.\nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.7.0*","page":"News","title":"Version 0.7.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Breaking\ndummy: To improve time computations, the output 'Y' is now a BitMatrix.\nfweight: remaned to wdis.\niqr: renamed to iqrv.\nmad: renamed to madv.\nnormw: renamed to normv.\nfweightdis: renamed to wdis.\nplotgrid: argument 'titleleg' renamed to 'legtitle'.\ntalworth: renamed to wtal.\nwdist: renamed to winvs.\nSparse Pca/Pls regression & discrimination:\nsplskern: renamed to splsr.\nArgument'meth' has changed.\nNew\nfweight: Weight each row of a matrix.\nsumv, meanv, stdv, varv: Vector operations.\nfrob2: Square of the Frobenius norm of a matrix.\nspcr: Sparse PCR.\nModifications\nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.6.2*","page":"News","title":"Version 0.6.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCorrection of a bug in function tab  introduced in 0.6.1\n(unfortunate changes in the mutiple dispatch, that impacted DA functions). \nCode cleaning.  ","category":"page"},{"location":"news/#*Version-0.6.1*","page":"News","title":"Version 0.6.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nconvertdf Convert the columns of a dataframe to given types.\nparsemiss Parsing a string vector allowing missing data.\nrecod_miss Declare data as missing in a dataset.\nBreaking\nmiss: renamed to findmiss.\ntabdf: renamed to tab.\naggstat, tabdf: arguments 'groups' and 'vars' renamed to 'vargroup' and 'vary'.\nModifications\nCode cleaning.    ","category":"page"},{"location":"news/#*Version-0.6.0*","page":"News","title":"Version 0.6.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Warning: Major breaking changes. The function model of the embedded syntax has been removed. The reason is: this function was frequently entering in conflict with the variable-name 'model' used  in many scripts built from other machine learning packages (Flux.jl, LearnAPI.jl, etc.). Sorry for any inconvenience.","category":"page"},{"location":"news/","page":"News","title":"News","text":"The previous syntax (<= 0.5.3):","category":"page"},{"location":"news/","page":"News","title":"News","text":"mod = model(plskern; nlv = 10)","category":"page"},{"location":"news/","page":"News","title":"News","text":"fit!(mod, X, Y)","category":"page"},{"location":"news/","page":"News","title":"News","text":"is now written (see README):","category":"page"},{"location":"news/","page":"News","title":"News","text":"model = plskern(nlv = 10)  # 'model' is now a variable (object) whose name can be changed ","category":"page"},{"location":"news/","page":"News","title":"News","text":"fit!(model, X, Y)","category":"page"},{"location":"news/","page":"News","title":"News","text":"or equivalently","category":"page"},{"location":"news/","page":"News","title":"News","text":"nlv = 10","category":"page"},{"location":"news/","page":"News","title":"News","text":"model = plskern(; nlv)","category":"page"},{"location":"news/","page":"News","title":"News","text":"fit!(model, X, Y)","category":"page"},{"location":"news/","page":"News","title":"News","text":"New\numap: new argument 'psamp'.\nBreaking\nsampdf: argument 'msamp' renamed to 'meth'.\nOutput 'fmpls' renamed to 'fmemb' in PLSDA functions.\nFunctions dtlo, dtpol, dtasls, dtarpls*, **dtairpls    renamed to detrend_lo, detrend_pol, detrend_asls, detrend_arpls,    detrend_airpls.\ndmnorm: one of the methods has been modified.\nArgument and object 'fun' has been renamed 'algo' everywhere.\nObject 'fm' has been renamed 'fitm' (fitted model) everywhere.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.5.3*","page":"News","title":"Version 0.5.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nloessr Compute a locally weighted regression model (LOESS)   (with package Loess.jl).\ndtlo Baseline correction of each row of X-data by LOESS regression.\nBreaking\ndetrend, asls, airpls, arpls: renamed to    dtpol, dtasls, dtairpls, dtarpls.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.5.2*","page":"News","title":"Version 0.5.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nairpls: Correction of the termination criterion.","category":"page"},{"location":"news/#*Version-0.5.1*","page":"News","title":"Version 0.5.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nBaseline correction of each row of X-data by\nasls  asymmetric least squares algorithm (ASLS).\nairpls adaptive iteratively reweighted penalized least squares algorithm (AIRPLS).\narpls asymmetrically reweighted penalized least squares smoothing (ARPLS).\nBreaking\nFunction defpar renamed to default.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.5.0*","page":"News","title":"Version 0.5.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nThe management of the arguments of the functions has been deeply modified. \nThe unique container 'Par' has been removed, and replaced by independent    containers between functions or groups of functions (eg. 'ParPca').\ndefpar Display the keyword arguments (with their default values) of a function.\numap: UMAP: Uniform manifold approximation and projection for \ndimension reduction (uses package UMAP.jl added as dependence).\nBreaking\nArgument 'nlvdis' removed from knnr and knnda (there is no possible dimension    reduction anymore).\nRenaming of functions:  \nfindindex to recod_catbyind.\nrecodcat2int to recod_catbyint\nrecodnum2int to recod_numbyint\nreplacebylev to recod_catbylev\nreplacebylev2 to recod_indbylev\nreplacedict to recod_catbydict\ntreer_dt, treeda_dt, rfr_dt, rfda_dt to treer, treeda,   rfr, rfda.   \nrp : argument 'mrp' renamed to 'meth'.\nrpmatli: argument 's_li' renamed to 's'.\nspca, splskern : argument 'msparse' renamed to 'meth'.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.4.3*","page":"News","title":"Version 0.4.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nouteucl: Outlierness from Euclidean distances to center.\ntalworth: Weight function.\npcapp, pcapp!: Robust PCA by projection pursuit. \npcaout, pcaout!: Robust PCA using outlierness.\nplsrout, plsrout!: Robust PLSR using outlierness.\nBreaking\nstah: Renamed to outstah and improved. \nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.4.2*","page":"News","title":"Version 0.4.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nsampwsp: Build training vs. test sets by WSP sampling.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.4.1*","page":"News","title":"Version 0.4.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Breaking\nsavgk: order of arguments have changed. \nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.4.0*","page":"News","title":"Version 0.4.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Warning: Major breaking changes with versions 0.3. The embedded syntax  has changed, with the use of the new function model:     - 'model = plskern(; nlv = 15)' is now writen as 'model = mod_(plskern; nlv = 15)'. ","category":"page"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode cleaning. The examples in th the function help pages have \nbeen corrected from typing errors. ","category":"page"},{"location":"news/#*Version-0.3.7*","page":"News","title":"Version 0.3.7","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \naggsum: Compute sub-total sums by class for a categorical variable.\nfindindex: Replace a vector containg levels by the indexes of a set of levels.\ngetknn: Add of angular and correlation distances.\nkplslda, kplsqda, kplskdeda: Kernel PLS-LDA, PLS-QDA, PLS-KDEDA.\ndkplslda, dkplsqda, dkplskdeda: Direct kernel PLS-LDA, PLS-QDA, PLS-KDEDA.\nmbplslda, mbplsqda, mbplskdeda: Multiblock PLS-LDA, PLS-QDA, PLS-KDEDA.\nmerrp: Mean intra-class classification error rate.\nmweightcla: Compute observation weights for a categorical variable,    given specified sub-total weights for the classes.\nrownorm: Row-wise norms.\nsnorm: Row-wise norming of X-data.\nBreaking\nconfusion: renamed to conf, and output 'accuracy' renamed to 'accpct'    and modified. \nModifications\nFDA and predictive DA functions: improvements to better take into account    for unbalanced classes.\nCode cleaning. ","category":"page"},{"location":"news/#*Version-0.3.6*","page":"News","title":"Version 0.3.6","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode improvement. ","category":"page"},{"location":"news/#*Version-0.3.5*","page":"News","title":"Version 0.3.5","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode improvement. ","category":"page"},{"location":"news/#*Version-0.3.4*","page":"News","title":"Version 0.3.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ncolmed: Column-wise medians.\nModifications\nCode improvement. ","category":"page"},{"location":"news/#*Version-0.3.3*","page":"News","title":"Version 0.3.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode improvement. \ncalds, calpds: Function 'fit!' was added.\nBreaking\nrmgap: works now with a function 'transf'.    Function rmgap! was removed.","category":"page"},{"location":"news/#*Version-0.3.2*","page":"News","title":"Version 0.3.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode improvement. \nBreaking\nhconcat renamed to mbconcat\nrecodnum2cla renamed to recodnum2int\nviperm! renamed to viperm","category":"page"},{"location":"news/#*Version-0.3.1*","page":"News","title":"Version 0.3.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nhconcat Transformer contatenating multi-block X-data.\nblockscal Scaling of multiblock X-data. Replace functions   'fblockscal_...\".\nmbplsrda Discrimination based on multiblock partial    least squares regression (MBPLSR-DA).\nModifications\nCode improvement. \nmbplsr, soplsr : correction when 'scal' = true.","category":"page"},{"location":"news/#*Version-0.3.0*","page":"News","title":"Version 0.3.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Warning: Major breaking changes. Package Jchemo has been deeply restructured, to enable an \"embedded\" syntax and facilitate pipelines  building. The previous \"direct\" syntax is still allowed (although arugments have changed for some functions, see the help pages) but is not favor anymore. Users who prefer the previous syntax will have to keep working with versions < 0.3.0. Sorry for any inconveniance. ","category":"page"},{"location":"news/","page":"News","title":"News","text":"Some typing errors may have been introduced due to the  restructuration. They will be corrected in versions > 0.3.0.","category":"page"},{"location":"news/","page":"News","title":"News","text":"Some specific modified points are:","category":"page"},{"location":"news/","page":"News","title":"News","text":"In the arguments, all String types have been replaced by Type Symbol (e.g. \"unif\" is replaced by :unif)\nThe 'weights' (row weighting in some functions, such as pcasvd etc.) argument must now be of type 'Weight', built from function 'mweight'. \nSampling functions 'samp...' have changed. \nFunction 'mtest' renamed to 'sampdf'.\nFunctions center, scale, cscale, blockscale renamed to fcenter, fscale, etc. Alternatively, new transformers center, scale and cscale have been created.\nFunctions isel and viperm remated to isel! and viperm!, and syntax changed.\nSyntax of tuning functions gridscore and gridcv   has changed, and the functions are now genereic    (no need anymore to call specific functions ...lv    and ...lb).\nRemoved function (temporary or not):\nbaggr and its utilities\ncplsravg\ngridcvmb, gridcvlvmb\ninterpl_mon\nlwmlrs, lwplsrs, lwmlrdas, lwlsrdas    (since can be built from pipelines)\nlwplsrdaavg, lwplsldaavg, lwplsqdaavg\nmavg_runmean\nmbunif, mbwcov\nnsc, nscrda, nscda\noccknndis, occlknndis\nplsrdaavg, plsldaavg, plsqdaavg","category":"page"},{"location":"news/#*Version-0.2.4*","page":"News","title":"Version 0.2.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode improvement.","category":"page"},{"location":"news/#*Version-0.2.3*","page":"News","title":"Version 0.2.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nsamprand: Build training vs. test sets by random sampling.\nsvmr, svmda: SVM (wrappers to LIBSVM.jl) were reset in    the package.\nModifications\nbaggr has been parallelized (multi-threading).\nCode improvement.\nBreaking changes\nsampks, sampdp, samprand, sampsys, sampcla: order of arguments have changed.\nmtest: Arguments have changed, and function has been improved. \nplotxy: The number of methods have been reduced (for simplification). \nThe aggregated syntax (matrice with two columns as input {x,y}) is not    allowed anymore. ","category":"page"},{"location":"news/#*Version-0.2.2*","page":"News","title":"Version 0.2.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\n@head: macro for function 'head'.\nModifications\nCode improvement.","category":"page"},{"location":"news/#*Version-0.2.1*","page":"News","title":"Version 0.2.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nsplsrda: Sparse PLSR-DA.\nsplslda: Sparse PLS-LDA.\nsplsqda: Sparse PLS-QDA.\nsplskdeda: Sparse PLS-KDE-DA.\nModifications\nCode improvement.","category":"page"},{"location":"news/#*Version-0.2.0*","page":"News","title":"Version 0.2.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nsplskern: Sparse PLSR.\nModifications\nCode improvement.","category":"page"},{"location":"news/#*Version-0.1.24*","page":"News","title":"Version 0.1.24","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ncolmeanskip, colstdskip, colsumskip, colvarskip: Column-wise operations allowing missing data.\nnsc: Nearest shrunken centroids (NSC).\nnscda: Discrimination by nearest shrunken centroids.\npcanipals: PCA by NIPALS algorithm.\npcanipalsmiss: PCA by NIPALS algorithm allowing missing data.\nplist Print each element of a list.\nrowmeanskip, rowstdskip, rowsumskip, rowvarskip: Row-wise operations allowing missing data.\nsoft: Soft thresholding.\nsoftmax: Softmax function.\nspca, snipals, snipalsh, snipalsmix: Sparse PCA (Shen & Hunag 2008).\nModifications\nCode improvement.\nBreaking changes\ncovsel has been removed and integrated into covselr that was   extended and made faster. ","category":"page"},{"location":"news/#*Version-0.1.23*","page":"News","title":"Version 0.1.23","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ndmnormlog: Logarithm of the normal probability density estimation.\nrda: Regularized discriminant analysis (RDA)\nModifications\nmatB, matW, lda, qda: new argument 'weights'.\nqda, plsqda, lwplsqda, plsqdaavg, lwplsqdaavg: new argument 'alpha'    (continuum from QDA toward LDA). \ndmnorm: new argument 'simpl'. \nCode improvement.\nBreaking changes\ncheckdupl, checkmiss: renamed to dupl and miss.\nfda, fdasvd : argument 'pseudo' has been replaced by 'lb'   (ridge regularization).","category":"page"},{"location":"news/#*Version-0.1.22*","page":"News","title":"Version 0.1.22","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ndifmean : Compute a detrimental matrix (for calibration transfer) by column    means difference.\nModifications\nCode improvement.\nBreaking changes\ncalds, calpds: Order of arguments 'Xt' and 'X' were inverted   (required to be useable by gridscore etc.).","category":"page"},{"location":"news/#*Version-0.1.21*","page":"News","title":"Version 0.1.21","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nkdeda: Discriminant analysis using non-parametric kernel Gaussian    density estimation (KDE-DA).\nplskdeda: PLS-KDE-DA \nModifications\nCode improvement.\nBreaking changes\nlda, qda: returned output 'ds' renamed to 'dens'.","category":"page"},{"location":"news/#*Version-0.1.20*","page":"News","title":"Version 0.1.20","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ndmkern: Gaussian kernel density estimation (KDE)..\nout: Return if elements of a vector are strictly outside of a given range.\npval: Compute p-value(s) for a distribution, an ECDF or a vector.\nModifications\nplotxy: accept a matrix (n, 2) as input.\nCode improvement.\nBreaking changes\ndens: removed and replaced by dmkern.","category":"page"},{"location":"news/#*Version-0.1.19*","page":"News","title":"Version 0.1.19","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nModifications\nDependance to unused package HypothesisTests.jl was removed. \nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.18*","page":"News","title":"Version 0.1.18","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nconfusion Confusion matrix.\nplotconf Plot confusion matrix.\nModifications\ncplsravg: new argument 'typda'. \nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.17*","page":"News","title":"Version 0.1.17","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nlwmlrda: k-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).\nlwmlrda_s: kNN-LWMLR-DA after preliminary (linear or non-linear) dimension reduction.\nlwplsrda_s kNN-LWPLSR-DA after preliminary (linear or non-linear) dimension reduction.\nModifications\nlwmlr_s: Add arguments 'psamp' and 'samp' for large nb. observations. \nCode cleaning.\nBreaking changes\nlwplsr_s: Arguments and pipeline changed to be consistent    with lwmlr_s. \nsampclas: remamed to sampcla.","category":"page"},{"location":"news/#*Version-0.1.16*","page":"News","title":"Version 0.1.16","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ndkplsrda Discrimination based on direct kernel partial least    squares regression (DKPLSR-DA)\ntreer_dt Regression tree (CART) with DecisionTree.jl\nrfr_dt Random forest regression with DecisionTree.jl\ntreeda_dt Discrimination tree (CART) with DecisionTree.jl\nrfda_dt Random forest discrimination with DecisionTree.jl\nModifications\nselwold :  add argument 'step'- \nCode cleaning.\nBreaking changes\nWarning: Difficult breaking bugs appeared in C++ dll from Julia v1.8.4    (still present in v1.9-betas) that removed the possibility to use packages    LIBSVM.jl and XGBoost.jl under Windows. For this reason, Jchemo.jl stopped to use    these two packages. All the related functions (SVM, RF and XGBoost models)    were removed. For CART models (trees), they were replaced by new functions    using package DecisionTree.jl.  ","category":"page"},{"location":"news/#*Version-0.1.15*","page":"News","title":"Version 0.1.15","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ncosm Cosinus between the columns of a matrix\ncosv Cosinus between two vectors\nlwmlr: k-Nearest-neighbours locally weighted multiple linear regression (kNN-LWMLR)\nlwmlr_s: kNN-LWMLR after preliminary (linear or non-linear) dimension reduction\npmod Short-cut for function 'Base.parentmodule'\ntabdupl Tabulate duplicated values in a vector\nModifications\nImprovement of vi_baggr\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.14*","page":"News","title":"Version 0.1.14","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nisel: Interval variable selection.\nmlev: Return the sorted levels of a dataset.\npcasph: Spherical PCA.\ntabdf: Compute the nb. occurences of groups in categorical variables of    a dataset.\nvip: Variable importance by permutation.\nModifications\nplotgrid: add of argument 'leg'. \nplotxy: add of arguments 'circle' and 'zeros'. \nCode cleaning.\nBreaking changes\naggstat has changed (arguments).\nbaggr_vi renamed to vi_baggr\nbaggr_oob renamed to oob_baggr\ngridcv and gridcv_mb: in output 'res_rep', colum 'rept' replaced    by column 'repl'.\niplsr was removed and replaced by the more generic function isel.\nmtest: Outputs 'idtrain' and 'idtest' renamed to 'train' and 'test'.\nrd: argument 'corr' chanfed to 'typ'.\ntabn was removed.\nvimp_xgb renamed to vi_xgb\nvip: outputs have been improved.","category":"page"},{"location":"news/#*Version-0.1.13*","page":"News","title":"Version 0.1.13","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nmtest Select indexes defining training and test sets for each column    of a dataframe.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.12*","page":"News","title":"Version 0.1.12","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nAll tree functions: Internal changes to adapt to modifications in XGBoost.jl library.","category":"page"},{"location":"news/#*Version-0.1.11*","page":"News","title":"Version 0.1.11","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nvip Variable importance on PLS projections (VIP).","category":"page"},{"location":"news/#*Version-0.1.10*","page":"News","title":"Version 0.1.10","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nmbwcov: Multiblock weighted covariate analysis regression (MBWCov) (Mangana et al. 2021).\nModifications\nCode cleaning.\nBreaking changes\nmbmang renamed to mbunif (Unified multiblock analysis).\nramang renamed to rrr (Reduced rank regression).","category":"page"},{"location":"news/#*Version-0.1.9*","page":"News","title":"Version 0.1.9","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nrasvd: Redundancy analysis - PCA on instrumental variables (PCAIV).\nramang Redundancy analysis regression = Reduced rank regression (RRR)\nmbplswest MBPLSR - Nipals algorithm (Westerhuis et al. 1998) \nBreaking changes\nAll the functions ..._avg and ..._stack renamed    to ...avg and ...stack (e.g. plsr_avg to    plsravg).\ncaltransf_ds and caltransf_pds remaned   to calds and calpds.\nfnorm renamed to frob.","category":"page"},{"location":"news/#*Version-0.1.8*","page":"News","title":"Version 0.1.8","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nplswold: PLSR Wold Nipals algorithm.\nccawold: CCA Wold Nipals algorithm.\nmbmang: Unified multiblock data analysis of Mangana et al. 2019.\nModifications\nmlrpinv_n renamed to mlrpinvn\npls renamed to plscan.\npls_svd renamed to plstuck.\nrcca renamed to cca (and argument 'alpha\" to 'tau').\nrpmat_gauss and rpmat_li renamed to rpmatgauss and rpmatli \nOutput 'Tbl' added in comdim and mbpca.\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.7*","page":"News","title":"Version 0.1.7","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"CairoMakie.jl was removed from the dependances, and replaced by Makie.jl.","category":"page"},{"location":"news/","page":"News","title":"News","text":"To display the plots, the user has to install and load one of the Makie's backend (e.g. CairoMakie).","category":"page"},{"location":"news/","page":"News","title":"News","text":"New\nrcca: Canonical correlation analysis. (RCCA).\npls: Canonical partial least squares regression (Symmetric PLS).\npls_svd: Tucker's inter-battery method of factor analysis (PLS-SVD).\ncolnorm2 was removed, replaced by colnorm: \nNorm of each column of a dataset.\nfnorm: Frobenius norm of a matrix.\nnorm2 was removed, replaced by normw: \nWeighted norm of a vector.\nModifications\nMajor changes in multiblock functions:\nRenamed functions:\nmbpca_cons ==> mbpca\nmbpcacomdims ==> comdim\nmbplsr_rosa ==> rosaplsr\nmbplsr_so ==> soplsr\nArgument 'X_bl' renamed to 'Xbl'\nVariable 'pc' in summary outputs of PCA and KPCA functions renamed to 'lv'. \nModification of all the tree functions to adapt to the new version of\nXGBoost.jl (>= 2.02) (https://juliahub.com/ui/Packages/XGBoost/rSeEh/2.0.2).    The new Jchemo functions does not work anymore with XGBoost.jl 1.5.2.    \nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.6*","page":"News","title":"Version 0.1.6","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Package Jchemo.jl has been registered.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.5*","page":"News","title":"Version 0.1.5","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.4*","page":"News","title":"Version 0.1.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\nhead: Display the first rows of a dataset.\nModifications\nRemove of side-effects in some functions of multi-bloc analyses.","category":"page"},{"location":"news/#*Version-0.1.3*","page":"News","title":"Version 0.1.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\ndetrend: argument 'degree' renamed to 'pol'.\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.2*","page":"News","title":"Version 0.1.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\ndetrend: new argument 'degree'\ngridcvlv: correction of a bug (typing error) inserted    in the last version.","category":"page"},{"location":"news/#*Version-0.1.1*","page":"News","title":"Version 0.1.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nblockscal: bug corrected in arguments.\nUse of multi-threading (package Threads)   in functions locw and locwlv, used in local models.","category":"page"},{"location":"news/#*Version-0.1.0*","page":"News","title":"Version 0.1.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nArgument 'scal' (X and/or Y column-scaling) added to various functions.\nblockscal: names of arguments have changed.\nplotgrid: argument 'indx' modified.","category":"page"},{"location":"news/#*Version-0.0.26*","page":"News","title":"Version 0.0.26","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New\ncscale: Center and scale each column of a matrix.\nModifications\nArgument 'scal' (X and/or Y column-scaling) added to various functions.   Work in progress. The argument will be available for all the concerned fonctions.\nOutput 'explvar' replaced by 'explvarx' in all the concerned functions.\nrd: New argument 'corr'.","category":"page"},{"location":"news/#*Version-0.0.25*","page":"News","title":"Version 0.0.25","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nrd: Redundancy coefficients between two matrices.\nModifications\nsummary for Plsr objects. See the example in ?plskern.","category":"page"},{"location":"news/#*Version-0.0.24*","page":"News","title":"Version 0.0.24","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nselwold: Argument \"plot\" renamed \"graph\" and bug fixed in plotting.","category":"page"},{"location":"news/#*Version-0.0.23*","page":"News","title":"Version 0.0.23","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \noccknndis: One-class classification using \"global\" k-nearest neighbors distances.\nocclknndis: One-class classification using \"local\" k-nearest neighbors distances.\nModifications\noccsd, occod, occsdod, occstah: The methods to compute the cutoff have changed.","category":"page"},{"location":"news/#*Version-0.0.22*","page":"News","title":"Version 0.0.22","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \ncolmad: Median absolute deviation (MAD) of each column of a matrix.\noccsdod: One-class classification using a consensus between PCA/PLS score (SD) and orthogonal (OD) distances.\nreplacedict: Replace the elements of a vector by levels defined in a dictionary.\nstah: Stahel-Donoho outlierness measure.\nModifications\ndens: outputs have been modified.\nodis and scordis have been rename to occsd and occod, and modified.\nplotxy: new argument \"bisect\".","category":"page"},{"location":"news/#*Version-0.0.21*","page":"News","title":"Version 0.0.21","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \ndens: Univariate kernel density estimation.\nModifications \nAll the datasets (examples) have been moved to package JchemoData    (https://github.com/mlesnoff/JchemoData.jl)\nplotsp: Argument 'nsamp' added.\ndatasets: removed and transferred to JchemoData.jl","category":"page"},{"location":"news/#*Version-0.0.20*","page":"News","title":"Version 0.0.20","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \ncovselr: Covsel regression (Covsel+Mlr).\nModifications \ncovsel, mlrvec: Arguments changed.","category":"page"},{"location":"news/#*Version-0.0.19*","page":"News","title":"Version 0.0.19","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nselwold : Wold's criterion to select dimensionality in LV (e.g. PLSR) models.\nplotxy : Scatter plot (x, y) data.\nModifications \nplotscore: Renamed to plotgrid.    ","category":"page"},{"location":"news/#*Version-0.0.18*","page":"News","title":"Version 0.0.18","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nplotgrid : Plot error or performance rates of model predictions.","category":"page"},{"location":"news/","page":"News","title":"News","text":"Modifications \nplotsp: argument 'size' was added.","category":"page"},{"location":"news/#*Version-0.0.17*","page":"News","title":"Version 0.0.17","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nreplacebylev2 : Replace the elements of an index-vector by levels.\nModifications \naggstat : Sorting order for dataframes.\ncheckdupl : bug corrected.\nmatB, matW : when requested, update of covm to cov, and aggstat output.\nplotsp : faster.\ntransfer_ds : renamed to caltransf_ds.\ntransfer_pds : renamed to caltransf_pds.\nrecodcat2num : renamed to recodcat2int\nsegmts : A seed (MersenneTwister) can be set for the random samplings.\nExamples added in the helps of every functions.\nDiscrimination functions: major updates.","category":"page"},{"location":"news/#*Version-0.0.16*","page":"News","title":"Version 0.0.16","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \ntransfer_ds : Calibration transfert with direct standardization (DS).\ntransfer_pds : Calibration transfert with piecewise direct standardization (PDS).\nModifications\nmlr functions : Argument 'noint' added.\nplsravgcv : Bug corrected.","category":"page"},{"location":"news/#*Version-0.0.15*","page":"News","title":"Version 0.0.15","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nplsr_stack : Stacking PLSR models\nModifications\naicplsr : BIC criterion added\nfweight\nplsr_avg : Stacking was added\nplsravgaic\nplsravgcv\nlwplsr_avg","category":"page"},{"location":"news/#*Version-0.0.14*","page":"News","title":"Version 0.0.14","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nlwplsr_s ","category":"page"},{"location":"news/#*Version-0.0.13*","page":"News","title":"Version 0.0.13","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nfweight \nrowmean, rowstd\nModifications\naicplsr\nlwplsr_avg\nplsr_avg\nsnv\nwshenk","category":"page"},{"location":"news/#*Version-0.0.12*","page":"News","title":"Version 0.0.12","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nnco, nro\nModifications\nmpars renamed to mpar\nAll functions terminating with \"...agg\" renamed to \"...avg\".","category":"page"},{"location":"news/#*Version-0.0.11*","page":"News","title":"Version 0.0.11","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nblockscal_mfa\ndatasets\nmbpca_cons\nlg\nssq\nModifications\nAll the functions terminating with a \"s\" have been renamed without \"s\"\n(e.g. colmeans was renamed to colmean)","category":"page"},{"location":"news/#*Version-0.0.10*","page":"News","title":"Version 0.0.10","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \ncolsum\nmbpcacomdims\nrowsum\nModifications\nnipals\nmse\nmbpls","category":"page"},{"location":"news/#*Version-0.0.9*","page":"News","title":"Version 0.0.9","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New \nblockscal_frob, blockscal_ncol, blockscal_sd\ncolnorm2\ncorm, covm\nnipals\nnorm2\nModifications\nblockscal \nmatcov renamed to covm and extended\nRemoved\nmbplsrmidavg\nmbplsr_mid\nmbplsrmidseq","category":"page"},{"location":"news/#*Version-0.0.8*","page":"News","title":"Version 0.0.8","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\ngridcv_mb\ngridcvlv_mb\nmbplsr_avg\nmbplsr_mid\nmbplsrmidseq\nModifications \nrosaplsr renamed to mbplsr_rosa\nsoplsr renamed to mbplsr_soplsr","category":"page"},{"location":"news/#*Version-0.0.7*","page":"News","title":"Version 0.0.7","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\nmbplsr\nsoplsr","category":"page"},{"location":"news/#*Version-0.0.6*","page":"News","title":"Version 0.0.6","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\ncolstd\nplsrosa\nplssimp\nrosaplsr\nrv\nrmrows, rmcols: renamed to rmrow, rmcol\nModifications \ninterpl, interpl_mon: changes in arguments\nplotsp: changes in outputs\naggstat (::AbstractMatrix): changes in arguments and outputs","category":"page"},{"location":"news/#*Version-0.0.5*","page":"News","title":"Version 0.0.5","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\nblockscal\npcr\nrp\nrpmatgauss\nrpmat_li","category":"page"},{"location":"news/#*Version-0.0.4*","page":"News","title":"Version 0.0.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\niplsr\nModification of covsel","category":"page"},{"location":"news/#*Version-0.0.3*","page":"News","title":"Version 0.0.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\ninterpl\ncheckdupl, checkmiss","category":"page"},{"location":"news/#*Version-0.0.2*","page":"News","title":"Version 0.0.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"New functions\ncovsel\ninterpl has been replaced by interpl_mon\nChange in output of vi_xgb","category":"page"},{"location":"news/#*Version-0.0.1*","page":"News","title":"Version 0.0.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"First version of the package","category":"page"},{"location":"see_jchemodata/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"see_jchemodata/","page":"Datasets","title":"Datasets","text":"See package JchemoData.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"DocTestSetup  = quote\n    using Jchemo\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Select a category in the left navigation bar.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See also the related projects:","category":"page"},{"location":"","page":"Home","title":"Home","text":"JchemoData.jl: Datasets repository (used in the examples)\nJchemoDemo: Training material","category":"page"},{"location":"","page":"Home","title":"Home","text":"[Return to Jchemo.jl]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [Jchemo]\nOrder   = [:function, :type]","category":"page"}]
}
