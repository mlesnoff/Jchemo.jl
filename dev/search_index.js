var documenterSearchIndex = {"docs":
[{"location":"api/#Jchemo.jl-functions","page":"Index of functions","title":"Jchemo.jl functions","text":"","category":"section"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"Here is a list of all exported functions from Jchemo.jl. ","category":"page"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"For more details, click on the link and you'll be directed to the function help.","category":"page"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"","category":"page"},{"location":"api/","page":"Index of functions","title":"Index of functions","text":"Modules = [Jchemo]\nOrder   = [:function, :type]","category":"page"},{"location":"api/#Base.summary-Tuple{Jchemo.Cca, Union{DataFrames.DataFrame, Vector, Matrix}, Union{DataFrames.DataFrame, Vector, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::Cca, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.CcaWold, Union{DataFrames.DataFrame, Vector, Matrix}, Union{DataFrames.DataFrame, Vector, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::CcaWold, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Comdim, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Comdim, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Fda}","page":"Index of functions","title":"Base.summary","text":"summary(object::Fda, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Kpca}","page":"Index of functions","title":"Base.summary","text":"summary(object::Kpca, X)\n\nSummarize the maximal (i.e. with maximal nb. PCs) fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Mbpca, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Mbpca, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.MbplsWest, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::MbplsWest, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Mbplsr, Any}","page":"Index of functions","title":"Base.summary","text":"summary(object::Mbplsr, Xbl)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nXbl : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Pca, Union{DataFrames.DataFrame, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::Pca, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.PlsCan, Union{DataFrames.DataFrame, Vector, Matrix}, Union{DataFrames.DataFrame, Vector, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::PlsCan, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.PlsTuck, Union{DataFrames.DataFrame, Vector, Matrix}, Union{DataFrames.DataFrame, Vector, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::PlsTuck, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Plsr, Union{DataFrames.DataFrame, Vector, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::Plsr, X)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.summary-Tuple{Jchemo.Rasvd, Union{DataFrames.DataFrame, Vector, Matrix}, Union{DataFrames.DataFrame, Vector, Matrix}}","page":"Index of functions","title":"Base.summary","text":"summary(object::Rasvd, X, Y)\n\nSummarize the fitted model.\n\nobject : The fitted model.\nX : The X-data that was used to fit the model.\nY : The Y-data that was used to fit the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aggstat-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.aggstat","text":"aggstat(X, group; fun = mean)\naggstat(X::DataFrame; vars, groups, fun = mean)\n\nCompute column-wise statistics (e.g. mean), by group in a dataset.\n\nX : Data.\ngroup : A variable defining the groups.\nvars : Names of the variables to summarize.\ngroups : Names of the group variables to consider.\nfun : Function to compute.\n\nVariables defined in vars and groups must be columns of X.\n\nExamples\n\nusing DataFrame, Statistics\n\nn, p = 20, 5\nX = rand(n, p)\ndf = DataFrame(X, :auto)\ngroup = rand(1:3, n)\nres = aggstat(X, group; fun = sum)\nres.X\naggstat(df, group; fun = sum).X\n\nn, p = 20, 5\nX = rand(n, p)\ndf = DataFrame(X, string.(\"v\", 1:p))\ndf.gr1 = rand(1:2, n)\ndf.gr2 = rand([\"a\", \"b\", \"c\"], n)\ndf\naggstat(df; vars = [:v1, :v2], \n    groups = [:gr1, :gr2], fun = mean)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aicplsr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.aicplsr","text":"aicplsr(X, y; nlv, correct = true, bic = false, scal = false)\n\nCompute Akaike's (AIC) and Mallows's (Cp) criteria for univariate PLSR models.\n\nX : X-data.\ny : Univariate Y-data.\nnlv : Nb. latent variables (LVs).\ncorrect : Define if the bias correction is applied.\nbic : Define is BIC is computed instead of AIC.\nscal : Boolean. If true, each column of X and y    is scaled by its uncorrected standard deviation.\n\nX and y are internally centered. \n\nThe function uses function dfplsr_cg. \n\nReferences\n\nHansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697\n\nHansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9\n\nLesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc \n\nnlv = 25\nres = aicplsr(X, y; nlv = nlv) \npnames(res)\nres.crit\nres.opt\nres.delta\n\nzaic = aicplsr(X, y; nlv = nlv).crit.aic\nf, ax = plotgrid(0:nlv, zaic;\n    xlabel = \"Nb. LVs\", ylabel = \"AIC\")\nscatter!(ax, 0:nlv, zaic)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.aov1-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.aov1","text":"aov1(x, Y)\nUnivariate anova test.\n\nx : Univariate categorical X-data.\nY : Y-data.\n\nExamples\n\nn = 100 ; p = 5\nx = rand(1:3, n)\nY = randn(n, p) \n\nres = aov1(x, Y)\npnames(res)\nres.SSF\nres.SSR \nres.F \nres.pval\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.baggr","page":"Index of functions","title":"Jchemo.baggr","text":"baggr(X, Y, weights = nothing, wcol = nothing; rep = 50, \n    rowsamp = .7, colsamp = 1, withr = false, \n    fun, kwargs...)\n\nBagging of regression models.\n\nX : X-data  (n, p).\nY : Y-data  (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nwcol : Weights (p) for the sampling of the variables.\nrep : Nb. of bagging repetitions.\nrowsamp : Proportion of rows sampled in X    at each repetition.\ncolsamp : Proportion of columns sampled (without replacement) in X    at each repetition.\nwithr: Type of sampling of the observations   (true => with replacement).\nfun : Name of the function computing the model to bagg.\nkwargs : Optional named arguments to pass in 'fun`.\n\nReferences\n\nBreiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140.  https://doi.org/10.1007/BF00058655\n\nBreiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324\n\nGenuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI.\n\nGey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nfm = baggr(Xtrain, ytrain; rep = 20, \n    rowsamp = .7, colsamp = .3, fun = mlr) ;\nres = Jchemo.predict(fm, Xtest) ;\nres.pred\nrmsep(ytest, res.pred)\nf, ax = scatter(vec(res.pred), ytest)\nablines!(ax, 0, 1)\nf\n\nres = oob_baggr(fm, Xtrain, ytrain; score = rmsep)\nres.scor\n\nres = vi_baggr(fm, Xtrain, ytrain; score = rmsep)\nres.imp\nlines(vec(res.imp), \n    axis = (xlabel = \"Variable\", ylabel = \"Importance\"))\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.bias-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.bias","text":"bias(pred, Y)\n\nCompute the prediction bias, i.e. the opposite of the mean prediction error.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nbias(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nbias(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.blockscal-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.blockscal","text":"blockscal(Xbl; bscales)\nblockscal_frob(Xbl)\nblockscal_frob(Xbl, weights = ones(nro(X[1]))\nblockscal_mfa(Xbl, weights = ones(nro(X[1]))\nblockscal_ncol(Xbl)\nblockscal_sd(Xbl, weights = ones(nro(X[1]))\n\nScale a list of blocks (matrices).\n\nXbl : List (vector) of blocks (matrices) of X-data. \nweights : Weights of the observations (rows of the blocks). \nbscales : A vector (of length equal to the nb. blocks) of the scalars diving the blocks.\n\nSpecificities of each function:\n\nblockscal: Each block X is tranformed to X / `bscales'.\nblockscal_frob: Let D be the diagonal matrix of vector weights,   standardized to sum to 1. Each block X is divided by its Frobenius norm    = sqrt(tr(X' * D * X)). After this scaling, tr(X' * D * X) = 1.\nblockscal_mfa: Each block X is divided by sqrt(lamda),   where lambda is the dominant eigenvalue of X (this is the \"MFA\" approach).\nblockscal_ncol: Each block X is divided by the nb. columns of the block.\nblockscal_sd: Each block X is divided by sqrt(sum(weighted variances of the block-columns)).   After this scaling, sum(weighted variances of the block-columns) = 1.\n\nThe functions return the scaled blocks and the scaling values.\n\nExamples\n\nn = 5 ; p = 10 \nX = rand(n, p) \nXnew = X[1:3, :]\n\nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl) \n\nXbl[1]\nXbl[2]\nXbl[3]\n\nbscales = ones(3)\nres = blockscal(Xbl, bscales) ;\nres.bscales\nres.X[3]\nXbl[3]\n\nw = ones(n)\n#w = collect(1:n)\nD = Diagonal(mweight(w))\nres = blockscal_frob(Xbl, w) ;\nres.bscales\nk = 3 ; tr(Xbl[k]' * D * Xbl[3])^.5\ntr(res.X[k]' * D * res.X[k])^.5\n\nw = ones(n)\n#w = collect(1:n)\nres = blockscal_mfa(Xbl, w) ;\nres.bscales\nk = 3 ; pcasvd(Xbl[k], w; nlv = 1).sv[1]\n\nres = blockscal_ncol(Xbl) ;\nres.bscales\nres.X[3]\nXbl[3] / size(Xbl[3], 2)\n\nw = ones(n)\n#w = collect(1:n)\nres = blockscal_sd(Xbl, w) ;\nres.bscales\nsum(colvar(res.X[3], w))\n\n# To concatenate the returned blocks\n\nX_concat = reduce(hcat, res.X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.calds-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.calds","text":"calds(Xt, X; fun = mlrpinv, kwargs...)\n\nDirect standardization (DS) for calibration transfer of spectral data.\n\nXt : Target spectra, (n, p).\nX : Spectra to transfer to the target, (n, p).\nfun : Function used for fitting the transfer model.  \nkwargs : Optional arguments for fun.\n\nXt and X must represent the same n standard samples.\n\nThe objective is to transform spectra X to spectra as close  as possible as the target Xt. The principle of the method is to fit models  predicting Xt from `X.\n\nReferences\n\nY. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\") \n@load db dat\npnames(dat)\n\n## Target\nXtcal = dat.X1cal\nXtval = dat.X1val\n## To predict\nXcal = dat.X2cal\nXval = dat.X2val\n\nn = nro(Xtcal)\nm = nro(Xtval)\n\nfm = calds(Xtcal, Xcal; fun = mlrpinv) ;\n#fm = calds(Xtcal, Xcal; fun = pcr, nlv = 15) ;\n#fm = calds(Xtcal, Xcal; fun = plskern, nlv = 15) ;\npred = Jchemo.predict(fm, Xval).pred     # Transfered spectra\n\ni = 1\nf = Figure(resolution = (500, 300))\nax = Axis(f[1, 1])\nlines!(Xtval[i, :]; label = \"xt\")\nlines!(ax, Xval[i, :]; label = \"x\")\nlines!(pred[i, :]; linestyle = \"--\", label = \"x_transf\")\naxislegend(position = :rb, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.calpds-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.calpds","text":"calpds(Xt, X; fun = mlrpinv, m = 5, kwargs...)\n\nPiecewise direct standardization (PDS) for calibration transfer of spectral data.\n\nXt : Target spectra, (n, p).\nX : Spectra to transfer to the target, (n, p).\nfun : Function used for fitting the transfer model.  \nm : Half-window size (nb. points left/right to the target wavelength) \nkwargs : Optional arguments for fun.\n\nXt and X must represent the same n standard samples.\n\nThe objective is to transform spectra X to spectra as close  as possible as the target Xt. The principle of the method is to fit models  predicting Xt from `X.\n\nTo predict wavelength i in Xt, the window used in X is :\n\ni - m, i - m + 1, ..., i, ..., i + m - 1, i + m\n\nReferences\n\nBouveresse, E., Massart, D.L., 1996. Improvement of the piecewise direct targetisation procedure  for the transfer of NIR spectra for multivariate calibration. Chemometrics and Intelligent Laboratory  Systems 32, 201–213. https://doi.org/10.1016/0169-7439(95)00074-7\n\nY. Wang, D. J. Veltkamp, and B. R. Kowalski, “Multivariate Instrument Standardization,”  Anal. Chem., vol. 63, no. 23, pp. 2750–2756, 1991, doi: 10.1021/ac00023a016.\n\nWülfert, F., Kok, W.Th., Noord, O.E. de, Smilde, A.K., 2000. Correction of Temperature-Induced  Spectral Variation by Continuous Piecewise Direct Standardization. Anal. Chem. 72, 1639–1644. https://doi.org/10.1021/ac9906835\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\") \n@load db dat\npnames(dat)\n\n## Target\nXtcal = dat.X1cal\nXtval = dat.X1val\n## To predict\nXcal = dat.X2cal\nXval = dat.X2val\n\nn = nro(Xtcal)\nm = nro(Xtval)\n\nfm = calpds(Xtcal, Xcal; fun = plskern, nlv = 1, m = 2) ;\npred = Jchemo.predict(fm, Xval).pred     # Transfered spectra\n\ni = 1\nf = Figure(resolution = (500, 300))\nax = Axis(f[1, 1])\nlines!(Xtval[i, :]; label = \"xt\")\nlines!(ax, Xval[i, :]; label = \"x\")\nlines!(pred[i, :]; linestyle = \"--\", label = \"x_transf\")\naxislegend(position = :rb, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cca","page":"Index of functions","title":"Jchemo.cca","text":"cca(X, Y, weights = ones(nro(X)); nlv, \n    bscal = \"none\", tau = 1e-8, scal = false)\ncca!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    bscal = \"none\", tau = 1e-8, scal = false)\n\nCanonical correlation Analysis (CCA).\n\nX : First block (matrix) of data.\nY : Second block (matrix) of data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThis function implements a CCA algorithm using SVD decompositions and  presented in Weenink 2003 section 2. \n\nA continuum regularization is available.  After block centering and scaling, the returned block scores (Tx and Ty)  are proportionnal to the eigenvectors of Projx * Projy  and Projy * Projx, respectively, defined as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\nCy = (1 - tau) * Y'DY + tau * Iy\nCxy = X'DY \nProjx = sqrt(D) * X * invCx * X' * sqrt(D)\nProjy = sqrt(D) * Y * invCx * Y' * sqrt(D)\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. tau = 1e-8)  to get similar results as with pseudo-inverses.  \n\nWith uniform weights, the normed scores returned  by the function are expected to be the same as those returned  by functions rcc of the R packages CCA (González et al.) and mixOmics  (Le Cao et al.) whith the parameters lambda1 and lambda2 set to:\n\nlambda1 = lambda2 = tau / (1 - tau) * n / (n - 1) \n\nReferences\n\nGonzález, I., Déjean, S., Martin, P.G.P., Baccini, A., 2008. CCA:  An R Package to Extend Canonical Correlation Analysis. Journal of Statistical  Software 23, 1-14. https://doi.org/10.18637/jss.v023.i12\n\nHotelling, H. (1936): “Relations between two sets of variates”, Biometrika 28: pp. 321–377.\n\nLe Cao, K.-A., Rohart, F., Gonzalez, I., Dejean, S., Abadi, A.J., Gautier, B., Bartolo, F.,  Monget, P., Coquery, J., Yao, F., Liquet, B., 2022. mixOmics: Omics Data Integration Project.  https://doi.org/10.18129/B9.bioc.mixOmics\n\nWeenink, D. 2003. Canonical Correlation Analysis, Institute of Phonetic Sciences,  Univ. of Amsterdam, Proceedings 25, 81-99.\n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n\ntau = 1e-8\nfm = cca(X, Y; nlv = 3, tau = tau)\npnames(fm)\n\nfm.Tx\ntransform(fm, X, Y).Tx\nscale(fm.Tx, colnorm(fm.Tx))\n\nres = summary(fm, X, Y)\npnames(res)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.ccawold","page":"Index of functions","title":"Jchemo.ccawold","text":"ccawold(X, Y, weights = ones(nro(X)); nlv,\n    bscal = \"none\", tau = 1e-8, \n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\nccawold!(X, Y, weights = ones(nro(X)); nlv,\n    bscal = \"none\", tau = 1e-8, \n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\n\nCanonical correlation analysis (RCCA) - Wold Nipals algorithm.\n\nX : First block (matrix) of data.\nY : Second block (matrix) of data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\ntol : Tolerance for the Nipals algorithm.\nmaxit : Maximum number of iterations for the Nipals algorithm.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThis function implements the Nipals CCA algorithm presented  by Tenenhaus 1998 p.204 (related to Wold et al. 1984). \n\nIn this implementation, after each step of LVs computation, X and Y are deflated  relatively to their respective scores (tx and ty). \n\nA continuum regularization is available.  After block centering and scaling, the covariances matrices are computed as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\nCy = (1 - tau) * Y'DY + tau * Iy\n\nwhere D is the observation (row) metric.   Value tau = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. tau = 1e-8)  to get similar results as with pseudo-inverses.    \n\nWith uniform weights, the normed scores returned  by the function are expected to be the same as those returned  by functions rgcca of the R package RGCCA (Tenenhaus & Guillemot 2017,  Tenenhaus et al. 2017). \n\nReferences\n\nTenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and Sparse Generalized Canonical  Correlation Analysis for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html \n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.\n\nTenenhaus, M., Tenenhaus, A., Groenen, P.J.F., 2017.  Regularized Generalized Canonical Correlation Analysis: A Framework for Sequential  Multiblock Component Methods. Psychometrika 82, 737–777.  https://doi.org/10.1007/s11336-017-9573-x\n\nWold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear  Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052\n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n\ntau = 1e-8\nfm = ccawold(X, Y; nlv = 3, tau = tau)\npnames(fm)\n\nfm.Tx\ntransform(fm, X, Y).Tx\nscale(fm.Tx, colnorm(fm.Tx))\n\nres = summary(fm, X, Y)\npnames(res)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.center-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.center","text":"center(X, v)\ncenter!(X, v)\n\nCenter each column of X.\n\nX : Data.\nv : Centering factors.\n\nexamples\n\nn, p = 5, 6\nX = rand(n, p)\nxmeans = colmean(X)\ncenter(X, xmeans)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cglsr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.cglsr","text":"cglsr(X, y; nlv, reorth = true, filt = false, scal = false)\ncglsr!(X::Matrix, y::Matrix; nlv, reorth = true, filt = false, scal = false)\n\nConjugate gradient algorithm for the normal equations (CGLS; Björck 1996).\n\nX : X-data  (n, p).\ny : Univariate Y-data (n).\nnlv : Nb. CG iterations.\nreorth : If true, a Gram-Schmidt reorthogonalization of the normal equation    residual vectors is done.\nfilt : Logical indicating if the CG filter factors are computed (output F).\nscal : Boolean. If true, each column of X and y    is scaled by its uncorrected standard deviation.\n\nX and y are internally centered. \n\nCGLS algorithm \"7.4.1\" Bjorck 1996, p.289. The part of the code computing the  re-orthogonalization (Hansen 1998) and filter factors (Vogel 1987, Hansen 1998)  is a transcription (with few adaptations) of the Matlab function cgls  (Saunders et al. https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).\n\nReferences\n\nBjörck, A., 1996. Numerical Methods for Least Squares Problems, Other Titles in Applied Mathematics.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9781611971484\n\nHansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697\n\nHansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9\n\nManne R. Analysis of two partial-least-squares algorithms for multivariate calibration. Chemometrics Intell. Lab. Syst. 1987; 2: 187–197.\n\nPhatak A, De Hoog F. Exploiting the connection between PLS, Lanczos methods and conjugate gradients: alternative proofs of some properties of PLS. J. Chemometrics 2002; 16: 361–367.\n\nVogel, C. R.,  \"Solving ill-conditioned linear systems using the conjugate gradient method\",  Report, Dept. of Mathematical Sciences, Montana State University, 1987.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 12 ;\nfm = cglsr(Xtrain, ytrain; nlv = nlv) ;\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.B\nJchemo.coef(fm; nlv = 7).B\n\nres = Jchemo.predict(fm, Xtest) ;\nres.pred\nrmsep(ytest, res.pred)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.checkdupl-Tuple{Any}","page":"Index of functions","title":"Jchemo.checkdupl","text":"checkdupl(X; digits = 3)\n\nFind replicated rows in a dataset.\n\nX : A dataset.\ndigits : Nb. digits used to round X before checking.\n\nExamples\n\nX = rand(5, 3)\nZ = vcat(X, X[1:3, :], X[1:1, :])\ncheckdupl(X)\ncheckdupl(Z)\n\nM = hcat(X, fill(missing, 5))\nZ = vcat(M, M[1:3, :])\ncheckdupl(M)\ncheckdupl(Z)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.checkmiss-Tuple{Any}","page":"Index of functions","title":"Jchemo.checkmiss","text":"checkmiss(X)\n\nFind rows with missing data in a dataset.\n\nX : A dataset.\n\nExamples\n\nX = rand(5, 4)\nzX = hcat(rand(2, 3), fill(missing, 2))\nZ = vcat(X, zX)\ncheckmiss(X)\ncheckmiss(Z)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Cglsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Cglsr)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Dkplsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Dkplsr; nlv = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Kplsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Kplsr; nlv = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Krr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Krr; lb = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nlb : A value of the regularization parameter \"lambda\".   If nothing, it is the parameter stored in the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Mlr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Mlr)\n\nCompute the coefficients of the fitted model.\n\nobject : The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Rosaplsr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Rosaplsr; nlv = nothing)\n\nCompute the X b-coefficients of a model fitted with nlv LVs.\n\nobject : The fitted model.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Jchemo.Rr}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Rr; lb = nothing)\n\nCompute the b-coefficients of a fitted model.\n\nobject : The fitted model.\nlb : A value of the regularization parameter \"lambda\".   If nothing, it is the parameter stored in the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.coef-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}","page":"Index of functions","title":"Jchemo.coef","text":"coef(object::Union{Plsr, Pcr}; nlv = nothing)\n\nCompute the X b-coefficients of a model fitted with nlv LVs.\n\nobject : The fitted model.\nnlv : Nb. LVs to consider.\n\nIf X is (n, p) and Y is (n, q), the returned object B is a matrix (p, q).  If nlv = 0, B is a matrix of zeros. The returned object int is the intercept.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colmad-Tuple{Any}","page":"Index of functions","title":"Jchemo.colmad","text":"colmad(X)\n\nCompute the median absolute deviation (MAD) of each column of X.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\n\ncolmad(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colmean-Tuple{Any}","page":"Index of functions","title":"Jchemo.colmean","text":"colmean(X)\ncolmean(X, w)\n\nCompute the mean of each column of X.\n\nX : Data (n, p).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nw = collect(1:n)\n\ncolmean(X)\ncolmean(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colnorm-Tuple{Any}","page":"Index of functions","title":"Jchemo.colnorm","text":"colnorm(X)\ncolnorm(X, w)\n\nCompute the norm of each column of a dataset X.\n\nX : Data (n, p).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nThe norm of a column x of X is:\n\nsqrt(x' * x), where D is the diagonal matrix of w.\n\nThe weighted norm is:\n\nsqrt(x' * D * x), where D is the diagonal matrix of w.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nw = collect(1:n)\n\ncolnorm(X)\ncolnorm(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colstd-Tuple{Any}","page":"Index of functions","title":"Jchemo.colstd","text":"colstd(X)\ncolstd(X, w)\n\nCompute the (uncorrected) standard deviation of each column of X.\n\nX : Data (n, p).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nw = collect(1:n)\n\ncolstd(X)\ncolstd(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colsum-Tuple{Any}","page":"Index of functions","title":"Jchemo.colsum","text":"colsum(X)\ncolsum(X, w)\n\nCompute the sum of each column of X.\n\nX : Data (n, p).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nw = collect(1:n)\n\ncolsum(X)\ncolsum(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.colvar-Tuple{Any}","page":"Index of functions","title":"Jchemo.colvar","text":"colvar(X)\ncolvar(X, w)\n\nCompute the (uncorrected) variance of each column of X.\n\nX : Data (n, p).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nw = collect(1:n)\n\ncolvar(X)\ncolvar(X, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.comdim","page":"Index of functions","title":"Jchemo.comdim","text":"comdim(Xbl, weights = ones(nro(Xbl[1])); nlv,\n    bscal = \"none\", tol = sqrt(eps(1.)), maxit = 200,\n    scal = false)\ncomdim!(Xbl, weights = ones(nro(Xbl[1])); nlv,\n    bscal = \"none\", tol = sqrt(eps(1.)), maxit = 200,\n    scal = false)\n\nCommon components and specific weights analysis (ComDim = CCSWA).\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\ntol : Tolerance value for convergence.\nmaxit : Maximum number of iterations.\nscal : Boolean. If true, each column of blocks in Xbl    is scaled by its uncorrected standard deviation    (before the block scaling).\n\n\"SVD\" algorithm of Hannafi & Qannari 2008 p.84.\n\nThe function returns several objects, in particular:\n\nT : The non normed global scores.\nU : The normed global scores.\nW : The global loadings.\nTbl : The block scores (grouped by blocks, in the original scale).\nTb : The block scores (grouped by LV, in the metric scale).\nWbl : The block loadings.\nlb : The specific weights (saliences) \"lambda\".\nmu : The sum of the squared saliences.\n\nFunction summary returns: \n\nexplvarx : Proportion of the X total inertia (sum of the squared norms of the    blocks) explained by each global score.\nexplvarxx : Proportion of the XX' total inertia (sum of the squared norms of the   products Xk * Xk') explained by each global score    (= indicator \"V\" in Qannari et al. 2000, Hanafi et al. 2008).\nsal2 : Proportion of the squared saliences (specific weights)   of each block within each global score. \ncontr_block : Contribution of each block to the global scores    (= proportions of the saliences \"lambda\" within each score)\nexplX : Proportion of the inertia of the blocks explained by each global score.\ncorx2t : Correlation between the global scores and the original variables.  \ncortb2t : Correlation between the global scores and the block scores.\nrv : RV coefficient. \nlg : Lg coefficient. \n\nReferences\n\nCariou, V., Qannari, E.M., Rutledge, D.N., Vigneau, E., 2018. ComDim: From multiblock data  analysis to path modeling. Food Quality and Preference, Sensometrics 2016:  Sensometrics-by-the-Sea 67, 27–34. https://doi.org/10.1016/j.foodqual.2017.02.012\n\nCariou, V., Jouan-Rimbaud Bouveresse, D., Qannari, E.M., Rutledge, D.N., 2019.  Chapter 7 - ComDim Methods for the Analysis of Multiblock Data in a Data Fusion  Perspective, in: Cocchi, M. (Ed.), Data Handling in Science and Technology,  Data Fusion Methodology and Applications. Elsevier, pp. 179–204.  https://doi.org/10.1016/B978-0-444-63984-4.00007-7\n\nGhaziri, A.E., Cariou, V., Rutledge, D.N., Qannari, E.M., 2016. Analysis of multiblock  datasets using ComDim: Overview and extension to the analysis of (K + 1) datasets.  Journal of Chemometrics 30, 420–429. https://doi.org/10.1002/cem.2810\n\nHanafi, M., 2008. Nouvelles propriétés de l’analyse en composantes communes et  poids spécifiques. Journal de la société française de statistique 149, 75–97.\n\nQannari, E.M., Wakeling, I., Courcoux, P., MacFie, H.J.H., 2000. Defining the underlying  sensory dimensions. Food Quality and Preference 11, 151–154.  https://doi.org/10.1016/S0950-3293(99)00069-5\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ham.jld2\") \n@load db dat\npnames(dat) \n\nX = dat.X\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X, listbl)\n# \"New\" = first two rows of Xbl \nXbl_new = mblock(X[1:2, :], listbl)\n\nbscal = \"none\"\n#bscal = \"frob\"\nfm = comdim(Xbl; nlv = 4, bscal = bscal) ;\nfm.U\nfm.T\nJchemo.transform(fm, Xbl)\nJchemo.transform(fm, Xbl_new) \n\nres = summary(fm, Xbl) ;\nfm.lb\nrowsum(fm.lb)\nfm.mu\nres.explvarx\nres.explvarxx\nres.explX # = fm.lb if bscal = \"frob\"\nrowsum(Matrix(res.explX))\nres.contr_block\nres.sal2\ncolsum(Matrix(res.sal2))\nres.corx2t \nres.cortb2t\nres.rv\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.confusion-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.confusion","text":"confusion(pred, y)\n\nConfusion matrix.\n\npred : Univariate predictions.\ny : Univariate observed data.\ndigits : Nb. digits used to round percentages.\n\nExamples\n\ny = [\"d\"; \"c\"; \"b\"; \"c\"; \"a\"; \"d\"; \"b\"; \"d\"; \n    \"b\"; \"b\"; \"a\"; \"a\"; \"c\"; \"d\"; \"d\"]\npred = [\"a\"; \"d\"; \"b\"; \"d\"; \"b\"; \"d\"; \"b\"; \"d\"; \n    \"b\"; \"b\"; \"a\"; \"a\"; \"d\"; \"d\"; \"d\"]\n#y = rand(1:10, 200); pred = rand(1:10, 200)\n\nres = confusion(pred, y) ;\npnames(res)\nres.cnt       # Counts (dataframe built from `A`) \nres.pct       # Row %  (dataframe built from `Apct`))\nres.A         \nres.Apct\nres.accuracy  # Overall accuracy (% classification successes)\nres.lev       # Levels\n\nplotconf(res).f\n\nplotconf(res; pct = true, ptext = false).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cor2-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.cor2","text":"cor2(pred, Y)\n\nCompute the squared linear correlation between data and predictions.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\ncor2(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\ncor2(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.corm-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.corm","text":"corm(X, w)\ncorm(X, Y, w)\n\nCompute correlation matrices.\n\nX : Data (n, p).\nY : Data (n, q).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nUncorrected correlation matrix \n\nof the columns of X: ==> (p, p) matrix \nor between columns of X and Y : ==> (p, q) matrix.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nY = rand(n, 3)\nw = collect(1:n)\n\ncorm(X, w)\ncorm(X, Y, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cosm-Tuple{Any}","page":"Index of functions","title":"Jchemo.cosm","text":"cosm(X)\n\nCosinus between the columns of a matrix.\n\nX : Data (n, p).\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\n\ncosm(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cosv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.cosv","text":"cosv(X)\n\nCosinus between two vectors.\n\nx : vector (n).\ny : vector (n).\n\nExamples\n\nn = 5\nx = rand(n)\ny = rand(n)\n\ncosv(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.covm-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.covm","text":"covm(X, w)\ncovm(X, Y, w)\n\nCompute covariance matrices.\n\nX : Data (n, p).\nY : Data (n, q).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nUncorrected covariance matrix \n\nof the columns of X: ==> (p, p) matrix \nor between columns of X and Y : ==> (p, q) matrix.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nY = rand(n, 3)\nw = collect(1:n)\n\ncovm(X, w)\ncovm(X, Y, w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.covsel-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.covsel","text":"covsel(X, Y; nlv = nothing, typ = \"cov\")\ncovsel!(X::Matrix, Y::Matrix; nlv = nothing, typ = \"cov\")\n\nVariable (feature) selection from partial correlation or covariance (Covsel).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlv : Nb. variables to select.\ntyp : Criterion used at each variable selection.    Possible values are: \"cov\" (squared covariance with Y, such as in Roger    et al. 2011) and \"cor\" (squared correlation with Y).\n\nThe selection is sequential. Once a variable is selected,  X and Y are orthogonolized to this variable,  and a new variable (the one showing the maximum value for the criterion) is selected.\n\nif Y is multivariate (q > 1), each column of Y is scaled by its  uncorrected stnandard deviation. This gives the same scales to the columns when computing the selection criterion.\n\nReferences\n\nHöskuldsson, A., 1992. The H-principle in modelling with applications  to chemometrics. Chemometrics and Intelligent Laboratory Systems,  Proceedings of the 2nd Scandinavian Symposium on Chemometrics 14,  139–153. https://doi.org/10.1016/0169-7439(92)80099-P\n\nRoger, J.M., Palagos, B., Bertrand, D., Fernandez-Ahumada, E., 2011.  covsel: Variable selection for highly multivariate and multi-response  calibration: Application to IR spectroscopy.  Chem. Lab. Int. Syst. 106, 216-223.\n\nWikipedia https://en.wikipedia.org/wiki/Partial_correlation\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\ny = dat.Y.tbc\n\nres = covsel(X, y; nlv = 10) ;\nres.sel\nres.cov2\n\nscatter(sqrt.(res.cov2), \n    axis = (xlabel = \"Variable\", ylabel = \"Importance\"))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.covselr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.covselr","text":"covselr(X, Y; nlv = nothing, typ = \"cov\")\n\nMLR on variables selected from partial correlation or covariance (Covsel).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlv : Nb. variables to select.\ntyp : Criterion used at each selection in Covsel (See ?covsel.). \n\nA number of nlv variables (X-columns) are selected with the Covsel method function covsel), and then a MLR is implemened on these variables. \n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n  \nnlv = 15\nfm = covselr(Xtrain, ytrain; nlv = nlv) ;\npnames(fm)\nJchemo.coef(fm.fm)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.cplsravg","page":"Index of functions","title":"Jchemo.cplsravg","text":"cplsravg(X, Y, cla = nothing; ncla = nothing, \n    typda = \"lda\", nlv_da, nlv, scal = false)\n\nClusterwise PLSR.\n\nX : X-data (n, p).\nY : Y-data (n, q).\ncla : A vector (n) defining the class membership (clusters). If cla = nothing,    a random k-means clustering is done internally and returns ncla clusters.\nncla : Only used if cla = nothing.    Number of clusters that has to be returned by the k-means clustering.\ntypda : Type of PLSDA. Possible values are \"lda\" (PLS-LDA; default) or \"qda\" (PLS-QDA).\nnlv_da : Nb. latent variables (LVs) for the PLSDA.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider in the PLSR-AVG models (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to   the single model with 10 LVs).\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\n\nA PLSR-AVG model (see ?plsravg) is fitted to predict Y for each of the clusters,  and a PLS-LDA is fitted to predict, for each cluster, the probability to belong to this cluster. The final prediction is the weighted average of the PLSR-AVG predictions, where the  weights are the probabilities predicted by the PLS-LDA model. \n\nReferences\n\nPreda, C., Saporta, G., 2005. Clusterwise PLS regression on a stochastic process.  Computational Statistics & Data Analysis 49, 99–108. https://doi.org/10.1016/j.csda.2004.05.002\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nncla = 5 ; nlv_da = 15 ; nlv = \"10:12\"\nfm = cplsravg(Xtrain, ytrain; \n    ncla = ncla, nlv_da = nlv_da, nlv = nlv) ;\npnames(fm)\nfm.lev\nfm.ni\n\nres = Jchemo.predict(fm, Xtest) \nres.posterior\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f  \n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.cscale-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.cscale","text":"cscale(X, u, v)\ncscale!(X, u, v)\n\nCenter and scale each column of X.\n\nX : Data.\nu : Centering factors.\nv : Scaling factors.\n\nexamples\n\nn, p = 5, 6\nX = rand(n, p)\nxmeans = colmean(X)\nxstds = colstd(X)\ncscale(X, xmeans, xstds)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.detrend-Tuple{Any}","page":"Index of functions","title":"Jchemo.detrend","text":"detrend(X; pol = 1)\ndetrend!(X::Matrix; pol = 1)\n\nDe-trend transformation of each row of a matrix X. \n\nX : X-data.\npol : Polynom order.\n\nThe function fits a polynomial regression to each observation and returns the residuals.\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nXp = detrend(X)\nplotsp(Xp[1:30, :], wl_num).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dfplsr_cg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.dfplsr_cg","text":"dfplsr_cg(X, y; nlv, reorth = true, scal = false)\n\nCompute the model complexity (df) of PLSR models with the CGLS algorithm.\n\nX : X-data.\ny : Univariate Y-data.\nnlv : Nb. latent variables (LVs).\nreorth : If true, a Gram-Schmidt reorthogonalization of the normal equation    residual vectors is done.\nscal : Boolean. If true, each column of X and y    is scaled by its uncorrected standard deviation.\n\nThe number of degrees of freedom (df) of the model is returned for 0, 1, ..., nlv LVs.\n\nReferences\n\nHansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation.  Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697\n\nHansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3.  Numer Algor 46, 189–194. https://doi.org/10.1007/s11075-007-9136-9\n\nLesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369\n\nExamples\n\n# The example below reproduces the numerical illustration\n# given by Kramer & Sugiyama 2011 on the Ozone data (Fig. 1, center).\n# Function \"pls.model\" used for df calculations\n# in the R package \"plsdof\" v0.2-9 (Kramer & Braun 2019)\n# automatically scales the X matrix before PLS.\n# The example scales X for consistency with plsdof.\n\nusing JchemoData, JLD2, DataFrames, CairoMakie \npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ozone.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\ndropmissing!(X) \nzX = rmcol(Matrix(X), 4) ;\ny = X[:, 4] \n\n# For consistency with plsdof\nxstds = colstd(zX)\nzXs = scale(zX, xstds)\n# End\n\nnlv = 12 \ndf = dfplsr_cg(zXs, y; nlv = nlv, reorth = true) \ndf_kramer = [1.000000, 3.712373, 6.456417, 11.633565, 12.156760, 11.715101, 12.349716,\n    12.192682, 13.000000, 13.000000, 13.000000, 13.000000, 13.000000]\nf, ax = plotgrid(0:nlv, df_kramer; step = 2,\n    xlabel = \"Nb. LVs\", ylabel = \"df\")\nscatter!(ax, 0:nlv, df.df; color = \"red\")\nablines!(ax, 1, 1; color = :grey, linestyle = :dot)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.dkplsr","page":"Index of functions","title":"Jchemo.dkplsr","text":"dkplsr(X, Y, weights = ones(nro(X)); nlv, \n    kern = \"krbf\", scal = false, kwargs...)\ndkplsr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv, \n    kern = \"krbf\", scal = scal, kwargs...)\n\nDirect kernel partial least squares regression (DKPLSR) (Bennett & Embrechts 2003).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to consider. \n'kern' : Type of kernel used to compute the Gram matrices.   Possible values are \"krbf\" of \"kpol\" (see respective functions krbf and kpol).\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\nkwargs : Named arguments to pass in the kernel function.\n\nThe method builds kernel Gram matrices and then runs a usual PLSR algorithm on them.  This is faster (but not equivalent) to the \"true\" Nipals KPLSR algorithm described  in Rosipal & Trejo (2001).\n\nReferences\n\nBennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial  least squares regression, in: Advances in Learning Theory: Methods, Models and Applications,  NATO Science Series III: Computer & Systems Sciences. IOS Press Amsterdam, pp. 227-250.\n\nRosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in  Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 20 ; gamma = 1e-1\nfm = dkplsr(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;\nfm.fm.T\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.B\nJchemo.coef(fm; nlv = 7).B\n\nJchemo.transform(fm, Xtest)\nJchemo.transform(fm, Xtest; nlv = 7)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\n\nres = Jchemo.predict(fm, Xtest; nlv = 1:2)\nres.pred[1]\nres.pred[2]\n\nfm = dkplsr(Xtrain, ytrain; nlv = nlv, kern = \"kpol\", degree = 2, gamma = 1e-1, coef0 = 10) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\n# Example of fitting the function sinc(x)\n# described in Rosipal & Trejo 2001 p. 105-106 \n\nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nfm = dkplsr(x, y; nlv = 2) ;\npred = Jchemo.predict(fm, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.dkplsrda","page":"Index of functions","title":"Jchemo.dkplsrda","text":"dkplsrda(X, y, weights = ones(nro(X)); nlv, kern = \"krbf\", \n    scal = false, kwargs...)\n\nDiscrimination based on direct kernel partial least squares regression (DKPLSR-DA).\n\nX : X-data.\ny : Univariate class membership.\nweights : Weights of the observations. Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nOther arguments to pass in the kernel: See ?kplsr.\n\nThis is the same approach as for plsrda except that the PLS2 step  is replaced by a non linear direct kernel PLS2 (DKPLS).\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\ngamma = .001 \nnlv = 15\nfm = dkplsrda(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;\npnames(fm)\ntypeof(fm.fm) # = KPLS2 model\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.posterior\nres.pred\nerr(res.pred, ytest)\n\nJchemo.transform(fm, Xtest; nlv = 2)\n\nJchemo.transform(fm.fm, Xtest)\nJchemo.coef(fm.fm)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.dmnorm","page":"Index of functions","title":"Jchemo.dmnorm","text":"dmnorm(X = nothing; mu = nothing, S = nothing)\n\nCompute normal probability density for multivariate data.\n\nX : X-data used to estimate the mean and    the covariance matrix of the population.    If nothing, mu and S must be provided.\nmu : Mean vector of the normal distribution.    If nothing, mu is computed by the column-means of X.\nS : Covariance matrix of the normal distribution.   If nothing, S is computed by cov(X).\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nsumm(X)\ntab(dat.X.species)\n\n## Studying of the Sepal two-dimensionnal \n## distribution of the class \"Setosa\"\nXtrain = Matrix(X[1:40, 1:2])\nXtest = Matrix(X[41:50, 1:2])\n\nfm = dmnorm(Xtrain) ;\nfm.mu\nfm.Uinv\nfm.detS\nJchemo.predict(fm, Xtest).pred    # densities\n\nmu = colmean(Xtrain)\nS = cov(Xtrain)\nfm = dmnorm(; mu = mu, S = S)\nfm.Uinv\nfm.detS\n\nk = 50\nx = Xtrain[:, 1]\ny = Xtrain[:, 2]\nx1 = range(.9 * minimum(x), 1.1 * maximum(x); length = k) \nx2 = range(.9 * minimum(y), 1.1 * maximum(y); length = k) \ngrid = reduce(hcat, mpar(x1 = x1, x2 = x2))\npred_grid = Jchemo.predict(fm, grid).pred    # densities\npred = Jchemo.predict(fm, Xtest).pred        # densities\nf = Figure(resolution = (600, 400))\nax = Axis(f[1, 1]; title = \"Dmnorm - Setosa\",\n    xlabel = \"Sepal length\", ylabel = \"Sepal width\") \nco = contour!(ax, grid[:, 1], grid[:, 2], vec(pred_grid); levels = 10)\nColorbar(f[1, 2], co; label = \"Density\")\n## Or:\n#contour!(ax, grid[:, 1], grid[:, 2], vec(pred_grid))\nscatter!(ax, Xtest[:, 1], Xtest[:, 2], vec(pred),\n    color = :red)\n#xlims!(ax, 2, 6) ;ylims!(ax, 2, 6)\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.dummy-Tuple{Any}","page":"Index of functions","title":"Jchemo.dummy","text":"dummy(y)\n\nBuild a table of dummy variables from a categorical variable.\n\ny : A categorical variable.\n\nExamples\n\ny = [\"d\", \"a\", \"b\", \"c\", \"b\", \"c\"]\n#y =  rand(1:3, 7)\nres = dummy(y)\npnames(res)\nres.Y\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ensure_df-Tuple{DataFrames.DataFrame}","page":"Index of functions","title":"Jchemo.ensure_df","text":"ensure_df(X)\n\nReshape X to a dataframe if necessary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ensure_mat-Tuple{AbstractMatrix}","page":"Index of functions","title":"Jchemo.ensure_mat","text":"ensure_mat(X)\n\nReshape X to a matrix if necessary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.eposvd-Tuple{Any}","page":"Index of functions","title":"Jchemo.eposvd","text":"eposvd(D; nlv)\n\nOrthogonalization for calibration transfer of spectral data.\n\nD : Data (m, p) containing the \"detrimental\" information on which the spectra   have to be orthogonalized.\nnlv : Nb. of first loadings vectors of D considered for the orthogonalization.\n\nThe general objective is to remove from a dataset X (n, p) some detrimental  information (e.g. humidity patterns in signals, multiple spectrometers, etc.)  defined by main directions contained in a dataset D (m, p).  The principle of the method is to orthogonalize the rows of X (observations)  to the detrimental sub-space defined by the first nlv  loadings vectors computed from a (non-centered) PCA of D.\n\nMatrix D can be built from different choices. Two common methods are:\n\nEPO (Roger et al. 2003, 2018): D is built from differences between spectra   collected from the different conditions. \nTOP (Andrew & Fearn 2004): Each row of D is the mean spectrum for an instrument.\n\nFunction eposvd makes a SVD factorization of D and returns  two matrices:\n\nM (p, p) : The orthogonalization matrix that can be used to correct any X-data.\nP (p, nlv) : The matrix of the loading vectors of D. \n\nAny X-data can be corrected from the detrimental information D by:\n\nX_corrected = X * M.\n\nA particular situation is the following. Assume that D was built from  differences between X1 and X2, and that a bilinear model (e.g. PLSR) has  been fitted on X1_corrected. For future predictions X2new, there is no need  to correct X2new.\n\nReferences\n\nAndrew, A., Fearn, T., 2004. Transfer by orthogonal projection: making near-infrared  calibrations robust to between-instrument variation. Chemometrics and Intelligent  Laboratory Systems 72, 51–56. https://doi.org/10.1016/j.chemolab.2004.02.004\n\nRoger, J.-M., Chauchard, F., Bellon-Maurel, V., 2003. EPO-PLS external parameter  orthogonalisation of PLS application to temperature-independent measurement  of sugar content of intact fruits.  Chemometrics and Intelligent Laboratory Systems 66, 191-204.  https://doi.org/10.1016/S0169-7439(03)00051-0\n\nRoger, J.-M., Boulet, J.-C., 2018. A review of orthogonal projections for calibration.  Journal of Chemometrics 32, e3045. https://doi.org/10.1002/cem.3045\n\nZeaiter, M., Roger, J.M., Bellon-Maurel, V., 2006. Dynamic orthogonal projection.  A new method to maintain the on-line robustness of multivariate calibrations.  Application to NIR-based monitoring of wine fermentations. Chemometrics and Intelligent  Laboratory Systems, 80, 227–235. https://doi.org/10.1016/j.chemolab.2005.06.011\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/caltransfer.jld2\") \n@load db dat\npnames(dat)\nX1cal = dat.X1cal\nX2cal = dat.X2cal\nX1val = dat.X1val\nX2val = dat.X2val\n\nD = X1cal - X2cal\nnlv = 2\nres = eposvd(D; nlv = nlv)\nres.M      # orthogonalization matrix\nres.P      # detrimental directions (columns of matrix P = loadings of D)\n\n## Corrected matrices\nzX1 = X1val * res.M    \nzX2 = X2val * res.M    \n\ni = 1\nf = Figure(resolution = (500, 300))\nax = Axis(f[1, 1])\nlines!(zX1[i, :]; label = \"x1_correct\")\nlines!(ax, zX2[i, :]; label = \"x2_correct\")\naxislegend(position = :cb, framevisible = false)\nf\n\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.err-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.err","text":"err(pred, y)\n\nCompute the classification error rate (ERR).\n\npred : Predictions.\ny : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nytrain = rand([\"a\" ; \"b\"], 10)\nXtest = rand(4, 5) \nytest = rand([\"a\" ; \"b\"], 4)\n\nfm = plsrda(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nerr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.euclsq-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.euclsq","text":"euclsq(X, Y)\n\nSquared Euclidean distances  between the rows of X and Y.\n\nX : Data.\nY : Data.\n\nFor X(n, p) and Y (m, p), the function returns an object (n, m) with:\n\ni, j = distance between row i of X and row j of Y.\n\nExamples\n\nX = rand(5, 3)\nY = rand(2, 3)\n\neuclsq(X, Y)\n\neuclsq(X[1:1, :], Y[1:1, :])\n\neuclsq(X[:, 1], 4)\neuclsq(1, 4)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.fda","text":"fda(X, y; nlv, pseudo = false, scal = false)\nfda!(X::Matrix, y; nlv, pseudo = false, scal = false)\n\nFactorial discriminant analysis (FDA).\n\nX : X-data (n, p).\ny : y-data (n) (class membership).\nnlv : Nb. discriminant components.\npseudo : If true, a MP pseudo-inverse is used (instead   of a usual inverse) for inverting W.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nEigen factorization of Inverse(W) * B. \n\nThe functions maximize the compromise p'Bp / p'Wp, i.e. max p'Bp with  constraint p'Wp = 1. Vectors p (columns of P) are the linear discrimant  coefficients \"LD\".\n\nExamples\n\nusing Jchemo, JLD2, StatsBase, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\n\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\ntab(ytrain)\ntab(ytest)\n\nfm = fda(Xtrain, ytrain; nlv = 2) ;\n#fm = fdasvd(Xtrain, ytrain; nlv = 2) ;\npnames(fm)\nlev = fm.lev\nnlev = length(lev)\n\nfm.T\n# Projections of the class centers to the score space\nct = fm.Tcenters \n\ngroup = copy(ytrain)\nf, ax = plotxy(fm.T[:, 1], fm.T[:, 2], group;\n    title = \"FDA\")\nscatter!(ax, ct[:, 1], ct[:, 2], \n    markersize = 10, color = :red)\nf\n\n# Projection of Xtest to the score space\nJchemo.transform(fm, Xtest)\n\n# X-loadings matrix\n# = coefficients of the linear discriminant function\n# = \"LD\" of function lda of package MASS\nfm.P\n\nfm.eig\nfm.sstot\n# Explained variance by PCA of the class centers \n# in transformed scale\nBase.summary(fm)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fdasvd-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.fdasvd","text":"fdasvd(X, y; nlv, pseudo = false, scal = false)\nfdasvd!(X, y; nlv, pseudo = false, scal = false)\n\nFactorial discriminant analysis (FDA).\n\nX : X-data.\ny : Univariate class membership.\nnlv : Nb. discriminant components.\npseudo : If true, a MP pseudo-inverse is used (instead   of a usual inverse) for inverting W.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nWeighted SVD factorization of the matrix of the class centers.\n\nSee ?fda for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fdif-Tuple{Any}","page":"Index of functions","title":"Jchemo.fdif","text":"fdif(X; f = 2)\nfdif!(M::Matrix, X::Matrix; f = 2)\n\nCompute finite differences for each row of a matrix X. \n\nX : X-data (n, p).\nM : Pre-allocated output matrix (n, p - f + 1).\nf : Size of the window (nb. points involved) for the finite differences.   The range of the window (= nb. intervals of two successive colums) is f - 1.\n\nThe finite differences can be used for computing discrete derivates. The method reduces the column-dimension: (n, p) –> (n, p - f + 1). \n\nThe in-place function stores the output in M.\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nXp = fdif(X; f = 10)\nplotsp(Xp[1:30, :]).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.findmax_cla","page":"Index of functions","title":"Jchemo.findmax_cla","text":"findmax_cla(x, weights = nothing)\n\nFind the most occurent level in x.\n\nx : A categorical variable.\n\nIf ex-aequos, the function returns the first.\n\nExamples\n\nx = rand(1:3, 10)\ntab(x)\nfindmax_cla(x)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.frob-Tuple{Any}","page":"Index of functions","title":"Jchemo.frob","text":"frob(X)\nfrob(X, w)\n\nFrobenius norm of a matrix.\n\nX : A matrix (n, p).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nThe Frobenius norm of X is:\n\nsqrt(tr(X' * X)).\n\nThe weighted norm is:\n\nsqrt(tr(X' * D * X)), where D is the diagonal matrix of vector w.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.fweight-Tuple{Any}","page":"Index of functions","title":"Jchemo.fweight","text":"fweight(d; typw = \"bisquare\", alpha = 0)\n\nComputation of weights from distances.\n\nd : Vector of distances.\ntypw : Define the weight function.\nalpha : Parameter of the weight function, see below.\n\nThe returned weight vector is: \n\nw = f(d / q) where f is the weight function and q the 1-alpha \n\nquantile of d (Cleveland & Grosse 1991).\n\nPossible values for typw are: \n\n\"bisquare\": w = (1 - x^2)^2 \n\"cauchy\": w = 1 / (1 + x^2) \n\"epan\": w = 1 - x^2 \n\"fair\": w =  1 / (1 + x)^2 \n\"invexp\": w = exp(-x) \n\"invexp2\": w = exp(-x / 2) \n\"gauss\": w = exp(-x^2)\n\"trian\": w = 1 - x  \n\"tricube\": w = (1 - x^3)^3  \n\nReferences\n\nCleveland, W.S., Grosse, E., 1991. Computational methods for local regression.  Stat Comput 1, 47–62. https://doi.org/10.1007/BF01890836\n\nExamples\n\nusing CairoMakie, Distributions\n\ncols = cgrad(:tab10, collect(1:9)) ;\nd = sort(sqrt.(rand(Chi(1), 1000)))\nalpha = 0\ntypw = \"bisquare\"\nw = fweight(d; typw = typw, alpha = alpha)\nf = Figure(resolution = (600, 500))\nax = Axis(f, xlabel = \"Distance\", ylabel = \"Weight\")\nlines!(ax, d, w, label = typw, color = cols[1])\ntypw = \"cauchy\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[2])\ntypw = \"epan\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[3])\ntypw = \"fair\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[4])\ntypw = \"gauss\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[5])\ntypw = \"trian\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[6])\ntypw = \"invexp\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[7])\ntypw = \"invexp2\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[8])\ntypw = \"tricube\"\nw = fweight(d; typw = typw, alpha = alpha)\nlines!(ax, d, w, label = typw, color = cols[9])\naxislegend(\"Function\")\nf[1, 1] = ax\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.getknn-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.getknn","text":"getknn(Xtrain, X; k = 1, metric = \"eucl\")\n\nReturn the k nearest neighbors in Xtrain of each row of X.\n\nXtrain : Training X-data.\nX : Query X-dta.\nmetric : Type of distance used for the query.    Possible values are \"eucl\" or \"mahal\".\n\nThe distances (not squared) are also returned.\n\nExamples\n\nXtrain = rand(5, 3)\nX = rand(2, 3)\nx = X[1:1, :]\n\nk = 3\nres = getknn(Xtrain, X; k = k)\nres.ind  # indexes\nres.d    # distances\n\nres = getknn(Xtrain, x; k = k)\nres.ind\n\nres = getknn(Xtrain, X; k = k, metric = \"mahal\")\nres.ind\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcv","text":"gridcv(X, Y; segm, score, fun, pars, verbose = false)\n\nCross-validation (CV) over a grid of parameters.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nsegm : Segments of the CV (output of functions    segmts, segmkf etc.).\nscore : Function (e.g. msep) computing a prediction score.\nfun : Function computing the prediction model.\npars : tuple of named vectors (arguments of fun)    defining the grid of parameters (e.g. output of function mpar).\nverbose : If true, fitting information are printed.\n\nCompute a prediction score (= error rate) for a given model over a grid of parameters.\n\nThe score is computed over the training sets X and Y for each combination  of the grid defined in pars. \n\nThe vectors in pars must have same length.\n\nThe function returns two outputs: res (mean results) and res_p (results per replication).\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\n# Building Train (years <= 2012) and Test  (year = 2012)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\n\n# KNNR models\n\nK = 5 ; rep = 1\nsegm = segmkf(ntrain, K; rep = rep)\n\nnlvdis = 15 ; metric = [\"mahal\" ]\nh = [1 ; 2.5] ; k = [5 ; 10 ; 20 ; 50] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) \nlength(pars[1]) \nres = gridcv(Xtrain, ytrain; segm = segm, \n    score = rmsep, fun = knnr, pars = pars, verbose = true).res ;\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\n\nfm = knnr(Xtrain, ytrain;\n    nlvdis = res.nlvdis[u], metric = res.metric[u],\n    h = res.h[u], k = res.k[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n################# PLSR models\n\nK = 5 ; rep = 1\nsegm = segmkf(ntrain, K; rep = rep)\n\nnlv = 0:20\nres = gridcvlv(Xtrain, ytrain; segm = segm, \n    score = rmsep, fun = plskern, nlv = nlv).res\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nplotgrid(res.nlv, res.y1;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\n\nfm = plskern(Xtrain, ytrain; nlv = res.nlv[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n# LWPLSR models\n\nK = 5 ; rep = 1\nsegm = segmkf(ntrain, K; rep = rep)\n\nnlvdis = 15 ; metric = [\"mahal\" ]\nh = [1 ; 2.5 ; 5] ; k = [50 ; 100] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nlength(pars[1]) \nnlv = 0:20\nres = gridcvlv(Xtrain, ytrain; segm = segm, \n    score = rmsep, fun = lwplsr, pars = pars, nlv = nlv, verbose = true).res\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\ngroup = string.(\"h=\", res.h, \" k=\", res.k)\nplotgrid(res.nlv, res.y1, group;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSECV\").f\n\nfm = lwplsr(Xtrain, ytrain;\n    nlvdis = res.nlvdis[u], metric = res.metric[u],\n    h = res.h[u], k = res.k[u], nlv = res.nlv[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n################# RR models\n\nK = 5 ; rep = 1\nsegm = segmkf(ntrain, K; rep = rep)\n\nlb = (10.).^collect(-5:1:-1)\nres = gridcvlb(Xtrain, ytrain; segm = segm, \n    score = rmsep, fun = rr, lb = lb).res\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nplotgrid(log.(res.lb), res.y1;\n    xlabel = \"Lambda\", ylabel = \"RMSECV\").f\n\nfm = rr(Xtrain, ytrain; lb = res.lb[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n################# KRR models\n\nK = 5 ; rep = 1\nsegm = segmkf(ntrain, K; rep = rep)\n\ngamma = (10.).^collect(-4:1:4)\npars = mpar(gamma = gamma)\nlength(pars[1]) \nlb = (10.).^collect(-5:1:-1)\nres = gridcvlb(Xtrain, ytrain; segm = segm, \n    score = rmsep, fun = krr, pars = pars, lb = lb).res\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\ngroup = string.(\"gamma=\", res.gamma)\nplotgrid(log.(res.lb), res.y1, group;\n    xlabel = \"Lambda\", ylabel = \"RMSECV\").f\n\nfm = krr(Xtrain, ytrain; gamma = res.gamma[u], lb = res.lb[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcv_mb-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcv_mb","text":"gridcv_mb(Xbl, Y; segm, score, fun, pars, verbose = false)\n\nSee gridcv.\n\nSame as gridcv but specific to multiblock regression.\n\nSee ?gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcvlb-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcvlb","text":"gridcvlb(X, Y; segm, score, fun, lb, pars, verbose = false)\n\nSee gridcv.\nlb : Value, or collection of values, of the ridge regularization parameter \"lambda\".\n\nSame as gridcv but specific to (and much faster for) models  using ridge regularization (e.g. RR).\n\nArgument pars must not contain lb.\n\nSee ?gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcvlv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcvlv","text":"gridcvlv(X, Y; segm, score, fun, nlv, pars, verbose = false)\n\nSee gridcv.\nnlv : Nb., or collection of nb., of latent variables (LVs).\n\nSame as gridcv but specific to (and much faster for) models  using latent variables (e.g. PLSR).\n\nArgument pars must not contain nlv.\n\nSee ?gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridcvlv_mb-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.gridcvlv_mb","text":"gridcvlv_mb(Xbl, Y; segm, score, fun, nlv, pars, verbose = false)\n\nSee gridcv.\n\nSame as gridcvlv but specific to multiblock regression.\n\nSee ?gridcv for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscore-NTuple{4, Any}","page":"Index of functions","title":"Jchemo.gridscore","text":"gridscore(Xtrain, Ytrain, X, Y; score, fun, pars, verbose = FALSE)\n\nModel validation over a grid of parameters.\n\nXtrain : Training X-data (n, p).\nYtrain : Training Y-data (n, q).\nX : Validation X-data (m, p).\nY : Validation Y-data (m, q).\nscore : Function (e.g. msep) computing the prediction score.\nfun : Function computing the prediction model.\npars : tuple of named vectors (= arguments of fun) of same length   involved in the calculation of the score (e.g. output of function mpar).\nverbose : If true, fitting information are printed.\n\nCompute a prediction score (= error rate) for a given model over a grid of parameters.\n\nThe score is computed over the validation sets X and Y for each combination  of the grid defined in pars. \n\nThe vectors in pars must have same length.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\n# Building Train (years <= 2012) and Test  (year == 2012)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\n\n# Building Cal and Val within Train\n\nnval = 80\ns = sample(1:ntrain, nval; replace = false)\nXcal = rmrow(Xtrain, s)\nycal = rmrow(ytrain, s)\nXval = Xtrain[s, :]\nyval = ytrain[s]\n\n# KNNR models\n\nnlvdis = 15 ; metric = [\"mahal\" ]\nh = [1 ; 2.5] ; k = [5 ; 10 ; 20 ; 50] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) \nlength(pars[1]) \nres = gridscore(Xcal, ycal, Xval, yval;\n    score = rmsep, fun = knnr, pars = pars, verbose = true)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\n\nfm = knnr(Xtrain, ytrain;\n    nlvdis = res.nlvdis[u], metric = res.metric[u],\n    h = res.h[u], k = res.k[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n################# PLSR models\n\nnlv = 0:20\nres = gridscorelv(Xcal, ycal, Xval, yval;\n    score = rmsep, fun = plskern, nlv = nlv)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nplotgrid(res.nlv, res.y1;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\n\nfm = plskern(Xtrain, ytrain; nlv = res.nlv[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n# LWPLSR models\n\nnlvdis = 15 ; metric = [\"mahal\" ]\nh = [1 ; 2.5 ; 5] ; k = [50 ; 100] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nlength(pars[1]) \nnlv = 0:20\nres = gridscorelv(Xcal, ycal, Xval, yval;\n    score = rmsep, fun = lwplsr, pars = pars, nlv = nlv, verbose = true)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\ngroup = string.(\"h=\", res.h, \" k=\", res.k)\nplotgrid(res.nlv, res.y1, group;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSECV\").f\n\nfm = lwplsr(Xtrain, ytrain;\n    nlvdis = res.nlvdis[u], metric = res.metric[u],\n    h = res.h[u], k = res.k[u], nlv = res.nlv[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n################# RR models\n\nlb = (10.).^collect(-5:1:-1)\nres = gridscorelb(Xcal, ycal, Xval, yval;\n    score = rmsep, fun = rr, lb = lb)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\nplotgrid(log.(res.lb), res.y1;\n    xlabel = \"Lambda\", ylabel = \"RMSECV\").f\n\nfm = rr(Xtrain, ytrain; lb = res.lb[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n################# KRR models\n\ngamma = (10.).^collect(-4:1:4)\npars = mpar(gamma = gamma)\nlength(pars[1]) \nlb = (10.).^collect(-5:1:-1)\nres = gridscorelb(Xcal, ycal, Xval, yval;\n    score = rmsep, fun = krr, pars = pars, lb = lb)\nu = findall(res.y1 .== minimum(res.y1))[1] \nres[u, :]\ngroup = string.(\"gamma=\", res.gamma)\nplotgrid(log.(res.lb), res.y1, group;\n    xlabel = \"Lambda\", ylabel = \"RMSECV\").f\n\nfm = krr(Xtrain, ytrain; gamma = res.gamma[u], lb = res.lb[u]) ;\npred = Jchemo.predict(fm, Xtest).pred \nrmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscorelb-NTuple{4, Any}","page":"Index of functions","title":"Jchemo.gridscorelb","text":"gridscorelb(Xtrain, Ytrain, X, Y; score, fun, lb, pars, verbose = FALSE)\n\nSee gridscore.\nlb : Value, or collection of values, of the ridge regularization parameter \"lambda\".\n\nSame as gridscore but specific to (and much faster for) models  using ridge regularization (e.g. RR).\n\nArgument pars must not contain lb.\n\nSee ?gridscore for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.gridscorelv-NTuple{4, Any}","page":"Index of functions","title":"Jchemo.gridscorelv","text":"gridscorelv(Xtrain, Ytrain, X, Y; score, fun, nlv, pars, verbose = FALSE)\n\nSee gridscore.\nnlv : Nb., or collection of nb., of latent variables (LVs).\n\nSame as gridscore but specific to (and much faster for) models  using latent variables (e.g. PLSR).\n\nArgument pars must not contain nlv.\n\nSee ?gridscore for examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.head-Tuple{Any}","page":"Index of functions","title":"Jchemo.head","text":"head(X)\n\nDisplay the first rows of a dataset.\n\nExamples\n\nX = rand(100, 5)\nhead(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.interpl-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.interpl","text":"interpl(X, wl; wlfin, fun = cubic_spline)\n\nSampling signals by interpolation.\n\nX : Matrix (n, p) of signals (rows).\nwl : Values representing the column \"names\" of X.    Must be a numeric vector of length p, or an AbstractRange.\nwlfin : Final values where to interpolate within the range of wl.   Must be a numeric vector, or an AbstractRange.\nfun : Function defining the interpolation method.\n\nThe function uses package DataInterpolations.jl.\n\nPossible values of fun (methods) are:\n\nlinear_int: A linear interpolation (LinearInterpolation).\nquadratic_int: A quadratic interpolation (QuadraticInterpolation).\nquadratic_spline: A quadratic spline interpolation(QuadraticSpline).\ncubic_spline: A cubic spline interpolation (CubicSpline)\n\nReferences\n\nPackage Interpolations.jl https://github.com/PumasAI/DataInterpolations.jl https://htmlpreview.github.io/?https://github.com/PumasAI/DataInterpolations.jl/blob/v2.0.0/example/DataInterpolations.html\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nwl = names(X)\nwl_num = parse.(Float64, wl) \n\nplotsp(X[1:10,:], wl_num).f\n\nwlfin = collect(range(500, 2400, length = 10))\n#wlfin = range(500, 2400, length = 10)\nXp = interpl(X[1:10, :], wl_num; wlfin = wlfin) \nplotsp(Xp, wlfin).f\n\nXp = interpl_mon(X[1:10, :], wl_num; wlfin = wlfin) ;\nplotsp(Xp, wlfin).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.interpl_mon-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.interpl_mon","text":"interpl_mon(X, wl; wlfin, fun = FritschCarlsonMonotonicInterpolation)\n\nSampling signals by monotonic interpolation.\n\nX : Matrix (n, p) of signals (rows).\nwl : Values representing the column \"names\" of X.    Must be a numeric vector of length p, or an AbstractRange.\nwlfin : Values where to interpolate within the range of wl.   Must be a numeric vector, or an AbstractRange.\nfun : Function defining the interpolation method.\n\nSee e.g. https://en.wikipedia.org/wiki/Monotonecubicinterpolation.\n\nThe function uses package Interpolations.jl.\n\nPossible values of fun (methods) are:\n\nLinearMonotonicInterpolation\nFiniteDifferenceMonotonicInterpolation : Classic cubic\nCardinalMonotonicInterpolation\nFritschCarlsonMonotonicInterpolation\nFritschButlandMonotonicInterpolation\nSteffenMonotonicInterpolation\n\nSee https://github.com/JuliaMath/Interpolations.jl/pull/243/files#diff-92e3f2a374c9a54769084bad1bbfb4ff20ee50716accf008074cda7af1cd6149\n\nSee '?interpl' for examples. \n\nReferences\n\nPackage Interpolations.jl https://github.com/JuliaMath/Interpolations.jl\n\nFritsch & Carlson (1980), \"Monotone Piecewise Cubic Interpolation\",  doi:10.1137/0717021.\n\nFritsch & Butland (1984), \"A Method for Constructing Local Monotone Piecewise  Cubic Interpolants\", doi:10.1137/0905021.\n\nSteffen (1990), \"A Simple Method for Monotonic Interpolation  in One Dimension\", http://adsabs.harvard.edu/abs/1990A%26A...239..443S\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.iqr-Tuple{Any}","page":"Index of functions","title":"Jchemo.iqr","text":"iqr(x)\n\nCompute the interquartile interval (IQR).\n\nExamples\n\nx = rand(100)\niqr(x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.isel","page":"Index of functions","title":"Jchemo.isel","text":"isel(X, Y, wl = 1:nco(X); rep = 1, \n    nint = 5, psamp = 1/3, score = rmsep, \n    fun, kwargs...)\n\nInterval variable selection.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nwl : Optional numeric labels (p, 1) of the X-columns.  \nrep : Number of replications. \nnint : Nb. intervals. \npsamp : Proportion of data used as test set to compute the score   (default: n/3 of the data).\nscore : Function computing the prediction score (= error rate; e.g. msep).\nfun : Function defining the prediction model.\nkwarg : Optional other arguments to pass to funtion defined in fun.\n\nThe principle is as follows:\n\nData (X, Y) are splitted randomly to a training and a test set.\nRange 1:p in X is segmented to nint intervals of equal (when possible) size. \nThe model is fitted on the training set and the score (error rate) on the test set,    firtsly accounting for all the p variables (reference) and secondly    for each of the nint intervals. \nThis process is replicated rep times. Average results are provided in the outputs,   as well the results per replication. \n\nReferences\n\nNørgaard, L., Saudland, A., Wagner, J., Nielsen, J.P., Munck, L., \n\nEngelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419. https://doi.org/10.1366/0003702001949500\n\nExamples\n\nusing JchemoData, DataFrames, JLD2\nusing CairoMakie\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/tecator.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nY = dat.Y \nwl = names(X)\nwl_num = parse.(Float64, wl) \ntyp = Y.typ\ny = Y.fat\n\nf = 15 ; pol = 3 ; d = 2 \nXp = savgol(snv(X); f = f, pol = pol, d = d) \n\ns = typ .== \"train\"\nXtrain = Xp[s, :]\nytrain = y[s]\n\nnint = 10\nnlv = 5\nres = isel(Xtrain, ytrain, wl_num; rep = 20, \n    nint = nint, fun = plskern, nlv = nlv) ;\nres.res_rep\nres.res0_rep\nzres = res.res\nzres0 = res.res0\n\nf = Figure(resolution = (900, 400))\nax = Axis(f[1, 1],\n    xlabel = \"Wawelength (nm)\", ylabel = \"RMSEP\",\n    xticks = zres.lo)\nscatter!(ax, zres.mid, zres.y1; color = (:red, .5))\nvlines!(ax, zres.lo; color = :grey,\n    linestyle = :dash, linewidth = 1)\nhlines!(ax, zres0.y1, linestyle = :dash)\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.knnda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.knnda","text":"knnda(X, y; nlvdis = 0, metric = \"eucl\", h = Inf, k = 1, tol = 1e-4)\n\nk-Nearest-Neighbours weighted discrimination (kNN-DA).\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) computations.\n\nFor each new observation to predict:\n\ni) a number of k nearest neighbors (= \"weighting 1\") is selected\nii) a weigthed (= \"weighting 2\") vote is then computed in this neighborhood    to select the most frequent class. \n\nWeightings 1 and 2 are computed from the dissimilarities between the observation  to predict and the training observations. Depending on argument nlvdis,  the computation is done from the raw X-data or after a dimension reduction.  In the last case, global PLS2 scores (LVs) are  computed from {X, Y-dummy} (where Y-dummy is the dummy table build from y),  and the dissimilarities are computed over these scores. \n\nIn general, for high dimensional X-data, using the Mahalanobis distance requires  preliminary dimensionality reduction of the data.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 10\nfm = knnda(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.knnr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.knnr","text":"knnr(X, Y; nlvdis = 0, metric = \"eucl\", h = Inf, k = 1, \n    tol = 1e-4, scal = false)\n\nk-Nearest-Neighbours regression (KNNR).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlvdis : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) computations.\n\nThe function uses functions getknn and locw;  see the code for details. Many other variants of kNNR pipelines can be built.\n\nThe general principle of the method is as follows.\n\nFor each new observation to predict, the prediction is the weighted mean over the selected neighborhood (in X). Within the selected neighborhood, the weights are defined from the dissimilarities between the new observation  and the neighborhood, and are computed from function 'wdist'.\n\nIn general, for high dimensional X-data, using the Mahalanobis distance requires  preliminary dimensionality reduction of the data.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlvdis = 20 ; metric = \"mahal\" \nh = 2 ; k = 100 ; nlv = 15\nfm = knnr(Xtrain, ytrain; nlvdis = nlvdis,\n    metric = metric, h = h, k = k) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nf, ax = scatter(vec(pred), ytest)\nablines!(ax, 0, 1)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.kpca","page":"Index of functions","title":"Jchemo.kpca","text":"kpca(X, weights = ones(nro(X)); nlv, \n    kern = \"krbf\", scal = false, kwargs...)\n\nKernel PCA  (Scholkopf et al. 1997, Scholkopf & Smola 2002, Tipping 2001).\n\nX : X-data.\nweights : vector (n,).\nnlv : Nb. principal components (PCs), or collection of nb. PCs, to consider. \nkern : Type of kernel used to compute the Gram matrices.   Possible values are \"krbf\" of \"kpol\" (see respective    functions krbf and kpol).\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\nkwargs : Named arguments to pass in the kernel function.    See ?krbf, ?kpol.\n\nThe method is implemented by SVD factorization of the weighted Gram matrix  D^(1/2) * Phi(X) * Phi(X)' * D^(1/2), where D is a diagonal matrix of weights for  the observations (rows of X).\n\nThe kernel Gram matrices are internally centered. \n\nReferences\n\nScholkopf, B., Smola, A., MÃ¼ller, K.-R., 1997. Kernel principal component analysis,  in: Gerstner, W., Germond, A., Hasler, M., Nicoud, J.-D. (Eds.), Artificial Neural Networks,  ICANN 97, Lecture Notes in Computer Science. Springer, Berlin, Heidelberg,  pp. 583-588. https://doi.org/10.1007/BFb0020217\n\nScholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines, regularization,  optimization, and beyond, Adaptive computation and machine learning. MIT Press, Cambridge, Mass.\n\nTipping, M.E., 2001. Sparse kernel principal component analysis. Advances in neural information  processing systems, MIT Press. http://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = dat.X[:, 1:4]\nn = nro(X)\n\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nXtest = rmrow(X, s)\n\nnlv = 3 ; gamma = 1e-4\nfm = kpca(Xtrain; nlv = nlv, gamma = gamma) ;\npnames(fm)\nfm.T\nfm.T' * fm.T\nfm.P' * fm.P\n\nJchemo.transform(fm, Xtest)\n\nres = Base.summary(fm) ;\npnames(res)\nres.explvarx\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.kplsr","page":"Index of functions","title":"Jchemo.kplsr","text":"kplsr(X, Y, weights = ones(nro(X)); \n    nlv, kern = \"krbf\", tol = 1.5e-8, maxit = 100, \n    scal = false, kwargs...)\nkplsr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); \n    nlv, kern = \"krbf\", tol = 1.5e-8, maxit = 100, \n    scal = false, kwargs...)\n\nKernel partial least squares regression (KPLSR) implemented with a Nipals  algorithm (Rosipal & Trejo, 2001).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to consider. \n'kern' : Type of kernel used to compute the Gram matrices.   Possible values are \"krbf\" or \"kpol\" (see respective functions krbf and kpol).\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\ntol : Tolerance value for stopping the iterations.\nmaxit : Maximum nb. iterations.\nkwargs : Named arguments to pass in the kernel function.\n\nThis algorithm becomes slow for n > 1000.\n\nThe kernel Gram matrices are internally centered. \n\nReferences\n\nRosipal, R., Trejo, L.J., 2001. Kernel Partial Least Squares Regression in  Reproducing Kernel Hilbert Space. Journal of Machine Learning Research 2, 97-123.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 20 ; gamma = 1e-1\nfm = kplsr(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;\nfm.T\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.beta\nJchemo.coef(fm; nlv = 7).beta\n\nJchemo.transform(fm, Xtest)\nJchemo.transform(fm, Xtest; nlv = 7)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\nres = Jchemo.predict(fm, Xtest; nlv = 1:2)\nres.pred[1]\nres.pred[2]\n\nfm = kplsr(Xtrain, ytrain; nlv = nlv, kern = \"kpol\", degree = 2, \n    gamma = 1e-1, coef0 = 10) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\n\n# Example of fitting the function sinc(x)\n# described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nfm = kplsr(x, y; nlv = 2) ;\npred = Jchemo.predict(fm, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.kplsrda","page":"Index of functions","title":"Jchemo.kplsrda","text":"kplsrda(X, y, weights = ones(nro(X)); nlv, kern = \"krbf\", \n    scal = false, kwargs...)\n\nDiscrimination based on kernel partial least squares regression (KPLSR-DA).\n\nX : X-data.\ny : Univariate class membership.\nweights : Weights of the observations. Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nOther arguments to pass in the kernel: See ?kplsr.\n\nThis is the same approach as for plsrda except that the PLS2 step  is replaced by a non linear kernel PLS2 (KPLS).\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\ngamma = .001 \nnlv = 15\nfm = kplsrda(Xtrain, ytrain; nlv = nlv, gamma = gamma) ;\npnames(fm)\ntypeof(fm.fm) # = KPLS2 model\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.posterior\nres.pred\nerr(res.pred, ytest)\n\nJchemo.coef(fm.fm)\nJchemo.transform(fm.fm, Xtest)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.kpol-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.kpol","text":"kpol(X, Y; degree = 1, gamma = 1, coef0 = 0)\n\nCompute a polynomial kernel Gram matrix. \n\nX : Data.\nY : Data.\ndegree : Degree of the polynom.\ngamma : Scale of the polynom.\ncoef0 : Offset of the polynom.\n\nGiven matrices X and Yof sizes (n, p) and (m, p), respectively, the function returns the (n, m) Gram matrix K(X, Y) = Phi(X) * Phi(Y)'.\n\nThe polynomial kernel between two vectors x and y is computed by  (gamma * (x' * y) + coef0)^degree.\n\nReferences\n\nScholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines,  regularization, optimization, and beyond, Adaptive computation and machine learning.  MIT Press, Cambridge, Mass.\n\nExamples\n\nX = rand(5, 3)\nY = rand(2, 3)\nkpol(X, Y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.krbf-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.krbf","text":"krbf(X, Y; gamma = 1)\n\nCompute a Radial-Basis-Function (RBF) kernel Gram matrix. \n\nX : Data.\nY : Data.\ngamma : Scale parameter.\n\nGiven matrices X and Yof sizes (n, p) and (m, p), respectively, the function returns the (n, m) Gram matrix K(X, Y) = Phi(X) * Phi(Y)'.\n\nThe RBF kernel between two vectors x and y is computed by  exp(-gamma * ||x - y||^2).\n\nReferences\n\nScholkopf, B., Smola, A.J., 2002. Learning with kernels: support vector machines,  regularization, optimization, and beyond, Adaptive computation and machine learning.  MIT Press, Cambridge, Mass.\n\nExamples\n\nX = rand(5, 3)\nY = rand(2, 3)\nkrbf(X, Y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.krr","page":"Index of functions","title":"Jchemo.krr","text":"krr(X, Y, weights = ones(nro(X)); \n    lb = .01, kern = \"krbf\", scal = false, kwargs...)\nkrr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); \n    lb, kern = \"krbf\", scal = false, kwargs...)\n\nKernel ridge regression (KRR) implemented by SVD factorization.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nlb : A value of the regularization parameter \"lambda\".\n'kern' : Type of kernel used to compute the Gram matrices.   Possible values are \"krbf\" of \"kpol\" (see respective functions krbf and kpol.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nkwargs : Named arguments to pass in the kernel function.\n\nKRR is also referred to as least squared SVM regression (LS-SVMR). The method is close to the particular case of SVM regression  where there is novmarges excluding the observations (epsilon coefficient  set to zero). The difference is that a L2-norm optimization is done,  instead of L1 in SVM.\n\nThe kernel Gram matrices are internally centered. \n\nReferences\n\nBennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression,  in: Advances in Learning Theory: Methods, Models and Applications,  NATO Science Series III: Computer & Systems Sciences. IOS Press Amsterdam, pp. 227-250.\n\nCawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel Ridge Regression.  Neural Processing Letters 16, 293-302. https://doi.org/10.1023/A:1021798002258\n\nKrell, M.M., 2018. Generalizing, Decoding, and Optimizing Support Vector Machine Classification.  arXiv:1801.04929.\n\nSaunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression Learning Algorithm in Dual Variables,  in: In Proceedings of the 15th International Conference on Machine Learning. Morgan Kaufmann, pp. 515â521.\n\nSuykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse approximation using  least squares support vector machines. 2000 IEEE International Symposium on Circuits and Systems.  Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439\n\nWelling, M., n.d. Kernel ridge regression. Department of Computer Science,  University of Toronto, Toronto, Canada. https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nlb = 1e-3 ; gamma = 1e-1\nfm = krr(Xtrain, ytrain; lb = lb, gamma = gamma) ;\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.A \nzcoef.df\nJchemo.coef(fm; lb = 1e-6).df\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\nres = Jchemo.predict(fm, Xtest; lb = [.01 ; .001])\nres.pred[1]\nres.pred[2]\n\nfm = krr(Xtrain, ytrain; lb = lb, kern = \"kpol\", \n    degree = 2, gamma = 1e-1, coef0 = 10) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\n\n# Example of fitting the function sinc(x)\n# described in Rosipal & Trejo 2001 p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nfm = krr(x, y; lb = 1e-1, gamma = 1 / 3) ;\npred = Jchemo.predict(fm, x).pred \nf, ax = scatter(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\naxislegend(\"Method\")\nf\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.krrda","page":"Index of functions","title":"Jchemo.krrda","text":"krrda(X, y, weights = ones(nro(X)); lb, \n    scal = scal, kern = \"krbf\", kwargs...)\n\nDiscrimination based on kernel ridge regression (KRR-DA).\n\nX : X-data.\ny : Univariate class membership.\nweights : Weights of the observations. Internally normalized to sum to 1. \nlb : A value of the regularization parameter \"lambda\".\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nOther arguments to pass in the kernel: See ?kplsr.\n\nThe training variable y (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in y. Each column of Ydummy is a dummy (0/1) variable.  Then, a RR is implemented on the y and each column of Ydummy, returning predictions of the dummy variables (= object posterior returned by  function predict).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\ngamma = .01\nlb = .001\nfm = krrda(Xtrain, ytrain; lb = lb, gamma = gamma) ;    \npnames(fm)\npnames(fm.fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nJchemo.predict(fm, Xtest; lb = [.1; .01]).pred\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.lda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lda","text":"lda(X, y; prior = \"unif\", scal = false)\n\nLinear discriminant analysis  (LDA).\n\nX : X-data.\ny : y-data (class membership).\nprior : Type of prior probabilities for class membership.   Posible values are: \"unif\" (uniform), \"prop\" (proportional).\n\nExamples\n\nusing JchemoData, JLD2, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\n\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\ntab(ytrain)\ntab(ytest)\n\nprior = \"unif\"\n#prior = \"prop\"\nfm = lda(Xtrain, ytrain; prior = prior) ;\npnames(fm)\nprintln(typeof(fm))\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.ds\nres.posterior\nres.pred\nerr(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lg","text":"lg(X, Y; centr = true)\nlg(Xbl; centr = true)\n\nCompute the Lg coefficient between matrices.\n\nX : Matrix (n, p).\nY : Matrix (n, q).\nXbl : A list (vector) of matrices.\ncentr : Logical indicating if the matrices are internally    centered or not.\n\nLg(X, Y) = Sumj(=1..p) Sumk(= 1..q) cov(xj, yk)^2\n\nRV(X, Y) = Lg(X, Y) / sqrt(Lg(X, X), Lg(Y, Y))\n\nReferences\n\nEscofier, B. & Pagès, J. 1984. L’analyse factorielle multiple. Cahiers du Bureau  universitaire de recherche opérationnelle. Série Recherche, tome 42, p. 3-68\n\nEscofier, B. & Pagès, J. (2008). Analyses Factorielles Simples et Multiples : Objectifs, Méthodes et Interprétation. Dunod, 4e édition\n\nExamples\n\nX = rand(5, 10)\nY = rand(5, 3)\nlg(X, Y)\n\nX = rand(5, 15) \nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl)\nlg(Xbl)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.list-Tuple{Integer, Any}","page":"Index of functions","title":"Jchemo.list","text":"list(n::Integer, type)\n\nCreate a Vector{type}(undef, n).\n\nisassigned(object, i) can be used to check if cell i is empty.\n\nExamples\n\nlist(5, Float64)\nlist(5, Array{Float64})\nlist(5, Matrix{Float64})\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.list-Tuple{Integer}","page":"Index of functions","title":"Jchemo.list","text":"list(n::Integer)\n\nCreate a Vector{Any}(nothing, n).\n\nisnothing(object, i) can be used to check if cell i is empty.\n\nExamples\n\nlist(5)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.locw-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.locw","text":"locw(Xtrain, Ytrain, X ; \n    listnn, listw = nothing, fun, verbose = false, kwargs...)\n\nCompute predictions for a given kNN model.\n\nXtrain : Training X-data.\nYtrain : Training Y-data.\nX : X-data (m observations) to predict.\nlistnn : List of m vectors of indexes.\nlistw : List of m vectors of weights.\nfun : Function computing the model on the m neighborhoods.\nverbose : If true, fitting information are printed.\nkwargs : Keywords arguments to pass in function fun. \n\nEach component i of listnn and listw contains the indexes and weights, respectively, of the nearest neighbors of x_i in Xtrain. The sizes of the neighborhood  for i = 1,...,m can be different.\n\nAll the arguments in kwargs must have length = 1 (not collections).\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.locwlv-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.locwlv","text":"locwlv(Xtrain, Ytrain, X; \n    listnn, listw = nothing, fun, nlv, verbose = true, kwargs...)\n\nCompute predictions for a given kNN model.\n\nnlv : Nb. or collection of nb. of latent variables (LVs).\n\nSame as locw but specific (and much faster) for LV-based (e.g. PLSR) models.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwmlr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwmlr","text":"lwmlr(X, Y; metric = \"eucl\", h, k, \n    tol = 1e-4, verbose = false)\n\nk-Nearest-Neighbours locally weighted multiple linear regression (kNN-LWMLR).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ntol : For stabilization when very close neighbors.\nverbose : If true, fitting information are printed.\n\nThis is the same principle as function lwplsr except that MLR models are fitted (on the neighborhoods) instead of PLSR models.  The neighborhoods  are computed on X (there is no preliminary dimension reduction).\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 20\nzfm = pcasvd(Xtrain; nlv = nlv) ;\nTtrain = zfm.T \nTtest = Jchemo.transform(zfm, Xtest)\n\nfm = lwmlr(Ttrain, ytrain; metric = \"mahal\",\n    h = 2, k = 100) ;\npred = Jchemo.predict(fm, Ttest).pred\nprintln(rmsep(pred, ytest))\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed (Test)\").f  \n\n####### Example of fitting the function sinc(x)\n####### described in Rosipal & Trejo 2001 J of Machine Learning Res. p. 105-106 \nx = collect(-10:.2:10) \nx[x .== 0] .= 1e-5\nn = length(x)\nzy = sin.(abs.(x)) ./ abs.(x) \ny = zy + .2 * randn(n) \nfm = lwmlr(x, y; metric = \"eucl\", h = 1, k = 20) ;\npred = Jchemo.predict(fm, x).pred \nf = Figure(resolution = (700, 300))\nax = Axis(f[1, 1])\nscatter!(x, y) \nlines!(ax, x, zy, label = \"True model\")\nlines!(ax, x, vec(pred), label = \"Fitted model\")\nf[1, 2] = Legend(f, ax, framevisible = false)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwmlr_s-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwmlr_s","text":"lwmlr_s(X, Y; nlv, reduc = \"pls\", \n    metric = \"eucl\", h, k, \n    gamma = 1, psamp = 1, samp = \"sys\", \n    tol = 1e-4, scal = false, verbose = false)\n\nkNN-LWMLR after preliminary (linear or non-linear) dimension      reduction (kNN-LWMLR-S).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlv : Nb. latent variables (LVs) for preliminary dimension reduction. \nreduc : Type of dimension reduction. Possible values are:   \"pca\" (PCA), \"pls\" (PLS; default), \"dkpls\" (direct Gaussian kernel PLS).\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ngamma : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function krbf.\npsamp : Proportion of observations sampled in X, Yto compute the    loadings used to compute the scores.\nsamp : Type of sampling applied for psamp. Possible values are:    \"sys\" (systematic grid sampling over rowsum(Y)) or \"random\" (random sampling).\ntol : For stabilization when very close neighbors.\nverbose : If true, fitting information are printed.\n\nThe principle is as follows. A preliminary dimension reduction (parameter nlv)  of the X-data (n, p) returns a score matrix T (n, nlv). Then, a kNN-LWMLR  is done on {T, Y}.\n\nThe dimension reduction can be linear (PCA, PLS) or non linear (DKPLS), defined  in argument reduc.\n\nWhen n is too large, the reduction dimension can become too costly, in particular for a kernel PLS (that requires to compute a matrix (n, n)). Argument psamp allows to sample a proportion of the observations that will be used to compute (approximate) scores T for the all X-data. \n\nThe case reduc = \"pca\" corresponds to the \"LWR\" algorithm proposed  by Naes et al. (1990).\n\nReferences\n\nNaes, T., Isaksson, T., Kowalski, B., 1990. Locally weighted regression and scatter correction for near-infrared reflectance data.  Analytical Chemistry 664–673.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nfm = lwmlr_s(Xtrain, ytrain; nlv = 20, reduc = \"pca\", \n    metric = \"eucl\", h = 2, k = 100) ;\npred = Jchemo.predict(fm, Xtest).pred\nprintln(rmsep(pred, ytest))\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed (Test)\").f  \n\nfm = lwmlr_s(Xtrain, ytrain; nlv = 20, reduc = \"dkpls\", \n    metric = \"eucl\", h = 2, k = 100, gamma = .01) ;\npred = Jchemo.predict(fm, Xtest).pred\nrmsep(pred, ytest)\n\nfm = lwmlr_s(Xtrain, ytrain; nlv = 20, reduc = \"dkpls\", \n    metric = \"eucl\", h = 2, k = 100, gamma = .01,\n    psamp = .5, samp = \"random\") ;\npred = Jchemo.predict(fm, Xtest).pred\nrmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwmlrda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwmlrda","text":"lwmlrda(X, y; metric = \"eucl\", h, k, \n    tol = 1e-4, verbose = false)\n\nk-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).\n\nX : X-data (n, p).\ny : Univariate class membership.\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ntol : For stabilization when very close neighbors.\nverbose : If true, fitting information are printed.\n\nThis is the same principle as function lwmlr except that local MLR-DA models are fitted instead of local MLR models.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlv = 20\nzfm = pcasvd(Xtrain; nlv = nlv) ;\nTtrain = zfm.T \nTtest = Jchemo.transform(zfm, Xtest)\n\nmetric = \"mahal\"\nh = 2 ; k = 100\nfm = lwmlrda(Ttrain, ytrain;\n    metric = metric, h = h, k = k) ;\nres = Jchemo.predict(fm, Ttest) ;\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwmlrda_s-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwmlrda_s","text":"lwmlrda_s(X, y; nlv, reduc = \"pls\", \n    metric = \"eucl\", h, k, \n    gamma = 1, psamp = 1, samp = \"cla\", \n    tol = 1e-4, scal = false, verbose = false)\n\nkNN-LWMLR-DA after preliminary (linear or non-linear) dimension      reduction (kNN-LWMLR-DA-S).\n\nX : X-data (n, p).\ny : Univariate class membership.\nnlv : Nb. latent variables (LVs) for preliminary dimension reduction. \nreduc : Type of dimension reduction. Possible values are:   \"pca\" (PCA), \"pls\" (PLS; default), \"dkpls\" (direct Gaussian kernel PLS).\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ngamma : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function krbf.\npsamp : Proportion of observations sampled in X, yto compute the    loadings used to compute the scores.\nsamp : Type of sampling applied for psamp. Possible values are    \"cla\" (stratified random sampling over the classes in y) or \"random\" (random sampling). \ntol : For stabilization when very close neighbors.\nverbose : If true, fitting information are printed.\n\nThis is the same principle as function lwmlr_s except that, locally, MLR-DA models are fitted instead of MLR models.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nfm = lwmlrda_s(Xtrain, ytrain; nlv = 20, reduc = \"pca\", \n    metric = \"eucl\", h = 2, k = 100) ;\npred = Jchemo.predict(fm, Xtest).pred\nerr(pred, ytest)\n\nfm = lwmlrda_s(Xtrain, ytrain; nlv = 20, reduc = \"dkpls\", \n    metric = \"eucl\", h = 2, k = 100, gamma = .01) ;\npred = Jchemo.predict(fm, Xtest).pred\nerr(pred, ytest)\n\nfm = lwmlrda_s(Xtrain, ytrain; nlv = 20, reduc = \"dkpls\", \n    metric = \"eucl\", h = 2, k = 100, gamma = .01,\n    psamp = .5, samp = \"cla\") ;\npred = Jchemo.predict(fm, Xtest).pred\nerr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplslda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplslda","text":"lwplslda(X, y; nlvdis, metric, h, k, nlv, prior = \"unif\", \n    tol = 1e-4, scal = false, verbose = false)\n\nkNN-LWPLS-LDA models.\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : Nb. latent variables (LVs).\nprior : Type of prior probabilities for class membership   (unif: uniform; prop: proportional).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same methodology as for lwplsr except that  PLSR is replaced by PLS-LDA.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 100\nnlv = 15\nfm = lwplslda(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k, nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsldaavg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsldaavg","text":"lwplsldaavg(X, y; nlvdis, metric, h, k, nlv, \n    tol = 1e-4, scal = false, verbose = false)\n\nAveraging of kNN-LWPLSR-DA models with different numbers of      latent variables (LVs).\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before calculating the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to    the single model with 10 LVs).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same methodology as for lwplsravg except that  PLSR is replaced by PLS-LDA, and the mean is replaced by votes.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the most occurent class within the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 100\n## mininum nlv must be >= 1, \n## conversely to lwplsrdaavg (nlv >= 0)\nnlv = \"1:20\"       \nfm = lwplsldaavg(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k, nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsqda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsqda","text":"lwplsqda(X, y; nlvdis, metric, h, k, nlv, prior = \"unif\", \n    tol = 1e-4, scal = false, verbose = false)\n\nkNN-LWPLS-QDA models.\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : Nb. latent variables (LVs).\nprior : Type of prior probabilities for class membership   (unif: uniform; prop: proportional).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same methodology as for lwplsr except that  PLSR is replaced by PLS-QDA.\n\nThe present version of the function suffers from frequent stops due to non positive definite matrices when doing local QDA.  The present recommandation is to select a sufficiant large number of neighbors. This will be fixed in the future.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 1000\nnlv = 15\nfm = lwplsqda(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k, nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsqdaavg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsqdaavg","text":"lwplsqdaavg(X, y; nlvdis, metric, h, k, nlv, \n    tol = 1e-4, scal = false, verbose = false)\n\nAveraging of kNN-LWPLSR-DA models with different numbers of      latent variables (LVs).\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before calculating the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to    the single model with 10 LVs).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same methodology as for lwplsravg except that  PLSR is replaced by PLS-QDA, and the mean is replaced by votes.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the most occurent class within the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nThe present version of the function suffers from frequent stops due to non positive definite matrices when doing local QDA.  The present recommandation is to select a sufficiant large number of neighbors. This will be fixed in the future.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 1000\n## mininum nlv must be >= 1, \n## conversely to lwplsrdaavg (nlv >= 0)\nnlv = \"1:20\"       \nfm = lwplsqdaavg(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k, nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsr","text":"lwplsr(X, Y; nlvdis, metric, h, k, nlv, tol = 1e-4, verbose = false)\n\nk-Nearest-Neighbours locally weighted partial least squares regression (kNN-LWPLSR).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlvdis : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before computing the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : Nb. latent variables (LVs).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nFunction lwplsr fits kNN-LWPLSR models (Lesnoff et al., 2020).  The function uses functions getknn, locw and a PLSR function;  see the code for details. Many other variants of kNN-LWPLSR pipelines can be built.\n\nThe general principles of the method are as follows.\n\nLWPLSR is a particular case of weighted PLSR (WPLSR) (e.g. Schaal et al. 2002).  In WPLSR, a priori weights, different from the usual 1/n (standard PLSR),  are given to the n training observations. These weights are used for calculating  (i) the scores and loadings of the WPLS and (ii) the regression model that fits  (by weighted least squares) the Y-response(s) to the WPLS scores.  The specificity of LWPLSR (compared to WPLSR) is that the weights are computed  from dissimilarities (e.g. distances) between the new observation to predict  and the training observations (\"L\" in LWPLSR comes from \"localized\").  Note that in LWPLSR the weights and therefore the fitted WPLSR model  change for each new observation to predict.\n\nIn the original LWPLSR, all the n training observations are used for each  observation to predict (e.g. Sicard & Sabatier 2006, Kim et al 2011).  This can be very time consuming, in particular for large n.  A faster and often more efficient strategy is to preliminary select,  in the training set, a number of k nearest neighbors to the observation to predict  (= \"weighting 1\") and then to apply LWPLSR only to this pre-selected  neighborhood (= \"weighting 2\"). This strategy corresponds to a kNN-LWPLSR  and is the one implemented in function lwplsr.\n\nIn lwplsr, the dissimilarities used for weightings 1 and 2 are  computed from the raw X-data or after a dimension reduction, depending on argument nlvdis. In the last case, global PLS2 scores (LVs) are  computed from {X, Y} and the dissimilarities are computed over these scores. \n\nIn general, for high dimensional X-data, using the Mahalanobis distance requires  preliminary dimensionality reduction of the data.\n\nReferences\n\nKim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active  pharmaceutical ingredients content using locally weighted partial least squares  and statistical wavelength selection. Int. J. Pharm., 421, 269-274.\n\nLesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS  strategies for regression and discrimination on agronomic NIR data.  Journal of Chemometrics, e3209. https://doi.org/10.1002/cem.3209\n\nSchaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric  statistics for the real time robot learning. Applied Intell., 17, 49-60.\n\nSicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression  and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlvdis = 20 ; metric = \"mahal\" \nh = 1 ; k = 100 ; nlv = 15\nfm = lwplsr(Xtrain, ytrain; nlvdis = nlvdis,\n    metric = metric, h = h, k = k, nlv = nlv) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed (Test)\").f  \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsr_s-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsr_s","text":"lwplsr_s(X, Y; nlv0, reduc = \"pls\", \n    metric = \"eucl\", h, k, gamma = 1, psamp = 1, samp = \"sys\", \n    nlv, tol = 1e-4, scal = false, verbose = false)\n\nkNN-LWPLSR after preliminary (linear or non-linear) dimension      reduction (kNN-LWPLSR-S).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlv0 : Nb. latent variables (LVs) for preliminary dimension reduction. \nreduc : Type of dimension reduction. Possible values are:   \"pca\" (PCA), \"pls\" (PLS; default), \"dkpls\" (direct Gaussian kernel PLS).\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ngamma : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function krbf.\npsamp : Proportion of observations sampled in X, Yto compute the    loadings used to compute the scores.\nsamp : Type of sampling applied for psamp. Possible values are:    \"sys\" (systematic grid sampling over rowsum(Y)) or \"random\" (random sampling).\nnlv : Nb. latent variables (LVs) for the models fitted on preliminary    scores.\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThe principle is as follows. A preliminary dimension reduction (parameter nlv0)  of the X-data (n, p) returns a score matrix T (n, nlv). Then, a kNN-LWPLSR  is done on {T, Y}. This is a fast approximation of kNN-LWPLSR using the same  principle as in Shen et al 2019.\n\nThe dimension reduction can be linear (PCA, PLS) or non linear (DKPLS), defined  in argument reduc.\n\nWhen n is too large, the reduction dimension can become too costly, in particular for a kernel PLS (that requires to compute a matrix (n, n)). Argument psamp allows to sample a proportion of the observations that will be used to compute (approximate) scores T for the all X-data. \n\nSetting nlv = nlv0 returns the same predicions as function lwmlr_s.\n\nReferences\n\nLesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS  strategies for regression and discrimination on agronomic NIR data.  Journal of Chemometrics, e3209. https://doi.org/10.1002/cem.3209\n\nShen, G., Lesnoff, M., Baeten, V., Dardenne, P., Davrieux, F., Ceballos, H., Belalcazar, J.,  Dufour, D., Yang, Z., Han, L., Pierna, J.A.F., 2019. Local partial least squares based on global PLS scores.  Journal of Chemometrics 0, e3117. https://doi.org/10.1002/cem.3117\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv0 = 20 ; metric = \"mahal\" \nh = 2 ; k = 100 ; nlv = 10\nfm = lwplsr_s(Xtrain, ytrain; nlv0 = nlv0,\n    metric = metric, h = h, k = k, nlv = nlv) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed (Test)\").f  \n\nfm = lwplsr_s(Xtrain, ytrain; nlv0 = nlv0,\n    reduc = \"dkpls\", metric = metric, \n    h = h, k = k, gamma = .1, nlv = nlv) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed (Test)\").f  \n\nfm = lwplsr_s(Xtrain, ytrain; nlv0 = nlv0,\n    reduc = \"dkpls\", metric = metric, \n    h = h, k = k, gamma = .1, psamp = .8,\n    samp = \"random\", nlv = nlv) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsravg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsravg","text":"lwplsravg(X, Y; nlvdis, metric, h, k, nlv, \n    typf = \"unif\", typw = \"bisquare\", alpha = 0, K = 5, rep = 10,\n    tol = 1e-4, scal = false, verbose = false)\n\nAveraging kNN-LWPLSR models with different numbers of      latent variables (LVs).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nnlvdis : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before calculating the dissimilarities.        If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to    the single model with 10 LVs).   \ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\nOther arguments: see ?plsravg.\n\nEnsemblist method where the predictions of each local model are computed  are computed by averaging or stacking the predictions of a set of models  built with different numbers of latent variables (LVs).\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the average (eventually weighted) or stacking of the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nSee ?plsravg.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlvdis = 20 ; metric = \"mahal\" \nh = 1 ; k = 100 ; nlv = \"5:15\"\nfm = lwplsravg(Xtrain, ytrain; nlvdis = nlvdis,\n    metric = metric, h = h, k = k, nlv = nlv) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nf, ax = scatter(vec(res.pred), ytest)\nablines!(ax, 0, 1)\nf\n\nfm = lwplsravg(Xtrain, ytrain; nlvdis = nlvdis,\n    metric = metric, h = h, k = k, nlv = nlv,\n    typf = \"cv\") ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsrda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsrda","text":"lwplsrda(X, y; nlvdis, metric, h, k, nlv, tol = 1e-4,\n    scal = false, verbose = false)\n\nkNN-LWPLSR-DA models.\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the    global PLS used for the dimension reduction before    calculating the dissimilarities. If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : Nb. latent variables (LVs).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same methodology as for lwplsr except that  PLSR is replaced by PLSR-DA.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 100\nnlv = 15\nfm = lwplsrda(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k, nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsrda_s-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsrda_s","text":"lwplsrda_s(X, y; nlv0, reduc = \"pls\", \n    metric = \"eucl\", h, k, gamma = 1, psamp = 1, samp = \"cla\", \n    nlv, tol = 1e-4, scal = false, verbose = false)\n\nkNN-LWPLSR-DA after preliminary (linear or non-linear) dimension      reduction (kNN-LWPLSR-DA-S).\n\nX : X-data (n, p).\ny : Univariate class membership.\nnlv0 : Nb. latent variables (LVs) for preliminary dimension reduction. \nreduc : Type of dimension reduction. Possible values are:   \"pca\" (PCA), \"pls\" (PLS; default), \"dkpls\" (direct Gaussian kernel PLS).\nmetric : Type of dissimilarity used to select the neighbors and compute   the weights. Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\ngamma : Scale parameter for the Gaussian kernel when a KPLS is used    for dimension reduction. See function krbf.\npsamp : Proportion of observations sampled in X, Yto compute the    loadings used to compute the scores.\nsamp : Type of sampling applied for psamp. Possible values are    \"cla\" (stratified random sampling over the classes in y) or \"random\" (random sampling). \nnlv : Nb. latent variables (LVs) for the models fitted on preliminary    scores.\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same principle as function lwplsr_s except that, locally, PLSR-DA models are fitted instead of PLSR models.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nfm = lwmlrda_s(Xtrain, ytrain; nlv0 = 20, \n    reduc = \"pca\", metric = \"eucl\", h = 2, k = 100,\n    nlv = 10) ;\npred = Jchemo.predict(fm, Xtest).pred\nerr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.lwplsrdaavg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.lwplsrdaavg","text":"lwplsrdaavg(X, y; nlvdis, metric, h, k, nlv, \n    tol = 1e-4, scal = false, verbose = false)\n\nAveraging of kNN-LWPLSR-DA models with different numbers of      latent variables (LVs).\n\nX : X-data.\ny : y-data (class membership).\nnlvdis : Number of latent variables (LVs) to consider in the global PLS    used for the dimension reduction before calculating the dissimilarities.    If nlvdis = 0, there is no dimension reduction.\nmetric : Type of dissimilarity used to select the neighbors.    Possible values are \"eucl\" (default; Euclidean distance)    and \"mahal\" (Mahalanobis distance).\nh : A scalar defining the shape of the weight function. Lower is h,    sharper is the function. See function wdist.\nk : The number of nearest neighbors to select for each observation to predict.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to    the single model with 10 LVs).\ntol : For stabilization when very close neighbors.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.   The scaling is implemented for the global (distances) and local (i.e. inside   each neighborhood) computations.\nverbose : If true, fitting information are printed.\n\nThis is the same methodology as for lwplsravg except that  PLSR is replaced by PLSR-DA, and the mean is replaced by votes.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the most occurent class within the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nExamples\n\nusing JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlvdis = 25 ; metric = \"mahal\"\nh = 2 ; k = 100\nnlv = \"0:20\"\nfm = lwplsrdaavg(Xtrain, ytrain;\n    nlvdis = nlvdis, metric = metric,\n    h = h, k = k, nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nres.listnn\nres.listd\nres.listw\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mad-Tuple{Any}","page":"Index of functions","title":"Jchemo.mad","text":"mad(x)\n\nCompute the median absolute deviation (MAD), adjusted by a factor (1.4826) for asymptotically normal consistency. \n\nExamples\n\nx = rand(100)\nmad(x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mahsq-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mahsq","text":"mahsq(X, Y)\nmahsq(X, Y, Sinv)\n\nSquared Mahalanobis distances  between the rows of X and Y.\n\nX : Data.\nY : Data.\nSinv : Inverse of a covariance matrix S.   If not given, this is the uncorrected covariance matrix of X.\n\nWhen X and Yare (n, p) and (m, p), repectively, it returns an object (n, m) with:\n\ni, j = distance between row i of X and row j of Y.\n\nExamples\n\nX = rand(5, 3)\nY = rand(2, 3)\n\nmahsq(X, Y)\n\nS = cov(X, corrected = false)\nSinv = inv(S)\nmahsq(X, Y, Sinv)\nmahsq(X[1:1, :], Y[1:1, :], Sinv)\n\nmahsq(X[:, 1], 4)\nmahsq(1, 4, 2.1)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mahsqchol-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mahsqchol","text":"mahsqchol(X, Y)\nmahsqchol(X, Y, Uinv)\n\nCompute the squared Mahalanobis distances (with a Cholesky factorization) between the observations (rows) of X and Y.\n\nX : Data.\nY : Data.\nUinv : Inverse of the upper matrix of a Cholesky factorization    of a covariance matrix S.   If not given, the factorization is done on S, the uncorrected covariance matrix of X.\n\nWhen X and Yare (n, p) and (m, p), repectively, it returns an object (n, m) with:\n\ni, j = distance between row i of X and row j of Y.\n\nExamples\n\nX = rand(5, 3)\nY = rand(2, 3)\n\nmahsqchol(X, Y)\n\nU = cholesky(Hermitian(S)).U \nUinv = inv(U)\nmahsqchol(X, Y, Uinv)\nmahsq(X[1:1, :], Y[1:1, :], Sinv)\n\nmahsqchol(X[:, 1], 4)\nmahsqchol(1, 4, sqrt(2.1))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.matB","page":"Index of functions","title":"Jchemo.matB","text":"matB(X, y; fun = mean)\n\nCompute the between covariance matrix (\"B\") of X.\n\nX : X-data (n, p).\ny : A vector (n) defing the class memberships.\n\nExamples\n\nn = 10 ; p = 3\nX = rand(n, p)\nX\ny = rand(1:3, n)\n#y = [3 ; ones(n - 2) ; 10]\nres = matB(X, y)\nres.B\nres.lev\nres.ni\n\nres = matW(X, y)\nres.W \nres.Wi\n\nmatW(X, y).W + matB(X, y).B \ncov(X; corrected = false)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.matW","page":"Index of functions","title":"Jchemo.matW","text":"matW(X, y; fun = mean)\n\nCompute the within covariance matrix (\"W\") of X.\n\nX : X-data (n, p).\ny : A vector (n) defing the class memberships.\n\nIf class \"i\" contains only one observation,  W_i is computed as cov(X; corrected = false).\n\nFor examples, see ?matB. \n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mavg-Tuple{Any}","page":"Index of functions","title":"Jchemo.mavg","text":"mavg(X; f)\nmavg!(X::Matrix; f)\n\nMoving averages smoothing of each row of X-data.\n\nX : X-data.\nf : Size (nb. points involved) of the filter.\n\nThe smoothing is computed by convolution (with padding), with function  imfilter of package ImageFiltering.jl. The centered kernel is ones(f) / f. Each returned point is located on the center of the kernel.\n\nReferences\n\nPackage ImageFiltering.jl https://github.com/JuliaImages/ImageFiltering.jl\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nXp = mavg(X; f = 10) \nplotsp(Xp[1:30, :], wl_num).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mavg_runmean-Tuple{Any}","page":"Index of functions","title":"Jchemo.mavg_runmean","text":"mavg_runmean(X, f)\nmavg_runmean!(M::Matrix, X::Matrix; f)\n\nMoving average smoothing of each row of a matrix X.\n\nX : X-data (n, p).\nM : Pre-allocated output matrix (n, p - f + 1).\nf : Size (nb. points involved) of the filter.\n\nThe smoothing is computed by convolution, without padding (which reduces  the column dimension). The function is an adaptation/simplification of function  runmean (V. G. Gumennyy) of package Indicators.jl. See https://github.com/dysonance/Indicators.jl/blob/a449c1d68487c3a8fea0008f7abb3e068552aa08/src/run.jl. The kernel is ones(f) / f. Each returned point is located on the 1st unit of the kernel. In general, this function can be faster than mavg, especialy for in-place versions.\n\nThe in-place function stores the output in M.\n\nReferences\n\nPackage Indicators.jl https://github.com/dysonance/Indicators.jl\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nXp = mavg_runmean(X; f = 10) \nplotsp(Xp[1:30, :]).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mblock-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mblock","text":"mblock(X, listbl)\n\nMake blocks from a matrix.\n\nX : X-data.\nlistbl : A vector whose each component defines the colum numbers   defining a block in X. The length of listbl is the number   of blocks.\n\nThe function returns a list (vector) of blocks.\n\nExamples\n\nn = 5 ; p = 10 \nX = rand(n, p) \nlistbl = [3:4, 1, [6; 8:10]]\n\nXbl = mblock(X, listbl)\nXbl[1]\nXbl[2]\nXbl[3]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mbpca","page":"Index of functions","title":"Jchemo.mbpca","text":"mbpca(Xbl, weights = ones(nro(Xbl[1])); nlv,\n    bscal = \"none\", tol = sqrt(eps(1.)), maxit = 200,\n    scal = false)\nmbpca!(Xbl, weights = ones(nro(Xbl[1])); nlv,\n    bscal = \"none\", tol = sqrt(eps(1.)), maxit = 200,\n    scal = false)\n\nConsensus principal components analysis (CPCA = MBPCA).\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\", \"mfa\").    See functions blockscal.\ntol : Tolerance value for convergence.\nmaxit : Maximum number of iterations.\nscal : Boolean. If true, each column of blocks in Xbl    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThe global scores are equal to the scores of the PCA of  the horizontal concatenation X = [X1 X2 ... Xk].\n\nThe function returns several objects, in particular:\n\nT : The non normed global scores.\nU : The normed global scores.\nW : The global loadings.\nTbl : The block scores (grouped by blocks, in the original scale).\nTb : The block scores (grouped by LV, in the metric scale).\nWbl : The block loadings.\nlb : The specific weights \"lambda\".\nmu : The sum of the specific weights (= eigen value of the global PCA).\n\nFunction summary returns: \n\nexplvarx : Proportion of the total inertia of X (sum of the squared norms of the    blocks) explained by each global score.\ncontr_block : Contribution of each block to the global scores \nexplX : Proportion of the inertia of the blocks explained by each global score.\ncorx2t : Correlation between the global scores and the original variables.  \ncortb2t : Correlation between the global scores and the block scores.\nrv : RV coefficient. \nlg : Lg coefficient. \n\nReferences\n\nMangamana, E.T., Cariou, V., Vigneau, E., Glèlè Kakaï, R.L., Qannari, E.M., 2019.  Unsupervised multiblock data analysis: A unified approach and extensions. Chemometrics and  Intelligent Laboratory Systems 194, 103856. https://doi.org/10.1016/j.chemolab.2019.103856\n\nWesterhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis of multiblock and hierarchical  PCA and PLS models. Journal of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5<301::AID-CEM515>3.0.CO;2-S\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ham.jld2\") \n@load db dat\npnames(dat) \n\nX = dat.X\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X, listbl)\n# \"New\" = first two rows of Xbl \nXbl_new = mblock(X[1:2, :], listbl)\n\nbscal = \"frob\"\nfm = mbpca(Xbl; nlv = 4, bscal = bscal) ;\nfm.U\nfm.T\nJchemo.transform(fm, Xbl).T\nJchemo.transform(fm, Xbl_new).T \n\nres = summary(fm, Xbl) ;\nfm.lb\nrowsum(fm.lb)\nfm.mu\nres.explvarx\nres.explX # = fm.lb if bscal = \"frob\"\nrowsum(Matrix(res.explX))\nres.contr_block\nres.corx2t \nres.cortb2t\nres.rv\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mbplswest","page":"Index of functions","title":"Jchemo.mbplswest","text":"mbplswest(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, \n    bscal = \"none\", tol = sqrt(eps(1.)), maxit = 200, \n    scal = false)\nmbplswest!(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, \n    bscal = \"none\", tol = sqrt(eps(1.)), maxit = 200, \n    scal = false)\n\nMultiblock PLSR - Nipals algorithm (Westerhuis et al. 1998).\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nY : Y-data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nbscal : Type of Xbl block scaling (\"none\", \"frob\").    See functions blockscal.\ntol : Tolerance value for convergence.\nmaxit : Maximum number of iterations.\nscal : Boolean. If true, each column of blocks in Xbl and    of Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nMBPLSR is equivalent to the the PLSR (X, Y) where X is the horizontal  concatenation of the blocks in Xbl. The function gives the same results as function mbplsr.\n\nReferences\n\nWesterhuis, J.A., Kourti, T., MacGregor, J.F., 1998. Analysis of multiblock and hierarchical  PCA and PLS models. Journal of Chemometrics 12, 301–321.  https://doi.org/10.1002/(SICI)1099-128X(199809/10)12:5<301::AID-CEM515>3.0.CO;2-S\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ham.jld2\") \n@load db dat\npnames(dat) \n\nX = dat.X\nY = dat.Y\ny = dat.Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X, listbl)\n# \"New\" = first two rows of Xbl \nXbl_new = mblock(X[1:2, :], listbl)\n\nbscal = \"none\"\nnlv = 5\nfm = mbplswest(Xbl, y; nlv = nlv, bscal = bscal) ;\npnames(fm)\nfm.T\nJchemo.transform(fm, Xbl_new)\n[y Jchemo.predict(fm, Xbl).pred]\nJchemo.predict(fm, Xbl_new).pred\n\nsummary(fm, Xbl)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mbunif","page":"Index of functions","title":"Jchemo.mbunif","text":"mbunif(Xbl, weights = ones(nro(Xbl[1])); nlv,\n    bscal = \"none\", tau = 1e-8, wcov = false, deflat = \"global\", \n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\nmbunif!(Xbl, weights = ones(nro(Xbl[1])); nlv,\n    bscal = \"none\", tau = 1e-8, wcov = false, deflat = \"global\", \n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\n\nUnified multiblock analysis of Mangana et al. 2019.\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\", \"mfa\").    See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\nwcov : If false, the global score is proportionnal to the sum of    the block scores. If true, it is proportionnal to the weighted    sum of the block scores (with weights proportionnal to the covariances   between the block scores and the global scores).\ndeflat : Possible values are \"global (deflation to the global scores)   or \"can\" (deflation to the block scores).\ntol : Tolerance value for convergence.\nmaxit : Maximum number of iterations.\nscal : Boolean. If true, each column of blocks in Xbl    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nSee Mangana et al. 2019. The regularization parameter tau is defined as \"1 - gamma\" in  Managana et al. 2019, section 2.1.3.\n\nValue tau = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. tau = 1e-8)  to get similar results as with pseudo-inverses.  \n\nReferences\n\nMangamana, E.T., Cariou, V., Vigneau, E., Glèlè Kakaï, R.L., Qannari, E.M., 2019.  Unsupervised multiblock data analysis: A unified approach and extensions. Chemometrics and  Intelligent Laboratory Systems 194, 103856. https://doi.org/10.1016/j.chemolab.2019.103856\n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nXbl = [dat.X, dat.Y]\n\ntau = 1e-8\nfm = mbunif(Xbl; nlv = 3, tau = tau)\npnames(fm)\n\nfm.T\ntransform(fm, Xbl).T\n\nres = summary(fm, Xbl)\npnames(res)\n\n## MBPCA\nfm = mbunif(Xbl; nlv = 3,\n    tau = 1, wcov = false, deflat = \"global\") ;\n\n## ComDim\nfm = mbunif(Xbl; nlv = 3,\n    tau = 1, wcov = true, deflat = \"global\") ;\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mbwcov","page":"Index of functions","title":"Jchemo.mbwcov","text":"mbwcov(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, \n    bscal = \"none\", wcov = true, tau = 1,\n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\nmbwcov!(Xbl, Y, weights = ones(nro(Xbl[1])); nlv, \n    bscal = \"none\", wcov = true, tau = 1,\n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\n\nMultiblock weighted covariate analysis regression (MBWCov) (Mangana et al. 2021)\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nY : Y-data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\nwcov : Logical. If true (default), a MBWCov is done, else   a MBPLSR is done.\ntau : Regularization parameter (∊ [0, 1]).     \ntol : Tolerance value for convergence.\nmaxit : Maximum number of iterations.\nscal : Boolean. If true, each column of blocks in Xbl and    of Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nSee Mangamana et al. 2021.\n\nThe regularization is implemented as a continuum. After block centering  and scaling, the block covariances matrices (k = 1,...,K blocks)  are computed as follows: \n\nCk = (1 - tau) * Xk' D Xk + tau * Ik\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. tau = 1e-8)  to get similar results as with pseudo-inverses.   \n\nWhen tau = 1, this is the PLS framework.\nWhen tau ~ 0, this is the redundancy analysis (RA) framework.\n\nReferences\n\nTchandao Mangamana, E., Glèlè Kakaï, R., Qannari, E.M., 2021. A general strategy for setting  up supervised methods of multiblock data analysis. Chemometrics and Intelligent Laboratory Systems 217, \n\nhttps://doi.org/10.1016/j.chemolab.2021.104388\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ham.jld2\") \n@load db dat\npnames(dat) \n\nX = dat.X\nY = dat.Y\ny = dat.Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X, listbl)\n# \"New\" = first two rows of Xbl \nXbl_new = mblock(X[1:2, :], listbl)\n\nbscal = \"none\"\nnlv = 5\nfm = mbwcov(Xbl, y; nlv = nlv, bscal = bscal) ;\npnames(fm)\nfm.T\nJchemo.transform(fm, Xbl_new)\n[y Jchemo.predict(fm, Xbl).pred]\nJchemo.predict(fm, Xbl_new).pred\n\nsummary(fm, Xbl)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mlev-Tuple{Any}","page":"Index of functions","title":"Jchemo.mlev","text":"mlev(x)\n\nReturn the sorted levels of a dataset. \n\nExamples\n\nx = rand([\"a\";\"b\";\"c\"], 20)\nlev = mlev(x)\nnlev = length(lev)\n\nX = reshape(x, 5, 4)\nmlev(X)\n\ndf = DataFrame(g1 = rand(1:2, n), \n    g2 = rand([\"a\"; \"c\"], n))\nmlev(df)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mlr","page":"Index of functions","title":"Jchemo.mlr","text":"mlr(X, Y, weights = ones(nro(X)); noint = false)\nmlr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); noint::Bool = false)\n\nCompute a mutiple linear regression model (MLR) by using the QR algorithm.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nnoint : Define if the model is computed with an intercept or not.\n\nSafe but can be little slower than other methods.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = Matrix(dat.X[:, 2:4]) \ny = dat.X[:, 1]\nn = nro(X)\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nfm = mlr(Xtrain, ytrain) ;\n#fm = mlrchol(Xtrain, ytrain) ;\n#fm = mlrpinv(Xtrain, ytrain) ;\n#fm = mlrpinvn(Xtrain, ytrain) ;\npnames(fm)\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\nzcoef = Jchemo.coef(fm) \nzcoef.int \nzcoef.B \n\nfm = mlr(Xtrain, ytrain; noint = true) ;\nzcoef = Jchemo.coef(fm) \nzcoef.int \nzcoef.B\n\nfm = mlr(Xtrain[:, 1], ytrain) ;\n#fm = mlrvec(Xtrain[:, 1], ytrain) ;\nzcoef = Jchemo.coef(fm) \nzcoef.int \nzcoef.B\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mlrchol","page":"Index of functions","title":"Jchemo.mlrchol","text":"mlrchol(X, Y, weights = ones(nro(X)))\nmlrchol!(X::Matrix, Y::Matrix, weights = ones(nro(X)))\n\nCompute a mutiple linear regression model (MLR)  using the Normal equations and a Choleski factorization.\n\nX : X-data, with nb. columns >= 2 (required by function cholesky).\nY : Y-data.\nweights : Weights of the observations. Internally normalized to sum to 1. \n\nCompute a model with intercept.\n\nFaster but can be less accurate (squared element X'X).\n\nSee ?mlr for examples.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mlrda","page":"Index of functions","title":"Jchemo.mlrda","text":"mlrda(X, y, weights = ones(nro(X)))\n\nDiscrimination based on multple linear regression (MLR-DA).\n\nX : X-data.\ny : Univariate class membership.\nweights : Weights of the observations. Internally normalized to sum to 1. \n\nThe training variable y (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in y. Each column of Ydummy is a dummy (0/1) variable.  Then, a multiple linear regression (MLR) is run between the X and and each column  of Ydummy, returning predictions of the dummy variables (= object posterior  returned by fuction predict).   These predictions can be  considered as unbounded  estimates (i.e. eventuall outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.\n\nExamples\n\nusing JchemoData, JLD2, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\n\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\ntab(ytrain)\ntab(ytest)\n\nfm = mlrda(Xtrain, ytrain) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.posterior\nres.pred\nerr(res.pred, ytest)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mlrpinv","page":"Index of functions","title":"Jchemo.mlrpinv","text":"mlrpinv(X, Y, weights = ones(nro(X)); noint = false)\nmlrpinv!(X::Matrix, Y::Matrix, weights = ones(nro(X)); noint::Bool = false)\n\nCompute a mutiple linear regression model (MLR)  by using a pseudo-inverse. \n\nX : X-data.\nY : Y-data.\nweights : Weights of the observations. Internally normalized to sum to 1. \nnoint : Define if the model is computed with an intercept or not.\n\nSafe but can be slower.  \n\nSee ?mlr for examples.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mlrpinvn","page":"Index of functions","title":"Jchemo.mlrpinvn","text":"mlrpinvn(X, Y, weights = ones(nro(X)))\nmlrpinvn!(X::Matrix, Y::Matrix, weights = ones(nro(X)))\n\nCompute a mutiple linear regression model (MLR)  by using the Normal equations and a pseudo-inverse.\n\nX : X-data.\nY : Y-data.\nweights : Weights of the observations. Internally normalized to sum to 1. \n\nSafe and fast for p not too large.\n\nCompute a model with intercept.\n\nSee ?mlr for examples.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mpar","page":"Index of functions","title":"Jchemo.mpar","text":"mpar(; kwargs...)\n\nReturn a tuple with all the combinations of the parameter values defined in kwargs.\n\nkwargs : vector(s) of the parameter(s) values.\n\nExamples\n\nnlvdis = 25 ; metric = [\"mahal\"] \nh = [1 ; 2 ; Inf] ; k = [500 ; 1000] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) \nlength(pars[1])\nreduce(hcat, pars)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mse-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.mse","text":"mse(pred, Y; digits = 3)\n\nSummary of model performance for regression.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nmse(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nmse(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.msep-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.msep","text":"msep(pred, Y)\n\nCompute the mean of the squared prediction errors (MSEP).\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nmsep(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.mtest","page":"Index of functions","title":"Jchemo.mtest","text":"mtest(Y::DataFrame, id = 1:nro(Y); test, rep = 1)\n\nSelect indexes defining training and test sets for each column      of a dataframe.\n\nY : DataFrame (n, p). Typically responses variables to predict.    Missing values are allowed.\nid : Vector (n) of IDs.\ntest : Nb. (if Int64) or proportion (if Float64)   of observations in each test set, within the non-missing    observations of the considered Y column.\nrep : Nb. repetitions of training and test sets for each Y column.\n\nExamples\n\nusing DataFrames\n\nY = hcat([rand(5); missing; rand(6)],\n   [rand(2); missing; missing; rand(7); missing])\nY = DataFrame(Y, :auto)\nn = nro(Y)\n\nres = mtest(Y; test = 3, rep = 4) ;\n#res = mtest(Y; test = .3, rep = 4) ;\n#res = mtest(Y, string.(1:n); test = 3, rep = 4) ;\npnames(res)\nres.nam\nlength(res.test)\nlength(res.test[1])\ni = 1\nres.test[i]\nres.train[i]\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.mweight-Tuple{Any}","page":"Index of functions","title":"Jchemo.mweight","text":"mweight(w)\n\nReturn a vector of weights that sums to 1.\n\nExamples\n\nx = rand(10)\nw = mweight(x)\nsum(w)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nco-Tuple{Any}","page":"Index of functions","title":"Jchemo.nco","text":"nco(X)\n\nReturn the nb. columns of X.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nipals-Tuple{Any}","page":"Index of functions","title":"Jchemo.nipals","text":"nipals(X; tol = sqrt(eps(1.)), maxit = 200)\n\nNipals to compute the first score and loading vectors of a matrix.\n\nX : X-data (n, p).\ntol : Tolerance value for stopping the iterations.\nmaxit : Maximum nb. iterations.\n\nThe function finds {u, v, s} = argmin(||X - u * s * v'||), with the constraints  ||u|| = ||v|| = 1.\n\nX ~ u * s * v', where:\n\nu, v : left and right singular vectors (scores and loadings, repectively)\ns : singular value.\n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip,  Paris, France.\n\nExamples\n\nX = rand(5, 3)\n\nres = nipals(X)\nres.u\nsvd(X).U[:, 1] \nres.v\nsvd(X).V[:, 1] \nres.sv\nsvd(X).S[1] \nres.niter\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.normw-Tuple{Any, Vector}","page":"Index of functions","title":"Jchemo.normw","text":"normw(x, w)\n\nReturn the squared weighted norm of a vector.\n\nx : A vector (n).\nw : Weights (n) of the observations.\n\nw is internally normalized to sum to 1.\n\nThe squared weighted norm of vector x is:\n\nx' * D * x, where D is the diagonal matrix of vector w.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.nro-Tuple{Any}","page":"Index of functions","title":"Jchemo.nro","text":"nro(X)\n\nReturn the nb. rows of X.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occknndis-Tuple{Any}","page":"Index of functions","title":"Jchemo.occknndis","text":"occknndis(X; nlv, nsamp, k, \n    typc = \"mad\", cri = 3, alpha = .025,\n    scal = false, kwargs...)\n\nOne-class classification using global k-nearest neighbors distances.\n\nX : X-data (training).\nnlv : Nb. PCA components for the distances computations.\nnsamp : Nb. of observations (rows) sampled in the training X   used to compute the H0 empirical distribution of outlierness.\nk : Nb. of neighbors used to compute the outlierness.\ntypc : Type of cutoff (\"mad\" or \"q\"). See Thereafter.\ncri : When typc = \"mad\", a constant. See thereafter.\nalpha : When typc = \"q\", a risk-I level. See thereafter.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\nkwargs : Optional arguments to pass in function kde of KernelDensity.jl   (see function kde1).\n\nLet us note q a given observation, and o[q] a neighbor of q  within the training data X. The k nearest neighbors of q define the neighborhood NNk(q) = {o.1[q], ...., o.k[q]}  (if q belongs to the training X, q is removed from NNk(q)). \n\nThe global outlierness of any observation q relatively to X, say dk(q), is computed as the median distance to NNk(q):\n\ndk(q) = median{dist(q, o.j[q]), j = 1,...,k}.\n\nOutlierness dk(q) is then compared to the outlierness distribution estimated for the training data X, say distribution H0.    If dk(q) is extreme compared to H0, observation q may come from a  different distribution than the training data X.\n\nH0 is estimated by Monte Carlo, as follows:\n\nA number of nsamp observations (rows) are sampled without replacement    within the training data X.\nFor each of these nsamp training observations, say q.j {j = 1, ..., nsamp},   outlierness dk(q.j) is computed. This returns a vector of nsamp    outlierness values {dk(q.j), j = 1,...,nsamp}. \nThis vector defines the empirical outlierness distribution of    observations assumed to come from the same distribution as    the training data X (\"hypothesis H0\"). \n\nThen, function predict computes outlierness dk(q) for each  new observation q.       \n\nIn the function, distances are computed as Mahalanobis distances in a  PCA score space (internally computed), cf. argument nlv.\n\nSee ?occsd for details on outputs.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X    \nY = dat.Y\nf = 21 ; pol = 3 ; d = 2 ;\nXp = savgol(snv(X); f = f, pol = pol, d = d) \ns = Bool.(Y.test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\n\ng1 = \"EHH\" ; g2 = \"PEE\"\n#g1 = \"EHH\" ; g2 = \"EHH\"\ns1 = Ytrain.typ .== g1\ns2 = Ytest.typ .== g2\nzXtrain = Xtrain[s1, :]    \nzXtest = Xtest[s2, :] \nntrain = nro(zXtrain)\nntest = nro(zXtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\nfm = pcasvd(zXtrain, nlv = 5) ; \nTtrain = fm.T\nTtest = Jchemo.transform(fm, zXtest)\nT = vcat(Ttrain, Ttest)\ngroup = vcat(repeat([\"0-Train\"], ntrain), repeat([\"1-Test\"], ntest))\ni = 1\nplotxy(T[:, i:(i + 1)], group;\n    xlabel = string(\"PC\", i), ylabel = string(\"PC\", i + 1)).f\n\n#### End data\n\nnlv = 30\nnsamp = 300\nk = round(.7 * ntrain)\nfm = occknndis(zXtrain; nlv = nlv, \n    nsamp = nsamp, k = k) ;\nfm.d\nhist(fm.d.dstand; bins = 50)\n\nres = Jchemo.predict(fm, zXtest) ;\nres.d\nres.pred\ntab(res.pred)\n\nd1 = fm.d.dstand\nd2 = res.d.dstand\nd = vcat(d1, d2)\ngroup = [repeat([\"0-Train\"], length(d1)); repeat([\"1-Test\"], length(d2))]\nf, ax = plotxy(1:length(d), d, group; \n    resolution = (600, 400), xlabel = \"Obs. index\", \n    ylabel = \"Standardized distance\")\nhlines!(ax, 1)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occlknndis-Tuple{Any}","page":"Index of functions","title":"Jchemo.occlknndis","text":"occlknndis(X; nlv, nsamp, k, \n    typc = \"mad\", cri = 3, alpha = .025,\n    scal = false, kwargs...)\n\nOne-class classification using local k-nearest neighbors distances.\n\nX : X-data (training).\nnlv : Nb. PCA components for the distances computations.\nnsamp : Nb. of observations (rows) sampled in the training X   used to compute the H0 empirical distribution of outlierness.\nk : Nb. of neighbors used to compute the outlierness.\ntypc : Type of cutoff (\"mad\" or \"q\"). See Thereafter.\ncri : When typc = \"mad\", a constant. See thereafter.\nalpha : When typc = \"q\", a risk-I level. See thereafter.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\nkwargs : Optional arguments to pass in function kde of KernelDensity.jl   (see function kde1).\n\nLet us note q a given observation, and o[q] a neighbor of q  within the training data X. The k nearest neighbors of q define the neighborhood NNk(q) = {o.1[q], ...., o.k[q]}  (if q belongs to the training X, q is removed from NNk(q)). \n\nThe median distance of any observation q to its neighborhood NNk(q) is dk(q) = median{dist(q, o.j[q]), j = 1,...,k}, referred to as global outlerness in function occknndis.\n\nThe local outlierness of any observation q relatively to X,  say ldk(q), is computed as follows:\n\nNNk(q) is selected and dk(q) is computed.\nFor each neighbor o.j[q] in NNk(q), global outlierness dk(o.j[q]) is    computed. This returns the vector {dk(o.j[q]), j = 1,...,k}.\nThe local outlierness of q is then computed as    dk(q) / median{dk(o.j[q]), j = 1,...,k}.\n\nOutlierness ldk(q) is then compared to the outlierness distribution estimated for the training data X, say distribution H0.    If ldk(q) is extreme compared to H0, observation q may come from a  different distribution than the training data X.\n\nH0 is estimated by Monte Carlo, as follows:\n\nA number of nsamp observations (rows) are sampled without replacement    within the training data X.\nFor each of these nsamp training observations, say q.j {j = 1,...,nsamp},   outlierness ldk(q.j) is computed. This returns a vector of nsamp    outlierness values {ldk(q.1), j = 1,...,nsamp}. \nThis vector defines the empirical outlierness distribution of    observations assumed to come from the same distribution as    the training data X (hypothesis H0). \n\nThen, function predict computes outlierness ldk(q) for each  new observation q. \n\nIn the function, distances are computed as Mahalanobis distances in a  PCA score space (internally computed), cf. argument nlv.\n\nSee ?occsd for details on outputs.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X    \nY = dat.Y\nf = 21 ; pol = 3 ; d = 2 ;\nXp = savgol(snv(X); f = f, pol = pol, d = d) \ns = Bool.(Y.test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\n\ng1 = \"EHH\" ; g2 = \"PEE\"\n#g1 = \"EHH\" ; g2 = \"EHH\"\ns1 = Ytrain.typ .== g1\ns2 = Ytest.typ .== g2\nzXtrain = Xtrain[s1, :]    \nzXtest = Xtest[s2, :] \nntrain = nro(zXtrain)\nntest = nro(zXtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\nfm = pcasvd(zXtrain, nlv = 5) ; \nTtrain = fm.T\nTtest = Jchemo.transform(fm, zXtest)\nT = vcat(Ttrain, Ttest)\ngroup = vcat(repeat([\"0-Train\"], ntrain), repeat([\"1-Test\"], ntest))\ni = 1\nplotxy(T[:, i:(i + 1)], group;\n    xlabel = string(\"PC\", i), ylabel = string(\"PC\", i + 1)).f\n\n#### End data\n\nnlv = 30\nnsamp = 50 ; k = 5\nfm = Jchemo.occlknndis(zXtrain; nlv = nlv, \n    nsamp = nsamp, k = k, typc = \"mad\") ;\nfm.d\nhist(fm.d.dstand; bins = 50)\n\nres = Jchemo.predict(fm, zXtest) ;\nres.d\nres.pred\ntab(res.pred)\n\nd1 = fm.d.dstand\nd2 = res.d.dstand\nd = vcat(d1, d2)\ngroup = [repeat([\"0-Train\"], length(d1)); repeat([\"1-Test\"], length(d2))]\nf, ax = plotxy(1:length(d), d, group; \n    resolution = (600, 400), xlabel = \"Obs. index\", \n    ylabel = \"Standardized distance\")\nhlines!(ax, 1)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}","page":"Index of functions","title":"Jchemo.occod","text":"occod(object::Union{Pca, Plsr}, X; nlv = nothing, \n    typc = \"mad\", cri = 3, alpha = .025, kwargs...)\n\nOne-class classification using PCA/PLS orthognal distance (OD).\n\nobject : The model (e.g. PCA) that was fitted on the training data,   assumed to represent the training class.\nX : X-data (training) that were used to fit the model.\nnlv : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components of the fitted model.\ntypc : Type of cutoff (\"mad\" or \"q\"). See Thereafter.\ncri : When typc = \"mad\", a constant. See thereafter.\nalpha : When typc = \"q\", a risk-I level. See thereafter.\nkwargs : Optional arguments to pass in function kde of    KernelDensity.jl (see function kde1).\n\nIn this method, the outlierness d of an observation is the orthogonal distance (OD =  \"X-residuals\") of this observation, ie. the Euclidean distance between the observation and its projection on the  score plan defined by the fitted (e.g. PCA) model (e.g. Hubert et al. 2005,  Van Branden & Hubert 2005 p. 66, Varmuza & Filzmoser 2009 p. 79).\n\nSee ?occsd for details on outputs, and examples.\n\nReferences\n\nM. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach  to robust principal components analysis. Technometrics, 47, 64-79.\n\nK. Vanden Branden, M. Hubert (2005). Robuts classification in high dimension based  on the SIMCA method. Chem. Lab. Int. Syst, 79, 10-21.\n\nK. Varmuza, P. Filzmoser (2009). Introduction to multivariate statistical analysis  in chemometrics. CRC Press, Boca Raton.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occsd-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}}","page":"Index of functions","title":"Jchemo.occsd","text":"occsd(object::Union{Pca, Plsr}; nlv = nothing,\n    typc = \"mad\", cri = 3, alpha = .025, kwargs...)\n\nOne-class classification using PCA/PLS score distance (SD).\n\nobject : The model (e.g. PCA) that was fitted on the training data,   assumed to represent the training class.\nnlv : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components of the fitted model.\ntypc : Type of cutoff (\"mad\" or \"q\"). See Thereafter.\ncri : When typc = \"mad\", a constant. See thereafter.\nalpha : When typc = \"q\", a risk-I level. See thereafter.\nkwargs : Optional arguments to pass in function kde of    KernelDensity.jl (see function kde1).\n\nIn this method, the outlierness d of an observation is defined by its  score distance (SD), ie. the Mahalanobis distance between the projection of  the observation on the score plan defined by the fitted (e.g. PCA) model and  the center of the score plan.\n\nIf a new observation has d higher than a given cutoff, the observation  is assumed to not belong to the training class.  The cutoff is computed with non-parametric heuristics.  Noting [d] the vector of outliernesses computed on the training class:\n\nIf typc = \"mad\", then cutoff = median([d]) + cri * mad([d]). \nIf typc = \"q\", thencutoffis estimated from the empirical cumulative   density function computed on [d], for a given risk-I (alpha`). \n\nAlternative approximate cutoffs have been proposed in the literature  (e.g.: Nomikos & MacGregor 1995, Hubert et al. 2005, Pomerantsev 2008). Typically and whatever the approximation method, it is recommended to tune  the cutoff, depending on detection objectives. \n\nOutputs\n\npval: Estimate of p-value (see functions kde1 and pval) computed    from the KDE of distribution [d], provided for each data observation. \ndstand: standardized distance defined as d / cutoff.    A value dstand > 1 may be considered as extreme compared to the distribution   of the training data.  Output gh is the Winisi \"GH\" (usually, GH > 3 is    considered as \"extreme\").\npred (fonction predict): class prediction\ndstand <= 1 ==> 0: the observation is expected to belong    the training class, \ndstand > 1  ==> 1: extreme value, possibly outside of the training class. \n\nReferences\n\nM. Hubert, P. J. Rousseeuw, K. Vanden Branden (2005). ROBPCA: a new approach to robust  principal components analysis. Technometrics, 47, 64-79.\n\nNomikos, P., MacGregor, J.F., 1995. Multivariate SPC Charts for Monitoring Batch Processes.  null 37, 41-59. https://doi.org/10.1080/00401706.1995.10485888\n\nPomerantsev, A.L., 2008. Acceptance areas for multivariate classification derived by  projection methods. Journal of Chemometrics 22, 601-609. https://doi.org/10.1002/cem.1147\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2018.jld2\") \n@load db dat\npnames(dat)\nX = dat.X    \nY = dat.Y\nf = 21 ; pol = 3 ; d = 2 ;\nXp = savgol(snv(X); f = f, pol = pol, d = d) \ns = Bool.(Y.test)\nXtrain = rmrow(Xp, s)\nYtrain = rmrow(Y, s)\nXtest = Xp[s, :]\nYtest = Y[s, :]\n\ng1 = \"EHH\" ; g2 = \"PEE\"\n#g1 = \"EHH\" ; g2 = \"EHH\"\ns1 = Ytrain.typ .== g1\ns2 = Ytest.typ .== g2\nzXtrain = Xtrain[s1, :]    \nzXtest = Xtest[s2, :] \nntrain = nro(zXtrain)\nntest = nro(zXtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\nfm = pcasvd(zXtrain, nlv = 5) ; \nTtrain = fm.T\nTtest = Jchemo.transform(fm, zXtest)\nT = vcat(Ttrain, Ttest)\ngroup = vcat(repeat([\"0-Train\"], ntrain), repeat([\"1-Test\"], ntest))\ni = 1\nplotxy(T[:, i:(i + 1)], group;\n    xlabel = string(\"PC\", i), ylabel = string(\"PC\", i + 1)).f\n\n#### End data\n\nnlv = 10\nfm0 = pcasvd(zXtrain; nlv = nlv) ;\n\nfm = occsd(fm0) ;\n#fm = occsd(fm0; typc = \"q\", alpha = .025) ;\n#fm = occod(fm0, zXtrain) ;\n#fm = occsdod(fm0, zXtrain) ;\n#fm = occstah(zXtrain)\nfm.d\nhist(fm.d.dstand; bins = 50)\n\nres = Jchemo.predict(fm, zXtest) ;\nres.d\nres.pred\ntab(res.pred)\n\nd1 = fm.d.dstand\nd2 = res.d.dstand\nd = vcat(d1, d2)\nf, ax = plotxy(1:length(d), d, group; \n    resolution = (600, 400), xlabel = \"Obs. index\", \n    ylabel = \"Standardized distance\")\nhlines!(ax, 1)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occsdod-Tuple{Union{Jchemo.Pca, Jchemo.Plsr}, Any}","page":"Index of functions","title":"Jchemo.occsdod","text":"occsdod(object::Union{Pca, Plsr}, X; \n    nlv_sd = nothing, nlv_od = nothing, \n    typc = \"mad\", cri = 3, alpha = .025, kwargs...)\n\nOne-class classification using a compromise between PCA/PLS score (SD) and      orthogonal (OD) distances.\n\nobject : The model (e.g. PCA) that was fitted on the training data,   assumed to represent the training class.\nX : X-data (training) that were used to fit the model.\nnlv_sd : Nb. components (PCs or LVs) to consider for SD. If nothing,    it is the maximum nb. of components of the fitted model.\nnlv_od : Nb. components (PCs or LVs) to consider for OD. If nothing,    it is the maximum nb. of components of the fitted model.\ntypc : Type of cutoff (\"mad\" or \"q\"). See Thereafter.\ncri : When typc = \"mad\", a constant. See thereafter.\nalpha : When typc = \"q\", a risk-I level. See thereafter.\nkwargs : Optional arguments to pass in function kde of    KernelDensity.jl (see function kde1).\n\nIn this method, the outlierness d of a given observation is a compromise between the score distance (SD) and the orthogonal distance (OD). The compromise is computed from the  standardized distances by: \n\ndstand = sqrt(dstand_sd * dstand_od).\n\nSee ?occsd for details on outputs, and examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.occstah-Tuple{Any}","page":"Index of functions","title":"Jchemo.occstah","text":"occstah(X; a = 2000, typc = \"mad\", cri = 3, \n    alpha = .025, scal = true, kwargs...)\n\nOne-class classification using the Stahel-Donoho outlierness.\n\nX : X-data (training).\na : Nb. dimensions simulated for the projection-pursuit method.\ntypc : Type of cutoff (\"mad\" or \"q\"). See Thereafter.\ncri : When typc = \"mad\", a constant. See thereafter.\nalpha : When typc = \"q\", a risk-I level. See thereafter.\nscal : Boolean. If true, matrix X is centred (by median)    and scaled (by MAD) before computing the outlierness.\nkwargs : Optional arguments to pass in function kde of    KernelDensity.jl (see function kde1).\n\nIn this method, the outlierness d of a given observation is the Stahel-Donoho outlierness (see ?stah).\n\nSee ?occsd for details on outputs, and examples. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.oob_baggr-Tuple{Jchemo.Baggr, Any, Any}","page":"Index of functions","title":"Jchemo.oob_baggr","text":"oob_baggr(object::Baggr, X, Y; score = rmsep)\n\nCompute the out-of-bag (OOB) error after bagging a regression model.\n\nobject : Output of a bagging.\nX : X-data used in the bagging.\nY : Y-data used in the bagging.\nscore : Function computing the prediction error (default: msep).\n\nSee ?baggr for examples.\n\nReferences\n\nBreiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140.  https://doi.org/10.1007/BF00058655 Breiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324 Genuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI. Gey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.out-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.out","text":"out(x)\n\nReturn if elements of a vector are strictly outside of a given range.\n\nx : Univariate data.\nlims : Limits (lower, upper) defining the range.\n\nReturn a BitVector.\n\nExamples\n\nx = [-200.; -100; -1; 0; 1; 200]\nout(x, (-1, 1))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pcaeigen","page":"Index of functions","title":"Jchemo.pcaeigen","text":"pcaeigen(X, weights = ones(nro(X)); nlv, scal = false)\npcaeigen!(X::Matrix, weights = ones(nro(X)); nlv, scal = false)\n\nPCA by Eigen factorization.\n\nX : X-data (n, p).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. principal components (PCs).\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nLet us note D the (n, n) diagonal matrix of weights and X the centered matrix in metric D.  The function minimizes ||X - T * P'||^2  in metric D, by  computing an Eigen factorization of X' * D * X. \n\nSee ?pcasvd for examples.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.pcaeigenk","page":"Index of functions","title":"Jchemo.pcaeigenk","text":"pcaeigenk(X, weights = ones(nro(X)); nlv, scal = false)\npcaeigenk!(X::Matrix, weights = ones(nro(X)); nlv, scal = false)\n\nPCA by Eigen factorization of the kernel form (XX').\n\nX : X-data (n, p).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. principal components (PCs).\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nThis is the \"kernel cross-product\" version of the PCA algorithm (e.g. Wu et al. 1997).  For wide matrices (n << p, where p is the nb. columns) and n not too large,  this algorithm can be much faster than the others.\n\nLet us note D the (n, n) diagonal matrix of weights and X the centered matrix in metric D.  The function minimizes ||X - T * P'||^2  in metric D, by  computing an Eigen factorization of D^(1/2) * X * X' D^(1/2).\n\nSee ?pcasvd for examples.\n\nReferences\n\nWu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA algorithms for wide data.  Part I: Theory and algorithms. Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.pcasph","page":"Index of functions","title":"Jchemo.pcasph","text":"pcasph(X, weights = ones(nro(X)); nlv, typc = \"medspa\", \n    delta = .001, scal = false)\npcasph!(X, weights = ones(nro(X)); nlv, typc = \"medspa\", \n    delta = .001, scal = false)\n\nSpherical PCA.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. principal components (PCs).\ntypc : Type of centering.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nSpherical PCA (Locantore et al. 1990, Maronna 2005, Daszykowski et al. 2007).  The spatial median used for centering matrix \u001bqn{X} is calculated by function Jchemo.colmedspa.\n\nReferences\n\nDaszykowski, M., Kaczmarek, K., Vander Heyden, Y., Walczak, B., 2007.  Robust statistics in data analysis - A review. Chemometrics and Intelligent  Laboratory Systems 85, 203-219. https://doi.org/10.1016/j.chemolab.2006.06.016\n\nLocantore N., Marron J.S., Simpson D.G., Tripoli N., Zhang J.T., Cohen K.L. Robust principal component analysis for functional data, Test 8 (1999) 1–7\n\nMaronna, R., 2005. Principal components and orthogonal regression based on  robust scales, Technometrics, 47:3, 264-273, DOI: 10.1198/004017005000000166\n\nExamples\n\nusing JLD2, JchemoData\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/octane.jld2\") \n@load db dat\npnames(dat)\n  \nX = dat.X \nwl = names(X)\nwl_num = parse.(Float64, wl)\nn = nro(X)\n\nnlv = 6\nfm = pcasph(X; nlv = nlv) ; \n#fm = pcasvd(X; nlv = nlv) ; \npnames(fm)\nT = fm.T\n\ni = 1\nplotxy(T[:, i:(i + 1)]; zeros = true,\n    xlabel = \"PC1\", ylabel = \"PC2\").f\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.pcasvd","page":"Index of functions","title":"Jchemo.pcasvd","text":"pcasvd(X, weights = ones(nro(X)); nlv, scal = false)\npcasvd!(X::Matrix, weights = ones(nro(X)); nlv, scal = false)\n\nPCA by SVD factorization.\n\nX : X-data (n, p). \nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. principal components (PCs).\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nLet us note D the (n, n) diagonal matrix of weights and X the centered matrix in metric D. The function minimizes ||X - T * P'||^2  in metric D, by  computing a SVD factorization of sqrt(D) * X:\n\nsqrt(D) * X ~ U * S * V'\n\nOutputs are:\n\nT = D^(-1/2) * U * S\nP = V\nThe diagonal of S   \n\nExamples\n\nusing JchemoData, JLD2, CairoMakie, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = dat.X[:, 1:4]\nn = nro(X)\n\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nXtest = rmrow(X, s)\n\nnlv = 3\nfm = pcasvd(Xtrain; nlv = nlv) ;\n#fm = pcaeigen(Xtrain; nlv = nlv) ;\n#fm = pcaeigenk(Xtrain; nlv = nlv) ;\npnames(fm)\nfm.T\nfm.T' * fm.T\nfm.P' * fm.P\n\nJchemo.transform(fm, Xtest)\n\nres = Base.summary(fm, Xtrain) ;\npnames(res)\nres.explvarx\nres.contr_var\nres.coord_var\nres.cor_circle\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.pcr","page":"Index of functions","title":"Jchemo.pcr","text":"pcr(X, Y, weights = ones(nro(X)); nlv,\n    scal = false)\npcr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    scal = false)\n\nPrincipal component regression (PCR) with a SVD factorization.\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X   is scaled by its uncorrected standard deviation.\n\nX and Y are internally centered. \n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nfm = pcr(Xtrain, ytrain; nlv = nlv) ;\npnames(fm)\nfm.T\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.B\nJchemo.coef(fm; nlv = 7).B\n\nfm_pca = fm.fm_pca ;\ntransform(fm_pca, Xtest)\ntransform(fm_pca, Xtest; nlv = 7)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f   \n\nres = Jchemo.predict(fm, Xtest; nlv = 1:2)\nres.pred[1]\nres.pred[2]\n\n# See ?pcasvd\nres = Base.summary(fm_pca, Xtrain)\nres.explvarx\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plotconf-Tuple{Any}","page":"Index of functions","title":"Jchemo.plotconf","text":"plotconf(object; cnt = true, ptext = true, \n    fontsize = 15, coldiag = :red, resolution = (500, 400))\n\nPlot a confusion matrix.\n\nobject : Output of function confusion.\ncnt : Boolean. If true (default), plot the occurrences,    else plot the row %s.\nptext : Boolean. If true (default), display the value in each cell.\nfontsize : Font size when ptext = true.\ncoldiag : Font color when ptext = true.\nresolution : Resolution (horizontal, vertical) of the figure.\n\nSee examples in help page of function confusion. ```\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plotgrid-Tuple{Vector, Any}","page":"Index of functions","title":"Jchemo.plotgrid","text":"plotgrid(indx::Union{Vector{Integer}, Vector{Int64}, Vector{Real}, Vector{Float64}}, r; \n    resolution = (500, 350), step = 5, \n    color = nothing, kwargs...)\nplotgrid(indx::Union{Vector{Integer}, Vector{Int64}, Vector{Real}, Vector{Float64}}, r, \n    group; \n    resolution = (500, 350), step = 5, \n    color = nothing, leg = true, kwargs...)\n\nPlot error or performance rates of model predictions.\n\nindx : A numeric variable representing the grid of model parameters,    e.g. nb. LVs if PLSR models.\nr : The error/performance rates for the values of x. \ngroup : Categorical variable defining groups.    A separate line is plotted for each level of group.\nresolution : Resolution (horizontal, vertical) of the figure.\nstep : Step used for defining the xticks.\ncolor : Set color. If group if used, must be a vector of same length   as the number of levels in group.\nleg : Boolean. If group is used, display a legend or not.\nkwargs : Optional arguments to pass in Axis of CairoMakie.\n\nThe user has to specify a backend (e.g. CairoMakie).\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 0:20\nres = gridscorelv(Xtrain, ytrain, Xtest, ytest;\n    score = rmsep, fun = plskern, nlv = nlv)\nplotgrid(res.nlv, res.y1;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\n\nnlvdis = 15 ; metric = [\"mahal\" ]\nh = [1 ; 2.5 ; 5] ; k = [50 ; 100] \npars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)\nnlv = 0:20\nres = gridscorelv(Xtrain, ytrain, Xtest, ytest;\n    score = rmsep, fun = lwplsr, pars = pars, nlv = nlv)\ngroup = string.(\"h=\", res.h, \" k=\", res.k)\nplotgrid(res.nlv, res.y1, group;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSECV\").f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plotsp","page":"Index of functions","title":"Jchemo.plotsp","text":"plotsp(X, wl = 1:size(X, 2); resolution = (500, 350),\n    color = nothing, nsamp, kwargs...)\n\nPlotting spectra.\n\nX : X-data.\nwl : Column names of X. Must be numeric.\ncolor : Set a unique color (and eventually transparency) to the spectra.\n'resolution' : Resolution (horizontal, vertical) of the figure.\nnsamp : Nb. spectra to plot. If nothing (default), all spectra are plotted.\nkwargs : Optional arguments to pass in Axis of CairoMakie.\n\nThe function plots the rows of X.\n\nThe user has to specify a backend (e.g. CairoMakie).\n\nExamples\n\n    using JchemoData, JLD2, CairoMakie\n    path_jdat = dirname(dirname(pathof(JchemoData)))\n    db = joinpath(path_jdat, \"data/cassav.jld2\") \n    @load db dat\n    pnames(dat)\n    \n    X = dat.X\n    wl = names(X)\n    wl_num = parse.(Float64, wl) \n    plotsp(X).f\n    plotsp(X; color = (:red, .2)).f\n    plotsp(X, wl_num; xlabel = \"Wavelength (nm)\",\n        ylabel = \"Absorbance\").f\n\n    f, ax = plotsp(X)\n    vlines!(ax, 200)\n    f\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plotxy-Tuple{AbstractVector, AbstractVector}","page":"Index of functions","title":"Jchemo.plotxy","text":"plotxy(x::AbstractVector, y::AbstractVector; resolution = (600, 400), \n    color = nothing, ellipse::Bool = false, prob = .95, \n    circle::Bool = false, bisect::Bool = false, zeros::Bool = false,\n    xlabel = \"\", ylabel = \"\", title = \"\",\n    kwargs...)\nplotxy(x::AbstractVector, y::AbstractVector, group::Vector; resolution = (600, 400), \n    color = nothing, ellipse::Bool = false, prob = .95, \n    circle::Bool = false, bisect::Bool = false, zeros::Bool = false,\n    xlabel = \"\", ylabel = \"\", title = \"\", leg::Bool = true,\n    kwargs...)\nplotxy(X::Union{Matrix, DataFrame}; resolution = (600, 400), \n    color = nothing, ellipse::Bool = false, prob = .95, \n    circle::Bool = false, bisect::Bool = false, zeros::Bool = false,\n    xlabel = \"\", ylabel = \"\", title = \"\", \n    kwargs...)\nplotxy(X::Union{Matrix, DataFrame}, group::Vector; resolution = (600, 400), \n    color = nothing, ellipse::Bool = false, prob = .95, \n    circle::Bool = false, bisect::Bool = false, zeros::Bool = false,\n    xlabel = \"\", ylabel = \"\", title = \"\", leg::Bool = true, \n    kwargs...)\n\nScatter plot of (x, y) data\n\nx : A x-vector (n).\ny : A y-vector (n). \nX : A matrix (n, 2) (col1 = x, col2 = y). \ngroup : Categorical variable defining groups. \nresolution : Resolution (horizontal, vertical) of the figure.\ncolor : Set color(s). If group if used, color must be a vector of    same length as the number of levels in group.\nellipse : Boolean. Draw an ellipse of confidence, assuming a Ch-square distribution   with df = 2. If group is used, one ellipse is drawn per group.\nprob : Probability for the ellipse of confidence (default = .95).\nbisect : Boolean. Draw a bisector.\nzeros : Boolean. Draw horizontal and vertical axes passing through origin (0, 0).\nxlabel : Label for the x-axis.\nylabel : Label for the y-axis.\ntitle : Title of the graphic.\nleg : Boolean. If group is used, display a legend or not.\nkwargs : Optional arguments to pass in function scatter of Makie.\n\nThe user has to specify a backend (e.g. CairoMakie).\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\nlev = mlev(year)\nnlev = length(lev)\n\nfm = pcasvd(X, nlv = 3) ; \nT = fm.T\n\nplotxy(T[:, 1], T[:, 2]; color = (:red, .5)).f\n\nplotxy(T[:, 1:2]; color = (:red, .5)).f\n\nplotxy(T[:, 1], T[:, 2], year; ellipse = true).f\n\nplotxy(T[:, 1:2], year; color = (:red, .5)).f\n\ni = 1\ncolm = cgrad(:Dark2_5, nlev; categorical = true)\nplotxy(T[:, i:(i + 1)], year; \n    color = colm,\n    xlabel = string(\"PC\", i), ylabel = string(\"PC\", i + 1),\n    zeros = true, ellipse = true).f\n\nplotxy(T[:, 1], T[:, 2], year).lev\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.plscan","page":"Index of functions","title":"Jchemo.plscan","text":"plscan(X, Y, weights = ones(nro(X)); nlv,\n    bscal = \"none\", scal = false)\nplscan!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    bscal = \"none\", scal = false)\n\nCanonical partial least squares regression (Canonical PLS)\n\nX : First block (matrix) of data.\nY : Second block (matrix) of data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nCanonical PLS with the Nipals algorithm (Wold 1984, Tenenhaus 1998 chap.11), referred to as PLS-W2A (i.e. Wold PLS mode A) in Wegelin 2000. The two blocks X and X play a symmetric role.   After each step of scores computation, X and Y are deflated by the x- and  y-scores, respectively. \n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.\n\nWegelin, J.A., 2000. A Survey of Partial Least Squares (PLS) Methods, with Emphasis  on the Two-Block Case (No. 371). University of Washington, Seattle, Washington, USA.\n\nWold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in Linear  Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses.  SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052\n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n\nfm = plscan(X, Y; nlv = 3)\npnames(fm)\n\nfm.Tx\ntransform(fm, X, Y).Tx\nscale(fm.Tx, colnorm(fm.Tx))\n\nres = summary(fm, X, Y)\npnames(res)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plskern","page":"Index of functions","title":"Jchemo.plskern","text":"plskern(X, Y, weights = ones(nro(X)); nlv,\n    scal = false)\nplskern!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    scal = false)\n\nPartial Least Squares Regression (PLSR) with the  \"improved kernel algorithm #1\" (Dayal & McGegor, 1997).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nAbout the row-weighting in PLS algorithms (weights), see in particular Schaal et al. 2002,  Siccard & Sabatier 2006, Kim et al. 2011, and Lesnoff et al. 2020. \n\nReferences\n\nDayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms.  Journal of Chemometrics 11, 73-85.\n\nKim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active  pharmaceutical ingredients content using locally weighted partial  least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.\n\nLesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally weighted  PLS strategies for regression and discrimination on agronomic NIR Data.  Journal of Chemometrics. e3209.  https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209\n\nSchaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques  from nonparametric statistics for the real time robot learning.  Applied Intell., 17, 49-60.\n\nSicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression  and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 15\nfm = plskern(Xtrain, ytrain; nlv = nlv) ;\n#fm = plsnipals(Xtrain, ytrain; nlv = nlv) ;\n#fm = plsrosa(Xtrain, ytrain; nlv = nlv) ;\n#fm = plssimp(Xtrain, ytrain; nlv = nlv) ;\npnames(fm)\nfm.T\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.B\nJchemo.coef(fm; nlv = 7).B\n\nJchemo.transform(fm, Xtest)\nJchemo.transform(fm, Xtest; nlv = 7)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f    \n\nres = Jchemo.predict(fm, Xtest; nlv = 1:2)\nres.pred[1]\nres.pred[2]\n\nres = summary(fm, Xtrain) ;\npnames(res)\nz = res.explvarx\nlines(z.nlv, z.cumpvar,\n    axis = (xlabel = \"Nb. LVs\", ylabel = \"Prop. Explained X-Variance\"))\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plslda","page":"Index of functions","title":"Jchemo.plslda","text":"plslda(X, y, weights = ones(nro(X)); nlv, \n    prior = \"unif\", scal = false)\n\nLDA on PLS latent variables (PLS-LDA).\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations.    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nprior : Type of prior probabilities for class membership.   Posible values are: \"unif\" (uniform), \"prop\" (proportional).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe training variable y (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in y. Each column of Ydummy is a dummy variable (0/1).  Then, a PLS2 is implemented on X and Ydummy,  returning nlv latent variables (LVs). Finally, a LDA is run on these LVs and y. \n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\n## nlv must be >=1 \n## (conversely to plsrda for which nlv >= 0)\nnlv = 20      \nfm = plslda(Xtrain, ytrain; nlv = nlv) ;    \n#fm = plsqda(Xtrain, ytrain; nlv = nlv) ;\npnames(fm)\npnames(fm.fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nJchemo.transform(fm, Xtest)\nJchemo.transform(fm, Xtest; nlv = 2)\n\nfm_pls = fm.fm.fm_pls ;\nJchemo.transform(fm_pls, Xtest)\nsummary(fm_pls, Xtrain)\nJchemo.coef(fm_pls).B\nJchemo.coef(fm_pls, nlv = 1).B\nJchemo.coef(fm_pls, nlv = 2).B\n\nfm_da = fm.fm.fm_da ;\nT = Jchemo.transform(fm_pls, Xtest)\nJchemo.predict(fm_da[nlv], T).pred\n\nJchemo.predict(fm, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsldaavg","page":"Index of functions","title":"Jchemo.plsldaavg","text":"plsldaavg(X, y, weights = ones(nro(X)); nlv,\n    scal = false)\n\nAveraging of PLS-LDA models with different numbers of      latent variables (LVs).\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations.   Internally normalized to sum to 1. \nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to   the single model with 10 LVs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nEnsemblist method where the predictions are calculated by \"averaging\"  the predictions of a set of models built with different numbers of  LVs.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the most occurent class within the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\n## minimum of nlv must be >=1 \n## (conversely to plsrdaavg)\nfm = plsldaavg(Xtrain, ytrain; nlv = \"1:40\") ;    \n#fm = plsldaavg(Xtrain, ytrain; nlv = \"1:20\") ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsnipals","page":"Index of functions","title":"Jchemo.plsnipals","text":"plsnipals(X, Y, weights = ones(nro(X)); nlv,\n    scal = false)\nplsnipals!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    scal = false)\n\nPartial Least Squares Regression (PLSR) with the Nipals algorithm \n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to consider.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nIn this function, for PLS2 (multivariate Y), the Nipals iterations are replaced by a  direct computation of the PLS weights (w) by SVD decomposition of matrix X'Y  (Hoskuldsson 1988 p.213).\n\nSee ?plskern for examples.\n\nReferences\n\nHoskuldsson, A., 1988. PLS regression methods. Journal of Chemometrics 2, 211-228. https://doi.org/10.1002/cem.1180020306\n\nTenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique. Editions Technip,  Paris, France.\n\nWold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression: a basic tool  for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsqda","page":"Index of functions","title":"Jchemo.plsqda","text":"plsqda(X, y, weights = ones(nro(X)); nlv, \n    prior = \"unif\", scal = false)\n\nQDA on PLS latent variables.\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations.    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nprior : Type of prior probabilities for class membership.   Posible values are: \"unif\" (uniform), \"prop\" (proportional).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe training variable y (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in y. Each column of Ydummy is a dummy variable (0/1).  Then, a PLS2 is implemented on X and Ydummy,  returning nlv latent variables (LVs). Finally, a QDA is run on these LVs and y.\n\nSee ?plslda for examples.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsqdaavg","page":"Index of functions","title":"Jchemo.plsqdaavg","text":"plsqdaavg(X, y, weights = ones(nro(X)); nlv,\n    scal = false)\n\nAveraging of PLS-QDA models with different numbers of      latent variables (LVs).\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to   the single model with 10 LVs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nEnsemblist method where the predictions are calculated by \"averaging\"  the predictions of a set of models built with different numbers of LVs.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the most occurent class within the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nSee ?plsldaavg for examples.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsravg","page":"Index of functions","title":"Jchemo.plsravg","text":"plsravg(X, Y, weights = ones(nro(X)); nlv, \n    typf = \"unif\", typw = \"bisquare\",\n    alpha = 0, K = 5, rep = 10, scal = false)\nplsravg!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv, \n    typf = \"unif\", typw = \"bisquare\", \n    alpha = 0, K = 5, rep = 10, scal = false)\n\nAveraging and stacking PLSR models with different numbers of      latent variables (LVs).\n\nX : X-data (n, p).\nY : Y-data (n, q). Must be univariate (q = 1) if typw != \"unif\".\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to   the single model with 10 LVs).\ntypf : Type of averaging. \nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nFor typf in {\"aic\", \"bic\", \"cv\"}\n\ntypw : Type of weight function. \nalpha : Parameter of the weight function.\n\nFor typf = \"stack\"\n\nK : Nb. of folds segmenting the data in the (K-fold) CV.\nrep : Nb. of repetitions of the K-fold CV. \n\nEnsemblist method where the predictions are computed by averaging  or stacking the predictions of a set of models built with different numbers of  LVs.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the average (eventually weighted) or stacking of the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nPossible values of typf are: \n\n\"unif\" : Simple average.\n\"aic\" : Weighted average based on AIC computed for each model.\n\"bic\" : Weighted average based on BIC computed for each model.\n\"cv\" : Weighted average based on RMSEP_CV computed for each model.\n\"shenk\" : Weighted average using \"Shenk et al.\" weights computed for each model.\n\"stack\" : Linear stacking. A K-fold CV (eventually repeated) is done and \n\nthe CV predictions are regressed (multiple linear model without intercept) on the observed response data.\n\nFor arguments typw and alpha (weight function): see ?fweight.\n\nReferences\n\nLesnoff, M., Roger, J.-M., Rutledge, D.N., 2021. Monte Carlo methods for estimating  Mallows’s Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data.  Journal of Chemometrics n/a, e3369. https://doi.org/10.1002/cem.3369\n\nLesnoff, M., Andueza, D., Barotin, C., Barre, P., Bonnal, L., Fernández Pierna, J.A., Picard,  F., Vermeulen, P., Roger, J.-M., 2022. Averaging and Stacking Partial Least Squares Regression Models  to Predict the Chemical Compositions and the Nutritive Values of Forages from Spectral Near  Infrared Data. Applied Sciences 12, 7850. https://doi.org/10.3390/app12157850\n\nShenk, J., Westerhaus, M., Berzaghi, P., 1997. Investigation of a LOCAL calibration  procedure for near infrared instruments.  Journal of Near Infrared Spectroscopy 5, 223. https://doi.org/10.1255/jnirs.115\n\nShenk et al. 1998 United States Patent (19). Patent Number: 5,798.526.\n\nZhang, M.H., Xu, Q.S., Massart, D.L., 2004. Averaged and weighted average partial  least squares. Analytica Chimica Acta 504, 279–289. https://doi.org/10.1016/j.aca.2003.10.056\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages2.jld2\") \n@load db dat\npnames(dat)\n  \nX = dat.X \nY = dat.Y\nsumm(Y)\ny = Y.ndf\n#y = Y.dm\n\ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(y, s)\nXtest = X[s, :]\nytest = y[s]\nntrain = nro(Xtrain)\nntest = nro(Xtest)\n(ntot = ntot, ntrain, ntest)\n\nnlv = \"0:50\"\nfm = plsravg(Xtrain, ytrain; nlv = nlv) ;\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f   \n\nfm = plsravg(Xtrain, ytrain; nlv = nlv,\n    typf = \"cv\") ;\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsrda","page":"Index of functions","title":"Jchemo.plsrda","text":"plsrda(X, y, weights = ones(nro(X)); nlv,\n    scal = false)\n\nDiscrimination based on partial least squares regression (PLSR-DA).\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations. Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThis is the usual \"PLSDA\".  The training variable y (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in y. Each column of Ydummy is a dummy (0/1) variable.  Then, a PLS2 is implemented on X and Ydummy, returning nlv LVs. Finally, a multiple linear regression (MLR) is run between the LVs and each  column of Ydummy, returning predictions of the dummy variables  (= object posterior returned by function predict).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlv = 15\nfm = plsrda(Xtrain, ytrain; nlv = nlv) ;\npnames(fm)\ntypeof(fm.fm) # = PLS2 model\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.posterior\nres.pred\nerr(res.pred, ytest)\n\nJchemo.transform(fm, Xtest)\n\nJchemo.transform(fm.fm, Xtest)\nJchemo.coef(fm.fm)\nsummary(fm.fm, Xtrain)\n\nJchemo.predict(fm, Xtest; nlv = 1:2).pred\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsrdaavg","page":"Index of functions","title":"Jchemo.plsrdaavg","text":"plsrdaavg(X, y, weights = ones(nro(X)); nlv)\n\nAveraging of PLSR-DA models with different numbers of      latent variables (LVs).\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations.\nnlv : A character string such as \"5:20\" defining the range of the numbers of LVs    to consider (\"5:20\": the predictions of models with nb LVS = 5, 6, ..., 20    are averaged). Syntax such as \"10\" is also allowed (\"10\": correponds to   the single model with 10 LVs).\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nEnsemblist method where the predictions are calculated by \"averaging\"  the predictions of a set of models built with different numbers of LVs.\n\nFor instance, if argument nlv is set to nlv = \"5:10\", the prediction for  a new observation is the most occurent class within the predictions  returned by the models with 5 LVS, 6 LVs, ... 10 LVs, respectively.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nnlv = \"0:40\"\nfm = plsrdaavg(Xtrain, ytrain; nlv = nlv) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plsrosa","page":"Index of functions","title":"Jchemo.plsrosa","text":"plsrosa(X, Y, weights = ones(nro(X)); nlv,\n    scal = false)\nplsrosa!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    scal = false)\n\nPartial Least Squares Regression (PLSR) with the ROSA algorithm (Liland et al. 2016).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to consider.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nNote: The function has the following differences with the original  algorithm of Liland et al. (2016):\n\nScores T (LVs) are not normed.\nMultivariate Y is allowed.\n\nSee ?plskern for examples.\n\nReferences\n\nLiland, K.H., Næs, T., Indahl, U.G., 2016. ROSA—a fast extension of partial least  squares regression for multiblock data analysis. Journal of Chemometrics 30,  651–662. https://doi.org/10.1002/cem.2824\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plssimp","page":"Index of functions","title":"Jchemo.plssimp","text":"plssimp(X, Y, weights = ones(nro(X)); nlv,\n    scal = false)\nplssimp!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    scal = false)\n\nPartial Least Squares Regression (PLSR) with the SIMPLS algorithm (de Jong 1993).\n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to compute.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nNote: In this function, scores T (LVs) are not normed, conversely to the original  algorithm of de Jong (2013)\n\nReferences\n\nde Jong, S., 1993. SIMPLS: An alternative approach to partial least squares  regression. Chemometrics and Intelligent Laboratory Systems 18, 251–263.  https://doi.org/10.1016/0169-7439(93)85002-X\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plstuck","page":"Index of functions","title":"Jchemo.plstuck","text":"plstuck(X, Y, weights = ones(nro(X)); nlv,\n    bscal = \"none\", scal = false)\nplstuck!(X::Matrix, Y::Matrix, weights = ones(nro(X)); nlv,\n    bscal = \"none\", scal = false)\n\nTucker's inter-battery method of factor analysis\n\nX : First block (matrix) of data.\nY : Second block (matrix) of data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nInter-battery method of factor analysis (Tucker 1958, Tenenhaus 1998 chap.3),  The two blocks X and X play a symmetric role.  This method is referred to  as PLS-SVD in Wegelin 2000. The basis of the method is to factorize the covariance  matrix X'Y by SVD. \n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris.\n\nTishler, A., Lipovetsky, S., 2000. Modelling and forecasting with robust  canonical analysis: method and application. Computers & Operations Research 27,  217–232. https://doi.org/10.1016/S0305-0548(99)00014-3\n\nTucker, L.R., 1958. An inter-battery method of factor analysis. Psychometrika 23, 111–136. https://doi.org/10.1007/BF02289009\n\nWegelin, J.A., 2000. A Survey of Partial Least Squares (PLS) Methods, with Emphasis  on the Two-Block Case (No. 371). University of Washington, Seattle, Washington, USA.\n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n\nfm = plstuck(X, Y; nlv = 3)\npnames(fm)\n\nfm.Tx\ntransform(fm, X, Y).Tx\nscale(fm.Tx, colnorm(fm.Tx))\n\nres = summary(fm, X, Y)\npnames(res)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.plswold","page":"Index of functions","title":"Jchemo.plswold","text":"plswold(X, Y, weights = ones(nro(X)); nlv,\n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\nplswold!(X, Y, weights = ones(nro(X)); nlv,\n    tol = sqrt(eps(1.)), maxit = 200, scal = false)\n\nPartial Least Squares Regression (PLSR) with the Wold algorithm \n\nX : X-data (n, p).\nY : Y-data (n, q).\nweights : Weights (n) of the observations.    Internally normalized to sum to 1.\nnlv : Nb. latent variables (LVs) to consider.\ntol : Tolerance for the Nipals algorithm.\nmaxit : Maximum number of iterations for the Nipals algorithm.\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation.\n\nWold Nipals PLSR algorithm: Tenenhaus 1998 p.204.\n\nSee ?plskern for examples.\n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: thÃ©orie et pratique. Editions Technip,  Paris, France.\n\nWold, S., Ruhe, A., Wold, H., Dunn, III, W.J., 1984. The Collinearity Problem in  Linear Regression. The Partial Least Squares (PLS). Approach to  Generalized Inverses. SIAM Journal on Scientific and Statistical Computing 5, 735–743.  https://doi.org/10.1137/0905052\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.pmod-Tuple{Any}","page":"Index of functions","title":"Jchemo.pmod","text":"pmod(foo)\n\nShortcut for function parentmodule.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pnames-Tuple{Any}","page":"Index of functions","title":"Jchemo.pnames","text":"pnames(x)\n\nReturn the names of the elements of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.CalDs, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::CalDs, X; kwargs...)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nkwargs : Optional arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.CalPds, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::CalPds, X; kwargs...)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nkwargs : Optional arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Cglsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Cglsr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. iterations, or collection of nb. iterations, to consider. \n\nIf nothing, it is the maximum nb. iterations.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Covselr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Covselr, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.CplsrAvg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::CplrAvg, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Dkplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Dkplsr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Dkplsrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Dkplsrda, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\nIf nothing, it is the maximum nb. LVs.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Knnda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Knnda1, X)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Knnr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Knnr, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Kplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Kplsr, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\nIf nothing, it is the maximum nb. LVs.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Krr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Krr, X; lb = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nlb : Regularization parameter, or collection of regularization parameters, \"lambda\" to consider.    If nothing, it is the parameter stored in the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lda, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwmlr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwmlr, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwmlrS, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwmlrS, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwmlrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwmlrda, X)\n\nCompute y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwmlrdaS, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwmlrdaS, X)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsLda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsLda, X; nlv = nothing)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsQda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsQda, X; nlv = nothing)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsldaAvg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsldaAvg, X)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsqdaAvg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsqdaAvg, X)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplsr, X; nlv = nothing)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsrAvg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsrAvg, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsrS, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsrS, X; nlv = nothing)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Lwplsrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Lwplsrda, X; nlv = nothing)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsrdaAvg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsrdaAvg, X)\n\nCompute the y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.LwplsrdaS, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::LwplsrdaS, X; nlv = nothing)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Mlr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Mlr, X)\n\nCompute the Y-predictions from the fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Mlrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Mlrda, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occknndis, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occknndis, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occlknndis, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occlknndis, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occod, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occod, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occsd, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occsd, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occsdod, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occsdod, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Occstah, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Occstah, X)\n\nCompute predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plsdaavg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plsdaavg, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plslda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plslda, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plsravg, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plsravg, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Plsrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Plsrda, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Qda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Qda, X)\n\nCompute y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rosaplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Rosaplsr, Xbl; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list (vector) of X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Rr, X; lb = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nlb : Regularization parameter, or collection of regularization parameters,    \"lambda\" to consider. If nothing, it is the parameter stored in the    fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Rrda, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Rrda, X; lb = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nlb : Regularization parameter, or collection of regularization parameters,    \"lambda\" to consider. If nothing, it is the parameter stored in the    fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.Soplsr, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Soplsr, Xbl)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list (vector) of X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.TreedaDt, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::TreedaDt, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Jchemo.TreerDt, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::TreerDt, X)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Union{MbplsWest, Mbplsr}, Xbl; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nXbl : A list (vector) of X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.predict-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}, Any}","page":"Index of functions","title":"Jchemo.predict","text":"predict(object::Union{Plsr, Pcr}, X; nlv = nothing)\n\nCompute Y-predictions from a fitted model.\n\nobject : The fitted model.\nX : X-data for which predictions are computed.\nnlv : Nb. LVs, or collection of nb. LVs, to consider. \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.psize-Tuple{Any}","page":"Index of functions","title":"Jchemo.psize","text":"psize(x)\n\nReturn the type and size of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.pval-Tuple{Distributions.Distribution, Any}","page":"Index of functions","title":"Jchemo.pval","text":"pval(d::Distribution, q)\npval(x::Array, q)\npval(e_cdf::ECDF, q)\npval(object::Kde1, q::Real)\n\nCompute p-value(s) for a distribution, an ECDF or vector.\n\nd : A distribution computed from Distribution.jl.\nx : Univariate data.\ne_cdf : An ECDF computed from StatsBase.jl.\nq : Value(s) for which to compute the p-value(s).\n\nCompute or estimate the p-value of quantile q, ie. P(Q > q) where Q is the random variable.\n\nExamples\n\nusing Distributions, StatsBase\n\nd = Distributions.Normal(0, 1)\nq = 1.96\n#q = [1.64; 1.96]\nDistributions.cdf(d, q)    # cumulative density function (CDF)\nDistributions.ccdf(d, q)   # complementary CDF (CCDF)\npval(d, q)                 # Distributions.ccdf\n\nx = rand(5)\ne_cdf = StatsBase.ecdf(x)\ne_cdf(x)                # empirical CDF computed at each point of x (ECDF)\np_val = 1 .- e_cdf(x)   # complementary ECDF at each point of x\nq = .3\n#q = [.3; .5; 10]\npval(e_cdf, q)          # 1 .- e_cdf(q)\npval(x, q)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.qda-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.qda","text":"qda(X, y; prior = \"unif\", scal = false)\n\nQuadratic discriminant analysis  (QDA).\n\nX : X-data.\ny : y-data (class membership).\nprior : Type of prior probabilities for class membership.   Posible values are: \"unif\" (uniform), \"prop\" (proportional).\n\nExamples\n\nusing JchemoData, JLD2, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n\nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nn = nro(X)\n\nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\ntab(ytrain)\ntab(ytest)\n\nprior = \"unif\"\n#prior = \"prop\"\nfm = qda(Xtrain, ytrain; prior = prior) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.ds\nres.posterior\nres.pred\nerr(res.pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.r2-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.r2","text":"r2(pred, Y)\n\nCompute the R2 coefficient.\n\npred : Predictions.\nY : Observed data.\n\nThe rate R2 is calculated by R2 = 1 - MSEP(current model) / MSEP(null model),  where the \"null model\" is the overall mean.  For predictions over CV or test sets, and/or for non linear models,  it can be different from the square of the correlation coefficient (cor2)  between the true data and the predictions. \n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nr2(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nr2(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rasvd","page":"Index of functions","title":"Jchemo.rasvd","text":"rasvd(X, Y, weights = ones(nro(X)); nlv,\n    bscal = \"none\", tau = 1e-8, scal = false)\nrasvd!(X, Y, weights = ones(nro(X)); nlv,\n    bscal = \"none\", tau = 1e-8, scal = false)\n\nRedundancy analysis (RA) - PCA on instrumental variables (PCAIV)\n\nX : First block of data (explicative variables).\nY : Second block of data (dependent variables).\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\nbscal : Type of block scaling (\"none\", \"frob\").    See functions blockscal.\ntau : Regularization parameter (∊ [0, 1]).\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nSee e.g. Bougeard et al. 2011a,b and Legendre & Legendre 2012.  Let Yhat be the fitted values of the regression of Y on X.  The scores Ty are the PCA scores of Yhat. The scores Tx are  the fitted values of the regression of Ty on X.\n\nA continuum regularization is available.   After block centering and scaling, the covariances matrices are  computed as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. tau = 1e-8)  to get similar results as with pseudo-inverses.    \n\nReferences\n\nBougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011. Multiblock redundancy  analysis from a user's perspective. Application in veterinary epidemiology.  Electronic Journal of Applied Statistical Analysis 4, 203-214.  https://doi.org/10.1285/i20705948v4n2p203\n\nBougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy analysis:  interpretation tools and application in epidemiology. Journal of Chemometrics 25,  467-475. https://doi.org/10.1002/cem.1392\n\nLegendre, P., Legendre, L., 2012. Numerical Ecology. Elsevier,  Amsterdam, The Netherlands.\n\nTenenhaus, A., Guillemot, V. 2017. RGCCA: Regularized and Sparse Generalized Canonical  Correlation Analysis for Multiblock Data Multiblock data analysis. https://cran.r-project.org/web/packages/RGCCA/index.html \n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/linnerud.jld2\") \n@load db dat\npnames(dat)\nX = dat.X \nY = dat.Y\n\ntau = 1e-8\nfm = rasvd(X, Y; nlv = 3, tau = tau)\npnames(fm)\n\nfm.Tx\ntransform(fm, X, Y).Tx\nscale(fm.Tx, colnorm(fm.Tx))\n\nres = summary(fm, X, Y)\npnames(res)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rd-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rd","text":"rd(X, Y; typ = \"cor\")\nrd(X, Y, weights; typ = \"cor\")\n\nCompute redundancy coefficients between two matrices.\n\nX : Matrix (n, p).\nY : Matrix (n, q).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\ntyp : If \"cor\" (default), correlation is used, else uncorrected covariance is used. \n\nReturns the redundancy coefficient between X and each column of Y, i.e.: \n\n(1 / p) * [Sum(j=1, .., p) cor(xj, y1)^2 ; ... ; Sum(j=1, .., p) cor(xj, yq)^2] \n\nSee Tenenhaus 1998 section 2.2.1 p.10-11.\n\nReferences\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.\n\nExamples\n\nX = rand(5, 10)\nY = rand(5, 3)\nrd(X, Y)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recodcat2int-Tuple{Any}","page":"Index of functions","title":"Jchemo.recodcat2int","text":"recodcat2int(x; start = 1)\n\nRecode a categorical variable to a integer variable\n\nx : Variable to recode.\nstart : Integer value that will be set to the first category.\n\nThe numeric codes returned by the function are Int64 and  correspond to the sorted categories of x.\n\nExamples\n\nx = [\"b\", \"a\", \"b\"]   \n[x recodcat2int(x)]\nrecodcat2int(x; start = 0)\nrecodcat2int([25, 1, 25])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.recodnum2cla-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.recodnum2cla","text":"recodnum2cla(x, q)\n\nRecode a continuous variable to classes.\n\nx : Variable to recode.\nq : Values separating the classes. \n\nExamples\n\nusing Statistics\nx = [collect(1:10); 8.1 ; 3.1] \nq = [3; 8]\nzx = recodnum2cla(x, q)  \n[x zx]\nprobs = [.33; .66]\nq = quantile(x, probs) \nzx = recodnum2cla(x, q)  \n[x zx]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.replacebylev-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.replacebylev","text":"replacebylev(x, lev)\n\nReplace the elements of a vector by levels of corresponding order.\n\nx : Vector (n) of values to replace.\nlev : Vector (nlev) containing the levels.\n\nWarning: x and lev must contain the same number (nlev) of levels.\n\nThe ith sorted level in x is replaced by the ith sorted level of lev.\n\nExamples\n\nx = [10; 4; 3; 3; 4; 4]\nlev = [\"B\"; \"C\"; \"AA\"]\nsort(lev)\n[x replacebylev(x, lev)]\nzx = string.(x)\n[zx replacebylev(zx, lev)]\n\nlev = [3; 0; -1]\n[x replacebylev(x, lev)]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.replacebylev2-Tuple{Union{Int64, Array{Int64}}, Array}","page":"Index of functions","title":"Jchemo.replacebylev2","text":"replacebylev2(x::Union{Int64, Array{Int64}}, lev::Array)\n\nReplace the elements of an index-vector by levels.\n\nx : Vector (n) of values to replace.\nlev : Vector (nlev) containing the levels.\n\nWarning: Let us note \"nlev\" the number of levels in lev.  Vector x must contain integer values between 1 and nlev. \n\nEach element x[i] (i = 1,...,n) is replaced by lev[x[i]].\n\nExamples\n\nx = [2; 1; 2; 2]\nlev = [\"B\"; \"C\"; \"AA\"]\nsort(lev)\n[x replacebylev2(x, lev)]\nreplacebylev2([2], lev)\nreplacebylev2(2, lev)\n\nx = [2; 1; 2]\nlev = [3; 0; -1]\nreplacebylev2(x, lev)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.replacedict-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.replacedict","text":"replacedict(x, dict)\n\nReplace the elements of a vector by levels defined in a dictionary.\n\nx : Vector (n) of values to replace.\ndict : A dictionary of the correpondances betwwen the old and new values.\n\nExamples\n\ndict = Dict(\"a\" => 1000, \"b\" => 1, \"c\" => 2)\n\nx = [\"c\"; \"c\"; \"a\"; \"a\"; \"a\"]\nreplacedict(x, dict)\n\nx = [\"c\"; \"c\"; \"a\"; \"a\"; \"a\"; \"e\"]\nreplacedict(x, dict)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.residcla-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.residcla","text":"residcla(pred, y)\n\nCompute classification prediction error (0 = no error, 1 = error).\n\npred : Predictions.\ny : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nytrain = rand([\"a\" ; \"b\"], 10)\nXtest = rand(4, 5) \nytest = rand([\"a\" ; \"b\"], 4)\n\nfm = plsrda(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nresidcla(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.residreg-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.residreg","text":"residreg(pred, Y)\n\nCompute regression prediction errors.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nresidreg(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nresidreg(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rfda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}","page":"Index of functions","title":"Jchemo.rfda_dt","text":"rfda_dt(X, yy::Union{Array{Int64}, Array{String}}; \n    n_trees = 10,\n    partial_sampling = .7,  \n    n_subfeatures = -1,\n    max_depth = -1, min_samples_leaf = 5, \n    min_samples_split = 2, scal = false, \n    mth = true, kwargs...)\n\nRandom forest discrimination with DecisionTree.jl.\n\nX : X-data (n obs., p variables).\ny : Univariate Y-data (n obs.).\nn_trees : Nb. trees built for the forest. \npartial_sampling : Proportion of sampled observations for each tree.\nn_subfeatures : Nb. variables to select at random at each split (default: -1 ==> sqrt(#variables)).\nmax_depth : Maximum depth of the decision trees (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples each leaf needs to have.\nmin_sample_split : Minimum number of observations in needed for a split.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nmth : Boolean indicating if a multi-threading is done when new data are    predicted with function predict.\nkwargs : Optional named arguments to pass in function build_forest    of DecisionTree.jl.\n\nThe function fits a random forest discrimination model using package  `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140.  https://doi.org/10.1007/BF00058655\n\nBreiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGenuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI.\n\nGey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245\n\nExamples\n\nusing JchemoData, JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n  \nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nntot, p = size(X)\n  \nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntest = ntot - ntrain\n(ntot = ntot, ntrain, ntest)\n\ntab(ytrain)\ntab(ytest)\n\nn_subfeatures = 2 \nfm = rfda_dt(Xtrain, ytrain; n_trees = 100,\n    n_subfeatures = n_subfeatures) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nerr(res.pred, ytest) \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rfr_dt-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rfr_dt","text":"rfr_dt(X, y; n_trees = 10,\n    partial_sampling = .7,  \n    n_subfeatures = -1,\n    max_depth = -1, min_samples_leaf = 5, \n    min_samples_split = 2, scal = false, \n    mth = true, kwargs...)\n\nRandom forest regression with DecisionTree.jl.\n\nX : X-data (n obs., p variables).\ny : Univariate y-data (n obs.).\nn_trees : Nb. trees built for the forest. \npartial_sampling : Proportion of sampled observations for each tree.\nn_subfeatures : Nb. variables to select at random at each split (default: -1 ==> sqrt(#variables)).\nmax_depth : Maximum depth of the decision trees (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples each leaf needs to have.\nmin_sample_split : Minimum number of observations in needed for a split.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nmth : Boolean indicating if a multi-threading is done when new data are    predicted with function predict.\nkwargs : Optional named arguments to pass in function build_forest    of DecisionTree.jl.\n\nThe function fits a random forest regression model using package  `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., 1996. Bagging predictors. Mach Learn 24, 123–140.  https://doi.org/10.1007/BF00058655\n\nBreiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGenuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI.\n\nGey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2021.jld2\")\n@load db dat\npnames(dat)\n\nXtrain = dat.Xtrain\nYtrain = dat.Ytrain\nytrain = Ytrain.y\ns = dat.Ytest.inst .== 1 \nXtest = dat.Xtest[s, :]\nYtest = dat.Ytest[s, :]\nytest = Ytest.y\nwl = names(Xtrain) \nwl_num = parse.(Float64, wl) \nntrain, p = size(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\nf = 21 ; pol = 3 ; d = 2 \nXptrain = savgol(snv(Xtrain); f, pol, d) \nXptest = savgol(snv(Xtest); f, pol, d) \n\nn_subfeatures = p / 3 \nfm = rfr_dt(Xptrain, ytrain; n_trees = 100,\n    n_subfeatures = n_subfeatures) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xptest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f  \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmcol-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}","page":"Index of functions","title":"Jchemo.rmcol","text":"rmcol(X, s)\n\nRemove the columns of a matrix or the components of a vector  having indexes s.\n\nX : Matrix or vector.\ns : Vector of the indexes.\n\nExamples\n\nX = rand(5, 3) \nrmcol(X, [1, 3])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmgap-Tuple{Any}","page":"Index of functions","title":"Jchemo.rmgap","text":"rmgap(X; indexcol, k = 5)\nrmgap!(X; indexcol, k = 5)\n\nRemove vertical gaps in spectra , e.g. for ASD.  \n\nX : X-data.\nindexcol : The indexes of the columns where are located the gaps. \nk : The number of columns used on the left side        of the gaps for fitting the linear regressions.\n\nFor each observation (row of matrix X), the corrections are done by extrapolation from simple linear regressions  computed on the left side of the defined gaps. \n\nFor instance, If two gaps are observed between indexes 651-652 and  between indexes 1425-1426, respectively, then the syntax should  be indexcol = [651 ; 1425].\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/asdgap.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nz = [1000 ; 1800] \nu = findall(in(z).(wl_num))\nf, ax = plotsp(X, wl_num)\nvlines!(ax, z; linestyle = :dash, color = (:grey, .8))\nf\n\n# Corrected data\n\nu = findall(in(z).(wl_num))\nzX = rmgap(X; indexcol = u, k = 5)  \nf, ax = plotsp(zX, wl_num)\nvlines!(ax, z; linestyle = :dash, color = (:grey, .8))\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmrow-Tuple{Union{DataFrames.DataFrame, AbstractMatrix}, Union{Number, Vector, BitVector}}","page":"Index of functions","title":"Jchemo.rmrow","text":"rmrow(X, s)\n\nRemove the rows of a matrix or the components of a vector  having indexes s.\n\nX : Matrix or vector.\ns : Vector of the indexes.\n\nExamples\n\nX = rand(5, 2) \nrmrow(X, [1, 4])\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmsep-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rmsep","text":"rmsep(pred, Y)\n\nCompute the square root of the mean of the squared prediction  errors (RMSEP).\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrmsep(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrmsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rmsepstand-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rmsepstand","text":"rmsepstand(pred, Y)\n\nCompute the standardized square root of the mean of the squared  prediction errors (RMSEP_stand).\n\npred : Predictions.\nY : Observed data.\n\nRMSEP is standardized to Y: RMSEP_stand = RMSEP ./ Y.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrmsepstand(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrmsepstand(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rosaplsr","page":"Index of functions","title":"Jchemo.rosaplsr","text":"rosaplsr(Xbl, Y, weights = ones(nro(Xbl[1])); nlv)\nrosaplsr!(Xbl, Y, weights = ones(nro(Xbl[1])); nlv)\n\nMultiblock PLSR with the ROSA algorithm (Liland et al. 2016).\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nY : Y-data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to consider.\nscal : Boolean. If true, each column of blocks in Xbl and    of Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nThe function has the following differences with the original  algorithm of Liland et al. (2016):\n\nScores T are not normed to 1.\nMultivariate Y is allowed. In such a case, the squared residuals are summed    over the columns for finding the winning block for each global LV    (therefore Y-columns should have the same scale).\n\nReferences\n\nLiland, K.H., Næs, T., Indahl, U.G., 2016. ROSA — a fast extension of partial least  squares regression for multiblock data analysis. Journal of Chemometrics 30,  651–662. https://doi.org/10.1002/cem.2824\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ham.jld2\") \n@load db dat\npnames(dat) \n\nX = dat.X\ny = dat.Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X, listbl)\n# \"New\" = first two rows of Xbl \nXbl_new = mblock(X[1:2, :], listbl)\n\nnlv = 5\nfm = rosaplsr(Xbl, y; nlv = nlv) ;\npnames(fm)\nfm.T\nJchemo.transform(fm, Xbl_new)\n[y Jchemo.predict(fm, Xbl).pred]\nJchemo.predict(fm, Xbl_new).pred\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rowmean-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowmean","text":"rowmean(X)\n\nCompute the mean of each row of X.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nrowmean(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rowstd-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowstd","text":"rowstd(X)\n\nCompute the (uncorrected) standard deviation of each row of X.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nn, p = 5, 6\nX = rand(n, p)\nrowstd(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rowsum-Tuple{Any}","page":"Index of functions","title":"Jchemo.rowsum","text":"rowsum(X)\n\nCompute the sum of each row of X.\n\nX : Data (n, p).\n\nReturn a vector.\n\nExamples\n\nX = rand(5, 2) \nrowsum(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rp","page":"Index of functions","title":"Jchemo.rp","text":"rp(X, weights = ones(nro(X)); nlv, fun = rpmatli, scal = false, kwargs ...)\nrp!(X::Matrix, weights = ones(nro(X)); nlv, fun = rpmatli, scal = false, kwargs ...)\n\nMake a random projection of matrix X.\n\nX : X-data (n, p).\nweights : Weights (n) of the observations. Internally normalized to sum to 1.\nnlv : Nb. dimensions on which X is projected.\nfun : A function of random projection.\nkwargs : Optional arguments of function fun.\nscal : Boolean. If true, each column of X is scaled   by its uncorrected standard deviation.\n\nExamples\n\nX = rand(5, 10)\nnlv = 3\nfm = rp(X; nlv = nlv)\npnames(fm)\nsize(fm.P) \nfm.P\nfm.T # = X * fm.P \nJchemo.transform(fm, X[1:2, :])\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rpd-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rpd","text":"rpd(pred, Y)\n\nCompute the ratio \"deviation to model performance\" (RPD).\n\npred : Predictions.\nY : Observed data.\n\nThis is the ratio of the deviation to the model performance to the deviation, defined by RPD = Std(Y) / RMSEP, where Std(Y) is the standard deviation. \n\nSince Std(Y) = RMSEP(null model) where the null model is the simple average, this also gives RPD = RMSEP(null model) / RMSEP. \n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrpd(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrpd(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rpdr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rpdr","text":"rpdr(pred, Y)\n\nCompute a robustified RPD.\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrpdr(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nrpdr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rpmatgauss-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rpmatgauss","text":"rpmatgauss(p, nlv)\n\nBuild a gaussian random projection matrix.\n\np : Nb. variables (attributes) to project.\nnlv : Nb. of simulated projection dimensions.\n\nThe function returns a random projection matrix P of dimension  p x nlv. The projection of a given matrix X of size n x p is given by X * P.\n\nP is simulated from i.i.d. N(0, 1)/sqrt(nlv).\n\nReferences\n\nLi, P., Hastie, T.J., Church, K.W., 2006. Very sparse random projections,  in: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge  Discovery and Data Mining, KDD ’06. Association for Computing Machinery, New York, NY, USA, pp. 287–296. https://doi.org/10.1145/1150402.1150436\n\nExamples\n\np = 10 ; nlv = 3\nrpmatgauss(p, nlv)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rpmatli-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rpmatli","text":"rpmatli(p, nlv; s = sqrt(p))\n\nBuild a sparse random projection matrix (Achlioptas 2001, Li et al. 2006).\n\np : Nb. variables (attributes) to project.\nnlv : Nb. final dimensions, i.e. after projection.\ns : Coefficient defining the sparsity of the returned matrix    (higher is s, higher is the sparsity).\n\nThe function returns a random projection matrix P of dimension  p x nlv. The projection of a given matrix X of size n x p is given by X * P.\n\nP is simulated from i.i.d. \"p_ij\" = \n\n1 with prob. 1/(2 * s)\n0 with prob. 1 - 1 / s\n-1 with prob. 1/(2 * s)\n\nUsual values for s are:\n\nsqrt(p)       (Li et al. 2006)\np / log(p)  (Li et al. 2006)\n1               (Achlioptas 2001)\n3               (Achlioptas 2001) \n\nReferences\n\nAchlioptas, D., 2001. Database-friendly random projections,  in: Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on  Principles of Database Systems, PODS ’01. Association for Computing Machinery,  New York, NY, USA, pp. 274–281. https://doi.org/10.1145/375551.375608\n\nLi, P., Hastie, T.J., Church, K.W., 2006. Very sparse random projections,  in: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge  Discovery and Data Mining, KDD ’06. Association for Computing Machinery, New York, NY, USA, pp. 287–296. https://doi.org/10.1145/1150402.1150436\n\nExamples\n\np = 10 ; nlv = 3\nrpmatli(p, nlv)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.rr","page":"Index of functions","title":"Jchemo.rr","text":"rr(X, Y, weights = ones(nro(X)); lb = .01,\n    scal = false)\nrr!(X::Matrix, Y::Matrix, weights = ones(nro(X)); lb = .01,\n    scal = false)\n\nRidge regression (RR) implemented by SVD factorization.\n\nX : X-data.\nY : Y-data.\nweights : Weights of the observations. Internally normalized to sum to 1. \nlb : A value of the regularization parameter \"lambda\".\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nX and Y are internally centered. The model is computed with an intercept. \n\nReferences\n\nCule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice  of ridge parameter in ridge regression. arXiv:1205.0686.\n\nHastie, T., Tibshirani, R., 2004. Efficient quadratic regularization  for expression arrays. Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010\n\nHastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.\n\nHoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.  Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nlb = 10^(-2)\nfm = rr(Xtrain, ytrain; lb = lb) ;\n#fm = rrchol(Xtrain, ytrain; lb = lb) ;\npnames(fm)\n\nzcoef = Jchemo.coef(fm)\nzcoef.int\nzcoef.B\n# Only for rr\nJchemo.coef(fm; lb = .1).B\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(pred, ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f    \n\n# Only for rr\nres = Jchemo.predict(fm, Xtest; lb = [.1 ; .01])\nres.pred[1]\nres.pred[2]\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rrchol","page":"Index of functions","title":"Jchemo.rrchol","text":"rrchol(X, Y, weights = ones(nro(X)); lb = .01, \n    scal = false)\nrrchol!(X::Matrix, Y::Matrix, weights = ones(nro(X)); lb = .01,\n    scal = false)\n\nRidge regression (RR) using the Normal equations and a Cholesky factorization.\n\nX : X-data.\nY : Y-data.\nweights : Weights of the observations. Internally normalized to sum to 1. \nlb : A value of the regularization parameter \"lambda\".\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nX and Y are internally centered. The model is computed with an intercept. \n\nSee ?rr for eaxamples.\n\nReferences\n\nCule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice  of ridge parameter in ridge regression. arXiv:1205.0686.\n\nHastie, T., Tibshirani, R., 2004. Efficient quadratic regularization  for expression arrays. Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010\n\nHastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,  inference, and prediction, 2nd ed. Springer, New York.\n\nHoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.  Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rrda","page":"Index of functions","title":"Jchemo.rrda","text":"rrda(X, y, weights = ones(nro(X)); lb)\n\nDiscrimination based on ridge regression (RR-DA).\n\nX : X-data.\ny : y-data (class membership).\nweights : Weights of the observations. Internally normalized to sum to 1. \nlb : A value of the regularization parameter \"lambda\".\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\n\nThe training variable y (univariate class membership) is transformed to a dummy table (Ydummy) containing nlev columns, where nlev is the number  of classes present in y. Each column of Ydummy is a dummy (0/1) variable.  Then, a RR is implemented on the y and each column of Ydummy, returning predictions of the dummy variables (= object posterior returned by  function predict).  These predictions can be considered as unbounded  estimates (i.e. eventually outside of [0, 1]) of the class membership probabilities. For a given observation, the final prediction is the class corresponding  to the dummy variable for which the probability estimate is the highest.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/forages.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \nY = dat.Y \ns = Bool.(Y.test)\nXtrain = rmrow(X, s)\nytrain = rmrow(Y.typ, s)\nXtest = X[s, :]\nytest = Y.typ[s]\n\ntab(ytrain)\ntab(ytest)\n\nlb = .001\nfm = rrda(Xtrain, ytrain; lb = lb) ;    \npnames(fm)\npnames(fm.fm)\n\nres = Jchemo.predict(fm, Xtest) ;\npnames(res)\nres.pred\nerr(res.pred, ytest)\n\nJchemo.predict(fm, Xtest; lb = [.1; .01]).pred\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rrr","page":"Index of functions","title":"Jchemo.rrr","text":"rrr(X, Y, weights = ones(nro(X)); nlv,\n    tau = 1e-5, tol = sqrt(eps(1.)), maxit = 200, \n    scal = false)\nrrr(X, Y, weights = ones(nro(X)); nlv,\n    tau = 1e-5, tol = sqrt(eps(1.)), maxit = 200, \n    scal = false)\n\nReduced rank regression (RRR).\n\nX : First block of data.\nY : Second block of data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs = scores T) to compute.\ntau : Regularization parameter (∊ [0, 1]).\nscal : Boolean. If true, each column of X and Y    is scaled by its uncorrected standard deviation    (before the block scaling).\n\nReduced rank regression, also referred to as redundancy analysis  (RA) regression. In this function, the RA uses the Nipals algorithm  presented in Mangamana et al 2021, section 2.1.1.\n\nA continuum regularization is available.  After block centering and scaling, the covariances matrices are  computed as follows: \n\nCx = (1 - tau) * X'DX + tau * Ix\n\nwhere D is the observation (row) metric.  Value tau = 0 can generate unstability when inverting the covariance matrices.  A better alternative is generally to use an epsilon value (e.g. tau = 1e-8)  to get similar results as with pseudo-inverses.  \n\nReferences\n\nBougeard, S., Qannari, E.M., Lupo, C., Chauvin, C., 2011. Multiblock redundancy  analysis from a user’s perspective. Application in veterinary epidemiology.  Electronic Journal of Applied Statistical Analysis 4, 203-214–214.  https://doi.org/10.1285/i20705948v4n2p203\n\nBougeard, S., Qannari, E.M., Rose, N., 2011. Multiblock redundancy analysis:  interpretation tools and application in epidemiology. Journal of Chemometrics 25,  467–475. https://doi.org/10.1002/cem.1392 \n\nTchandao Mangamana, E., Glèlè Kakaï, R., Qannari, E.M., 2021. A general  strategy for setting up supervised methods of multiblock data analysis.  Chemometrics and Intelligent Laboratory Systems 217, 104388.  https://doi.org/10.1016/j.chemolab.2021.104388\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie, StatsBase\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntrain = nro(Xtrain)\n# Building Cal and Val within Train\nnval = 80\ns = sample(1:ntrain, nval; replace = false)\nXcal = rmrow(Xtrain, s)\nycal = rmrow(ytrain, s)\nXval = Xtrain[s, :]\nyval = ytrain[s]\n\npars = mpar(tau = 1e-4)\nnlv = 0:20\nres = gridscorelv(Xcal, ycal, Xval, yval;\n    score = rmsep, fun = rrr, nlv = nlv, pars = pars)\nu = findall(res.y1 .== minimum(res.y1))[1]\nres[u, :]\nplotgrid(res.nlv, res.y1;\n    xlabel = \"Nb. LVs\", ylabel = \"RMSEP\").f\n\ntau = 1e-4\nnlv = 1\nfm = rrr(Xtrain, ytrain; nlv = nlv, tau = tau) ;\nres = Jchemo.predict(fm, Xtest)\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", \n    ylabel = \"Observed\").f\n    \n## PLSR \ntau = 1\nfm = rrr(Xtrain, ytrain; nlv = 3, tau = tau) ;\nhead(Jchemo.predict(fm, Xtest).pred)\nfm = plskern(Xtrain, ytrain; nlv = 3) ;\nhead(Jchemo.predict(fm, Xtest).pred)\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.rv-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.rv","text":"rv(X, Y)\nrv(Xbl)\n\nCompute the RV coefficient between matrices.\n\nX : Matrix (n, p).\nY : Matrix (n, q).\nXbl : A list (vector) of matrices.\ncentr : Logical indicating if the matrices are internally    centered or not.\n\nRV is bounded in [0, 1]. \n\nA dissimilarty measure between X and Y can be computed by d = sqrt(2 * (1 - RV)).\n\nReferences\n\nEscoufier, Y., 1973. Le Traitement des Variables Vectorielles. Biometrics 29, 751–760.  https://doi.org/10.2307/2529140\n\nJosse, J., Holmes, S., 2016. Measuring multivariate association and beyond.  Stat Surv 10, 132–167. https://doi.org/10.1214/16-SS116\n\nJosse, J., Pagès, J., Husson, F., 2008. Testing the significance of the RV coefficient.  Computational Statistics & Data Analysis 53, 82–91. https://doi.org/10.1016/j.csda.2008.06.012\n\nKazi-Aoual, F., Hitier, S., Sabatier, R., Lebreton, J.-D., 1995. Refined approximations  to permutation tests for multivariate inference. Computational Statistics & Data Analysis  20, 643–656. https://doi.org/10.1016/0167-9473(94)00064-2\n\nMayer, C.-D., Lorent, J., Horgan, G.W., 2011. Exploratory Analysis of Multiple Omics  Datasets Using the Adjusted RV Coefficient. Statistical Applications in Genetics and Molecular Biology 10. https://doi.org/10.2202/1544-6115.1540\n\nSmilde, A.K., Kiers, H.A.L., Bijlsma, S., Rubingh, C.M., van Erk, M.J., 2009.  Matrix correlations for high-dimensional data: the modified RV-coefficient.  Bioinformatics 25, 401–405. https://doi.org/10.1093/bioinformatics/btn634\n\nRobert, P., Escoufier, Y., 1976. A Unifying Tool for Linear Multivariate Statistical Methods:  The RV-Coefficient. Journal of the Royal Statistical Society: Series C (Applied Statistics)  25, 257–265. https://doi.org/10.2307/2347233\n\nExamples\n\nX = rand(5, 10)\nY = rand(5, 3)\nrv(X, Y)\n\nX = rand(5, 15) \nlistbl = [3:4, 1, [6; 8:10]]\nXbl = mblock(X, listbl)\nrv(Xbl)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampcla","page":"Index of functions","title":"Jchemo.sampcla","text":"sampcla(x, y = nothing; k)\n\nStratified sampling.  \n\nx : Classes of the observations.\ny : Quantitative variable used if systematic sampling.\nk : Nb. observations to sample in each class ==> output train. \n\nThe length of k must be either 1 (k = equal number of training observations  to select per class) or the number of classes in x.\n\nIf y = nothing (default), the sampling is random, else it is  systematic (grid over y).\n\nReferences\n\nNaes, T., 1987. The design of calibration in near infra-red reflectance analysis by clustering.  Journal of Chemometrics 1, 121-134.\n\nExamples\n\nx = string.(repeat(1:5, 3))\ntab(x)\nres = sampcla(x; k = 2)\nres.train\nx[res.train]\ntab(x[res.train])\n\nx = string.(repeat(1:5, 3))\nn = length(x) ; y = rand(n) \nres = sampcla(x, y; k = 2)\nres.train\nx[res.train]\ntab(x[res.train])\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.sampdp-Tuple{Any}","page":"Index of functions","title":"Jchemo.sampdp","text":"sampdp(X; k, metric = \"eucl\")\n\nDUPLEX sampling.  \n\nX : X-data (n, p).\nk : Nb. pairs of observations to sample. Must be <= n / 2. \nmetric : Metric used for the distance computation.   Possible values are: \"eucl\", \"mahal\".\n\nThe function divides the data X in two sets of equal size,  train vs test, using the DUPLEX algorithm (Snee, 1977 p.421). The two sets are expected to cover approximately the same region and have similar statistical properties. \n\nThe user may add (a posteriori) the eventual remaining observations  (output remain) to train.\n\nReferences\n\nKennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.\n\nSnee, R.D., 1977. Validation of Regression Models: Methods and Examples.  Technometrics 19, 415-428. https://doi.org/10.1080/00401706.1977.10489581\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nn = nro(X)\n\nk = 140\nres = sampdp(X; k = k)\npnames(res)\nres.train \nres.test\nres.remain\n\nfm = pcasvd(X; nlv = 15)\nT = fm.T\nres = sampdp(T; k = k, metric = \"mahal\")\n\nn = 10 ; k = 25 \nX = [repeat(1:n, inner = n) repeat(1:n, outer = n)] \nX = Float64.(X) \nX .= X + .1 * randn(nro(X), nco(X))\ns = sampks(X; k = k).train \nf, ax = scatter(X[:, 1], X[:, 2])\nscatter!(X[s, 1], X[s, 2], color = \"red\") \nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampks-Tuple{Any}","page":"Index of functions","title":"Jchemo.sampks","text":"sampks(X; k, metric = \"eucl\")\n\nKennard-Stone sampling.  \n\nX : X-data.\nk : Nb. observations to sample ==> output train. \nmetric : Metric used for the distance computation.   Possible values: \"eucl\", \"mahal\".\n\nThe function divides the data X in two sets, train vs test,  using the Kennard-Stone (KS) algorithm (Kennard & Stone, 1969).  The two sets have different underlying probability distributions:  train has higher dispersion than test.\n\nReferences\n\nKennard, R.W., Stone, L.A., 1969. Computer aided design of experiments.  Technometrics, 11(1), 137-148.\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\n\nk = 200\nres = sampks(X; k = k)\npnames(res)\nres.train \nres.test\n\nfm = pcasvd(X; nlv = 15)\nT = fm.T\nres = sampks(T; k = k, metric = \"mahal\")\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sampsys-Tuple{Any}","page":"Index of functions","title":"Jchemo.sampsys","text":"sampsys(y; k)\n\nSystematic sampling over a quantitative variable.  \n\ny : Quantitative variable to sample.\nk : Nb. observations to sample ==> output train. Must be >= 2.\n\nSystematic sampling (regular grid) over y.\n\nThe minimum and maximum of y are always sampled.\n\nExamples\n\ny = rand(7)\n[y sort(y)]\nres = sampsys(y; k = 3)\ny[res.train]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.savgk-Tuple{Any, Any, Any}","page":"Index of functions","title":"Jchemo.savgk","text":"savgk(m, pol, d)\n\nCompute the kernel of the Savitzky-Golay filter.\n\nm : Nb. points of the half window (m >= 1)    –> the size of the kernel is odd (f = 2 * m + 1):    x[-m], x[-m+1], ..., x[0], ...., x[m-1], x[m].\npol : Polynom order (1 <= pol <= 2 * m).   The case \"pol = 0\" (simple moving average) is not allowed by the funtion.\nd : Derivation order (0 <= d <= pol).   If d = 0, there is no derivation (only polynomial smoothing).\n\nReferences\n\nLuo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation  filter for even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002\n\nExamples\n\nres = savgk(21, 3, 2)\npnames(res)\nres.S \nres.G \nres.kern\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.savgol-Tuple{Any}","page":"Index of functions","title":"Jchemo.savgol","text":"savgol(X; f, pol, d)\nsavgol!(X::Matrix; f, pol, d)\n\nSavitzky-Golay smoothing of each row of a matrix X.\n\nX : X-data (n, p).\nf : Size of the filter (nb. points involved in the kernel). Must be odd and >= 3.   The half-window size is m = (f - 1) / 2.\npol : Polynom order (1 <= pol <= f - 1).\nd : Derivation order (0 <= d <= pol).\n\nThe smoothing is computed by convolution (with padding), with function  imfilter of package ImageFiltering.jl. Each returned point is located on the center  of the kernel. The kernel is computed with function savgk.\n\nReferences\n\nLuo, J., Ying, K., Bai, J., 2005. Savitzky–Golay smoothing and differentiation filter for  even number data. Signal Processing 85, 1429–1434. https://doi.org/10.1016/j.sigpro.2005.02.002\n\nSavitzky, A., Golay, M.J.E., 2002. Smoothing and Differentiation of Data by Simplified Least  Squares Procedures. [WWW Document]. https://doi.org/10.1021/ac60214a047\n\nSchafer, R.W., 2011. What Is a Savitzky-Golay Filter? [Lecture Notes].  IEEE Signal Processing Magazine 28, 111–117. https://doi.org/10.1109/MSP.2011.941097\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nf = 21 ; pol = 3 ; d = 2 ; \nXp = savgol(X; f = f, pol = pol, d = d) \nplotsp(Xp[1:30, :], wl_num).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.scale-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.scale","text":"scale(X, v)\nscale!(X, v)\n\nScale each column of X.\n\nX : Data.\nv : Scaling factors.\n\nExamples\n\nX = rand(5, 2) \nscale(X, colstd(X))\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.segmkf-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.segmkf","text":"segmkf(n, K; rep = 1)\nsegmkf(n, K, group; rep = 1)\n\nBuild segments for K-fold cross-validation.  \n\nn : Total nb. observations in the dataset. The sampling    is implemented with 1:n.\nK : Nb. folds (segments) splitting the data. \ngroup : A vector (n) defining blocks.\nrep : Nb. replications of the sampling.\n\nBuild K segments splitting the data that can be used to validate a model.  The sampling can be replicated (rep).\n\nIf group is used (must be a vector of length n), the function samples entire  blocks of observations instead of observations. Such a block-sampling is required  when data is structured by blocks and when the response to predict is  correlated within blocks. It prevents underestimation of the generalization error.\n\nThe function returns a list (vector) of rep elements.  Each element of the list contains K segments (= K vectors). Each segment contains the indexes (position within 1:n) of the sampled observations.    \n\nExamples\n\nn = 10 ; K = 3 ; rep = 4 \nsegm = segmkf(n, K; rep) \ni = 1 \nsegm[i] # = replication \"i\"\n\n# Block-sampling\n\nn = 11 \ngroup = [\"A\", \"B\", \"C\", \"D\", \"E\", \"A\", \"B\", \"C\", \"D\", \"E\", \"A\"]    # = blocks of the observations\nunique(group)   \nK = 3 ; rep = 4 \nsegm = segmkf(n, K, group; rep = rep)\ni = 1 \nsegm[i]\ngroup[segm[i][1]]\ngroup[segm[i][2]]\ngroup[segm[i][3]]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.segmts-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.segmts","text":"segmts(n, m; rep = 1, seed = nothing)\nsegmts(n, m, group; rep = 1, seed = nothing)\n\nBuild segments for \"test-set\" validation.\n\nn : Total nb. observations in the dataset. The sampling    is implemented within 1:n.\nm : Nb. observations, or groups (if group is used), returned in each segment.\nseed : Can set the seet for the Random.MersenneTwister generator.    Must be of length = rep. When nothing, the seed is random at each sampling.\ngroup : A vector (n) defining blocks of observations.\nrep : Nb. replications of the sampling.\n\nBuild a test set that can be used to validate a model. The sampling can be replicated (rep).\n\nIf group is used (must be a vector of length n), the function samples entire  groups (= blocks) of observations instead of observations.  Such a block-sampling is required when data is structured by blocks and  when the response to predict is correlated within blocks.  This prevents underestimation of the generalization error.\n\nThe function returns a list (vector) of rep elements.  Each element of the list contains K segments (= K vectors). Each segment contains the indexes (position within 1:n) of the sampled observations.  \n\nExamples\n\nn = 10 ; m = 3 ; rep = 4 \nsegm = segmts(n, m; rep) \ni = 1 \nsegm[i]\n\nsegmts(10, 4; seed = 3)\nsegmts(10, 4; rep = 2, seed = [1 ; 3])\n\n# Block-sampling\n\nn = 11\ngroup = [\"A\", \"B\", \"C\", \"D\", \"E\", \"A\", \"B\", \"C\", \"D\", \"E\", \"A\"]    # = blocks of the observations\ntab(group)\nm = 2 ; rep = 4 \nsegm = segmts(n, m, group; rep)\ni = 1 \nsegm[i]\nsegm[i][1]\ngroup[segm[i][1]]\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.selwold-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.selwold","text":"selwold(indx, r; smooth = true, \n    f = 5, alpha = .05, digits = 3, graph = true, \n    step = 2, xlabel = \"Index\", ylabel = \"Value\", \n    title = \"Score\")\n\nWold's criterion to select dimensionality in LV (e.g. PLSR) models.\n\nindx : A variable representing the model parameter(s), e.g. nb. LVs if PLSR models.\nr : A vector of error rates (n), e.g. RMSECV.\nsmooth : Boolean. If true,  the selection is done on the smoothed R.\n'f' : Window of the savitzky-Golay filter use for the smoothing (function savgol).\nalpha : Proportion alpha used as threshold for R.\ndigits : Number of digits in the outputs.\ngraph : Boolean. If true, outputs are plotted.\nstep : Step used for defining the xticks in the graphs.\nxlabel : Horizontal label for the plots.\nylabel : Vertical label for the plots.\ntitle : Title of the left plot.\n\nThe slection criterion is the \"precision gain ratio\" R = 1 - r(a+1) / r(a), where r is an observed error rate quantifying the model performance (e.g. RMSEP, * classification error rate, etc.) and a the model dimensionnality (= nb. LVs).  r can also represent other indicators such as the eigenvalues of a PCA.\n\nR is the relative gain in perforamnce efficiency after a new LV is added to the model.  The iterations continue until R becomes lower than a threshold value alpha.  By default and only as an indication, the default alpha=.05  is set in the function, but the user should set any other value depending  on his data and parsimony objective.\n\nIn his original article, Wold (1978; see also Bro et al. 2008) used the  ratio of cross-validated over training residual sums of squares, i.e. PRESS over SSR.  Instead, function selwold compares values of consistent nature (the successive values in  the input vector r). For instance, r was set to PRESS values in Li et al. (2002) and  Andries et al. (2011), which is equivalent to the \"punish factor\" described in Westad & Martens (2000).\n\nThe ratio R can be erratic (particulary when r is the error rate of a discrimination model), making difficult the dimensionnaly selection.  In such a situation, function selwold proposes to calculate a smoothing of R  (argument smooth).\n\nThe function returns two outputs (in addition to eventual plots):\n\nopt : The index corresponding to the minimum value of r.\nsel : The index of the selection from the R (or smoothed R) threshold.\n\nReferences\n\nAndries, J.P.M., Vander Heyden, Y., Buydens, L.M.C., 2011. Improved variable reduction in partial least squares modelling based on Predictive-Property-Ranked  Variables and adaptation of partial least squares complexity.  Analytica Chimica Acta 705, 292-305. https://doi.org/10.1016/j.aca.2011.06.037\n\nBro, R., Kjeldahl, K., Smilde, A.K., Kiers, H.A.L., 2008. Cross-validation of  component models: A critical look at current methods. Anal Bioanal Chem 390, 1241-1251.  https://doi.org/10.1007/s00216-007-1790-1\n\nLi, B., Morris, J., Martin, E.B., 2002. Model selection for partial least squares regression.  Chemometrics and Intelligent Laboratory Systems 64, 79-89. https://doi.org/10.1016/S0169-7439(02)00051-5\n\nWestad, F., Martens, H., 2000. Variable Selection in near Infrared Spectroscopy Based  on Significance Testing in Partial Least Squares Regression. J. Near Infrared Spectrosc., JNIRS 8, 117â124.\n\nWold S. Cross-Validatory Estimation of the Number of Components in Factor and Principal  Components Models. Technometrics. 1978;20(4):397-405\n\nExamples\n\nusing JchemoData, JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc\nyear = dat.Y.year\ntab(year)\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nn = nro(Xtrain)\nsegm = segmts(n, 50; rep = 10)\n\nnlv = 0:20\nres = gridcvlv(Xtrain, ytrain; segm = segm, nlv = nlv, \n    score = rmsep, fun = plskern, verbose = false).res\n\nzres = selwold(res.nlv, res.y1; smooth = true, \n    graph = true) ;\nzres.opt     # Nb. LVs correponding to the minimal error rate\nzres.sel     # Nb LVs selected with the Wold's criterion\nzres.f       # plots\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.sep-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.sep","text":"sep(pred, Y)\n\nCompute the corrected SEP (\"SEP_c\"), i.e. the standard deviation of the prediction errors.\n\npred : Predictions.\nY : Observed data.\n\nReferences\n\nBellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B., Roger, J.-M., McBratney, A., 2010. Critical review of chemometric indicators commonly used for assessing the quality of  the prediction of soil attributes by NIR spectroscopy.  TrAC Trends in Analytical Chemistry 29, 1073–1081. https://doi.org/10.1016/j.trac.2010.05.006\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nsep(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nsep(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.snv-Tuple{Any}","page":"Index of functions","title":"Jchemo.snv","text":"snv(X; cent = true, scal = true)\nsnv!(X::Matrix; cent = true, scal = true)\n\nStandard-normal-variate (SNV) transformation of each row of X-data.\n\nX : X-data.\ncent : Logical indicating if the centering in done.\nscal : Logical indicating if the scaling in done.\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nwl = names(dat.X)\nwl_num = parse.(Float64, wl)\n\nXp = snv(X) \nplotsp(Xp[1:30, :], wl_num).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.soplsr","page":"Index of functions","title":"Jchemo.soplsr","text":"soplsr(Xbl, Y, weights = ones(size(Xbl, 1)); nlv,\n    scal = false)\n\nMultiblock sequentially orthogonalized PLSR (SO-PLSR).\n\nXbl : List (vector) of blocks (matrices) of X-data.    Each component of the list is a block.\nY : Y-data.\nweights : Weights of the observations (rows).    Internally normalized to sum to 1. \nnlv : Nb. latent variables (LVs) to consider for each block.    A vector having a length equal to the nb. blocks.\nscal : Boolean. If true, each column of blocks in Xbl and    of Y is scaled by its uncorrected standard deviation    (before the block scaling).\n\nReferences\n\nBiancolillo et al. , 2015. Combining SO-PLS and linear discriminant analysis    for multi-block classification. Chemometrics and Intelligent Laboratory Systems,    141, 58-67.\nBiancolillo, A. 2016. Method development in the area of multi-block analysis focused on    food analysis. PhD. University of copenhagen.\nMenichelli et al., 2014. SO-PLS as an exploratory tool for path modelling.    Food Quality and Preference, 36, 122-134.\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/ham.jld2\") \n@load db dat\npnames(dat) \n\nX = dat.X\ny = dat.Y.c1\ngroup = dat.group\nlistbl = [1:11, 12:19, 20:25]\nXbl = mblock(X, listbl)\n# \"New\" = first two rows of Xbl \nXbl_new = mblock(X[1:2, :], listbl)\n\nnlv = [2; 1; 2]\nfm = soplsr(Xbl, y; nlv = nlv) ;\npnames(fm)\nfm.T\nJchemo.transform(fm, Xbl_new)\n[y Jchemo.predict(fm, Xbl).pred]\nJchemo.predict(fm, Xbl_new).pred\n\n\n\n\n\n","category":"function"},{"location":"api/#Jchemo.sourcedir-Tuple{Any}","page":"Index of functions","title":"Jchemo.sourcedir","text":"sourcedir(path)\n\nInclude all the files contained in a directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ssq-Tuple{Any}","page":"Index of functions","title":"Jchemo.ssq","text":"ssq(X)\n\nCompute the total inertia of a matrix.\n\nX : Matrix.\n\nSum of all the squared components of X (= norm(X)^2; Squared Frobenius norm). \n\nExamples\n\nX = rand(5, 2) \nssq(X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.ssr-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.ssr","text":"ssr(pred, Y)\n\nCompute the sum of squared prediction errors (SSR).\n\npred : Predictions.\nY : Observed data.\n\nExamples\n\nXtrain = rand(10, 5) \nYtrain = rand(10, 2)\nytrain = Ytrain[:, 1]\nXtest = rand(4, 5) \nYtest = rand(4, 2)\nytest = Ytest[:, 1]\n\nfm = plskern(Xtrain, Ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nssr(pred, Ytest)\n\nfm = plskern(Xtrain, ytrain; nlv = 2)\npred = Jchemo.predict(fm, Xtest).pred\nssr(pred, ytest)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.stah-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.stah","text":"stah(X, a; scal = true)\n\nStahel-Donoho outlierness.\n\nX : X-data.\na : Nb. dimensions simulated for the projection pursuit method.\nscal : Boolean. If true, matrix X is centred (by median)    and scaled (by MAD) before computing the outlierness.\n\nThe outlierness measure is computed from a projection-pursuit approach:\n\ndirections in the column-X space (linear combinations of the columns    of X) are randomly simulated, \nand the observations (rows of X) are projected on these directions.\n\nSee Maronna and Yohai (1995) for details. \n\nReferences\n\nMaronna, R.A., Yohai, V.J., 1995. The Behavior of the Stahel-Donoho Robust Multivariate Estimator.  Journal of the American Statistical Association 90, 330–341. https://doi.org/10.1080/01621459.1995.10476517\n\nExamples\n\nusing StatsBase\n\nn = 300 ; p = 700 ; m = 80 ; ntot = n + m\nX1 = randn(n, p)\nX2 = randn(m, p) .+ sample(1:3, p)'\nX = vcat(X1, X2)\n\na = 100\nres = stah(X, a; scal = true) ;\nres.d # outlierness\n\nplotxy(1:nro(X), res.d).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.summ-Tuple{Any}","page":"Index of functions","title":"Jchemo.summ","text":"summ(X; digits = 3)\nsumm(X, group; digits = 1)\n\nSummarize a dataset (or a variable).\n\ngroup : A vector (n,) defing the groups.\ndigits : Nb. digits in the outputs.\n\nExamples\n\nX = rand(10, 3) \nres = summ(X)\npnames(res)\nsumm(X[:, 2]).res\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.tab-Tuple{Any}","page":"Index of functions","title":"Jchemo.tab","text":"tab(x)\n\nUnivariate tabulation.\n\nx : Categorical variable.\n\nThe output cointains sorted levels.\n\nExamples\n\nx = rand([\"a\";\"b\";\"c\"], 20)\nres = tab(x)\nres.keys\nres.vals\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.tabdf-Tuple{Any}","page":"Index of functions","title":"Jchemo.tabdf","text":"tabdf(X; groups = nothing)\n\nCompute the nb. occurences of groups in categorical variables of      a dataset.\n\nX : Data.\ngroups : Names of the group variables to consider    in X (by default: all the columns of X).\n\nThe output (dataframe) contains sorted levels.\n\nExamples\n\nn = 20\nX =  hcat(rand(1:2, n), rand([\"a\", \"b\", \"c\"], n))\ntabdf(X)\ntabdf(X[:, 2])\n\ndf = DataFrame(X, [:v1, :v2])\ntabdf(df)\ntabdf(df; groups = [:v1, :v2])\ntabdf(df; groups = :v2)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.tabdupl-Tuple{Any}","page":"Index of functions","title":"Jchemo.tabdupl","text":"tabdupl(x)\n\nTabulate duplicated values in a vector.\n\nx : Categorical variable.\n\nExamples\n\nx = [\"a\", \"b\", \"c\", \"a\", \"b\", \"b\"]\ntab(x)\nres = tabdupl(x)\nres.keys\nres.vals\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Cca, Any, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Cca, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.CcaWold, Any, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::CcaWold, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Comdim, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Comdim, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and X-data.\n\nobject : The fitted model.\nXbl : A list (vector) of blocks (matrices) of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Dkplsr, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Dkplsr, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Dkplsrda, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Dkplsrda, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and a matrix X.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Kpca, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Kpca, X; nlv = nothing)\n\nCompute PCs (scores T) from a fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which PCs are computed.\nnlv : Nb. PCs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Kplsr, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Kplsr, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Mbpca, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Mbpca, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and X-data.\n\nobject : The fitted model.\nXbl : A list (vector) of blocks (matrices) of X-data for which LVs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.PlsCan, Any, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::PlsCan, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.PlsTuck, Any, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::PlsSVd, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Plslda, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Plslda, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and a matrix X.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Plsr, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Plsr, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and a matrix X.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Plsrda, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Plsrda, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and a matrix X.\n\nobject : The fitted model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Rasvd, Any, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Rasvd, X, Y; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and (X, Y)-data.\n\nobject : The fitted model.\nX : X-data for which components (LVs) are computed.\nY : Y-data for which components (LVs) are computed.\nnlv : Nb. LVs to compute. If nothing, it is the maximum number   from the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Rosaplsr, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Rosaplsr, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list (vector) of blocks (matrices) of X-data for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Rp, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Rp, X; nlv = nothing)\n\nCompute \"scores\" T from a random projection model and a matrix X.\n\nobject : The random projection model.\nX : Matrix (m, p) for which LVs are computed.\nnlv : Nb. dimensions to consider. If nothing, it is the maximum nb. dimensions.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Jchemo.Soplsr, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Soplsr, Xbl)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list (vector) of blocks (matrices) for which LVs are computed.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Union{Jchemo.Fda, Jchemo.Pca}, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Pca, X; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model and X-data.\n\nobject : The fitted model.\nX : X-data for which PCs are computed.\nnlv : Nb. LVs to compute.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.transform-Tuple{Union{Jchemo.MbplsWest, Jchemo.Mbplsr}, Any}","page":"Index of functions","title":"Jchemo.transform","text":"transform(object::Union{MbplsWest, Mbplsr}, Xbl; nlv = nothing)\n\nCompute latent variables (LVs = scores T) from a fitted model.\n\nobject : The fitted model.\nXbl : A list (vector) of blocks (matrices) for which LVs are computed.\nnlv : Nb. LVs to consider.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.treeda_dt-Tuple{Any, Union{Array{Int64}, Array{String}}}","page":"Index of functions","title":"Jchemo.treeda_dt","text":"treeda_dt(X, yy::Union{Array{Int64}, Array{String}}; \n    n_subfeatures = 0,\n    max_depth = -1, min_samples_leaf = 5, \n    min_samples_split = 2, scal = false, \n    kwargs...)\n\nDiscrimination tree (CART) with DecisionTree.jl.\n\nX : X-data (n obs., p variables).\ny : Univariate y-data (n obs.).\nn_subfeatures : Nb. variables to select at random at each split (default: 0 ==> keep all).\nmax_depth : Maximum depth of the decision tree (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples each leaf needs to have.\nmin_sample_split : Minimum number of observations in needed for a split.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nkwargs : Optional named arguments to pass in function build_tree    of DecisionTree.jl.\n\nThe function fits a single discrimination tree (CART) using package  `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. Classification And Regression Trees. Chapman & Hall, 1984.\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245\n\nExamples\n\nusing JchemoData, JLD2\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/iris.jld2\") \n@load db dat\npnames(dat)\nsumm(dat.X)\n  \nX = dat.X[:, 1:4] \ny = dat.X[:, 5]\nntot, p = size(X)\n  \nntrain = 120\ns = sample(1:n, ntrain; replace = false) \nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\nntest = ntot - ntrain\n(ntot = ntot, ntrain, ntest)\n\ntab(ytrain)\ntab(ytest)\n\nn_subfeatures = 2 \nmax_depth = 6\nfm = treeda_dt(Xtrain, ytrain;\n    n_subfeatures = n_subfeatures, max_depth = max_depth) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xtest)\nres.pred\nerr(res.pred, ytest) \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.treer_dt-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.treer_dt","text":"treer_dt(X, y; n_subfeatures = 0,\n    max_depth = -1, min_samples_leaf = 5, \n    min_samples_split = 2, scal = false, \n    kwargs...)\n\nRegression tree (CART) with DecisionTree.jl.\n\nX : X-data (n obs., p variables).\ny : Univariate y-data (n obs.).\nn_subfeatures : Nb. variables to select at random at each split (default: 0 ==> keep all).\nmax_depth : Maximum depth of the decision tree (default: -1 ==> no maximum).\nmin_sample_leaf : Minimum number of samples each leaf needs to have.\nmin_sample_split : Minimum number of observations in needed for a split.\nscal : Boolean. If true, each column of X    is scaled by its uncorrected standard deviation.\nkwargs : Optional named arguments to pass in function build_tree    of DecisionTree.jl.\n\nThe function fits a single regression tree (CART) using package  `DecisionTree.jl'.\n\nReferences\n\nBreiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. Classification And Regression Trees. Chapman & Hall, 1984.\n\nDecisionTree.jl https://github.com/JuliaAI/DecisionTree.jl\n\nGey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression (These de doctorat).  Paris 11. http://www.theses.fr/2002PA112245\n\nExamples\n\nusing JchemoData, JLD2, CairoMakie\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/challenge2021.jld2\")\n@load db dat\npnames(dat)\n\nXtrain = dat.Xtrain\nYtrain = dat.Ytrain\nytrain = Ytrain.y\ns = dat.Ytest.inst .== 1 \nXtest = dat.Xtest[s, :]\nYtest = dat.Ytest[s, :]\nytest = Ytest.y\nwl = names(Xtrain) \nwl_num = parse.(Float64, wl) \nntrain, p = size(Xtrain)\nntest = nro(Xtest)\nntot = ntrain + ntest\n(ntot = ntot, ntrain, ntest)\n\nf = 21 ; pol = 3 ; d = 2 \nXptrain = savgol(snv(Xtrain); f, pol, d) \nXptest = savgol(snv(Xtest); f, pol, d) \n\nn_subfeatures = p / 3 \nmax_depth = 20\nfm = treer_dt(Xptrain, ytrain; \n    n_subfeatures = n_subfeatures, \n    max_depth = max_depth) ;\npnames(fm)\n\nres = Jchemo.predict(fm, Xptest)\nres.pred\nrmsep(res.pred, ytest)\nplotxy(vec(res.pred), ytest; color = (:red, .5),\n    bisect = true, xlabel = \"Prediction\", ylabel = \"Observed\").f  \n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vcatdf-Tuple{Any}","page":"Index of functions","title":"Jchemo.vcatdf","text":"vcatdf(dat; cols = :intersect)\n\nVertical concatenation of a list of dataframes.\n\ndat : List (vector) of dataframes.\ncols : Determines the columns of the returned data frame.   See ?DataFrames.vcat.\n\nExamples\n\nusing DataFrames\ndat1 = DataFrame(rand(5, 2), [:v3, :v1]) \ndat2 = DataFrame(100 * rand(2, 2), [:v3, :v1])\ndat = (dat1, dat2)\nJchemo.vcatdf(dat)\n\ndat2 = DataFrame(100 * rand(2, 2), [:v1, :v3])\ndat = (dat1, dat2)\nJchemo.vcatdf(dat)\n\ndat2 = DataFrame(100 * rand(2, 3), [:v3, :v1, :a])\ndat = (dat1, dat2)\nJchemo.vcatdf(dat)\nJchemo.vcatdf(dat; cols = :union)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vcol-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.vcol","text":"vcol(X::Matrix, j)\nvcol(X::DataFrame, j)\nvcol(x::Vector, j)\n\nView of the j-th column(s) of a matrix X, or of the j-th element(s) of vector x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vi_baggr-Tuple{Jchemo.Baggr, Any, Any}","page":"Index of functions","title":"Jchemo.vi_baggr","text":"vi_baggr(object::Baggr, X, Y; score = rmsep)\n\nVariable importance after bagging (Out-of-bag permutations method).\n\nobject : Output of a bagging.\nX : X-data that was used in the model bagging.\nY : Y-data that was used in the model bagging.\n\nVariances importances are computed  by permuting sucessively each column of the out-of-bag (X_OOB), and by looking at the effect on the error rate (e.g. RMSEP).   See ?baggr for examples.\n\nReferences\n\nBreiman, L., 2001. Random Forests. Machine Learning 45, 5–32.  https://doi.org/10.1023/A:1010933404324 Genuer, R., 2010. Forêts aléatoires : aspects théoriques,  sélection de variables et applications. PhD Thesis. Université Paris Sud - Paris XI. Gey, S., 2002. Bornes de risque, détection de ruptures, boosting :  trois thèmes statistiques autour de CART en régression. PhD Thesis.  Univ. Paris 11. http://www.theses.fr/2002PA112245\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vip-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}}","page":"Index of functions","title":"Jchemo.vip","text":"vip(object::Union{Pcr, Plsr}; nlv = nothing)\nvip(object::Union{Pcr, Plsr}, Y; nlv = nothing)\n\nVariable importance on Projections (VIP).\n\nobject : The fitted model (object of structure Plsr).\nY : The Y-data that was used to fit the model.\nnlv : Nb. latent variables (LVs) to consider.\n\nFor a PLS (or PCR, etc.) model (X, Y) with a number of A latent variables,  and variable xj (column j of X): \n\nVIP(xj) = Sum(a=1,...,A) R2(Yc, ta) waj^2 / Sum(a=1,...,A) R2(Yc, ta) (1 / p) \n\nwhere:\n\nYc is the centered Y, \nta is the ath X-score, \nand R2(Yc, ta) the proportion of Yc-variance explained by ta,    i.e. ||Yc.hat||^2 / ||Yc||^2 (where Yc.hat is the LS estimate of Yc by ta).  \n\nWhen Y is used as argument, R2(Yc, ta) is replaced by the redundancy Rd(Yc, ta) (see function rd), such as in Tenenhaus 1998 p.139. \n\nReferences\n\nChong, I.-G., Jun, C.-H., 2005. Performance of some variable selection methods when  multicollinearity is present. Chemometrics and Intelligent Laboratory Systems 78, 103–112.  https://doi.org/10.1016/j.chemolab.2004.12.011\n\nMehmood, T., Sæbø, S., Liland, K.H., 2020. Comparison of variable selection methods  in partial least squares regression. Journal of Chemometrics 34, e3226.  https://doi.org/10.1002/cem.3226\n\nTenenhaus, M., 1998. La régression PLS: théorie et pratique.  Editions Technip, Paris.\n\nExamples\n\nX = [1. 2 3 4; 4 1 6 7; 12 5 6 13; 27 18 7 6; 12 11 28 7] \nY = [10. 11 13; 120 131 27; 8 12 4; 1 200 8; 100 10 89] \ny = Y[:, 1] \nycla = [1; 1; 1; 2; 2]\n\nnlv = 3\nfm = plskern(X, Y; nlv = nlv) ;\nres = vip(fm)\npnames(res)\nmean(res.imp.^2)\nvip(fm; nlv = 1).imp\n\nnlv = 2\nfm = plsrda(X, ycla; nlv = nlv) ;\nfmpls = fm.fm\nvip(fmpls).imp\nYdummy = dummy(ycla).Y\nvip(fmpls, Ydummy).imp\n\nnlv = 2\nfm = plslda(X, ycla; nlv = nlv) ;\nfmpls = fm.fm.fm_pls\nvip(fmpls).imp\nYdummy = dummy(ycla).Y\nvip(fmpls, Ydummy).imp\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.viperm-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.viperm","text":"viperm(X, Y; perm = 50,\n    psamp = 1/3, score = rmsep, fun, \n    kwargs...)\n\nVariable importance by direct permutations.\n\nX : X-data (n, p).\nY : Y-data (n, q).  \nperm : Number of replications. \nnint : Nb. intervals. \npsamp : Proportion of data used as test set to compute the score   (default: n/3 of the data).\nscore : Function computing the prediction score (= error rate; e.g. msep).\nfun : Function defining the prediction model.\nkwarg : Optional other arguments to pass to funtion defined in fun.\n\nThe principle is as follows:\n\nData (X, Y) are splitted randomly to a training and a test set.\nThe model is fitted on Xtrain, and the score (error rate) on Xtest.   This gives the reference error rate.\nRows of a given variable (feature) j in Xtest are randomly permutated   (the rest of Xtest is unchanged). The score is computed on    the permuted Xtest and the new score is computed. The importance   is computed by the difference between this score and the referece score.\nThis process is run for each variable separately and replicated perm times.   Average results are provided in the outputs, as well the results per    replication. \n\nIn general, this method returns similar results as the out-of-bag permutation method used in random forests (Breiman, 2001).\n\nReferences\n\nNørgaard, L., Saudland, A., Wagner, J., Nielsen, J.P., Munck, L., \n\nEngelsen, S.B., 2000. Interval Partial Least-Squares Regression (iPLS):  A Comparative Chemometric Study with an Example from Near-Infrared  Spectroscopy. Appl Spectrosc 54, 413–419. https://doi.org/10.1366/0003702001949500\n\nExamples\n\nusing JchemoData, DataFrames, JLD2\nusing CairoMakie\n\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/tecator.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X\nY = dat.Y \nwl = names(X)\nwl_num = parse.(Float64, wl) \ntyp = Y.typ\ny = Y.fat\n\nf = 15 ; pol = 3 ; d = 2 \nXp = savgol(snv(X); f = f, pol = pol, d = d) \n\ns = typ .== \"train\"\nXtrain = Xp[s, :]\nytrain = y[s]\n\nres = viperm(Xtrain, ytrain; perm = 50, \n    score = rmsep, fun = plskern, nlv = 9)\nz = vec(res.imp)\nf = Figure(resolution = (500, 400))\nax = Axis(f[1, 1];\n    xlabel = \"Wavelength (nm)\", \n    ylabel = \"Importance\")\nscatter!(ax, wl_num, vec(z); color = (:red, .5))\nu = [910; 950]\nvlines!(ax, u; color = :grey, linewidth = 1)\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.vrow-Tuple{Any, Any}","page":"Index of functions","title":"Jchemo.vrow","text":"vrow(X::Matrix, i)\nvrow(X::DataFrame, i)\nvrow(x::Vector, i)\n\nView of the i-th row(s) of a matrix X, or of the i-th element(s) of vector x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.wdist-Tuple{Any}","page":"Index of functions","title":"Jchemo.wdist","text":"wdist(d; h = 2, cri = 4, squared = false)\nwdist!(d; h = 2, cri = 4, squared = false)\n\nCompute weights from distances, using a decreasing exponential function.\n\nd : A vector of distances.\nh : A scaling positive scalar defining the shape of the function. \ncri : A positive scalar defining outliers in the distances vector.\nsquared: If true, distances are replaced by the squared distances;   the weight function is then a Gaussian (RBF) kernel function.\n\nWeights are computed by exp(-d / (h * MAD(d))), or are set to 0 for  distances > Median(d) + cri * MAD(d). This is an adaptation of the weight function presented in Kim et al. 2011.\n\nThe weights decrease with increasing distances. Lower is h, sharper is the decreasing function.  Weights are set to 0 for outliers (extreme distances).\n\nReferences\n\nKim S, Kano M, Nakagawa H, Hasebe S. Estimation of active pharmaceutical ingredients content  using locally weighted partial least squares and statistical wavelength selection.  Int J Pharm. 2011;421(2):269-274. https://doi.org/10.1016/j.ijpharm.2011.10.007\n\nExamples\n\nusing CairoMakie, Distributions\n\nx1 = rand(Chisq(10), 100) ;\nx2 = rand(Chisq(40), 10) ;\nd = [sqrt.(x1) ; sqrt.(x2)]\nh = 2 ; cri = 3\nw = wdist(d; h = h, cri = cri) ;\nf = Figure(resolution = (600, 400))\nax1 = Axis(f, xlabel = \"Distance\", ylabel = \"Nb. observations\")\nhist!(ax1, d, bins = 30)\nax2 = Axis(f, xlabel = \"Distance\", ylabel = \"Weight\")\nscatter!(ax2, d, w)\nf[1, 1] = ax1 \nf[1, 2] = ax2 \nf\n\nd = collect(0:.5:15) ;\nh = [.5, 1, 1.5, 2.5, 5, 10, Inf] ;\n#h = [1, 2, 5, Inf] ;\nw = wdist(d; h = h[1]) ;\nf = Figure(resolution = (600, 500))\nax = Axis(f, xlabel = \"Distance\", ylabel = \"Weight\")\nlines!(ax, d, w, label = string(\"h = \", h[1]))\nfor i = 2:length(h)\n    w = wdist(d; h = h[i])\n    lines!(ax, d, w, label = string(\"h = \", h[i]))\nend\naxislegend(\"Values of h\")\nf[1, 1] = ax\nf\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.wshenk-Tuple{Union{Jchemo.Pcr, Jchemo.Plsr}, Any}","page":"Index of functions","title":"Jchemo.wshenk","text":"wshenk(object::Union{Pcr, Plsr}, X; nlv = nothing)\n\nCompute the Shenk et al. (1997) \"LOCAL\" PLSR weights\n\nobject : The fitted model.\nX : X-data on which the weights are computed.\nnlv : Nb. latent variables (LVs) to consider. If nothing,    it is the maximum nb. of components.\n\nFor each observation (row) of X, the weights are returned  for the models with 1, ..., nlv LVs. \n\nReferences\n\nShenk, J., Westerhaus, M., Berzaghi, P., 1997. Investigation of a LOCAL calibration  procedure for near infrared instruments.  Journal of Near Infrared Spectroscopy 5, 223. https://doi.org/10.1255/jnirs.115\n\nShenk et al. 1998 United States Patent (19). Patent Number: 5,798.526.\n\nZhang, M.H., Xu, Q.S., Massart, D.L., 2004. Averaged and weighted average partial  least squares. Analytica Chimica Acta 504, 279–289. https://doi.org/10.1016/j.aca.2003.10.056\n\nExamples\n\nusing JLD2\npath_jdat = dirname(dirname(pathof(JchemoData)))\ndb = joinpath(path_jdat, \"data/cassav.jld2\") \n@load db dat\npnames(dat)\n\nX = dat.X \ny = dat.Y.tbc \nyear = dat.Y.year\n\ns = year .<= 2012\nXtrain = X[s, :]\nytrain = y[s]\nXtest = rmrow(X, s)\nytest = rmrow(y, s)\n\nnlv = 30\nfm = plskern(Xtrain, ytrain; nlv = nlv) ;\nres = Jchemo.wshenk(fm, Xtest) ;\npnames(res) \nplotsp(res.w, 1:nlv).f\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.xfit-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}, Any}","page":"Index of functions","title":"Jchemo.xfit","text":"xfit(object::Union{Pca, Pcr, Plsr}, X; nlv = nothing)\nxfit!(object::Union{Pca, Pcr, Plsr}, X::Matrix; nlv = nothing)\n\nMatrix fitting from a PCA, PCR or PLS model\n\nobject : The fitted model.\nX : X-data to be approximatred from the model.\nnlv : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components.\n\nCompute an approximate of matrix X (X_fit) from a PCA, PCR  or PLS fitted on X.\n\nX and X_fit are in the original scale, i.e. before centering and eventual scaling.\n\nExamples\n\nn, p = 5, 3\nX = rand(n, p)\ny = rand(n)\n\nnlv = 2 ;\nfm = pcasvd(X; nlv = nlv) ;\n#fm = plskern(X, y; nlv = nlv) ;\nxfit(fm, X)\nxfit(fm, X, nlv = 0)\nxfit(fm, X, nlv = 1)\n\nfm = pcasvd(X; nlv = min(n, p)) ;\nxfit(fm, X)\nxresid(fm, X)\n\n\n\n\n\n","category":"method"},{"location":"api/#Jchemo.xresid-Tuple{Union{Jchemo.Pca, Jchemo.Pcr, Jchemo.Plsr}, Any}","page":"Index of functions","title":"Jchemo.xresid","text":"xresid(object::Union{Pca, Pcr, Plsr}, X; nlv = nothing)\nxresid!(object::Union{Pca, Pcr, Plsr}, X::Matrix; nlv = nothing)\n\nResidual matrix after fitting by a PCA, PCR or PLS model\n\nobject : The fitted model.\nX : X-data for which the residuals have to be computed.\nnlv : Nb. components (PCs or LVs) to consider. If nothing,    it is the maximum nb. of components.\n\nCompute the residual matrix E = X - X_fit.\n\nX and X_fit are in the original scale, i.e. before centering and eventual scaling.\n\nExamples\n\nn, p = 5, 3\nX = rand(n, p)\ny = rand(n)\n\nnlv = 2 ;\nfm = pcasvd(X; nlv = nlv) ;\n#fm = plskern(X, y; nlv = nlv) ;\nxresid(fm, X)\nxresid(fm, X, nlv = 0)\nxresid(fm, X, nlv = 1)\n\nfm = pcasvd(X; nlv = min(n, p)) ;\nxfit(fm, X)\nxresid(fm, X)\n\n\n\n\n\n","category":"method"},{"location":"domains/#Available-methods","page":"Available methods","title":"Available methods","text":"","category":"section"},{"location":"domains/#MULTIVARIATE-EXPLORATORY-ANALYSES","page":"Available methods","title":"MULTIVARIATE EXPLORATORY ANALYSES","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Principal component analysis (PCA) ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"pcaeigen Eigen decomposition\npcaeigenk Eigen decomposition for wide matrices (kernel form)\npcasvd SVD decomposition\nRobust PCA\npcasph Spherical (with spatial median)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Utilities for PCA and PLS ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"xfit Matrix fitting \nxresid Residual matrix ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Random projections","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"rp Random projection\nrpmatgauss Gaussian random projection matrix \nrpmatli Sparse random projection matrix ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear PCA","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kpca Kernel (KPCA) Scholkopf et al. 2002","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Multiblock","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"2 blocks\ncca: Canonical correlation analysis (CCA)\nccawold: CCA - Wold (1984) Nipals algorithm  \nplscan: Canonical partial least squares regression (Symmetric PLS)\nplstuck: Tucker's inter-battery method of factor analysis (PLS-SVD)\nrasvd: Redundancy analysis (RA) - PCA on instrumental variables (PCAIV)\n2 or more blocks \nmbpca Multiblock PCA (MBPCA = CPCA Consensus principal component analysis)\ncomdim Common components and specific weights analysis (ComDim = CCSWA = HPCA)\nmbunif: Unified multiblock data analysis of Mangana et al. 2019\nUtilities\nmblock Make blocks from a matrix\nblockscal_col, _frob, _mfa, _sd Scaling blocks\nrd Redundancy coefficients between two matrices\nlg Lg coefficient\nrv RV correlation coefficient","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Factorial discrimination analysis (FDA)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"fda Eigen decomposition of the compromise \"inter/intra\"\nfdasvd Weighted SVD decomposition of the class centers","category":"page"},{"location":"domains/#REGRESSION","page":"Available methods","title":"REGRESSION","text":"","category":"section"},{"location":"domains/#**Linear-models**","page":"Available methods","title":"Linear models","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Multiple linear regression (MLR)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mlr QR algorithm\nmlrchol Normal equations and Choleski factorization\nmlrpinv Pseudo-inverse\nmlrpinvn Normal equations and pseudo-inverse\nmlrvec Simple linear regression (Univariate x)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Anova","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"aov1 One factor ANOVA","category":"page"},{"location":"domains/#**Partial-least-squares-(PLSR)**","page":"Available methods","title":"Partial least squares (PLSR)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"PLSR","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plskern Fast \"improved kernel #1\" algorithm of Dayal & McGregor 1997\nplsnipals Nipals\nplswold Nipals Wold 1984\nplsrosa ROSA Liland et al. 2016\nplssimp SIMPLS de Jong 1993","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Variants ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"cglsr Conjugate gradient for the least squares normal equations (CGLS)\nrrr Reduced rank regression (RRR)  (= redundancy analysis regression) \npcr Principal components regression (SVD factorization)\ncovselr MLR on variables selected from Covsel","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kplsr Non linear kernel (KPLSR) Rosipal & Trejo 2001\ndkplsr Direct non linear kernel (DKPLSR) Bennett & Embrechts 2003","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Averaging and stacking","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plsravg Averaging and stacking PLSR models with different numbers of    latent variables (LVs) (PLSR-AVG)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Multiblock","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mbplsr Multiblock PLSR (MBPLSR) - Fast version (PLSR on concatenated blocks)\nmbplswest MBPLSR - Nipals algorithm (Westerhuis et al. 1998) \nmbwcov Multiblock weighted covariate analysis regression (MBWCov) (Mangana et al. 2021) \nrosaplsr ROSA Liland et al. 2016\nsoplsr Sequentially orthogonalized (SO-PLSR) ","category":"page"},{"location":"domains/#**Ridge-(RR,-KRR)**","page":"Available methods","title":"Ridge (RR, KRR)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"RR","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"rr Pseudo-inverse (RR)\nrrchol Choleski factorization (RR)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"krr Non linear kernel (KRR) = Least squares SVM (LS-SVMR)","category":"page"},{"location":"domains/#**Local-models**","page":"Available methods","title":"Local models","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"knnr kNNR\nlwmlr kNN locally weighted multiple linear regression (kNN-LWMLR)\nlwplsr kNN locally weighted PLSR (kNN-LWPLSR)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"With preliminary dimension reduction","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lwmlr_s: kNN-LWMLR-S\nlwplsr_s kNN-LWPLSR-S","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Averaging and stacking","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lwplsravg kNN-LWPLSR-AVG \ncplsravg Clustered PLSR-AVG","category":"page"},{"location":"domains/#**Trees**-from-DecisionTree.jl","page":"Available methods","title":"Trees from DecisionTree.jl","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"treer_dt Single tree\nrfr_dt Random forest","category":"page"},{"location":"domains/#**Generic-bagging**","page":"Available methods","title":"Generic bagging","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"baggr Bagging \noob_baggr Out-of-bag (OOB) error rate","category":"page"},{"location":"domains/#DISCRIMINATION-ANALYSIS-(DA)","page":"Available methods","title":"DISCRIMINATION ANALYSIS (DA)","text":"","category":"section"},{"location":"domains/#One-Class-Classification-(OCC)","page":"Available methods","title":"One-Class Classification (OCC)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"From a PCA or PLS score space","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"occsd Score distance (SD)\noccod Orthogonal distance (OD) \noccsdod Compromise between SD and OD ","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Other methods","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"occknndis: Global k-nearest neighbors distances.\nocclknndis: Local k-nearest neighbors distances.\noccstah Stahel-Donoho outlierness.\nstah Compute Stahel-Donoho outlierness.","category":"page"},{"location":"domains/#DA-based-on-predicted-Y-dummy-table","page":"Available methods","title":"DA based on predicted Y-dummy table","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mlrda Y-dummy MLR predictions (MLR-DA)\nplsrda Y-dummy PLSR predictions (PLSR-DA; = common \"PLSDA\")\nplsrdaavg Averaging PLSR-DA models with different numbers of    latent variables (LVs) (PLSR-DA-AVG)\nrrda Y-dummy RR predictions (RR-DA)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Non linear","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"kplsrda Y-dummy KPLSR predictions (KPLSR-DA)\ndkplsrda Y-dummy DKPLSR predictions (DKPLSR-DA)\nkrrda Y-dummy KRR predictions (KRR-DA)","category":"page"},{"location":"domains/#Probabilistic","page":"Available methods","title":"Probabilistic","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lda Linear discriminant analysis (LDA)\nqda Quadratic discriminant analysis (QDA)\nplslda LDA on PLS latent variables (PLS-LDA)\nplsqda QDA on PLS latent variables (PLS-QDA)\nplsqdaavg Averaging PLS-QDA models with different numbers of    latent variables (LVs) (PLS-QDA-AVG)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Averaging","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plsldaavg Averaging PLS-LDA models with different numbers of    latent variables (LVs) (PLS-LDA-AVG)","category":"page"},{"location":"domains/#**Local-models**-2","page":"Available methods","title":"Local models","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"knnda kNN-DA (Vote within neighbors)\nlwmlrda kNN locally weighted MLR-DA (kNN-LWMLR-DA)\nlwplsrda kNN Locally weighted PLSR-DA (kNN-LWPLSR-DA)\nlwplslda kNN Locally weighted PLS-LDA (kNN-LWPLS-LDA)\nlwplsqda kNN Locally weighted PLS-QDA (kNN-LWPLS-QDA)","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"With preliminary dimension reduction","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lwmlrda_s kNN-LWMLR-DA-S\nlwplsrda_s kNN-LWPLSR-DA-S","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Averaging","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"lwplsrdaavg kNN-LWPLSR-DA-AVG\nlwplsldaavg kNN-LWPLS-LDA-AVG\nlwplsqdaavg kNN-LWPLS-QDA-AVG","category":"page"},{"location":"domains/#**Trees**-from-DecisionTree.jl-2","page":"Available methods","title":"Trees from DecisionTree.jl","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"treeda_dt Single tree\nrfda_dt Random forest","category":"page"},{"location":"domains/#DISTRIBUTIONS","page":"Available methods","title":"DISTRIBUTIONS","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"dmnorm Compute normal probability density for multivariate data\nkdem Multivariate kernel density estimation (KDE)\npval Compute p-value(s) for a distribution, a vector or an ECDF\nout Return if elements of a vector are strictly outside of a given range","category":"page"},{"location":"domains/#VARIABLE-IMPORTANCE","page":"Available methods","title":"VARIABLE IMPORTANCE","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"covsel Variable selection from partial covariance or correlation (Covsel)\nisel Interval variable selection (e.g. Interval PLSR).\nvip Variable importance on projections (VIP)\nvi_baggr Variable importance after bagging (OOB permutations method)\nviperm Variable importance by direct permutations","category":"page"},{"location":"domains/#TUNING-MODELS","page":"Available methods","title":"TUNING MODELS","text":"","category":"section"},{"location":"domains/#**Grid**","page":"Available methods","title":"Grid","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mpar Expand a grid of parameter values","category":"page"},{"location":"domains/#**Validation**","page":"Available methods","title":"Validation","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"gridscore Any model\ngridscorelv Models with LVs (faster)\ngridscorelb Models with ridge parameter (faster)","category":"page"},{"location":"domains/#**Cross-validation-(CV)**","page":"Available methods","title":"Cross-validation (CV)","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"gridcv Any model\ngridcvlv Models with LVs (faster)\ngridcvlb Models with ridge parameter (faster)  \ngridcv_mb Multiblock models \ngridcvlv_mb Multiblock models with LVs \nsegmkf Building segments for K-fold CV\nsegmts Building segments for test-set validation","category":"page"},{"location":"domains/#**Performance-scores**","page":"Available methods","title":"Performance scores","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Regression","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"ssr SSR\nmsep MSEP\nrmsep, rmsepstand RMSEP\nsep SEP\nbias Bias\ncor2 Squared correlation coefficient\nr2 R2\nrpd, rpdr Ratio of performance to deviation\nmse Summary for regression\nconfusion Confusion matrix","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Discrimination","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"err Classification error rate","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Model dimensionality","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"aicplsr AIC and Cp for PLSR\nselwold Wold's criterion to select dimensionality in LV (e.g. PLSR) models","category":"page"},{"location":"domains/#DATA-MANAGEMENT","page":"Available methods","title":"DATA MANAGEMENT","text":"","category":"section"},{"location":"domains/#**Checking**","page":"Available methods","title":"Checking","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"checkdupl Finding replicated rows in a dataset\ncheckmiss Finding rows with missing data in a dataset\ntabdupl Tabulate duplicated values in a vector","category":"page"},{"location":"domains/#**Calibration-transfer**","page":"Available methods","title":"Calibration transfer","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"calds : Direct standardization (DS).\ncalpds : Piecewise direct standardization (PDS).\neposvd External parameter orthogonalization (EPO)","category":"page"},{"location":"domains/#**Pre-processing**","page":"Available methods","title":"Pre-processing","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"detrend Polynomial detrend\nfdif Finite differences\nmavg, mavg_runmean Smoothing by moving average\nrmgap Remove vertical gaps in spectra, e.g. for ASD NIR data\nsavgk, savgol Savitsky-Golay filtering\nsnv Standard-normal-deviation transformation","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"Interpolation","category":"page"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"interpl Sampling signals by interpolation – From DataInterpolations.jl\ninterpl_mon Sampling signals by monotonic interpolation – From Interpolations.jl","category":"page"},{"location":"domains/#**Sampling-observations**","page":"Available methods","title":"Sampling observations","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"mtest Select indexes defining training and test sets for each column    of a dataframe\nsampdp Duplex sampling \nsampks Kennard-Stone sampling \nsampsys Systematic sampling\nsampcla Stratified sampling","category":"page"},{"location":"domains/#PLOTTING","page":"Available methods","title":"PLOTTING","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"plotconf Plot confusion matrix\nplotgrid Plot error or performance rates of model predictions\nplotsp Plot spectra\nplotxy xy scatter plot","category":"page"},{"location":"domains/#UTILITIES","page":"Available methods","title":"UTILITIES","text":"","category":"section"},{"location":"domains/","page":"Available methods","title":"Available methods","text":"aggstat Compute column-wise statistics (e.g. mean), by group in a dataset\ncenter, scale, cscale Column-wise centering and scaling of a matrix\ncolmad, colmean, colnorm, colstd, colsum, colvar  Column-wise operations\ncovm, corm Covariance and correlation matrices\ncosv, cosm Cosinus between vectors\ndummy Build dummy table\neuclsq, mahsq, mahsqchol Distances (Euclidean, Mahalanobis) between rows of matrices\nfindmax_cla Find the most occurent level in a categorical variable\nfrob Frobenius norm of a matrix\nfweight Compute weights from distances\ngetknn Find nearest neighbours between rows of matrices\nhead Display the first rows of a dataset\niqr Interval inter-quartiles\nkrbf, kpol Build kernel Gram matrices\nlocw Working function for local (kNN) models\nmad Median absolute deviation (not exported)\nmatB, matW Between- and within-covariance matrices\nmlev Return the sorted levels of a dataset \nmweight Normalize a vector to sum to 1\nnco, nro, Nb. rows and columns of an object\nnormw Weighted norm of a vector\npnames Return the names of the elements of an object\npsize Return the type and size of a dataset\nrecodcat2int Recode a categorical variable to a numeric variable\nrecodnum2cla Recode a continuous variable to classes\nreplacebylev Replace the elements of a vector by levels of corresponding order\nreplacebylev2 Replace the elements of an index-vector by levels\nreplacedict Replace the elements of a vector by levels defined in a dictionary\nrmcol Remove the columns of a matrix or the components of a vector having indexes s\nrmrow Remove the rows of a matrix or the components of a vector having indexes s\nrowmean, rowstd, rowsum Row-wise operations\nsourcedir Include all the files contained in a directory\nssq Total inertia of a matrix\nsumm Summarize the columns of a dataset\ntab, tabdf, tabdupl Tabulations for categorical variables\nvcatdf Vertical concatenation of a list of dataframes\nwdist Compute weights from distances\nOther utility functions in file utility.jl","category":"page"},{"location":"news/#NEWS-for-package-**Jchemo**","page":"News","title":"NEWS for package Jchemo","text":"","category":"section"},{"location":"news/#*Version-0.1.20*","page":"News","title":"Version 0.1.20","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nkdem: Multivariate kernel density estimation.\nout: Return if elements of a vector are strictly outside of a given range.\npval: Compute p-value(s) for a distribution, an ECDF or a vector.\nModifications\nplotxy: accept a matrix (n, 2) as input.\nCode improvement.\nBreaking changes\ndens: removed and replaced by kde1.","category":"page"},{"location":"news/#*Version-0.1.19*","page":"News","title":"Version 0.1.19","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nModifications\nDependance to unused package HypothesisTests.jl was removed. \nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.18*","page":"News","title":"Version 0.1.18","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nconfusion Confusion matrix.\nplotconf Plot confusion matrix.\nModifications\ncplsravg: new argument 'typda'. \nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.17*","page":"News","title":"Version 0.1.17","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nlwmlrda: k-Nearest-Neighbours locally weighted MLR-based discrimination (kNN-LWMLR-DA).\nlwmlrda_s: kNN-LWMLR-DA after preliminary (linear or non-linear) dimension reduction.\nlwplsrda_s kNN-LWPLSR-DA after preliminary (linear or non-linear) dimension reduction.\nModifications\nlwmlr_s: Add arguments 'psamp' and 'samp' for large nb. observations. \nCode cleaning.\nBreaking changes\nlwplsr_s: Arguments and pipeline changed to be consistent    with lwmlr_s. \nsampclas: remamed to sampcla.","category":"page"},{"location":"news/#*Version-0.1.16*","page":"News","title":"Version 0.1.16","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\ndkplsrda Discrimination based on direct kernel partial least    squares regression (DKPLSR-DA)\ntreer_dt Regression tree (CART) with DecisionTree.jl\nrfr_dt Random forest regression with DecisionTree.jl\ntreeda_dt Discrimination tree (CART) with DecisionTree.jl\nrfda_dt Random forest discrimination with DecisionTree.jl\nModifications\nselwold :  add argument 'step'- \nCode cleaning.\nBreaking changes\nWarning: Difficult breaking bugs appeared in C++ dll from Julia v1.8.4    (still present in v1.9-betas) that removed the possibility to use packages    LIBSVM.jl and XGBoost.jl under Windows. For this reason, Jchemo.jl stopped to use    these two packages. All the related functions (SVM, RF and XGBoost models)    were removed. For CART models (trees), they were replaced by new functions    using package DecisionTree.jl.  ","category":"page"},{"location":"news/#*Version-0.1.15*","page":"News","title":"Version 0.1.15","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\ncosm Cosinus between the columns of a matrix\ncosv Cosinus between two vectors\nlwmlr: k-Nearest-neighbours locally weighted multiple linear regression (kNN-LWMLR)\nlwmlr_s: kNN-LWMLR after preliminary (linear or non-linear) dimension reduction\npmod Short-cut for function 'Base.parentmodule'\ntabdupl Tabulate duplicated values in a vector\nModifications\nImprovement of vi_baggr\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.14*","page":"News","title":"Version 0.1.14","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nisel: Interval variable selection.\nmlev: Return the sorted levels of a dataset.\npcasph: Spherical PCA.\ntabdf: Compute the nb. occurences of groups in categorical variables of    a dataset.\nvip: Variable importance by permutation.\nModifications\nplotgrid: add of argument 'leg'. \nplotxy: add of arguments 'circle' and 'zeros'. \nCode cleaning.\nBreaking changes\naggstat has changed (arguments).\nbaggr_vi renamed to vi_baggr\nbaggr_oob renamed to oob_baggr\ngridcv and gridcv_mb: in output 'res_rep', colum 'rept' replaced    by column 'repl'.\niplsr was removed and replaced by the more generic function isel.\nmtest: Outputs 'idtrain' and 'idtest' renamed to 'train' and 'test'.\nrd: argument 'corr' chanfed to 'typ'.\ntabn was removed.\nvimp_xgb renamed to vi_xgb\nvip: outputs have been improved.","category":"page"},{"location":"news/#*Version-0.1.13*","page":"News","title":"Version 0.1.13","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nmtest Select indexes defining training and test sets for each column    of a dataframe.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.12*","page":"News","title":"Version 0.1.12","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nAll tree functions: Internal changes to adapt to modifications in XGBoost.jl library.","category":"page"},{"location":"news/#*Version-0.1.11*","page":"News","title":"Version 0.1.11","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nvip Variable importance on PLS projections (VIP).","category":"page"},{"location":"news/#*Version-0.1.10*","page":"News","title":"Version 0.1.10","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nmbwcov: Multiblock weighted covariate analysis regression (MBWCov) (Mangana et al. 2021).\nModifications\nCode cleaning.\nBreaking changes\nmbmang renamed to mbunif (Unified multiblock analysis).\nramang renamed to rrr (Reduced rank regression).","category":"page"},{"location":"news/#*Version-0.1.9*","page":"News","title":"Version 0.1.9","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nrasvd: Redundancy analysis - PCA on instrumental variables (PCAIV).\nramang Redundancy analysis regression = Reduced rank regression (RRR)\nmbplswest MBPLSR - Nipals algorithm (Westerhuis et al. 1998) \nBreaking changes\nAll the functions ..._avg and ..._stack renamed    to ...avg and ...stack (e.g. plsr_avg to    plsravg).\ncaltransf_ds and caltransf_pds remaned   to calds and calpds.\nfnorm renamed to frob.","category":"page"},{"location":"news/#*Version-0.1.8*","page":"News","title":"Version 0.1.8","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nplswold: PLSR Wold Nipals algorithm.\nccawold: CCA Wold Nipals algorithm.\nmbmang: Unified multiblock data analysis of Mangana et al. 2019.\nModifications\nmlrpinv_n renamed to mlrpinvn\npls renamed to plscan.\npls_svd renamed to plstuck.\nrcca renamed to cca (and argument 'alpha\" to 'tau').\nrpmat_gauss and rpmat_li renamed to rpmatgauss and rpmatli \nOutput 'Tbl' added in comdim and mbpca.\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.7*","page":"News","title":"Version 0.1.7","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"CairoMakie.jl was removed from the dependances, and replaced by Makie.jl.","category":"page"},{"location":"news/","page":"News","title":"News","text":"To display the plots, the user has to install and load one of the Makie's backend (e.g. CairoMakie).","category":"page"},{"location":"news/","page":"News","title":"News","text":"News\nrcca: Canonical correlation analysis. (RCCA).\npls: Canonical partial least squares regression (Symmetric PLS).\npls_svd: Tucker's inter-battery method of factor analysis (PLS-SVD).\ncolnorm2 was removed, replaced by colnorm: \nNorm of each column of a dataset.\nfnorm: Frobenius norm of a matrix.\nnorm2 was removed, replaced by normw: \nWeighted norm of a vector.\nModifications\nMajor changes in multiblock functions:\nRenamed functions:\nmbpca_cons ==> mbpca\nmbpcacomdims ==> comdim\nmbplsr_rosa ==> rosaplsr\nmbplsr_so ==> soplsr\nArgument 'X_bl' renamed to 'Xbl'\nVariable 'pc' in summary outputs of PCA and KPCA functions renamed to 'lv'. \nModification of all the tree functions to adapt to the new version of\nXGBoost.jl (>= 2.02) (https://juliahub.com/ui/Packages/XGBoost/rSeEh/2.0.2).    The new Jchemo functions does not work anymore with XGBoost.jl 1.5.2.    \nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.6*","page":"News","title":"Version 0.1.6","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Package Jchemo.jl has been registered.\nModifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.5*","page":"News","title":"Version 0.1.5","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.4*","page":"News","title":"Version 0.1.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\nhead: Display the first rows of a dataset.\nModifications\nRemove of side-effects in some functions of multi-bloc analyses.","category":"page"},{"location":"news/#*Version-0.1.3*","page":"News","title":"Version 0.1.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\ndetrend: argument 'degree' renamed to 'pol'.\nCode cleaning.","category":"page"},{"location":"news/#*Version-0.1.2*","page":"News","title":"Version 0.1.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\ndetrend: new argument 'degree'\ngridcvlv: correction of a bug (typing error) inserted    in the last version.","category":"page"},{"location":"news/#*Version-0.1.1*","page":"News","title":"Version 0.1.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nblockscal: bug corrected in arguments.\nUse of multi-threading (package Threads)   in functions locw and locwlv, used in local models.","category":"page"},{"location":"news/#*Version-0.1.0*","page":"News","title":"Version 0.1.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nArgument 'scal' (X and/or Y column-scaling) added to various functions.\nblockscal: names of arguments have changed.\nplotgrid: argument 'indx' modified.","category":"page"},{"location":"news/#*Version-0.0.26*","page":"News","title":"Version 0.0.26","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News\ncscale: Center and scale each column of a matrix.\nModifications\nArgument 'scal' (X and/or Y column-scaling) added to various functions.   Work in progress. The argument will be available for all the concerned fonctions.\nOutput 'explvar' replaced by 'explvarx' in all the concerned functions.\nrd: New argument 'corr'.","category":"page"},{"location":"news/#*Version-0.0.25*","page":"News","title":"Version 0.0.25","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nrd: Redundancy coefficients between two matrices.\nModifications\nsummary for Plsr objects. See the example in ?plskern.","category":"page"},{"location":"news/#*Version-0.0.24*","page":"News","title":"Version 0.0.24","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Modifications\nselwold: Argument \"plot\" renamed \"graph\" and bug fixed in plotting.","category":"page"},{"location":"news/#*Version-0.0.23*","page":"News","title":"Version 0.0.23","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \noccknndis: One-class classification using \"global\" k-nearest neighbors distances.\nocclknndis: One-class classification using \"local\" k-nearest neighbors distances.\nModifications\noccsd, occod, occsdod, occstah: The methods to compute the cutoff have changed.","category":"page"},{"location":"news/#*Version-0.0.22*","page":"News","title":"Version 0.0.22","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \ncolmad: Median absolute deviation (MAD) of each column of a matrix.\noccsdod: One-class classification using a compromise between PCA/PLS score (SD) and orthogonal (OD) distances.\nreplacedict: Replace the elements of a vector by levels defined in a dictionary.\nstah: Stahel-Donoho outlierness measure.\nModifications\ndens: outputs have been modified.\nodis and scordis have been rename to occsd and occod, and modified.\nplotxy: new argument \"bisect\".","category":"page"},{"location":"news/#*Version-0.0.21*","page":"News","title":"Version 0.0.21","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \ndens: Univariate kernel density estimation.\nModifications \nAll the datasets (examples) have been moved to package JchemoData    (https://github.com/mlesnoff/JchemoData.jl)\nplotsp: Argument 'nsamp' added.\ndatasets: removed and transferred to JchemoData.jl","category":"page"},{"location":"news/#*Version-0.0.20*","page":"News","title":"Version 0.0.20","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \ncovselr: Covsel regression (Covsel+Mlr).\nModifications \ncovsel, mlrvec: Arguments changed.","category":"page"},{"location":"news/#*Version-0.0.19*","page":"News","title":"Version 0.0.19","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nselwold : Wold's criterion to select dimensionality in LV (e.g. PLSR) models.\nplotxy : Scatter plot (x, y) data.\nModifications \nplotscore: Renamed to plotgrid.    ","category":"page"},{"location":"news/#*Version-0.0.18*","page":"News","title":"Version 0.0.18","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nplotgrid : Plot error or performance rates of model predictions.","category":"page"},{"location":"news/","page":"News","title":"News","text":"Modifications \nplotsp: argument 'resolution' was added.","category":"page"},{"location":"news/#*Version-0.0.17*","page":"News","title":"Version 0.0.17","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nreplacebylev2 : Replace the elements of an index-vector by levels.\nModifications \naggstat : Sorting order for dataframes.\ncheckdupl : bug corrected.\nmatB, matW : when requested, update of covm to cov, and aggstat output.\nplotsp : faster.\ntransfer_ds : renamed to caltransf_ds.\ntransfer_pds : renamed to caltransf_pds.\nrecodcat2num : renamed to recodcat2int\nsegmts : A seed (MersenneTwister) can be set for the random samplings.\nExamples added in the helps of every functions.\nDiscrimination functions: major updates.","category":"page"},{"location":"news/#*Version-0.0.16*","page":"News","title":"Version 0.0.16","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \ntransfer_ds : Calibration transfert with direct standardization (DS).\ntransfer_pds : Calibration transfert with piecewise direct standardization (PDS).\nModifications\nmlr functions : Argument 'noint' added.\nplsravgcv : Bug corrected.","category":"page"},{"location":"news/#*Version-0.0.15*","page":"News","title":"Version 0.0.15","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nplsr_stack : Stacking PLSR models\nModifications\naicplsr : BIC criterion added\nfweight\nplsr_avg : Stacking was added\nplsravgaic\nplsravgcv\nlwplsr_avg","category":"page"},{"location":"news/#*Version-0.0.14*","page":"News","title":"Version 0.0.14","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nlwplsr_s ","category":"page"},{"location":"news/#*Version-0.0.13*","page":"News","title":"Version 0.0.13","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nfweight \nrowmean, rowstd\nModifications\naicplsr\nlwplsr_avg\nplsr_avg\nsnv\nwshenk","category":"page"},{"location":"news/#*Version-0.0.12*","page":"News","title":"Version 0.0.12","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nnco, nro\nModifications\nmpars renamed to mpar\nAll functions terminating with \"...agg\" renamed to \"...avg\".","category":"page"},{"location":"news/#*Version-0.0.11*","page":"News","title":"Version 0.0.11","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nblockscal_mfa\ndatasets\nmbpca_cons\nlg\nssq\nModifications\nAll the functions terminating with a \"s\" have been renamed without \"s\"\n(e.g. colmeans was renamed to colmean)","category":"page"},{"location":"news/#*Version-0.0.10*","page":"News","title":"Version 0.0.10","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \ncolsum\nmbpcacomdims\nrowsum\nModifications\nnipals\nmse\nmbpls","category":"page"},{"location":"news/#*Version-0.0.9*","page":"News","title":"Version 0.0.9","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News \nblockscal_frob, blockscal_ncol, blockscal_sd\ncolnorm2\ncorm, covm\nnipals\nnorm2\nModifications\nblockscal \nmatcov renamed to covm and extended\nRemoved\nmbplsrmidavg\nmbplsr_mid\nmbplsrmidseq","category":"page"},{"location":"news/#*Version-0.0.8*","page":"News","title":"Version 0.0.8","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\ngridcv_mb\ngridcvlv_mb\nmbplsr_avg\nmbplsr_mid\nmbplsrmidseq\nModifications \nrosaplsr renamed to mbplsr_rosa\nsoplsr renamed to mbplsr_soplsr","category":"page"},{"location":"news/#*Version-0.0.7*","page":"News","title":"Version 0.0.7","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\nmbplsr\nsoplsr","category":"page"},{"location":"news/#*Version-0.0.6*","page":"News","title":"Version 0.0.6","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\ncolstd\nplsrosa\nplssimp\nrosaplsr\nrv\nrmrows, rmcols: renamed to rmrow, rmcol\nModifications \ninterpl, interpl_mon: changes in arguments\nplotsp: changes in outputs\naggstat (::AbstractMatrix): changes in arguments and outputs","category":"page"},{"location":"news/#*Version-0.0.5*","page":"News","title":"Version 0.0.5","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\nblockscal\npcr\nrp\nrpmatgauss\nrpmat_li","category":"page"},{"location":"news/#*Version-0.0.4*","page":"News","title":"Version 0.0.4","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\niplsr\nModification of covsel","category":"page"},{"location":"news/#*Version-0.0.3*","page":"News","title":"Version 0.0.3","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\ninterpl\ncheckdupl, checkmiss","category":"page"},{"location":"news/#*Version-0.0.2*","page":"News","title":"Version 0.0.2","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"News functions\ncovsel\ninterpl has been replaced by interpl_mon\nChange in output of vi_xgb","category":"page"},{"location":"news/#*Version-0.0.1*","page":"News","title":"Version 0.0.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"First version of the package","category":"page"},{"location":"","page":"Home","title":"Home","text":"DocTestSetup  = quote\n    using Jchemo\nend","category":"page"},{"location":"#Jchemo.jl","page":"Home","title":"Jchemo.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Jchemo.jl.","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Jchemo.jl is a package for data exploration and prediction with focus on high dimensional data. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package was initially designed about partial least squares regression and discrimination models and variants, in particular locally weighted PLS models (LWPLS) (e.g. https://doi.org/10.1002/cem.3209). Then, it has been expanded to many other methods for  analyzing high dimensional data. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package was named Jchemo since it is orientated to chemometrics, but most of the provided methods are fully generic to other domains. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Functions such as transform, predict, coef and summary are available.  Tuning the predictive models is facilitated by generic functions gridscore (validation dataset) and  gridcv (cross-validation). Faster versions of these functions are also available for models based on latent variables (LVs)  (gridscorelv and gridcvlv) and ridge regularization (gridscorelb and gridcvlb).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Most of the functions of the package have a help page (providing an example), e.g.:","category":"page"},{"location":"","page":"Home","title":"Home","text":"?savgol","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples demonstrating Jchemo.jl are available in project JchemoDemo, used for training only. The datasets of the examples are stored in package JchemoData.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Some of the functions of the package (in particular those using kNN selections) use multi-threading  to speed the computations. Taking advantage of this requires to specify a relevant number  of threads (e.g. from the 'Settings' menu of the VsCode Julia extension and the file 'settings.json').","category":"page"},{"location":"","page":"Home","title":"Home","text":"Jchemo.jl uses Makie.jl for plotting. To install and load one of the Makie's backends (e.g. CairoMakie.jl) is required to display the plots. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Before to update the package, it is recommended to have a look on  What changed to avoid problems due to eventual breaking changes. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [Jchemo]\nOrder   = [:function, :type]","category":"page"}]
}
